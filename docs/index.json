
[{"content":" Hey, I'm David\nI'm a Linux system administrator and freelance writer.\nTo contact me, send me an email. To hear about new articles and notes, make sure you subscribe to my newsletter. ","date":"19 December 2025","externalUrl":null,"permalink":"/","section":"","summary":"\u003cdiv style=\"display:flex; gap:1rem; align-items:flex-start;\"\u003e\n  \u003cimg src=\"davidsquare.png\" style=\"width:18%; height:auto;\" /\u003e\n  \u003cdiv style=\"max-width:60%;\"\u003e\n    \u003ch2 class=\"mb-0\"\u003eHey, I'm David\u003c/h2\u003e\u003cbr\u003e\n    \u003cp class=\"mt-0\"\u003eI'm a Linux system administrator and freelance writer.\u003c/p\u003e\n    \u003cp\u003e\n      To contact me, \u003ca href=\"mailto:tdavetech@gmail.com\"\u003esend me an email\u003c/a\u003e.\n    \u003c/p\u003e\n    \u003cp\u003e\n      To hear about new articles and notes, make sure you subscribe to my newsletter.\n      \u003cscript async src=\"https://eomail5.com/form/d9b7d338-dbf3-11f0-bae4-65187d72ac9a.js\" data-form=\"d9b7d338-dbf3-11f0-bae4-65187d72ac9a\"\u003e\u003c/script\u003e\n    \u003c/p\u003e","title":"","type":"page"},{"content":"","date":"19 December 2025","externalUrl":null,"permalink":"/articles/","section":"Articles","summary":"","title":"Articles","type":"articles"},{"content":"What I am up to Now.\nQuarterly Quests # Get down to 190lbs.\nFinish website design and push to production.\nTaper off of caffeine.\nBuild study habit up to 45 minutes per day.\nWhat I am reading # ","date":"18 December 2025","externalUrl":null,"permalink":"/now/","section":"","summary":"\u003cp\u003eWhat I am up to \u003ca\n  href=\"https://nownownow.com/about\"\n    target=\"_blank\"\n  \u003eNow\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 class=\"relative group\"\u003eQuarterly Quests\n    \u003cdiv id=\"quarterly-quests\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#quarterly-quests\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eGet down to 190lbs.\u003cbr\u003e\n\u003cdel\u003eFinish website design and push to production.\u003c/del\u003e\u003cbr\u003e\nTaper off of caffeine.\u003cbr\u003e\nBuild study habit up to 45 minutes per day.\u003c/p\u003e","title":"Now","type":"page"},{"content":"","date":"6 November 2023","externalUrl":null,"permalink":"/tech/","section":"Teches","summary":"","title":"Teches","type":"tech"},{"content":"Run command ujust devmode\nAdd yourself to the right groups ujust dx-group\nhttps://docs.projectbluefin.io/bluefin-dx/\nFlatpaks\nnextcloud desktop Chrome Brave Slack Spotify Remmina Termius --- - name: Setup Development Environment hosts: localhost become: yes tasks: # Install Flatpak applications - name: Install Flatpak applications flatpak: name: \u0026#34;{{ item }}\u0026#34; state: present loop: - com.bitwarden.desktop - com.brave.Browser - org.gimp.GIMP - org.gnome.Snapshot - org.libreoffice.LibreOffice - org.remmina.Remmina - com.termius.Termius - com.slack.Slack - org.keepassxc.KeePassXC - md.obsidian.Obsidian - com.calibre_ebook.calibre - org.mozilla.Thunderbird - us.zoom.Zoom - org.wireshark.Wireshark - com.google.Chrome - io.github.shiftey.Desktop - io.github.dvlv.boxbuddyrs - com.github.tchx84.Flatseal - io.github.flattool.Warehouse - io.missioncenter.MissionCenter - com.github.rafostar.Clapper - com.mattjakeman.ExtensionManager - com.jgraph.drawio.desktop - org.adishatz.Screenshot - com.github.finefindus.eyedropper - com.github.johnfactotum.Foliate - com.obsproject.Studio - com.vivaldi.Vivaldi - com.vscodium.codium - io.podman_desktop.PodmanDesktop - org.kde.kdenlive - org.virt_manager.virt-manager - io.github.input_leap.input-leap - com.nextcloud.desktopclient.nextcloud Homebrew\nhugo bashrc\nexport PATH=$PATH:/usr/local/go/bin export PATH=$PATH:/home/davidt/Nextcloud/Documents/Scripts export HISTTIMEFORMAT=\u0026#39;%F %T \u0026#39; # Set up Vagrant provider export VAGRANT_DEFAULT_PROVIDER=libvirt # Tell libvirt to use system as default URI export LIBVIRT_DEFAULT_URI=\u0026#34;qemu:///system\u0026#34; alias server=\u0026#39;ssh 10.0.10.56\u0026#39; alias gitu=\u0026#39;git add . \u0026amp;\u0026amp; git commit -m \u0026#34;Commit\u0026#34; \u0026amp;\u0026amp; git push\u0026#39; alias sombrero=\u0026#39;ssh m104@sombrero\u0026#39; alias oort=\u0026#39;ssh m104@10.0.10.51\u0026#39; alias control=\u0026#39;ssh ansible@control\u0026#39; alias proxmox1=\u0026#39;ssh root@172.16.1.39\u0026#39; alias hachiwara=\u0026#39;ssh root@hachiwara\u0026#39; alias ansible1=\u0026#39;ssh ansible@ansible1\u0026#39; alias ansible2=\u0026#39;ssh ansible@ansible2\u0026#39; alias gitu=\u0026#39;git add . \u0026amp;\u0026amp; git commit -m \u0026#34;Commit\u0026#34; \u0026amp;\u0026amp; git push\u0026#39; alias ansd=\u0026#39;cd ~/Nextcloud/Documents/projects/base\u0026#39; alias labup=\u0026#39;sudo virsh start Ansible1 \u0026amp;\u0026amp; sudo virsh start Ansible2\u0026#39; alias labdown=\u0026#39;sudo virsh shutdown Ansible1 \u0026amp; sudo virsh shutdown Ansible2\u0026#39; alias scripts=\u0026#39;cd ~/Nextcloud/Documents/Scripts\u0026#39; alias oldoort=\u0026#39;ssh root@oldoort\u0026#39; alias cosharps=\u0026#39;sudo podman run -it --network host --rm almalinux:8 bash -c \u0026#34;yum install -y openssh-clients \u0026amp;\u0026amp; ssh -oHostKeyAlgorithms=+ssh-rsa -oPubkeyAcceptedKeyTypes=+ssh-rsa -L 8081:hachiwara:80 -p 55555 cpbuser@cutlass.psi.edu\u0026#34;\u0026#39; alias flushdns=\u0026#39;sudo systemd-resolve --flush-caches\u0026#39; Ansible:\npython3 -m pip install --user ansible For libvirt: rpm-ostree install --allow-inactive python3-libvirt python3-lxml\nGenerate public key: ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519\nVirt manager dark mode go to gnome tweaks and set the theme for lagacy apps to adw-gtk3-dark\nexport PATH=$PATH:/home/davidt/Nextcloud/Documents/Scripts export HISTTIMEFORMAT=\u0026#39;%F %T \u0026#39; export LIBVIRT_DEFAULT_URI=\u0026#34;qemu:///system\u0026#34; alias server=\u0026#39;ssh 10.0.10.56\u0026#39; alias gitu=\u0026#39;git add . \u0026amp;\u0026amp; git commit -m \u0026#34;Commit\u0026#34; \u0026amp;\u0026amp; git push\u0026#39; alias sombrero=\u0026#39;ssh dthomas@sombrero\u0026#39; alias oort=\u0026#39;ssh m104@10.0.10.51\u0026#39; alias control=\u0026#39;ssh ansible@control\u0026#39; alias proxmox1=\u0026#39;ssh root@172.16.1.39\u0026#39; alias hachiwara=\u0026#39;ssh root@hachiwara\u0026#39; alias web1=\u0026#39;ssh ansible@web1\u0026#39; alias db1=\u0026#39;ssh ansible@db1\u0026#39; alias ftp1=\u0026#39;ssh ansible@ftp1\u0026#39; alias gitu=\u0026#39;git add . \u0026amp;\u0026amp; git commit -m \u0026#34;Commit\u0026#34; \u0026amp;\u0026amp; git push\u0026#39; alias ansd=\u0026#39;cd ~/Nextcloud/Documents/projects/base\u0026#39; alias labup=\u0026#39;sudo virsh start control \u0026amp;\u0026amp; sudo virsh start db1 \u0026amp;\u0026amp; sudo virsh start ftp1 \u0026amp;\u0026amp; sudo virsh start web1\u0026#39; alias labdown=\u0026#39;sudo virsh shutdown Ansible1 \u0026amp; sudo virsh shutdown Ansible2\u0026#39; alias scripts=\u0026#39;cd ~/Nextcloud/Documents/Scripts\u0026#39; alias cosharps=\u0026#39;sudo podman run -it --network host --rm almalinux:8 bash -c \u0026#34;yum install -y openssh-clients \u0026amp;\u0026amp; ssh -oHostKeyAlgorithms=+ssh-rsa -oPubkeyAcceptedKeyTypes=+ssh-rsa -L 8081:hachiwara:80 -p 55555 cpbuser@cutlass.psi.edu\u0026#34;\u0026#39; alias flushdns=\u0026#39;sudo systemd-resolve --flush-caches\u0026#39; extensions: tiling shell need to copy and note settings used\nhotkeys extentions \u0026gt; tiling shell super + direction = move windows ctrl + direction = switch window focus\nGnome shortcuts alt + left/righ = switch between workspaces alt + t New terminal window alt + c Close selected Window\n","externalUrl":null,"permalink":"/tech/bluefin-setup/","section":"Teches","summary":"\u003cp\u003eRun command\n\u003ccode\u003eujust devmode\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAdd yourself to the right groups\n\u003ccode\u003eujust dx-group\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca\n  href=\"https://docs.projectbluefin.io/bluefin-dx/\"\n    target=\"_blank\"\n  \u003ehttps://docs.projectbluefin.io/bluefin-dx/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFlatpaks\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enextcloud desktop\u003c/li\u003e\n\u003cli\u003eChrome\u003c/li\u003e\n\u003cli\u003eBrave\u003c/li\u003e\n\u003cli\u003eSlack\u003c/li\u003e\n\u003cli\u003eSpotify\u003c/li\u003e\n\u003cli\u003eRemmina\u003c/li\u003e\n\u003cli\u003eTermius\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-yml\" data-lang=\"yml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nn\"\u003e---\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eSetup Development Environment\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003ehosts\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003elocalhost\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003ebecome\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003eyes\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003etasks\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c\"\u003e# Install Flatpak applications\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eInstall Flatpak applications\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eflatpak\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;{{ item }}\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003epresent\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eloop\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.bitwarden.desktop\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.brave.Browser\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.gimp.GIMP\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.gnome.Snapshot\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.libreoffice.LibreOffice\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.remmina.Remmina\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.termius.Termius\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.slack.Slack\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.keepassxc.KeePassXC\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003emd.obsidian.Obsidian\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.calibre_ebook.calibre\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.mozilla.Thunderbird\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eus.zoom.Zoom\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.wireshark.Wireshark\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.google.Chrome\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.github.shiftey.Desktop\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.github.dvlv.boxbuddyrs\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.github.tchx84.Flatseal\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.github.flattool.Warehouse\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.missioncenter.MissionCenter\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.github.rafostar.Clapper\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.mattjakeman.ExtensionManager\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.jgraph.drawio.desktop\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.adishatz.Screenshot\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.github.finefindus.eyedropper\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.github.johnfactotum.Foliate\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.obsproject.Studio\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.vivaldi.Vivaldi\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.vscodium.codium\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.podman_desktop.PodmanDesktop\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.kde.kdenlive\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.virt_manager.virt-manager\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.github.input_leap.input-leap\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.nextcloud.desktopclient.nextcloud\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHomebrew\u003c/p\u003e","title":"","type":"tech"},{"content":"--- - name: Setup Development Environment hosts: localhost become: yes become_user: davidt tasks: # Install Flatpak applications - name: Install Flatpak applications flatpak: name: \u0026#34;{{ item }}\u0026#34; state: present loop: - com.brave.Browser - org.gnome.Snapshot - org.libreoffice.LibreOffice - org.remmina.Remmina - com.termius.Termius - com.slack.Slack - org.keepassxc.KeePassXC - org.mozilla.Thunderbird - us.zoom.Zoom - org.wireshark.Wireshark - com.google.Chrome - io.github.shiftey.Desktop - com.jgraph.drawio.desktop - org.adishatz.Screenshot - com.github.finefindus.eyedropper - com.obsproject.Studio - com.vivaldi.Vivaldi - com.vscodium.codium - org.kde.kdenlive - io.github.input_leap.input-leap - com.spotify.Client - org.kde.kdenlive - com.github.johnfactotum.Foliate - be.alexandervanhee.gradia - name: Install homebrew applications homebrew: name: \u0026#34;{{ item }}\u0026#34; state: present loop: - hugo - name: Install pip modules ansible.builtin.pip: name: \u0026#34;{{ item }}\u0026#34; state: latest loop: - ansible-navigator ","externalUrl":null,"permalink":"/tech/desktop-yml/","section":"Teches","summary":"\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nn\"\u003e---\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eSetup Development Environment\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003ehosts\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003elocalhost\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003ebecome\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003eyes\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003ebecome_user\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003edavidt\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003etasks\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"c\"\u003e# Install Flatpak applications\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eInstall Flatpak applications\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eflatpak\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;{{ item }}\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003epresent\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eloop\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.brave.Browser\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.gnome.Snapshot\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.libreoffice.LibreOffice\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.remmina.Remmina\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.termius.Termius\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.slack.Slack\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.keepassxc.KeePassXC\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.mozilla.Thunderbird\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eus.zoom.Zoom\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.wireshark.Wireshark\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.google.Chrome\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.github.shiftey.Desktop\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.jgraph.drawio.desktop\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.adishatz.Screenshot\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.github.finefindus.eyedropper\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.obsproject.Studio\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.vivaldi.Vivaldi\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.vscodium.codium\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.kde.kdenlive\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eio.github.input_leap.input-leap\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.spotify.Client\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eorg.kde.kdenlive\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ecom.github.johnfactotum.Foliate\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ebe.alexandervanhee.gradia\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eInstall homebrew applications\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ehomebrew\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;{{ item }}\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003epresent\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eloop\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003ehugo\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eInstall pip modules\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eansible.builtin.pip\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;{{ item }}\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003elatest\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eloop\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003eansible-navigator\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"","type":"tech"},{"content":"","externalUrl":null,"permalink":"/tech/linux/","section":"Teches","summary":"","title":"","type":"tech"},{"content":"","externalUrl":null,"permalink":"/tech/networking/","section":"Teches","summary":"","title":"","type":"tech"},{"content":"Quick Commands\nBasic Device Management # SSH # crypto key generate rsa modulus (modulus value) show ip ssh show ssh DHCP # ip address dhcp show dhcp lease show interfaces vlan 1 show ip default-gateway History # show history terminal history size x history size x Logging and Domain Lookup # no logging console no ip domain-lookup exec timeout (minutes) data plane\nwork a switch does to forward frames generated by the devices connected to the switch control plane\nconfiguration and processes that control and change the choices made by the switch’s data plane management plane\ndeals with managing the device itself, rather than controlling what the device is doing SSH\ncrypto key generate rsa modulus modulus-value\n(Use at least a 768-bit key to support SSH version 2.) show ip ssh\nlists status information about the SSH server show ssh\nlists information about each SSH client currently connected into the switch Switch DHCP\nip address dhcp\nShow dhcp lease\nshow interfaces vlan 1\nshow ip default-gateway\nshow history\nlists the commands currently held in the history buffer. terminal history size x\nallows a single user to set the size of their history buffer just for this one login session history size x\nconsole or vty line configuration mode sets the default number of commands saved in the history buffer for the users of the console or vty lines Logging and domain lookup\nno logging console\nlogging console\nexec-timeout minutes seconds\nline subcommand enables you to set the length of that inactivity timer. In the lab (but not in production), you might want to use the special value of 0 minutes and 0 seconds meaning “never time out.” ","externalUrl":null,"permalink":"/tech/networking/management/","section":"Teches","summary":"\u003cp\u003eQuick Commands\u003c/p\u003e\n\n\u003ch1 class=\"relative group\"\u003eBasic Device Management\n    \u003cdiv id=\"basic-device-management\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#basic-device-management\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\n\u003ch2 class=\"relative group\"\u003eSSH\n    \u003cdiv id=\"ssh\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#ssh\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecrypto key generate rsa modulus (modulus value)\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow ip ssh\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow ssh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 class=\"relative group\"\u003eDHCP\n    \u003cdiv id=\"dhcp\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#dhcp\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eip address dhcp\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow dhcp lease\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow interfaces vlan 1\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow ip default-gateway\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 class=\"relative group\"\u003eHistory\n    \u003cdiv id=\"history\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#history\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow history\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eterminal history size x\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehistory size x\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 class=\"relative group\"\u003eLogging and Domain Lookup\n    \u003cdiv id=\"logging-and-domain-lookup\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#logging-and-domain-lookup\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eno logging console\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eno ip domain-lookup\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eexec timeout (minutes)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003edata plane\u003c/p\u003e","title":"","type":"tech"},{"content":" 10 NAT # Friday, October 8, 2021 7:12 AM\nNetwork Address Translation Concepts\nStatic NAT\nIP addresses statically mapped to each other.\nNAT router simply configures a oneregistered address that is used on its behalf. -to-one mapping between the private address and the\nSupporting a second IP host with static NAT requires a second static onesecond IP address in the public address range. -to-one mappingusing a\nthe router statically maps 10.1.1.2 to 200.1.1.2. Because the enterprise has a single registered Class C network, it can support at most 254 private IP addresses with NAT, with the usual two\nreserved numbers (the network number and network broadcast address).\ninside local for the private IP addresses in this example and inside global for the public IP addresses.\nDynamic NAT\nDynamic NAT sets up a pool of possible inside global addresses and defines matching criteria to determine which inside local IP addresses should be translated with NAT.\nHost 10.1.1.1 sends its first packet to the server at 170.1.1.1. As the packet enters the NAT router, the router applies some matching logic to decide whether As the packet enters the NAT router, the router applies some matching logic to decide whether the packet should have NAT applied. Because the logic has been configured to match source IP addresses that begin with 10.1.1, the router adds an entry in the NAT table for 10.1.1.1 as an inside local address. The NAT router needs to allocate an IP address from the pool of valid inside global addresses. It picks the first one available (200.1.1.1, in this case) and adds it to the NAT table to complete the entry. The NAT router translates the source IP address and forwards the packet. The dynamic entry stays in the table as long as traffic flows occasionally. You can configure a timeout value that defines how long the router should wait, having not translated any packets with that address, before removing the dynamic entry.entries from the table using the clear ip nat translation You can also * command.manually clear the dynamic NAT can be configured with more IP addresses in the inside local address list than in the inside global address pool. If a new packet arrives from yet another inside host, and it needs a NAT entry, but all the pooled IP addresses are in use, the router simply discards the packet. The user must try again until a NAT entry times out, at which point the NAT function works for the next host that sends a packet. Essentially, the inside global pool of addresses needs to be as large as the maximum number of concurrent hosts that need to use the Internet at the same timeexplained in the next section. —unless you use PAT, as is Overloading NAT with Port Address Translation\nNAT Overload feature, also called Port Address Translation (PAT)\nThe NAT router keeps a NAT table entry for every unique combination of inside local IP address and port, with translation to the inside global address and a unique port number associated with and port, with translation to the inside global address and a unique port number associated with the inside global address. NAT overload can use more than 65,000 port numbers PAT is by far the most popular option NAT Configuration and Troubleshooting # Static NAT Configuration\nEach static mapping between a local (private) address and a global (public) address must be configured.\nThose same interface subcommands tell NAT whether the interface is inside or outside.\nStep 1. Use thebe in the inside part of the NAT design. ip nat inside command in interface configuration mode to configure interfaces to\nStep 2. Use the be in the outside part of the NAT design. ip nat outside command in interface configuration mode to configure interfaces to\nStep 3. Use theconfiguration mode to configure the static mappings. ip nat inside source static inside-local inside-global command in global\nstatic mappings are created using the ip nat inside source static command. Source keyword means that NAT translates the source IP address of packets coming into its inside interfaces. The static keyword means that the parameters define a static entry, which should never be removed from the NAT table because of timeout.hosts—10.1.1.1 and 10.1.1.2—to have Internet access, two ip nat inside commands are needed.Because the design calls for two show ip nat translations The show ip nat statistics command lists the two static NAT entries created in the configuration. command lists statistics, listing things such as the number of currently active translation table entries. The statistics also include the number of hits, which increments for every packet for which NAT must translate addresses. Dynamic NAT Configuration\nDynamic NAT still requires that each interface be identified as either an inside or outside interface\nDynamic NAT uses an access control list (ACL) to identify which inside local (private) IP addresses need to have their addresses translated, and it defines a pool of registered public IP addresses to\nallocate.\nStep 1. Use the be in the inside part of the NAT design (just like with static NAT). ip nat inside command in interface configuration mode to configure interfaces to\nStep 2. Use the be in the outside part of the NAT design (just like with static NAT). ip nat outside command in interface configuration mode to configure interfaces to\nStep 3. be performed.Configure an ACL that matches the packets entering inside interfaces for which NAT should\nStep 4. Use the global configuration mode to configure the pool of public registered IP addresses. ip nat pool name first-address last-address netmask subnet-mask command in\nStep 5. Use theconfiguration mode to enable dynamic NAT. Note the command references the ACL (step 3) and ip nat inside source list acl-number pool pool-name command in global\npool (step 4) per previous steps.\n“misses,” as highlighted in the example. The first occurrence of this counter counts the number of times a new packet comes along, needing a NAT entry, and not finding one.\nThe second misses counter toward the end of the command output lists the number of misses in the pool. This counter increments only when dynamic NAT tries to allocate a new NAT table entry\nand finds no available addresses\ndebug ip nat packet has its address translated for NAT.command. This debug command causes the router to issue a message every time a\nNAT Overload (PAT) Configuration\nIf PAT uses a pool of inside global addresses, the configuration looks exactly like dynamic NAT, except the ip nat inside source list global command has an overload keyword added to the end. If\nPAT just needs to use one inside global IP address, the router can use one of its interface IP addresses\nconfiguration when using an interface IP address as the sole inside global IP address:\nStep 1. As with dynamic and static NAT, configure theto identify inside interfaces. ip nat inside interface sub-command\nStep 2. As with dynamic and static NAT, configure theto identify outside interfaces. ip nat outside interface subcommand\nStep 3. As with dynamic NAT, interfaces. configure an ACL that matches the packets entering inside\nStep 4. Configure the global configuration command, referring to the ACL created in step 3 and to the interface ip nat inside source list acl-number interface type/number overload\nwhose IP address will be used for translations.\nip nat inside source list 1 interface serial 0/0/0 overload command has several parameters The matching ACL 1 have their addresses translated. The list 1 parameter means the same thing as it does for dynamic NAT: inside local IP addresses interface serial 0/0/0 parameter means that the only inside global IP address available is the IP address of the NAT router’s interface serial 0/0/0. The router creates one NAT table entry for each unique combination of inside local IP address and port NAT Troubleshooting\nReversed inside and outside\nStatic NAT: Check the address first and the inside global IP address second. ip nat inside source static command to ensure it lists the inside local\naddress first and the inside global IP address second.\nDynamic NAT (ACL): Ensure that the ACL configured to match packets sent by the inside hosts match that host’s packets before any NAT translation has occurred.\nDynamic NAT (pool): For dynamic NAT without PAT, ensure that the pool has enough IP addresses\\\nA large or growing value in the second misses counter in thecommand output can indicate this problem. Also, compare the configured pool to the list of show ip nat statistics\naddresses in the NAT translation table the problem may be that the configuration intended to use PAT and is missing the overload (show ip nat translations ). Finally, if the pool is small,\nkeyword\nPAT: It is easy to forget to add the overload option\nperhaps NAT has been configured correctly, but the packets. an ACL exists on one of the interfaces, discarding\nIOS processes ACLs before NATafter translating the addresses with NAT.. For packets exiting an interface, IOS processes any outbound ACL\nUser traffic required\nIPv4 routing\n4.0 IP Services\n4.7 Explain the forwarding percongestion, policing, shaping -hop behavior (PHB) for QoS such as classification, marking, queuing,\nQoSthan storing and forwarding a message. defines these actions as per-hop behaviors (PHBs),which isa formal term to refer to actions other\n","externalUrl":null,"permalink":"/tech/networking/nat/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003e10 NAT\n    \u003cdiv id=\"10-nat\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#10-nat\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eFriday, October 8, 2021 7:12 AM\u003c/p\u003e\n\u003cp\u003eNetwork Address Translation Concepts\u003c/p\u003e\n\u003cp\u003eStatic NAT\u003cbr\u003e\nIP addresses statically mapped to each other.\u003c/p\u003e","title":"","type":"tech"},{"content":"Friday, October 8, 2021 11:48 AM\nQuality of Experience (QoE) -users’ perception of their use of the application on the\nnetwork.\nVoice and Video Applications\nAflowisall the data moving from one application to another over the network, with one\nflow for each direction\nFor example, if you open a website and connect to a web server, the web page content that moves from the server to the client is one flow.\nFrom a voice perspective, a phone call between two IP phones would create a flow for each direction\nStep 1. The phone user makes a phone call and begins speaking.\nStep 2. A chip called a codec processes (digitizes) the sound to create a binary code (160\nbytes with the G.711 codec, for example) for a certain time period (usually 20 ms).\nStep 3. The phone places the data into an IP packet.\nStep 4. The phone sends the packet to the destination IP phone.\nguidelines for interactive voice: Delay (one-way): 150 ms or less Jitter: 30 ms or less Loss: 1% or less requirements for video\nBandwidth: 384 Kbps to 20+ Mbps\nDelay (one-way): 200–400 ms\nJitter: 30–50 ms\nLoss: 0.1%–1%\nQoS as Mentioned in This Book\n“Classification and Marking” is about the marking of packets and the definition of trust\nboundaries.\n“Queuing” describes the scheduling of packets to give one type of packet priority over another.\n“Shaping and Policing” explains these two tools together because they are often used on opposite ends of a link.\n“devices get too busy.Congestion Avoidance” addresses how to manage the packet loss that occurs when network\nClassification and Marking\na type of QoS tool that by changing some bits in specific header fieldsclassifies packets based on their header contents then marks the message\nClassification Basics\nQoS tools sit in the path that packets take when being forwarded through a router or switch,\nmuch like the positioning of ACLs. Like ACLs, ACLs, QoS tools are enabled for a direction: packets entering the interface (before the QoS tools are enabled on an interface. Also like\nforwarding decision) or for messages exiting the interface (after the forwarding decision).\nclassification refers to the process of matching the fields in a message to make a choice to\ntake some QoS action\nQoS tools perform classification (matching of header fields) to decide which packets to take certain QoS actions against.Those actions include the other types of QoS tools discussed in\nthis chapter, such as queuing, shaping, policing, and so on.\nMatching (Classification) Basics Cisco and by RFCs, suggests doing complex matching early in the life of a packet and then marking the packet. Marking means that the QoS tool changes one or more header fields, setting a value in the header. Differentiated Services Code Point (DSCP) a 6-bit field in the IP header meant for QoS marking Classification on Routers with ACLs and NBAR\nmany QoS tools support the ability to simply refer to an IP ACL,with this kind of logic:\nFor any packet matched by the ACL with a permit action, consider that packet a match for\nQoS, so do a particular QoS action.\nAll these fields are matchable for QoS classification.\nnot every classification can be easily made by matching with an ACL. In more challenging cases, Cisco Network Based Application Recognition (NBAR)can be used. NBAR is basically in its second major version, called NBAR2, or nextpackets for classification in a large variety of ways that are very useful for QoS.-generation NBAR. In short,NBAR2 matches NBAR2 looks far beyond what an ACL can examine in a message Cisco also organizes what NBAR can match in ways that make it easy to separate the traffic into different classes. you might classify WebEx traffic and give it a unique DSCP marking. NBAR provides easy builtof applications.-in matching ability for WebEx, plus more than 1000 different subcategories NBAR refers to this idea of defining the characteristics of different applications as application signatures.) application signatures.) Marking IP DSCP and Ethernet CoS\nexample plan\nClassify all voice payload traffic that is used for business purposes as IP DSCP EF and\nCoS 5.\nClassify all video conferencing and other interactive video for business purposes as IP DSCP AF41 and CoS 4.\nClassify all business-critical data application traffic as IP DSCP AF21 and CoS 2\nMarking the IP Header\nMarking a QoS field in the IP header works well with QoS because the IP header exists for the entire trip from the source host to the destination host.\nIPv4 defines a Type of Service (ToS) byte in the IPv4 headerThe original RFC defined a 3-bit IP Precedence (IPP) field for QoS marking. That field , as shown in Figure 11-6.\ngave us eight separate valueswhen converted to decimal are decimals 0 through 7.—binary 000, 001, 010, and so on, through 111—which\nThose last 5 bits of the ToS byte per RFC 791 were mostly defined for some purpose but were not used in practice to any significant extent. DSCP increased the number of marking bits to 6 bits, allowing for64 unique values\nthat can be marked. The DiffServ RFCs, which became RFCs back in the late 1990s, have become accepted as the most common method to use when doing QoS, and\nusing the DSCP field for marking has become quite common.\nIPv6 has a similar field to mark as well.The 6-bit field also goes by the name DSCP,\nwith the byte in the IPv6 header being called the IPv6 Traffic Class byte.\nMarking the Ethernet 802.1Q Header\nmarking field exists in the 802.1Q header\nIt goes bytwo different names:Class of Service, orCoS, and Priority Code Point, or\nPCP.\nsits in the third byte of the 4possible valuesto mark (see Figure 11-byte 802.1Q header, as a 3-7). It goes by two different names: Class of -bit field,supplying eight\nService, or CoS, and Priority Code Point, or PCP.\n802.1Q header is not included in all Ethernet frames only exists when 802.1Q trunking is used on a link only for QoS features enabled on interfaces that use trunking, as shown Other Marking Fields\nDefining Trust Boundaries\nmost voice traffic is marked with a DSCP called Expediteddecimal value of 46 Forwarding (EF), which has a\ntrust boundary\nthe point in the path of a packet flowing through the network at which the networking devices can trust the current QoS markings\nwhen the access layer includes an IP Phone, the phone is typically the trust boundary IP Phones can set the CoS and DSCP fields of the messages created by the phone, as well as those forwarded from the PC through the phone. are actually configured on the attached access switch The specific marking values DiffServ Suggested Marking Values\nDiffServ architecture as defined originally by RFC 2475\nthree sets of DSCP values as used in DiffServ.\nExpedited Forwarding (EF)\nExpedited Forwarding (EF) DSCP value—a single value\nuse for packets that need low latency (delay), low jitter, and low loss.\ndecimal 46\nMost often QoS plans use EF to mark voice payloadpackets. With voice calls, some\npackets carry voice payload, and other packets carry call signaling messages.signaling messages set up (create) the voice call between two devices, and they do not Call\nrequire low delay, jitter, and loss.\nBy default, Cisco IP Phones mark voice pay-load with EF,and mark voice signaling\npackets sent by the phone with another value called CS3.\nAssured Forwarding (AF)\nThmeant to be used in concert with each othere Assured Forwarding (AF) DiffServ RFC (2597) defines a set of 12 DSCP values\ndefines the defines three levels of drop priority within each queue for use with congestion concept of four separate queues in a queuing system. Additionally, it\navoidance tools.need 12 different DSCP markings, one for each combination of queue and drop With four queues, and three drop priority classes per queue, you\npriority. (Queuing and congestion avoidance mechanisms are discussed later in this\nchapter.)\nand Y referring to the drop priority (1 through 3).The text names follow a format of AFXY, with X referring to the queue (1 through 4)\nClass Selector (CS) eight DSCP values for backward compatibility with IPP values. The DSCP values have the last 3 bits, as shown on the left side of the figure. CSx represents the text names, same first 3 bits as the IPP field, and with binary 0s for the where x is the matching IPP value (0 through 7). Guidelines for DSCP Marking Values\nhow all devices should mark data\nDSCP EF: Voice payload\nAF4x: Interactive video (for example, videoconferencing)\nAF3x: Streaming video\nAF2x: High priority (low latency) data\nCS0: Standard data\nQueuing\ninterfacethe QoS toolset for managing the queues that hold packets while they wait their turn to exit an To use multiple queues, the queuing system needs a classifierfunction to choose which packets\nare placed into which queue\nThe queuing system needs a interface becomes availablescheduleras well, to decide which message to take next when the\nPrioritizationrefers to the concept of giving priority to one queue over another\nRound-Robin Scheduling (Prioritization) (Two Types)\nweighting\nthe more preference to one queue over another.scheduler takes a different number of packets (or bytes) from each queue, giving\nClass-Based Weighted Fair Queuing (CBWFQ)\nEach class receives at least the amount of bandwidth configured during times of congestion, but maybe more\nLow Latency Queuing\ntells the scheduler to treat one or more queues as special priority queues if thethe voice queue? The scheduler never services the other queues (calledspeed of the interface is X bits per second, but more than X bits per second come into queue starvation).\nlimit the amount of traffic placed into the priority queue, using a feature called policing\na cap on the bandwidth used by the priority queue\nCall Admission Control (CAC) tools\nFind a way to this link, so that the policer never discards any of the trafficlimit the amount of voice and video that the network routes out\nCAC tools did not get a mention in the exam topics, so this chapter leaves those\ntools at a brief mention\nA Prioritization Strategy for Data, Voice, and Video\napproach queuing in their QoS plans:\nKey Topic.\nUse a round-robin queuing method like CBWFQ for data classes and for noninteractive voice and video. If faced with too little bandwidth compared to the typical amount of traffic, give data classes that support business-critical applications much more guaranteed bandwidth than is given to less important data classes. Use a priority queue with LLQ scheduling for interactive voice and video, to achieve low delay, jitter, and loss. Put voice in a separate queue from video so that the policing function applies separately to each. Define enough bandwidth for each priority queue so that the builtdiscard any messages from the priority queues. -in policer should not Use Call Admission Control (CAC) tools to avoid adding too much voice or video to the network, which would trigger the policer function. Shaping and Policing # not found in as many locations in a typical enterprise often used at the WAN edge in an enterprise network design. monitor the bit rate of the combined messages that flow through a device. notes each packet that passes and measures the number of bits per second over time attempt to keep the bit rate at or below the configured speed policersdiscard packets,and shapershold packets in queues to delay the packets. Does this next packet push the measured rate past the configured shaping rate or policing rate? If no: Let the packet keep moving through the normal path and do nothing extra to the packet. If yes: If shaping, delay the message by queuing it. If policing, either discard the message or mark it differently. policing, which discards or rewhich slows down messages that exceed the shaping rate.-marks messages that exceed the policing rate, followed by shaping, Policing # Policers activity.allow for a burst beyond the policing rate for a short time, after a period of low\nWhere to Use Policing\nPolicing makes sense only in certain cases, and as a general tool, it can be best used at\nthe edge between two networks.\na 200-Mbps committed information rate (CIR). the Mbps of traffic in each directionSP providing the WAN service agrees to allow the enterprise to send 200. However, remember that the enterprise routers transmit the data at the speed of the access link, or 1 Gbps in this case. Thethat the customer chooses for that link.SP can police incoming packets, setting the policing rate to match the CIR Policers can also re-mark packets. SP could mark the messages with a new marking value, with this strategy: Renetwork.-mark packets that exceed the policing rate, but let them into the SP’s If other SP network devices are experiencing congestion when they process the packet, the different marking means that device can discard the packet. However... ...if no other SP network devices are experiencing congestion when forwarding that re-marked packet, it gets through the SP network anyway. that re-marked packet, it gets through the SP network anyway. SP canstill protecting the SP’s network during times of stress.treat their customers a little better by discarding less traffic, while key features of policing: Key Topic. It rate.measures the traffic rate over time for comparison to the configured policing It allows for a burst of data after a period of inactivity. It is enabled on an interface, in either direction, but typically at ingress. It cancandidate for more aggressive discard later in its journey.discard excess messages but can also re-mark the message so that it is a Use a shaper to slow down the traffic shaping before sending data to an SP that is policing—is one of the typical uses of a shaper slows messages down by queuing the messages. then services the shaping queues, but not based on when the physical interface is available schedules messages from the shaping queues based on the shaping rate the shaper queues packets so that the sending rate through the shaper does not exceed the shaping rate; and then output queuing works as normal, if needed. shaping queuesapply the round-robin and priority queuing features of CBWFQ and LLQ, respectively, to the Setting a Good Shaping Time Interval for Voice and Video # you can (and should) configure a shaper’s setting that changes the internal operation of the shaper, which then reduces the delay and jitter caused to voice and video traffic. Shaping # traffic. A shaper’s time interval refers to its internal logic and how a shaper averages, over time, sending at a particular rate Because the interface transmits bits at 1 Gbps, it takes just .2 seconds, or 200 ms, to send all 200 million bits. Then the shaper must wait for the rest of the time interval, another 800 ms, before beginning the next time interval. configure a short time interval so there is less wait time Tc = 1 second (1000 ms): Send at 1 Gbps for 200 ms, rest for 800 ms Tc = .1 second (100 ms): Send at 1 Gbps for 20 ms, rest for 80 ms Tc = .01 second (10 ms): Send at 1 Gbps for 2 ms, rest for 8 ms When shaping, use a short time interval. By recommendation, use a interval to support voice and video. 10 - ms time key features of shapers: Key Topic. Shapers measure the traffic rate over time for comparison to the configured shaping rate. Shapers allow for bursting after a period of inactivity. Shapers areenabled on an interface for egress (outgoing packets). Shapers slow down packets by queuing them and over time releasing them from the queue at the shaping rate. Shapers use queuing tools to create and schedule the shaping queues, which is very important for the same reasons discussed for output queuing. Congestion Avoidance # attempts to reduce overall packet loss by preemptively discarding some packets used in TCP connections connections\nTCP Windowing Basics\ntail drop\none queue being totally full. Any new packets that arrive for that queue right now will\nbe dropped because there is no room at the tail of the queue (tail drop).\nCongestion Avoidance Tools\ndiscard some TCP segments before the queues fill, hoping that enough TCP connections will\nslow down, reducing congestion, and avoiding tail drop\nmonitor the average queue depth over time, triggering more severe actions the deeper the\nqueue\nthe queue depth passes the maximum thresholdcalled full drop. ,the tool drops all packets, in an action\ncongestion avoidance tools can classify messages to treat some packets better than others\n3.0 IP Connectivity 3.5 Describe the purpose of First Hop Redundancy Protocol 4.0 Infrastructure Services 4.4 Explain the function of SNMP in network operations 4.9 Describe the capabilities and function of TFTP/FTP in the network First Hop Redundancy Protocol # 11 QoS # Friday, October 8, 2021 11:48 AM\nQuality of Experience (QoE) -users’ perception of their use of the application on the\nnetwork.\nVoice and Video Applications\nAflowisall the data moving from one application to another over the network, with one\nflow for each direction\nFor example, if you open a website and connect to a web server, the web page content that moves from the server to the client is one flow.\nFrom a voice perspective, a phone call between two IP phones would create a flow for each direction\nStep 1. The phone user makes a phone call and begins speaking.\nStep 2. A chip called a codec processes (digitizes) the sound to create a binary code (160\nbytes with the G.711 codec, for example) for a certain time period (usually 20 ms).\nStep 3. The phone places the data into an IP packet.\nStep 4. The phone sends the packet to the destination IP phone.\nguidelines for interactive voice: Delay (one-way): 150 ms or less Jitter: 30 ms or less Loss: 1% or less requirements for video\nBandwidth: 384 Kbps to 20+ Mbps\nDelay (one-way): 200–400 ms\nJitter: 30–50 ms\nLoss: 0.1%–1%\nQoS as Mentioned in This Book\n“Classification and Marking” is about the marking of packets and the definition of trust\nboundaries.\n“Queuing” describes the scheduling of packets to give one type of packet priority over another.\n“Shaping and Policing” explains these two tools together because they are often used on opposite ends of a link.\n“devices get too busy.Congestion Avoidance” addresses how to manage the packet loss that occurs when network\nClassification and Marking\na type of QoS tool that by changing some bits in specific header fieldsclassifies packets based on their header contents then marks the message\nClassification Basics\nQoS tools sit in the path that packets take when being forwarded through a router or switch,\nmuch like the positioning of ACLs. Like ACLs, ACLs, QoS tools are enabled for a direction: packets entering the interface (before the QoS tools are enabled on an interface. Also like\nforwarding decision) or for messages exiting the interface (after the forwarding decision).\nclassification refers to the process of matching the fields in a message to make a choice to\ntake some QoS action\nQoS tools perform classification (matching of header fields) to decide which packets to take certain QoS actions against.Those actions include the other types of QoS tools discussed in\nthis chapter, such as queuing, shaping, policing, and so on.\nMatching (Classification) Basics Cisco and by RFCs, suggests doing complex matching early in the life of a packet and then marking the packet. Marking means that the QoS tool changes one or more header fields, setting a value in the header. Differentiated Services Code Point (DSCP) a 6-bit field in the IP header meant for QoS marking Classification on Routers with ACLs and NBAR\nmany QoS tools support the ability to simply refer to an IP ACL,with this kind of logic:\nFor any packet matched by the ACL with a permit action, consider that packet a match for\nQoS, so do a particular QoS action.\nAll these fields are matchable for QoS classification.\nnot every classification can be easily made by matching with an ACL. In more challenging cases, Cisco Network Based Application Recognition (NBAR)can be used. NBAR is basically in its second major version, called NBAR2, or nextpackets for classification in a large variety of ways that are very useful for QoS.-generation NBAR. In short,NBAR2 matches NBAR2 looks far beyond what an ACL can examine in a message Cisco also organizes what NBAR can match in ways that make it easy to separate the traffic into different classes. you might classify WebEx traffic and give it a unique DSCP marking. NBAR provides easy builtof applications.-in matching ability for WebEx, plus more than 1000 different subcategories NBAR refers to this idea of defining the characteristics of different applications as application signatures.) application signatures.) Marking IP DSCP and Ethernet CoS\nexample plan\nClassify all voice payload traffic that is used for business purposes as IP DSCP EF and\nCoS 5.\nClassify all video conferencing and other interactive video for business purposes as IP DSCP AF41 and CoS 4.\nClassify all business-critical data application traffic as IP DSCP AF21 and CoS 2\nMarking the IP Header\nMarking a QoS field in the IP header works well with QoS because the IP header exists for the entire trip from the source host to the destination host.\nIPv4 defines a Type of Service (ToS) byte in the IPv4 headerThe original RFC defined a 3-bit IP Precedence (IPP) field for QoS marking. That field , as shown in Figure 11-6.\ngave us eight separate valueswhen converted to decimal are decimals 0 through 7.—binary 000, 001, 010, and so on, through 111—which\nThose last 5 bits of the ToS byte per RFC 791 were mostly defined for some purpose but were not used in practice to any significant extent. DSCP increased the number of marking bits to 6 bits, allowing for64 unique values\nthat can be marked. The DiffServ RFCs, which became RFCs back in the late 1990s, have become accepted as the most common method to use when doing QoS, and\nusing the DSCP field for marking has become quite common.\nIPv6 has a similar field to mark as well.The 6-bit field also goes by the name DSCP,\nwith the byte in the IPv6 header being called the IPv6 Traffic Class byte.\nMarking the Ethernet 802.1Q Header\nmarking field exists in the 802.1Q header\nIt goes bytwo different names:Class of Service, orCoS, and Priority Code Point, or\nPCP.\nsits in the third byte of the 4possible valuesto mark (see Figure 11-byte 802.1Q header, as a 3-7). It goes by two different names: Class of -bit field,supplying eight\nService, or CoS, and Priority Code Point, or PCP.\n802.1Q header is not included in all Ethernet frames only exists when 802.1Q trunking is used on a link only for QoS features enabled on interfaces that use trunking, as shown Other Marking Fields\nDefining Trust Boundaries\nmost voice traffic is marked with a DSCP called Expediteddecimal value of 46 Forwarding (EF), which has a\ntrust boundary\nthe point in the path of a packet flowing through the network at which the networking devices can trust the current QoS markings\nwhen the access layer includes an IP Phone, the phone is typically the trust boundary IP Phones can set the CoS and DSCP fields of the messages created by the phone, as well as those forwarded from the PC through the phone. are actually configured on the attached access switch The specific marking values DiffServ Suggested Marking Values\nDiffServ architecture as defined originally by RFC 2475\nthree sets of DSCP values as used in DiffServ.\nExpedited Forwarding (EF)\nExpedited Forwarding (EF) DSCP value—a single value\nuse for packets that need low latency (delay), low jitter, and low loss.\ndecimal 46\nMost often QoS plans use EF to mark voice payloadpackets. With voice calls, some\npackets carry voice payload, and other packets carry call signaling messages.signaling messages set up (create) the voice call between two devices, and they do not Call\nrequire low delay, jitter, and loss.\nBy default, Cisco IP Phones mark voice pay-load with EF,and mark voice signaling\npackets sent by the phone with another value called CS3.\nAssured Forwarding (AF)\nThmeant to be used in concert with each othere Assured Forwarding (AF) DiffServ RFC (2597) defines a set of 12 DSCP values\ndefines the defines three levels of drop priority within each queue for use with congestion concept of four separate queues in a queuing system. Additionally, it\navoidance tools.need 12 different DSCP markings, one for each combination of queue and drop With four queues, and three drop priority classes per queue, you\npriority. (Queuing and congestion avoidance mechanisms are discussed later in this\nchapter.)\nand Y referring to the drop priority (1 through 3).The text names follow a format of AFXY, with X referring to the queue (1 through 4)\nClass Selector (CS) eight DSCP values for backward compatibility with IPP values. The DSCP values have the last 3 bits, as shown on the left side of the figure. CSx represents the text names, same first 3 bits as the IPP field, and with binary 0s for the where x is the matching IPP value (0 through 7). Guidelines for DSCP Marking Values\nhow all devices should mark data\nDSCP EF: Voice payload\nAF4x: Interactive video (for example, videoconferencing)\nAF3x: Streaming video\nAF2x: High priority (low latency) data\nCS0: Standard data\nQueuing\ninterfacethe QoS toolset for managing the queues that hold packets while they wait their turn to exit an To use multiple queues, the queuing system needs a classifierfunction to choose which packets\nare placed into which queue\nThe queuing system needs a interface becomes availablescheduleras well, to decide which message to take next when the\nPrioritizationrefers to the concept of giving priority to one queue over another\nRound-Robin Scheduling (Prioritization) (Two Types)\nweighting\nthe more preference to one queue over another.scheduler takes a different number of packets (or bytes) from each queue, giving\nClass-Based Weighted Fair Queuing (CBWFQ)\nEach class receives at least the amount of bandwidth configured during times of congestion, but maybe more\nLow Latency Queuing\ntells the scheduler to treat one or more queues as special priority queues if thethe voice queue? The scheduler never services the other queues (calledspeed of the interface is X bits per second, but more than X bits per second come into queue starvation).\nlimit the amount of traffic placed into the priority queue, using a feature called policing\na cap on the bandwidth used by the priority queue\nCall Admission Control (CAC) tools\nFind a way to this link, so that the policer never discards any of the trafficlimit the amount of voice and video that the network routes out\nCAC tools did not get a mention in the exam topics, so this chapter leaves those\ntools at a brief mention\nA Prioritization Strategy for Data, Voice, and Video\napproach queuing in their QoS plans:\nKey Topic.\nUse a round-robin queuing method like CBWFQ for data classes and for noninteractive voice and video. If faced with too little bandwidth compared to the typical amount of traffic, give data classes that support business-critical applications much more guaranteed bandwidth than is given to less important data classes. Use a priority queue with LLQ scheduling for interactive voice and video, to achieve low delay, jitter, and loss. Put voice in a separate queue from video so that the policing function applies separately to each. Define enough bandwidth for each priority queue so that the builtdiscard any messages from the priority queues. -in policer should not Use Call Admission Control (CAC) tools to avoid adding too much voice or video to the network, which would trigger the policer function. Shaping and Policing # not found in as many locations in a typical enterprise often used at the WAN edge in an enterprise network design. monitor the bit rate of the combined messages that flow through a device. notes each packet that passes and measures the number of bits per second over time attempt to keep the bit rate at or below the configured speed policersdiscard packets,and shapershold packets in queues to delay the packets. Does this next packet push the measured rate past the configured shaping rate or policing rate? If no: Let the packet keep moving through the normal path and do nothing extra to the packet. If yes: If shaping, delay the message by queuing it. If policing, either discard the message or mark it differently. policing, which discards or rewhich slows down messages that exceed the shaping rate.-marks messages that exceed the policing rate, followed by shaping, Policing # Policers activity.allow for a burst beyond the policing rate for a short time, after a period of low\nWhere to Use Policing\nPolicing makes sense only in certain cases, and as a general tool, it can be best used at\nthe edge between two networks.\na 200-Mbps committed information rate (CIR). the Mbps of traffic in each directionSP providing the WAN service agrees to allow the enterprise to send 200. However, remember that the enterprise routers transmit the data at the speed of the access link, or 1 Gbps in this case. Thethat the customer chooses for that link.SP can police incoming packets, setting the policing rate to match the CIR Policers can also re-mark packets. SP could mark the messages with a new marking value, with this strategy: Renetwork.-mark packets that exceed the policing rate, but let them into the SP’s If other SP network devices are experiencing congestion when they process the packet, the different marking means that device can discard the packet. However... ...if no other SP network devices are experiencing congestion when forwarding that re-marked packet, it gets through the SP network anyway. that re-marked packet, it gets through the SP network anyway. SP canstill protecting the SP’s network during times of stress.treat their customers a little better by discarding less traffic, while key features of policing: Key Topic. It rate.measures the traffic rate over time for comparison to the configured policing It allows for a burst of data after a period of inactivity. It is enabled on an interface, in either direction, but typically at ingress. It cancandidate for more aggressive discard later in its journey.discard excess messages but can also re-mark the message so that it is a Use a shaper to slow down the traffic shaping before sending data to an SP that is policing—is one of the typical uses of a shaper slows messages down by queuing the messages. then services the shaping queues, but not based on when the physical interface is available schedules messages from the shaping queues based on the shaping rate the shaper queues packets so that the sending rate through the shaper does not exceed the shaping rate; and then output queuing works as normal, if needed. shaping queuesapply the round-robin and priority queuing features of CBWFQ and LLQ, respectively, to the Setting a Good Shaping Time Interval for Voice and Video # you can (and should) configure a shaper’s setting that changes the internal operation of the shaper, which then reduces the delay and jitter caused to voice and video traffic. Shaping # traffic. A shaper’s time interval refers to its internal logic and how a shaper averages, over time, sending at a particular rate Because the interface transmits bits at 1 Gbps, it takes just .2 seconds, or 200 ms, to send all 200 million bits. Then the shaper must wait for the rest of the time interval, another 800 ms, before beginning the next time interval. configure a short time interval so there is less wait time Tc = 1 second (1000 ms): Send at 1 Gbps for 200 ms, rest for 800 ms Tc = .1 second (100 ms): Send at 1 Gbps for 20 ms, rest for 80 ms Tc = .01 second (10 ms): Send at 1 Gbps for 2 ms, rest for 8 ms When shaping, use a short time interval. By recommendation, use a interval to support voice and video. 10 - ms time key features of shapers: Key Topic. Shapers measure the traffic rate over time for comparison to the configured shaping rate. Shapers allow for bursting after a period of inactivity. Shapers areenabled on an interface for egress (outgoing packets). Shapers slow down packets by queuing them and over time releasing them from the queue at the shaping rate. Shapers use queuing tools to create and schedule the shaping queues, which is very important for the same reasons discussed for output queuing. Congestion Avoidance # attempts to reduce overall packet loss by preemptively discarding some packets used in TCP connections connections\nTCP Windowing Basics\ntail drop\none queue being totally full. Any new packets that arrive for that queue right now will\nbe dropped because there is no room at the tail of the queue (tail drop).\nCongestion Avoidance Tools\ndiscard some TCP segments before the queues fill, hoping that enough TCP connections will\nslow down, reducing congestion, and avoiding tail drop\nmonitor the average queue depth over time, triggering more severe actions the deeper the\nqueue\nthe queue depth passes the maximum thresholdcalled full drop. ,the tool drops all packets, in an action\ncongestion avoidance tools can classify messages to treat some packets better than others\n3.0 IP Connectivity 3.5 Describe the purpose of First Hop Redundancy Protocol 4.0 Infrastructure Services 4.4 Explain the function of SNMP in network operations 4.9 Describe the capabilities and function of TFTP/FTP in the network First Hop Redundancy Protocol # ","externalUrl":null,"permalink":"/tech/networking/qos/","section":"Teches","summary":"\u003cp\u003eFriday, October 8, 2021 11:48 AM\u003c/p\u003e\n\u003cp\u003eQuality of Experience (QoE) -users’ perception of their use of the application on the\u003cbr\u003e\nnetwork.\u003cbr\u003e\nVoice and Video Applications\u003cbr\u003e\nAflowisall the data moving from one application to another over the network, with one\u003cbr\u003e\nflow for each direction\u003cbr\u003e\nFor example, if you open a website and connect to a web server, the web page content that moves from the server to the client is one flow.\u003cbr\u003e\nFrom a voice perspective, a phone call between two IP phones would create a flow for each direction\u003cbr\u003e\nStep 1. The phone user makes a phone call and begins speaking.\u003cbr\u003e\nStep 2. A chip called a codec processes (digitizes) the sound to create a binary code (160\u003cbr\u003e\nbytes with the G.711 codec, for example) for a certain time period (usually 20 ms).\u003cbr\u003e\nStep 3. The phone places the data into an IP packet.\u003cbr\u003e\nStep 4. The phone sends the packet to the destination IP phone.\u003c/p\u003e","title":"","type":"tech"},{"content":"","externalUrl":null,"permalink":"/tech/podman/","section":"Teches","summary":"","title":"","type":"tech"},{"content":" ","externalUrl":null,"permalink":"/tech/rhce-exam/","section":"Teches","summary":"\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"\"\n    src=\"../../img/Pasted%20image%2020250916083925.png\"\n    \u003e\u003c/figure\u003e","title":"","type":"tech"},{"content":"https://plausible.io/ https://analytics.bookstackapp.com/bookstackapp.com\n","externalUrl":null,"permalink":"/tech/tools/add-plausible-to-hugo-site/","section":"Teches","summary":"\u003cp\u003e\u003ca\n  href=\"https://plausible.io/\"\n    target=\"_blank\"\n  \u003ehttps://plausible.io/\u003c/a\u003e\n\u003ca\n  href=\"https://analytics.bookstackapp.com/bookstackapp.com\"\n    target=\"_blank\"\n  \u003ehttps://analytics.bookstackapp.com/bookstackapp.com\u003c/a\u003e\u003c/p\u003e","title":"","type":"tech"},{"content":" Rhel # Config root: /etc/httpd Primary config file: /etc/httpd/conf/httpd.conf Module config: conf.modules.d virtual host config: conf.d Log: /var/log/httpd User: apache\nhttpd.conf\nused include to reference other files 3 categories Global settings virtualhost instructions for requests that do not match a VirtualHost defenition Virtual host examples:\nBasic authentication in Apache is configured in Location or Directory blocks\nSSL might have changed its name to TLS\nHere, the TLS certificate and key are located in Linux’s central system location, /etc/ssl. The public certificates can be readable by anyone, but the key should be accessible only to the Apache master-process user, typically root. We prefer to set permissions to 444 for the certificate and 400 for the key.\nAll versions of the actual SSL protocol (precursor to TLS) are known to be insecure and should be disabled with the SSLProtocol directive, shown above.\nfew ciphers have known weaknesses. You can configure the web server’s supported ciphers with the SSLCipherSuite directive\nMozilla Server Side TLS guide is the best resource that we are aware of for staying current\non best practices for TLS. It also has a handy configuration syntax reference for Apache, NGINX, and HAProxy.\ncertbot:\n/etc/letencrypt/\nsectigo\nprivate key /etc/pki/tls/private/\n/etc/httpd/conf/httpd.conf Document root /var/www/html\n/var/www/swim\nIfModule mod_ssl.c\ncat 00-ssl.conf LoadModule ssl_module modules/mod_ssl.so\ncat /usr/lib64/httpd/modules/mod_ssl.so (filled with gibberish)\n/etc/httpd/conf.d/vhost_marstherm-le-ssl.conf point to the let\u0026rsquo;s encrypt SSL config\n[root@marstherm conf.d]# cat vhost_marstherm-le-ssl.conf \u0026lt;IfModule mod_ssl.c\u0026gt; \u0026lt;VirtualHost *:443\u0026gt; ServerName marstherm.psi.edu Redirect / https://marstherm.psi.edu/ SSLCertificateFile /etc/letsencrypt/live/marstherm.psi.edu/cert.pem SSLCertificateKeyFile /etc/letsencrypt/live/marstherm.psi.edu/privkey.pem Include /etc/letsencrypt/options-ssl-apache.conf SSLCertificateChainFile /etc/letsencrypt/live/marstherm.psi.edu/chain.pem \u0026lt;/VirtualHost\u0026gt; \u0026lt;/IfModule\u0026gt; /etc/httpd/conf.d/ssl.conf point to old ssl files\n[root@marstherm conf.d]# cat vhost_swim-le-ssl.conf \u0026lt;IfModule mod_ssl.c\u0026gt; \u0026lt;VirtualHost *:443\u0026gt; ServerName swim.psi.edu Redirect / https://swim.psi.edu/ \u0026lt;Directory /var/www/swim/\u0026gt; AllowOverride FileInfo AuthConfig Limit Indexes Options Indexes FollowSymLinks IndexOptions FancyIndexing \u0026lt;/Directory\u0026gt; SSLCertificateFile /etc/letsencrypt/live/swim.psi.edu/cert.pem SSLCertificateKeyFile /etc/letsencrypt/live/swim.psi.edu/privkey.pem Include /etc/letsencrypt/options-ssl-apache.conf SSLCertificateChainFile /etc/letsencrypt/live/swim.psi.edu/chain.pem \u0026lt;/VirtualHost\u0026gt; \u0026lt;/IfModule\u0026gt; added the ssl certs to the normal ssl fileand renamed to letsencryptkettest.ssl.conf\nalso moved the original ssl.conf to current.ssl.conf\n","externalUrl":null,"permalink":"/tech/tools/apache/","section":"Teches","summary":"\u003ch4 class=\"relative group\"\u003eRhel\n    \u003cdiv id=\"rhel\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#rhel\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cp\u003eConfig root: /etc/httpd\nPrimary config file:  /etc/httpd/conf/httpd.conf\nModule config: conf.modules.d\nvirtual host config: conf.d\nLog: /var/log/httpd\nUser: apache\u003c/p\u003e","title":"","type":"tech"},{"content":"Install Calibre and Rsync\napt install -y calibre rsync Make a directory for your library\nmkdir /opt/calibre Either upload your existing library using rsync. For example to /opt/calibre/.\ncd ~/Documents rsync -avuP your-library-dir root@example.org:/opt/calibre/ Add a new user to protect your server:\ncalibre-server --manage-users /var/www/nextcloud/data/david/files/Documents/Calibre\nCreating a service # Create a new file /etc/systemd/system/calibre-server.service and add the following:\n[Unit] Description=Calibre library server After=network.target [Service] Type=simple User=root Group=root ExecStart=/usr/bin/calibre-server --enable-auth --enable-local-write /opt/calibre/your_library --listen-on 127.0.0.1 [Install] WantedBy=multi-user.target You can change the port with the --port prefix. Additional information man calibre-server.\nIssue systemctl daemon-reload to apply the changes.\nEnable and start the service.\nsystemctl enable calibre-server systemctl start calibre-server A reverse proxy with Nginx # Create a new file /etc/nginx/sites-available/calibre and enter the following:\nserver { listen 80; client_max_body_size 64M; # to upload large books server_name calibre.example.org ; location / { proxy_pass http://127.0.0.1:8080; } } Enable the site\nln -s /etc/nginx/sites-available/calibre /etc/nginx/sites-enabled systemctl reload nginx Issue a Let\u0026rsquo;s Encrypt certificate. Detailed instructions and additional information.\ncertbot --nginx Add CNAME record\nNow just go to calibre.example.org. The server will request an username and a password.\nAfter login you will see something like this.\nDe DRM Books\nDownload kindle verion 1.26 https://www.filehorse.com/download-kindle-for-pc/40502/download/\nfilename: KindleForPC-installer-1.26.55076.exe\nSHA-256: c9d104c4aad027a89ab92a521b7d64bdee422136cf562f8879 f0af96abd74511 Found here: https://www.mobileread.com/forums/showthread.php?t=283371\ncd ~/Dowloads shasum -a 256 KindleForPC-installer-1.26.55076.exe c9d104c4aad027a89ab92a521b7d64bdee422136cf562f8879f0af96abd74511 KindleForPC-installer-1.26.55076.exe Make sure the hash matches the above.\nInstall it with Wine\nBypass crash on startup\nmkdir -p $WINEPREFIX/drive_c/users/$USER/AppData/Local/Amazon/Kindle/crashdump Install\n","externalUrl":null,"permalink":"/tech/tools/calibre_server/","section":"Teches","summary":"\u003cp\u003eInstall Calibre and Rsync\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eapt install -y calibre rsync\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eMake a directory for your library\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003emkdir /opt/calibre\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEither upload your existing library using \u003ccode\u003ersync\u003c/code\u003e. For example to \u003ccode\u003e/opt/calibre/\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecd\u003c/span\u003e ~/Documents\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ersync -avuP your-library-dir root@example.org:/opt/calibre/\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAdd a new user to protect your server:\u003c/p\u003e","title":"","type":"tech"},{"content":"%% Zoottelkeeper: Beginning of the autogenerated index file list %%\n[[tech/desktop/_index|_index]] [[Learning Touch Typing|Learning Touch Typing]] [[My Fedora Setup|My Fedora Setup]] [[New setup|New setup]] [[Silverblue|Silverblue]] %% Zoottelkeeper: End of the autogenerated index file list %% ","externalUrl":null,"permalink":"/tech/tools/desktop/","section":"Teches","summary":"\u003cp\u003e%% Zoottelkeeper: Beginning of the autogenerated index file list  %%\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e[[tech/desktop/_index|_index]]\u003c/li\u003e\n\u003cli\u003e[[Learning Touch Typing|Learning Touch Typing]]\u003c/li\u003e\n\u003cli\u003e[[My Fedora Setup|My Fedora Setup]]\u003c/li\u003e\n\u003cli\u003e[[New setup|New setup]]\u003c/li\u003e\n\u003cli\u003e[[Silverblue|Silverblue]]\n%% Zoottelkeeper: End of the autogenerated index file list  %%\u003c/li\u003e\n\u003c/ul\u003e","title":"","type":"tech"},{"content":"Github: https://github.com/89luca89/distrobox\nRun native containers that have access to the host file system Creates an app launcher icon for the container\nLaunch gui apps from other distros natively. Even without spinning up the container first\nMake separate shortcuts for separate distros in the terminal\nMake a profile in the terminal for the container:\n`distrobox-enter \u0026ndash;name ubuntu-20\nMake sure to check the box \u0026ldquo;run a custom command instead of my shell\u0026rdquo;\nThen set a custom shortcut in your OS to that terminal profile gnome-terminal --profile=ubuntu\nHow to Install # curl -s https://raw.githubusercontent.com/89luca89/distrobox/main/install | sudo sh\nor\nsudo dnf -y install distrobox\nYou can find a list of container distros here: https://github.com/89luca89/distrobox/blob/main/docs/compatibility.md#host-distros\nCommands # Create a container # distrobox create --name aur --image archlinux or distrobox-create --name archlinux --image docker.io/library/archlinux or distrobox create -n ubuntu -i ubuntu:20.04\nAdditional Options # \u0026ndash;nvidia - add your host nvidia drivers\n\u0026ndash;additional-packages {package-name} - install additional packages\nEnter the container # distrobox enter aur\nAdd app from another distro to your app launcher/ application menu # `distrobox-export \u0026ndash;app app-name\nCreate a GUI app # distrobox-export --app {appname}\nCreate a temporary container distrobox-ephemeral \u0026ndash;image archlinux\nCreate a container for testing server distros distrobox create --image almalinux:latest\nCreate an arm container\ndistrobox create -i aarch64/fedora -n fedora-arm64 distrobox enter fedora-arm64 Add distro to your display manager to boot into the distribution\nhttps://cloudyday.tech.blog/2022/05/14/distrobox-is-awesome/ https://github.com/89luca89/distrobox/blob/main/docs/posts/run_latest_gnome_kde_on_distrobox.md\nSetting up a fedora distrobox in ublue # To create a Fedora 38 Distrobox, run:\ndistrobox create --image registry.fedoraproject.org/fedora-toolbox:38 --name fedora\nEnter the box: distrobox enter fedora\nTo create an Ubuntu 22.04 Distrobox, run:\ndistrobox create --image docker.io/library/ubuntu:22.04 --name ubuntu\nTo create an Arch Linux Distrobox, run:\ndistrobox create --image docker.io/library/archlinux:latest --name arch\nTo create an Debian Linux Distrobox, run:\ndistrobox create --image docker.io/library/debian:latest --name debian\nList of tested container images.\nWhen choosing an image for your container, consider which package manager and repos you want to use (e.g. apt vs. pacman), and pick one that you’re most comfortable with!\nExporting programs from Distrobox # You can export a GUI program (for example, mpv) by running the following from inside the Distrobox:\ndistrobox-export --app mpv\nYou can also export a binary or CLI program (for example, vim) by running the following from inside the Distrobox. You need to provide an export path and know where the original binary exists in the Distrobox’s filesystem. An easy way to find out is running which vim (replace vim with the name of the binary you want to export). The export command will create a shell script that runs the specified binary from inside the Distrobox.\ndistrobox-export --bin /usr/bin/vim --export-path ~/.local/bin\nRead more in the Distrobox documentation about distrobox-export\nIntegrating VSCode with Distrobox # There are two ways to integrate VSCode with a Distrobox.\nThe easiest way is to just install it inside your Distrobox and distrobox-export it. The integrated terminal and all addons will then be run inside that single Distrobox. The other way is with the Dev Containers extension.\nBoth ways are detailed inside the official Distrobox tutorial for integrating with VSCode\nUsing the host’s xdg-open inside Distrobox # Some GUI programs use xdg-open to open URLs, but it doesn’t work when running your browser on the host.\nYou can fix this by adding the following shell script to your ~/.local/bin/ and giving it executable permissions.\n#!/bin/bash if [ ! -e /run/.containerenv ] \u0026amp;\u0026amp; [ ! -e /.dockerenv ]; then # if not inside a container /usr/bin/xdg-open \u0026quot;$@\u0026quot; # run xdg-open normally else distrobox-host-exec /usr/bin/xdg-open \u0026quot;$@\u0026quot; # run xdg-open on the host fi\nIf you have xdg-open installed inside the Distrobox, you need to make sure that ~/.local/bin/ is in your $PATH before /usr/bin/.\nImmutable distros # Apps do not get root access to your o not get root access to your system.\nCloud native client # /usr is read only parts of the disk Able to roll back after updates\n","externalUrl":null,"permalink":"/tech/tools/distrobox/","section":"Teches","summary":"\u003cp\u003eGithub: \u003ca\n  href=\"https://github.com/89luca89/distrobox\"\n    target=\"_blank\"\n  \u003ehttps://github.com/89luca89/distrobox\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eRun native containers that have access to the host file system\nCreates an app launcher icon for the container\u003c/p\u003e\n\u003cp\u003eLaunch gui apps from other distros natively. Even without spinning up the container first\u003c/p\u003e","title":"","type":"tech"},{"content":"Get rid of lines with color reference: :g/^**rgb/d\nget rid of the \u0026gt; on each line: %s/\u0026gt; //g\nGet rid of dashed lines: :g/^---/d\nGet rid of consecutive empty lines: :%s/\\(\\n\\)\\{3,}/\\r\\r/g\n","externalUrl":null,"permalink":"/tech/tools/editing-foliate-highlights-with-vim/","section":"Teches","summary":"\u003cp\u003eGet rid of lines with color reference:\n\u003ccode\u003e:g/^**rgb/d\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eget rid of the \u003ccode\u003e\u0026gt; \u003c/code\u003e on each line:\n\u003ccode\u003e%s/\u0026gt; //g\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eGet rid of dashed lines:\n\u003ccode\u003e:g/^---/d\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eGet rid of consecutive empty lines:\n\u003ccode\u003e:%s/\\(\\n\\)\\{3,}/\\r\\r/g\u003c/code\u003e\u003c/p\u003e","title":"","type":"tech"},{"content":"Make sure your VSP unblocks port 25\nAllow ports using your firewall\nufw allow 25/tcp ufw allow 587/tcp Install mailutils\napt install -y mailutils postfix Select \u0026ldquo;internet site\u0026rdquo; in the gui.\nThen enter your domain name \u0026ldquo;example.com\u0026rdquo;\n(Screenshot)\nReverse DNS (rDNS) # Email servers may require a PTR record to prevent spam\nIn vultr (VPS) make sure your reverse DNS is set to your domain name. Do this for both ipv4 and ipv6 under settings.\nDKIM (Domain Keys Identified Mail) # Used to prevent people from changing their from address.\nOpen DKIM is a tool that creates cryptographic public and private keys for your server. This way yopu emails can be trusted. Other servers can check your public key to make sure you are really you.\nYou will also need server-side authorization settings to verify which account the email came from.\napt install opendkim opendkim-tools Generate DKIM Key, create directory, and edit permissions.\nmkdir -p /etc/postfix/dkim opendkim-genkey -D /etc/postfix/dkim/ -d example.org -s mail chgrp opendkim /etc/postfix/dkim/* chmod g+r /etc/postfix/dkim/* Create the key table\ntell OpenDKIM where the keys are\necho \u0026#34;mail._domainkey.example.org example.org:mail:/etc/postfix/dkim/mail.private\u0026#34; \u0026gt; /etc/postfix/dkim/keytable Create signing table\necho \u0026#34;*@example.org mail._domainkey.example.org\u0026#34; \u0026gt; /etc/postfix/dkim/signingtable Add trusted hosts\necho \u0026#34;127.0.0.1 10.1.0.0/16 1.2.3.4/24\u0026#34; \u0026gt; /etc/postfix/dkim/trustedhosts OpenDKIM configuration file\nopen up /etc/opendkim.conf add these lines to source the files created above\nKeyTable file:/etc/postfix/dkim/keytable SigningTable refile:/etc/postfix/dkim/signingtable InternalHosts refile:/etc/postfix/dkim/trustedhosts Canonicalization relaxed/simple Socket inet:12301@localhost the socket option will already be set in the configuration file. COmment it out or overwrite this.\nAllowing postfix to interface with OPenDKIM # set our OpenDKIM server, which will be running on port 12301, as a milter (mail filter).\npostconf -e \u0026#34;myhostname = $(cat /etc/mailname)\u0026#34; postconf -e \u0026#34;milter_default_action = accept\u0026#34; postconf -e \u0026#34;milter_protocol = 6\u0026#34; postconf -e \u0026#34;smtpd_milters = inet:localhost:12301\u0026#34; postconf -e \u0026#34;non_smtpd_milters = inet:localhost:12301\u0026#34; Restart and reload the services\nsystemctl restart opendkim systemctl enable opendkim systemctl reload postfix Add key as TXT record in your registrar # Grab the key\necho -e \u0026#34; v=DKIM1; k=rsa; $(tr -d \u0026#34; \u0026#34; \u0026lt;/etc/postfix/dkim/mail.txt | sed \u0026#34;s/k=rsa.* \\\u0026#34;p=/k=rsa; p=/;s/\\\u0026#34;\\s*\\\u0026#34;//;s/\\\u0026#34;\\s*).*//\u0026#34; | grep -o \u0026#34;p=.*\u0026#34;) \u0026#34; Grab the key that starts with v=DKIM1 and enter it as the TXT Value for the record.\nenter the host name: mail._domainkey\nTest\necho \u0026#34;Hi there. This is the text.\u0026#34; | mail -s \u0026#34;Email from the server\u0026#34; your@emailaddress.com For more help https://appmaildev.com/en/dkim\nDMARC (Domain-based Message Authentication Protocol) # Give email domain owners the ability to protect your domain from unauthorized use.\nAdd the dmarc user:\nuseradd -m -G mail dmarc make a new TXT record like we did with DKIM, except now use the output from the following command:\necho \u0026#34;_dmarc.$(cat /etc/mailname)\u0026#34; echo \u0026#34;v=DMARC1; p=reject; rua=mailto:dmarc@$(cat /etc/mailname); fo=1\u0026#34; The first line is the Host field. The latter is the TXT value.\nSender Policy Framework # Saving the easiest for last, we should add a TXT record for SPF, an email-authentication standard used to prevent spammers from sending messages that appear to come from a spoofed domain.\ncat /etc/mailname echo \u0026#34;v=spf1 mx a:mail.$(cat /etc/mailname) -all\u0026#34; The output of cat /etc/mailname is the Host field. The output of the second command is the TXT value.\nAgain, you can check that site to make sure your DKIM, DMARC, and SPF entries are valid. That’s it!\nSetting up an E-mail Inbox # In the article on SMTP and Postfix, we set up a simple Postfix server that we could use to programatically send mail with the mail command. In order to have a true and fully-functional mail server, users should be able to login to a mail client where they can read their inbox and send mail remotely. In order to achieve this we need Dovecot, which can store mails received by the server, authenticate user accounts and interact with mail.\nIf we’re setting up an inbox we will also want spam detection software, such as spam assassin.\nDovecot and Spamassassin # apt install dovecot-imapd dovecot-sieve spamassassin spamc Unblock the imap port:\nufw allow 993 Certificate # We will want a SSL certificate for the mail. subdomain. We can get this with Certbot. Assuming we are using Nginx for our server otherwise, run:\ncertbot --nginx certonly -d mail.example.org DNS # We also need two little DNS records set on your domain registrar’s site/DNS server:\nAn MX record. Just put your domain, example.org, in the “Points to” field. A CNAME record. Host field: mail.example.org. “Points to” field: example.org. Configuring Dovecot # Dovecot\u0026rsquo;s configuration file is in /etc/dovecot/docevot.conf. If you open that file, you will see this line: !include conf.d/*.conf which adds all the .conf files in /etc/dovecot/conf.d/ to the Dovecot configuration.\nOne can edit each of these files individually to get the needed configuration, but to make things easy here, delete or backup the main configuration file and we will replace it with one single config file with all important settings in it. Make sure you change ssl_cert and ssl_key accordingly.\n# Note that in the dovecot conf, you can use: # %u for username # %n for the name in name@domain.tld # %d for the domain # %h the user\u0026#39;s home directory # Connections between the mail client and Dovecot needs to be encrypted ssl = required ssl_cert = \u0026lt;/etc/letsencrypt/live/mail.example.org/fullchain.pem ssl_key = \u0026lt;/etc/letsencrypt/live/mail.example.org/privkey.pem ssl_min_protocol = TLSv1.2 ssl_cipher_list = EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA256:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EDH+aRSA+AESGCM:EDH+aRSA+SHA256:EDH+aRSA:EECDH:!aNULL:!eNULL:!MEDIUM:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!RC4:!SEED ssl_prefer_server_ciphers = yes ssl_dh = \u0026lt;/usr/share/dovecot/dh.pem auth_mechanisms = plain login auth_username_format = %n protocols = $protocols imap # Search for valid users in /etc/passwd userdb { driver = passwd } #Fallback: Use plain old PAM to find user passwords passdb { driver = pam } # Our mail for each user will be in ~/Mail, and the inbox will be ~/Mail/Inbox mail_location = maildir:~/Mail:INBOX=~/Mail/Inbox:LAYOUT=fs namespace inbox { inbox = yes mailbox Drafts { special_use = \\Drafts auto = subscribe } mailbox Junk { special_use = \\Junk auto = subscribe autoexpunge = 30d } mailbox Sent { special_use = \\Sent auto = subscribe } mailbox Trash { special_use = \\Trash } mailbox Archive { special_use = \\Archive } } # Here we let Postfix use Dovecot\u0026#39;s authetication system. service auth { unix_listener /var/spool/postfix/private/auth { mode = 0660 user = postfix group = postfix } } protocol lda { mail_plugins = \\$mail_plugins sieve } protocol lmtp { mail_plugins = \\$mail_plugins sieve } plugin { sieve = ~/.dovecot.sieve sieve_default = /var/lib/dovecot/sieve/default.sieve sieve_dir = ~/.sieve sieve_global_dir = /var/lib/dovecot/sieve/ } Settings Explained # Take a good look at the above settings to understand what\u0026rsquo;s going on. Some of the settings include:\nSSL settings to allow encrypted connections. The mail server will authenticate users against PAM/passwd, which means users you create on the server (so long as they are part of the mail group) will be able to receive and send mail. Default directories for a mail account: Inbox, Sent, Drafts, Junk, Trash and Archive. Create a unix_listener that will allow Postfix to authenticate users via Dovecot. Setup the Dovecot sieve plugin, which provides mail filtering facilities at time of final message delivery. Sieve scripts can be used to customize how messages are delivered, whether they’re forwarded or stored in special folders. Next, we can tell sieve to automatically move mail flagged as spam to the junk folder:\necho \u0026#34;require [\\\u0026#34;fileinto\\\u0026#34;, \\\u0026#34;mailbox\\\u0026#34;]; if header :contains \\\u0026#34;X-Spam-Flag\\\u0026#34; \\\u0026#34;YES\\\u0026#34; { fileinto \\\u0026#34;Junk\\\u0026#34;; }\u0026#34; \u0026gt; /var/lib/dovecot/sieve/default.sieve After that, we should create the vmail user and group, which will access the mails, and then update the sieve configuration:\ngrep -q \u0026#39;^vmail:\u0026#39; /etc/passwd || useradd vmail chown -R vmail:vmail /var/lib/dovecot sievec /var/lib/dovecot/sieve/default.sieve Then, enable pam authentication for Dovecot:\necho \u0026#34;auth required pam_unix.so nullok account required pam_unix.so\u0026#34; \u0026gt;\u0026gt; /etc/pam.d/dovecot Connecting Postfix and Dovecot # We need to tell Postfix to look to Dovecot for authenticating users/passwords. Dovecot will be putting an authentication socket in /var/spool/postfix/private/auth.\npostconf -e \u0026#39;smtpd_sasl_auth_enable = yes\u0026#39; postconf -e \u0026#39;smtpd_sasl_type = dovecot\u0026#39; postconf -e \u0026#39;smtpd_sasl_path = private/auth\u0026#39; postconf -e \u0026#39;mailbox_command = /usr/lib/dovecot/deliver\u0026#39; Connecting Postfix and Spamassassin # We will change /etc/postifx/master.cf so postfix can route mail through spamassassin. First we can cleanup the default configuration. Feel free to make a backup.\nsed -i \u0026#39;/^\\s*-o/d;/^\\s*submission/d;/^\\s*smtp/d\u0026#39; /etc/postfix/master.cf Finally, run this command to finish the configuration for spamassassin.\necho \u0026#34;smtp unix - - n - - smtp smtp inet n - y - - smtpd -o content_filter=spamassassin submission inet n - y - - smtpd -o syslog_name=postfix/submission -o smtpd_tls_security_level=encrypt -o smtpd_sasl_auth_enable=yes -o smtpd_tls_auth_only=yes smtps inet n - y - - smtpd -o syslog_name=postfix/smtps -o smtpd_tls_wrappermode=yes -o smtpd_sasl_auth_enable=yes spamassassin unix - n n - - pipe user=debian-spamd argv=/usr/bin/spamc -f -e /usr/sbin/sendmail -oi -f \\${sender} \\${recipient}\u0026#34; \u0026gt;\u0026gt; /etc/postfix/master.cf Make new mail accounts # This is the easy part. Let’s say we want to add a user Billy and let him receive mail, run this:\nuseradd -m -G mail billy passwd billy Any user added to the mail group will be able to receive mail. Suppose a user Cassie already exists and we want to let her receive mail too. Just run:\nusermod -a -G mail cassie Harden your E-mail Server # Hardening Postfix # Put restrictions on servers sending mail to you.\npostconf -e \u0026#39;smtpd_recipient_restrictions = permit_sasl_authenticated, permit_mynetworks, reject_unauth_destination, reject_unknown_recipient_domain\u0026#39; Anonymize Headers # Use some regular expressions to prevent some meta data like a client’s ip address from being leaked.\necho \u0026#34;/^Received:.*/ IGNORE /^X-Originating-IP:/ IGNORE /^User-Agent:/ IGNORE /^X-Mailer:/ IGNORE\u0026#34; \u0026gt;\u0026gt; /etc/postfix/header_checks Add this file to the postfix configuration:\npostconf -e \u0026#34;header_checks = regexp:/etc/postfix/header_checks\u0026#34; Fail2Ban # If you’re not familiar with fail2Ban, it’s essentially a program which blocks bot’s and hacker’s login requests after a few invalid attempts.\napt-get install fail2ban Make a local copy of the configuration file:\ncp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local Go down to the # Mail servers line and paste this:\n[postfix] enabled = true port = smtp,ssmtp,submission filter = postfix logpath = /var/log/mail.log [sasl] enabled = true port = smtp,ssmtp,submission,imap2,imap3,imaps,pop3,pop3s filter = postfix-sasl # You might consider monitoring /var/log/mail.warn instead if you are # running postfix since it would provide the same log lines at the # \u0026#34;warn\u0026#34; level but overall at the smaller filesize. logpath = /var/log/mail.warn maxretry = 1 bantime = 21600 [dovecot] enabled = true port = smtp,ssmtp,submission,imap2,imap3,imaps,pop3,pop3s filter = dovecot logpath = /var/log/mail.log This will only grant 2 login attempts and then block the requester for 6 hours. Now restart fail2ban:\nsystemctl restart fail2ban ","externalUrl":null,"permalink":"/tech/tools/emailserver/","section":"Teches","summary":"\u003cp\u003eMake sure your VSP unblocks port 25\u003c/p\u003e\n\u003cp\u003eAllow ports using your firewall\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eufw allow 25/tcp\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eufw allow 587/tcp\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eInstall mailutils\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eapt install -y mailutils postfix\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSelect \u0026ldquo;internet site\u0026rdquo; in the gui.\u003c/p\u003e","title":"","type":"tech"},{"content":"Following this guide: https://blog.while-true-do.io/podman-setup-gitea/\nCreate container network: sudo podman network create gitea-net\nEnable update timer for automatic updates: sudo systemctl enable --now podman-auto-update.timer\nDatabase setup # sudo vim /etc/systemd/system/container-gitea-db.service\n# container-gitea-db.service [Unit] Description=Podman container-gitea-db.service Wants=network.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 PIDFile=%t/container-gitea-db.pid Type=forking ExecStartPre=/bin/rm -f %t/container-gitea-db.pid %t/container-gitea-db.ctr-id ExecStart=/usr/bin/podman container run \\ --conmon-pidfile %t/container-gitea-db.pid \\ --cidfile %t/container-gitea-db.ctr-id \\ --cgroups=no-conmon \\ --replace \\ --detach \\ --tty \\ --env MARIADB_RANDOM_ROOT_PASSWORD=yes \\ --env MARIADB_DATABASE=gitea \\ --env MARIADB_USER={username} \\ --env MARIADB_PASSWORD={password} \\ --volume gitea-db-volume:/var/lib/mysql/:Z \\ --label \u0026#34;io.containers.autoupdate=registry\u0026#34; \\ --network gitea-net \\ --name gitea-db \\ docker.io/library/mariadb:10 ExecStop=/usr/bin/podman stop --ignore --cidfile %t/container-gitea-db.ctr-id -t 10 ExecStopPost=/usr/bin/podman rm --ignore -f --cidfile %t/container-gitea-db.ctr-id [Install] WantedBy=multi-user.target default.target Enable the service and verify:\nsudo systemctl daemon-reload sudo systemctl enable --now container-gitea-db sudo systemctl status container-gitea-db sudo podman ps Gitea setup # Create the system-d unit file: (use the same user and password from the database)\nsudo vim /etc/systemd/system/container-gitea-app.service [Unit] Description=Podman container-gitea-app.service Wants=network.target After=network-online.target RequiresMountsFor=/var/lib/containers/storage /var/run/containers/storage [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 PIDFile=%t/container-gitea-app.pid Type=forking ExecStartPre=/bin/rm -f %t/container-gitea-app.pid %t/container-gitea-app.ctr-id ExecStart=/usr/bin/podman container run \\ --conmon-pidfile %t/container-gitea-app.pid \\ --cidfile %t/container-gitea-app.ctr-id \\ --cgroups=no-conmon \\ --replace \\ --detach \\ --tty \\ --env DB_TYPE=mysql \\ --env DB_HOST=gitea-db:3306 \\ --env DB_NAME=gitea \\ --env DB_USER={username} \\ --env DB_PASSWD={password} \\ --volume gitea-data-volume:/var/lib/gitea:Z \\ --volume gitea-config-volume:/etc/gitea:Z \\ --network gitea-net \\ --publish 2222:2222 \\ --publish 3000:3000 \\ --label \u0026#34;io.containers.autoupdate=registry\u0026#34; \\ --name gitea-app \\ docker.io/gitea/gitea:1-rootless ExecStop=/usr/bin/podman container stop \\ --ignore \\ --cidfile %t/container-gitea-app.ctr-id \\ -t 10 ExecStopPost=/usr/bin/podman container rm \\ --ignore \\ -f \\ --cidfile %t/container-gitea-app.ctr-id [Install] WantedBy=multi-user.target default.target sudo systemctl daemon-reload sudo systemctl enable --now container-gitea-app sudo systemctl status container-gitea-app sudo podman ps Test the GUI site: http://IP-ADDRESS:3000\n","externalUrl":null,"permalink":"/tech/tools/gitea-setup/","section":"Teches","summary":"\u003cp\u003eFollowing this guide: \u003ca\n  href=\"https://blog.while-true-do.io/podman-setup-gitea/\"\n    target=\"_blank\"\n  \u003ehttps://blog.while-true-do.io/podman-setup-gitea/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCreate container network:\n\u003ccode\u003esudo podman network create gitea-net\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eEnable update timer for automatic updates:\n\u003ccode\u003esudo systemctl enable --now podman-auto-update.timer\u003c/code\u003e\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eDatabase setup\n    \u003cdiv id=\"database-setup\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#database-setup\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003esudo vim /etc/systemd/system/container-gitea-db.service\u003c/code\u003e\u003c/p\u003e","title":"","type":"tech"},{"content":" Commands # Version # $ git \u0026ndash;version\nShow version of git installed Config # $ git config \u0026ndash;global user.name \u0026ldquo;username\u0026rdquo; $ git config \u0026ndash;global user.email \u0026ldquo;email@email.com\u0026rdquo;\nLet\u0026rsquo;s git know who you are Inizialize # $ git init\ninitialize git on the current working directory Status # $ git status\ncheck the status of your git repo from your current working directory \u0026ndash;short flag will show a more compact version Short status flags are:\n?? - Untracked files A - Files added to stage M - Modified files D - Deleted files\nStaging # $ git add filename.txt\nstages the file $ git add \u0026ndash;all\nstages all files in the directory clone from a remote repo # git clone enter_repo_url_here\nPull # Use this to update current folder with new repo changes\ngit pull origin main Log # $ git log\nView history of commits for a repository. Help # $ git command -help (or \u0026ndash;help)\noptions for a specific command $ git help \u0026ndash;all\nSee all possible commands Link to Github # Add the repository and push. You need an access toekn as the password to log in.\n[davidthomas@fedora PerfectDarkMode]$ git remote add origin https://github.com/tdavetech/perfectdarkmode.git [davidthomas@fedora PerfectDarkMode]$ git branch -M main [davidthomas@fedora PerfectDarkMode]$ git push -u origin main Create a personal access Token; https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\nMake sure you are linking to HTTPS # https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#switching-remote-urls-from-ssh-to-https\nSummary # Version control system Tracking code changes Tracking who made changes Coding collaboration Manage projects with Repositories Clone a project to work on a local copy Staging and Committing Branch and Merge to allow for work on different parts and versions of a project Push local updates to the main project How to Use # Initialize Git on a folder, making it a Repository Git now creates a hidden folder to keep track of changes in that folder When a file is changed, added or deleted, it is considered modified You select the modified files you want to Stage The Staged files are Committed, which prompts Git to store a permanent snapshot of the files Git allows you to see the full history of every commit. You can revert back to any previous commit. Git does not store a separate copy of every file in every commit, but keeps track of changes made in each commit! Files in your Git repository folder can be in one of 2 states: # Tracked - files that Git knows about and are added to the repository Untracked - files that are in your working directory, but not added to the repository When you first add files to an empty repository, they are all untracked. To get Git to track them, you need to stage them, or add them to the staging environment.\nGit Staging Environment # Staged files are files that are ready to be committed to the repository you are working on.\nGit Commit # Adding commits keep track of changes. Each commit is a change point or \u0026ldquo;save point\u0026rdquo;. You can (and should) include a message with each commit.\n$ git commit\n$ git commit -m \u0026quot;add comment here\u0026quot; * commit and add a comment * -m flag adds a message\n$ git commit -a -m \u0026quot;add message here\u0026quot; * -a flag automatically stage every changed, already tracked file\nCommit without stage # -a option will automatically stage every changed, already tracked file. Can make you commit unwanted changes Git Branch # Branch = A new/separate version of the main repository. allow you to work on different parts of a project without impacting the main branch. a branch can be merged with the main project. switch between branches and work on different projects without them interfering with each other. Low storage/overhead $ git branch branch-name-here * create branch with name\nCheckout # Need to check out a branch before committing or the branch will not be included. $ git checkout newImage; git commit\ncreate a new branch AND check it out at the same time $ git checkout -b [yourbranchname]\nWhat is GitHub? # Git is not the same as GitHub. GitHub makes tools that use Git. GitHub is the largest host of source code in the world, and has been owned by Microsoft since 2018. Git Merge # Combine work from two different branches. Creates a special commit with two parents $ git merge branch-name - makes a new commit with two parent (Main and the chosen branch) - Doesn\u0026rsquo;t merge the branch unless you check out the branch first.\n$ git checkout branch-name; git merge main - checkout branch and merge with main\nGit Rebase # Another way of combining work between branches Takes a set of commits and copies them to somewhere else can be used to make a nice linear sequence of commits. commit log / history of the repository will be a lot cleaner if only rebasing is allowed. moves th $ git rebase main - adds selected branch as a commit over main $ git rebase branch-name - You will also need to checkout main and rebase it to the last commit\nHEAD # the symbolic name for the currently checked out commit always points to the most recent commit Detaching head attaching head to a commit instead of a branch $ git checkout commit-name - checks out the commit and attaches the head\nStore git credentials: git config credential.helper store\nUpdate current folder in one command git add . \u0026amp;\u0026amp; git commit -m \u0026quot;Message goes here\u0026quot; \u0026amp;\u0026amp; git push\n","externalUrl":null,"permalink":"/tech/tools/gitguide/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003eCommands\n    \u003cdiv id=\"commands\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#commands\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\n\u003ch2 class=\"relative group\"\u003eVersion\n    \u003cdiv id=\"version\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#version\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e$ git \u0026ndash;version\u003c/p\u003e","title":"","type":"tech"},{"content":" Convert .MOV to MP4 # All .MOV files in a folder\nfor f in *.MOV; do ffmpeg -i $f $f.mp4; done More info at Techwalla https://www.techwalla.com/articles/how-to-convert-mov-to-mp4-with-ffmpeg-on-linux\nConvert .HEIC to .jpg # Fedora Install\nsudo dnf install -y https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm sudo dnf install libheif sudo dnf install libheif-tools Convert all .HEIC files\nfor f in *.HEIC; do heif-convert -q 100 $f $f.jpg; done See https://linuxnightly.com/convert-heif-images-to-jpg-or-png-on-linux/ for more options.\n","externalUrl":null,"permalink":"/tech/tools/iphonemigrate/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eConvert .MOV to MP4\n    \u003cdiv id=\"convert-mov-to-mp4\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#convert-mov-to-mp4\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eAll .MOV files in  a folder\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003efor f in *.MOV; do ffmpeg -i $f $f.mp4; done\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eMore info at Techwalla \u003ca\n  href=\"https://www.techwalla.com/articles/how-to-convert-mov-to-mp4-with-ffmpeg-on-linux\"\n    target=\"_blank\"\n  \u003ehttps://www.techwalla.com/articles/how-to-convert-mov-to-mp4-with-ffmpeg-on-linux\u003c/a\u003e\u003c/p\u003e","title":"","type":"tech"},{"content":"Commands to manage Libvirt:\nvirsh\nManage VMs ","externalUrl":null,"permalink":"/tech/tools/libvirt/","section":"Teches","summary":"\u003cp\u003eCommands to manage Libvirt:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003evirsh\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eManage VMs\u003c/li\u003e\n\u003c/ul\u003e","title":"","type":"tech"},{"content":"First, you\u0026rsquo;ll need to create a github account. If you won\u0026rsquo;t be using a custom domain (You can add one later), make sure you name the account what you want your website to be. The domain will be {{ account-name.github.io }}\nOnce you have an account, the first thing you will do is generate a new ssh key and add it to github. This will give you passwordless access to your github account from your desktop. From my Fedora Linux machine, I will run:\nssh-keygen -t ed25519 -C \u0026quot;exampleemail@email.com\u0026quot;\nWhen it asks for the name of the key, give it a unique name to use for this account. I just picked the gitup account name to make it easy to identify:\nid_ed25519_example\nThen, grab your public key from ~/.ssh/id_ed25519_example.pub and add it in github under settings \u0026gt; SSH and GPG Keys \u0026gt; New SSH Key\nNext, go to your github homepage and select \u0026ldquo;Create repository\u0026rdquo;. The repository name needs to match your github account name from earlier. It also must have public visibility:\nFrom your desktop, create a directory with {{ reponame.github.io }}. You must use this format for this to work: mkdir everythingelsexyz.github.io \u0026amp;\u0026amp; cd everythingelsexyz.github.io\nFollow the instructions on github for \u0026ldquo;…or create a new repository on the command line\u0026rdquo;.\necho \u0026#34;# everythingelsexyz.github.io\u0026#34; \u0026gt;\u0026gt; README.md git init git add README.md git commit -m \u0026#34;first commit\u0026#34; git branch -M main git https://github.com/everythingelsexyz/everythingelsexyz.github.io.git git push -u origin main Then add credentials:\ngit config user.name \u0026#34;everythingelsexyz\u0026#34; git config user.email \u0026#34;everythingelsedt@gmail.com\u0026#34; If you have multiple github accounts, you\u0026rsquo;ll have to create aliases for each one in the ~/.ssh/config file:\nHost linuxreader.github.com HostName github.com IdentityFile ~/.ssh/id_ed25519 Host everythingelsexyz.github.com HostName github.com IdentityFile ~/.ssh/id_ed25519_everythingelsexyz Then add your private keys to the ssh agent:\nssh-add ~/.ssh/id_ed25519 ssh-add ~/.ssh/id_ed25519_everythingelsexyz Then add the username and email settings while in the new project directory:\neverythingelsexyz.github.io on  main ❯ git config user.name \u0026#34;everythingelsexyz\u0026#34; everythingelsexyz.github.io on  main ❯ git config user.email \u0026#34;example@email.com\u0026#34; Then you will need cd to each repo and add an alias:\nlinuxreader.github.io on  main [⇡] via 🐹 via  ❯ git remote set-url origin git@linuxreader.github.com:linuxreader/linuxreader.git everythingelsexyz.github.io on  main git remote set-url origin git@everythingelsexyz.github.com:everythingelsexyz/everythingelsexyz.git Load the keys into the agent:\nssh-add ~/.ssh/id_ed25519 ssh-add ~/.ssh/id_ed25519_everythingelsexyz Adding a Hugo Theme to your Github Repository # Host\nGo through quick setup here: https://pages.github.com/\nHugo Setup: https://gohugo.io/hosting-and-deployment/hosting-on-github/\nHere\u0026rsquo;s how to set up a free website using Github pages and Hugo. You can add a custom domain name using Cloudflare for about $10 per year.\nThe general steps include:\nCreate a Github repository for your site. Change your pages settings. Build the repo into a Hugo site. Add DNS records for a custom domain. ","externalUrl":null,"permalink":"/tech/tools/obsidian-hugo-github-website-stack-ohg/","section":"Teches","summary":"\u003cp\u003eFirst, you\u0026rsquo;ll need to create a github account. If you won\u0026rsquo;t be using a custom domain (You can add one later), make sure you name the account what you want your website to be. The domain will be {{ account-name.github.io }}\u003c/p\u003e","title":"","type":"tech"},{"content":"Sync obsidian dirs to hugo dirs: rsync -av --delete source destination \u0026ndash;delete flag deletes files in the destination directory if they do not exist in the source\n#!/bin/bash rsync -av --delete ~/Documents/content/booknotes ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/\u0026#39;health and fitness\u0026#39; ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/images ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/linux ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/networking ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/now ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/philosophy ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/python ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/Recipes ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/Recommended ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/tools ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/writing ~/Documents/blog/content/ rsync -av --delete ~/Documents/content/_index.md ~/Documents/blog/content/ cd /home/david/Documents/blog \u0026amp;\u0026amp; hugo \u0026amp;\u0026amp; if ! nmcli connection show --active | grep -q \u0026#34;Primary\u0026#34;; then echo \u0026#34;Starting VPN...\u0026#34; nmcli connection up id \u0026#34;Primary\u0026#34; sleep 5 fi rsync -rtvzP ~/Documents/blog/docs/ {username}@{ip-address}:/var/www/public Using inotify to monitor obsidian and run the update script whenever there are changes: # sudo dnf -y install inotify-tools\nScript to monitor the folder:\nvim ~/Documents/Scripts/obsidian_monitor.sh #!/bin/bash inotifywait -m -r -e modify,create,delete \u0026#34;~/Documents/content/\u0026#34; | while read path action file; do echo \u0026#34;Change detected: $action on $file\u0026#34; ~/Documents/Scripts/hugoupdate.sh done Make script executable: chmod +x ~/Documents/Scripts/obsidian_monitor.sh\nCreate startup script ~/.config/systemd/user/obsidian-monitor.service\nvim ~/.config/systemd/user/obsidian-monitor.service [Unit] Description=Obsidian Monitor After=network.target [Service] ExecStart=/home/david/Documents/Scripts/obsidian_monitor.sh Restart=always #User=david [Install] WantedBy=multi-user.target Reload systemd, enable, and start the service:\nsystemctl --user daemon-reload systemctl --user enable obsidian-monitor.service systemctl --user start obsidian-monitor.service Run hugo to build the html\nOn the server # Create the directory for the html files:\nmkdir -p /var/www/public From your laptop, yeet the files over to the /var/www directory on the server: `rsync -rtvzP ~/Documents/blog/docs/ {username}@{servername or ip}:/var/www/public\nUpdate file owner and permissions:\nsudo find /var/www/public -type d -exec chmod +x {} \\; sudo find /var/www/public -type f -exec chmod 644 {} \\; sudo chown -R apache:apache /var/www/public Check active vhosts:\nhttpd -D DUMP_VHOSTS Firewall Permissions # You will need to make sure your firewall allows port 80 and 443. Vultr installs the ufw program by default. But your can install it if you used a different provider. Beware, enabling a firewalll could block you from accessing your vm, so do your research before tinkering outside of these instructions.\nsudo firewall-cmd --add-service https --permanent sudo firewall-cmd --add-service http --permanent Setting up the Apache Server # Install apache: sudo dnf -y install httpd\nComment out ServerName and DocumentRoot directives in /etc/httpd/conf/httpd.conf file.\nGet rid of userdir.conf sudo mv /etc/httpd/conf.d/userdir.conf /etc/httpd/conf.d/userdir.conf.old\nComment out /etc/httpd/conf.d/welcome.conf:\n# # This configuration file enables the default \u0026#34;Welcome\u0026#34; page if there # is no default index page present for the root URL. To disable the # Welcome page, comment out all the lines below. # # NOTE: if this file is removed, it will be restored on upgrades. # #\u0026lt;LocationMatch \u0026#34;^/+$\u0026#34;\u0026gt; # Options -Indexes # ErrorDocument 403 /.noindex.html #\u0026lt;/LocationMatch\u0026gt; #\u0026lt;Directory /usr/share/httpd/noindex\u0026gt; # AllowOverride None # Require all granted #\u0026lt;/Directory\u0026gt; #Alias /.noindex.html /usr/share/httpd/noindex/index.html #Alias /poweredby.png /usr/share/httpd/icons/apache_pb3.png #Alias /system_noindex_logo.png /usr/share/httpd/icons/system_noindex_logo.png Create vhost file: sudo vim /etc/httpd/conf.d/vhost_{sitename}.com\n\u0026lt;VirtualHost *:80\u0026gt; ServerAdmin webmaster@{sitename}.com DocumentRoot /var/www/public ServerName {sitename}.com ServerAlias www.{sitename}.com Redirect permanent / https://{sitename}.com/ ErrorLog logs/{sitename}_error.log CustomLog logs/{sitename}_access.log combined \u0026lt;/VirtualHost\u0026gt; Enable the apache service now and at startup: sudo systemctl enable --now httpd\nUse certbot to generate certs for SSL/TLS: # Install certbot:\nSudo dnf -y install epel-release sudo dnf install certbot python3-certbot-apache -y Restore selinux contexts: sudo restorecon -Rv /var/www/public\nRestart the httpd service: sudo systemctl restart httpd\nSet Up a Cronjob to automatically Renew certbot certs # crontab -e Select a text editor and add this line to the end of the file. Then save and exit the file:\n0 0 1 * * certbot --httpd renew You now have a running website. Just make new posts locally, the run \u0026ldquo;hugo\u0026rdquo; to rebuild the site. And use the rsync alias to update the folder on your server. I will soon be making tutorials on making an email address for your domain, such as david@perfectdarkmode.com on my site. I will also be adding a comments section, RSS feed, email subscription, sidebar, and more.\nFeel free to reach out with any questions if you get stuck. This is meant to be an all encompassing guide. So I want it to work.\n","externalUrl":null,"permalink":"/tech/tools/obsidian-site-setup/","section":"Teches","summary":"\u003cp\u003eSync obsidian dirs to hugo dirs:\n\u003ccode\u003ersync -av --delete source destination\u003c/code\u003e\n\u0026ndash;delete flag deletes files in the destination directory if they do not exist in the source\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/bash\nrsync -av --delete ~/Documents/content/booknotes ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/\u0026#39;health and fitness\u0026#39; ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/images ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/linux ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/networking ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/now ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/philosophy ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/python ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/Recipes ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/Recommended ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/tools ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/writing ~/Documents/blog/content/\nrsync -av --delete ~/Documents/content/_index.md ~/Documents/blog/content/\n\ncd /home/david/Documents/blog \u0026amp;\u0026amp;\nhugo \u0026amp;\u0026amp;\nif ! nmcli connection show --active | grep -q \u0026#34;Primary\u0026#34;; \n\tthen\n\t\techo \u0026#34;Starting VPN...\u0026#34;\n\t\tnmcli connection up id \u0026#34;Primary\u0026#34;\n  sleep 5\nfi\n\nrsync -rtvzP ~/Documents/blog/docs/ {username}@{ip-address}:/var/www/public\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 class=\"relative group\"\u003eUsing inotify to monitor obsidian and run the update script whenever there are changes:\n    \u003cdiv id=\"using-inotify-to-monitor-obsidian-and-run-the-update-script-whenever-there-are-changes\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-inotify-to-monitor-obsidian-and-run-the-update-script-whenever-there-are-changes\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003esudo dnf -y install inotify-tools\u003c/code\u003e\u003c/p\u003e","title":"","type":"tech"},{"content":" Pandoc # Used for converting file formats\nhttps://ins3cure.com/pandoc-on-rhel-8/\nGo to https://github.com/jgm/pandoc/releases/latest, download the latest tarball (currently pandoc-2.17.0.1-linux-64.tar.gz) and run:\nsudo tar xvzf pandoc-2.17.0.1-linux-64.tar.gz --strip-components 1 -C /usr/local/ install TeX to be able to generate PDF output:\nsudo yum -y install texlive Basic useage # pandoc inputfile -f inputfiletype -t outputfiletype -o outputfile Convert full folder # for f in *; do pandoc \u0026#34;$f\u0026#34; -s -o \u0026#34;../bloghtml/${f%}.html\u0026#34;; done Convert epub to markdown and migrate to Obsidian # Run pandoc bookname.epub -o bookname.md --extract-media=images\nThis will create the file and an image folder. Place the image folder in your Obsidian attachments folder. Also, place the markdown file in your Obsidian vault and Obsidian will see all of the image references.\nThen, you can run Obsidian\u0026rsquo;s Text Extractor to pull the text from the image to your clipboard.\n","externalUrl":null,"permalink":"/tech/tools/pandoc/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003ePandoc\n    \u003cdiv id=\"pandoc\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#pandoc\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003eUsed for converting file formats\u003c/p\u003e\n\u003cp\u003e\u003ca\n  href=\"https://ins3cure.com/pandoc-on-rhel-8/\"\n    target=\"_blank\"\n  \u003ehttps://ins3cure.com/pandoc-on-rhel-8/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGo to \u003ca\n  href=\"https://github.com/jgm/pandoc/releases/latest\"\n    target=\"_blank\"\n  \u003ehttps://github.com/jgm/pandoc/releases/latest\u003c/a\u003e, download the latest tarball (currently pandoc-2.17.0.1-linux-64.tar.gz) and run:\u003c/p\u003e","title":"","type":"tech"},{"content":"Fedora SilverBlue Vimwiki Switch to Firefox? Switch to Android or graphene os\nShow disk usage on a specific directory:\ndu -sch /var/www/nextcloud/data/david/files/Games/battlenet/* Hugo Hugo: Add Table of Contents Anywhere in Markdown File (ruddra.com)\nBTL1 and CCD certifications (security)\nSway\nClear Terminal (picom) https://www.youtube.com/watch?v=t6Klg7CvUxA\nAlacritty (Terminal Emulator) https://www.youtube.com/watch?v=76GbxnD8wnM\nKitty (Terminal Emulator) https://www.youtube.com/watch?v=rCc_JQv6IyU\nMonkeytype (typing speed test) https://monkeytype.com/\nTouch typing https://www.youtube.com/watch?v=U8Qc_dzQTJ4 https://www.youtube.com/watch?v=6E1AiZEtV5c https://www.youtube.com/watch?v=1ArVtCQqQRE https://www.youtube.com/watch?v=JcLlQEkQk0A https://www.keybr.com/\nRICE https://www.youtube.com/watch?v=XK7gal3Wrtk https://github.com/maxhu08/dotfiles\nWhat is Hyperland? https://www.youtube.com/watch?v=cplCuf6H0ZY\nVIm Enhanced https://www.plothost.com/kb/how-to-install-vim-enhanced/\nStarship I use starship for the custom prompt and you could also change the colors using the LS_COLORS environment variable in your bashrc file. To get vibrant colors make sure to set rgb colors in the LS_COLORS variable. Just google it or ask chatgpt to customize it for and pop it into your bashrc file. https://www.youtube.com/watch?v=ZbgulVriTPE https://www.youtube.com/watch?v=G7aWxK4395Y https://www.youtube.com/watch?v=Xyr_EOmEB_g https://www.youtube.com/watch?v=4RuGK3w6Mbs\nNeoVim https://www.youtube.com/watch?v=m8C0Cq9Uv9o\nMigrate to Bitwarden for passwords\nHomebrew\nFZF\nfilesearching Atuin\nShell history database chezmoi\nmanage dotfiles sync to gitrepo Powertop\nMonitor power usage and tune Dust\nDetailed disk space consumption grub-reboot\ntell grub to boot to a specific system if you are dual booting. btop\nMonitor system resources bat\ncat but with synt ax highlighting shows tabs and spaces TLDR\nabridged version of man pages Zellij\ntmux alternative eza\nls but more info Containers\nopenshift\nKATE Stack https://blog.container-solutions.com/introducing-the-kate-stack\nFedora SilverBlue\nVimwiki\nSwitch to Firefox?\nSwitch to Android or graphene os\nHugo Hugo: Add Table of Contents Anywhere in Markdown File (ruddra.com)\nBTL1 and CCD certifications (security)\nHow to change google snippet in hugo\nhttps://gitlab.com/askyourself/dotfiles\nopen source analytics https://plausible.io/ https://analytics.bookstackapp.com/bookstackapp.com [[Add Plausible to Hugo Site]]\nSearx # [[Searx Setup Guide]]\nCreate my own searx search engine\ngo to https://searx.space/\nRSS # Create and RSS feed of my site and subscribe to other feeds.\nFollow these sites:\nhttps://denshi.org/ https://drewdevault.com/ https://opensourcemusings.com/support https://www.mrmoneymustache.com/\nDWM Tile Manager # http://thedarnedestthing.com/vim/wiki%20cheatsheet\nBrowser Sync # https://www.xbrowsersync.org/\nPrivacy tools # https://www.privacytools.io/\n","externalUrl":null,"permalink":"/tech/tools/projects/","section":"Teches","summary":"\u003cp\u003eFedora SilverBlue\nVimwiki\nSwitch to Firefox?\nSwitch to Android or graphene os\u003c/p\u003e\n\u003cp\u003eShow disk usage on a specific directory:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edu -sch /var/www/nextcloud/data/david/files/Games/battlenet/*\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHugo\n\u003ca\n  href=\"https://ruddra.com/hugo-add-toc-anywhere/\"\n    target=\"_blank\"\n  \u003eHugo: Add Table of Contents Anywhere in Markdown File (ruddra.com)\u003c/a\u003e\u003c/p\u003e","title":"","type":"tech"},{"content":"Add the following to the end of /etc/apt/sources.list\n# not for production use deb http://download.proxmox.com/debian/pve bullseye pve-no-subscription You may need to replace this for your current version. See https://pve.proxmox.com/wiki/Package_Repositories\nThis will download \u0026ldquo;unstable\u0026rdquo; updates for proxmox.\nNext, open pve-enterprise.list if you do not have a subscription. This lets you avoid subscription errors.\nvim /etc/apt/sources.list.d/pve-enterprise.list Then add a \u0026ldquo;#\u0026rdquo; to comment out this line: # deb https://enterprise.proxmox.com/debian/pve bullseye pve-enterprise\nThen, update the lists: apt-get update\nNext, Run a distribution upgrade and reboot: apt dist-upgrade reboot ``\nMake the network bridge vlan aware # network \u0026gt; select bridge \u0026gt; edit \u0026gt; check \u0026ldquo;vlan aware\u0026rdquo;\n","externalUrl":null,"permalink":"/tech/tools/proxmox_setup/","section":"Teches","summary":"\u003cp\u003eAdd the following to the end of /etc/apt/sources.list\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e# not for production use\ndeb http://download.proxmox.com/debian/pve bullseye pve-no-subscription\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou may need to replace this for your current version. See \u003ca\n  href=\"https://pve.proxmox.com/wiki/Package_Repositories\"\n    target=\"_blank\"\n  \u003ehttps://pve.proxmox.com/wiki/Package_Repositories\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis will download \u0026ldquo;unstable\u0026rdquo; updates for proxmox.\u003c/p\u003e","title":"","type":"tech"},{"content":" Using Rsync to sync a folder between two hosts # I was using github to sync my notebook with my website. But that introduces some extra steps that rsync can easily eliminate.\nInstall rsync # apt install rsync To upload files from your local computer to your server # rsync -rtvzP /path/to/file root@example.org:/path/on/the/server Turn the above script into an alias # We will be adding an alias so you do not have to type the entire command above every time. Add the following to ~/.bashrc\nalias rsyncp=\u0026#39;rsync -rtvzP ~/Documents/PerfectDarkMode_html/ root@perfectdarkmode.com:/var/www/PerfectDarkMode_html\u0026#39; Update the source for the new alias to take effect:\n$ source ~/.bashrc Now simply type \u0026ldquo;rsyncp\u0026rdquo; to run the command.\nOptions # -r - Recursive -t - tranfer modification times (lets you skip files that have not been modified) -v - visual -z - with file compression -p - pick up where you left off if a file fails during transfer Download files from another host # rsync -rtvzP root@example.org:/path/to/file /path/to/file ","externalUrl":null,"permalink":"/tech/tools/rsync/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003eUsing Rsync to sync a folder between two hosts\n    \u003cdiv id=\"using-rsync-to-sync-a-folder-between-two-hosts\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-rsync-to-sync-a-folder-between-two-hosts\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003eI was using github to sync my notebook with my website. But that introduces some extra steps that rsync can easily eliminate.\u003c/p\u003e","title":"","type":"tech"},{"content":"Using GTKterm app\nFind out what port your serial cable is plugged into:\ndthomas@lgfedora:~$ dmesg | grep tty [ 0.119494] printk: console [tty0] enabled [ 1.591268] serial8250: ttyS0 at I/O 0x3f8 (irq = 4, base_baud = 115200) is a 16550A [ 7.707751] systemd[1]: Created slice system-getty.slice - Slice /system/getty. [ 7.708786] systemd[1]: Reached target getty.target - Login Prompts. [ 424.829380] usb 3-6: FTDI USB Serial Device converter now attached to ttyUSB0 [ 498.255356] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0 [ 499.952411] usb 3-4: FTDI USB Serial Device converter now attached to ttyUSB0 [ 1020.298236] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0 [ 1022.774544] usb 3-6: FTDI USB Serial Device converter now attached to ttyUSB0 [ 1115.380773] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0 [ 1117.430371] usb 3-4: FTDI USB Serial Device converter now attached to ttyUSB0 See last line that says \u0026ldquo;now attached\u0026rdquo;\n","externalUrl":null,"permalink":"/tech/tools/serial-in-on-linux/","section":"Teches","summary":"\u003cp\u003eUsing GTKterm app\u003c/p\u003e\n\u003cp\u003eFind out what port your serial cable is plugged into:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edthomas@lgfedora:~$ dmesg \u003cspan class=\"p\"\u003e|\u003c/span\u003e grep tty\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e    0.119494\u003cspan class=\"o\"\u003e]\u003c/span\u003e printk: console \u003cspan class=\"o\"\u003e[\u003c/span\u003etty0\u003cspan class=\"o\"\u003e]\u003c/span\u003e enabled\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e    1.591268\u003cspan class=\"o\"\u003e]\u003c/span\u003e serial8250: ttyS0 at I/O 0x3f8 \u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eirq\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e 4, \u003cspan class=\"nv\"\u003ebase_baud\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e 115200\u003cspan class=\"o\"\u003e)\u003c/span\u003e is a 16550A\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e    7.707751\u003cspan class=\"o\"\u003e]\u003c/span\u003e systemd\u003cspan class=\"o\"\u003e[\u003c/span\u003e1\u003cspan class=\"o\"\u003e]\u003c/span\u003e: Created slice system-getty.slice - Slice /system/getty.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e    7.708786\u003cspan class=\"o\"\u003e]\u003c/span\u003e systemd\u003cspan class=\"o\"\u003e[\u003c/span\u003e1\u003cspan class=\"o\"\u003e]\u003c/span\u003e: Reached target getty.target - Login Prompts.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e  424.829380\u003cspan class=\"o\"\u003e]\u003c/span\u003e usb 3-6: FTDI USB Serial Device converter now attached to ttyUSB0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e  498.255356\u003cspan class=\"o\"\u003e]\u003c/span\u003e ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e  499.952411\u003cspan class=\"o\"\u003e]\u003c/span\u003e usb 3-4: FTDI USB Serial Device converter now attached to ttyUSB0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e 1020.298236\u003cspan class=\"o\"\u003e]\u003c/span\u003e ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e 1022.774544\u003cspan class=\"o\"\u003e]\u003c/span\u003e usb 3-6: FTDI USB Serial Device converter now attached to ttyUSB0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e 1115.380773\u003cspan class=\"o\"\u003e]\u003c/span\u003e ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e 1117.430371\u003cspan class=\"o\"\u003e]\u003c/span\u003e usb 3-4: FTDI USB Serial Device converter now attached to ttyUSB0\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSee last line that says \u0026ldquo;now attached\u0026rdquo;\u003c/p\u003e","title":"","type":"tech"},{"content":"Automated setup https://universal-blue.org/\nYou get all the benefits of using containers Separates system level packages from applications.\nSystem Level\nDesktop, kernel? Layering Apps at system level because containers aren\u0026rsquo;t as developed yet Locks to the fedora version you are on Layered package examples # - gnome shell extensions - distrobox Uses rpm-ostree? https://coreos.github.io/rpm-ostree/administrator-handbook/\nFlatpacks\nRemove fedora flatpack stuff and use flathub repos instead https://flatpak.org/setup/Fedora\nSystemd unit for automatic flatpack updates\nUpdate every 4 hours to mirror ubuntu\nflatseal adjust permissions of flatpacks\ncheck out apps.gnome.org\nRebase into Universal Blue # Rebase onto the \u0026ldquo;unsigned\u0026rdquo; image then reboot: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/silverblue-main:39 and\nThen the signed image and reboot: rpm-ostree rebase ostree-image-signed:docker://ghcr.io/ublue-os/silverblue-main:39\nThen do we you do after install, open the app store and install stuff via GUI or we\nBazzite Explanation # https://www.youtube.com/watch?v=BBDOhvS_3SM\nShove setup in a containerfile \u0026gt; put podman in a for loop \u0026gt; operating system\n","externalUrl":null,"permalink":"/tech/tools/silverblue-1/","section":"Teches","summary":"\u003cp\u003eAutomated setup \u003ca\n  href=\"https://universal-blue.org/\"\n    target=\"_blank\"\n  \u003ehttps://universal-blue.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYou get all the benefits of using containers\nSeparates system level packages from applications.\u003c/p\u003e\n\u003cp\u003eSystem Level\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDesktop, kernel?\u003c/li\u003e\n\u003cli\u003eLayering\n\u003cul\u003e\n\u003cli\u003eApps at system level because containers aren\u0026rsquo;t as developed yet\u003c/li\u003e\n\u003cli\u003eLocks to the fedora version you are on\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eLayered package examples\n    \u003cdiv id=\"layered-package-examples\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#layered-package-examples\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e- gnome shell extensions\n- distrobox\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUses rpm-ostree? \u003ca\n  href=\"https://coreos.github.io/rpm-ostree/administrator-handbook/\"\n    target=\"_blank\"\n  \u003ehttps://coreos.github.io/rpm-ostree/administrator-handbook/\u003c/a\u003e\u003c/p\u003e","title":"","type":"tech"},{"content":"Skaffold watches for code changes in a repo and builds, tests, and deployes and application in Kubernetes\nWhat questions are they asking? Headline melding. # How can I validate the structure of a container image?\nHow do you run tests on a new version of your application and automate the creation of a new container?\nSet up an automation that watches source code for changes. If there are changes, then you will have a set of step ready to go that deploy to a K8s cluster, VMs, or your chosen Cloud provider.\nBuild \u0026gt; test \u0026gt; deploy\nSkaffold is a CD tool for kubernetes-native applications. Container-structure test will check and verify the structure of and image after it is built.\ncan verify specific files exist Can run commands and verify output. Can verify container metadata set in a dockerfile such and environment variables, ports Skaffold will give you a template to build and test your code in the same way. This helps produce a consistent product and makes automatic deployment easier.\nDuring the testing phase,\nUnit tests Integration Tests Does the application integrate within the rest of you environment? Security Scans Check for known security vulnerabilities in software or imported container images. Artifact is then built and sent to a repo that the CD phase has access to.\nThe CD Takes the artifact and deploys it using one of three strategies:\nCanary Like sending a canary into a cole mine. If a select group of users survive, then the code is pushed out to everyone. Rolling Deploys new code slowly along side of current code until all the new code is fully deployed. blue-green Green (new) code is tested along side of blue (current) code. If the green code is good, then it is deployed. Monitoring is deployed to watch for errors, latency, etc. If a problem is detected then the code can be rolled back.\nskaffold.yaml\nProvides instructions on building, testing, and deploying an application. Located in root of project Keep this file under version control Three main sections: build how to build the image test tests to perform on the image deploy How to deploy to kubernetes kind: Config build: artifacts: - image: project/code local: {} test: - image: project/code custom: - command: \u0026#34;npm --prefix ./src test\u0026#34; structureTests: - ./container-structure-tool/structure-tool.yaml deploy: kubectl: manifests: - kubernetes/* Build section uses the Docker build command to create the container image. The image name is set to project/code with the image tag. This will match the image name in deployment.yaml\nSkaffold uses the current Git commit hash to calculate the image tag. The tag is added to the container image name automatically and set as an environment variable called $IMAGE.\ntest section Run any tests on the container image or application. The example file above runs a container structure test that checks the container for defects then a custom Node.js test to\ndeploy section Uses kubernetes manifest files in /kubernetes directory to release the deployment. A patch is placed on the current deployment and the container tag and image are replaced.\ncontainer-structure-test Run on the container after it is built and after the application has been tested.\nIn this case, the test is located in the application\u0026rsquo;s subdirectory /container-structure-tool/ in a file called structure-tool.yaml\ncommandTests: - name: \u0026#34;telnet-server\u0026#34; command: \u0026#34;./telnet-server\u0026#34; args: [\u0026#34;-i\u0026#34;] expectedOutput: [\u0026#34;telnet port :2323\\nMetrics Port: :9000\u0026#34;] metadataTest: env: - key: TELNET_PORT value: 2323 - key: METRIC_PORT value: 9000 cmd: [\u0026#34;./telnet-server\u0026#34;] workdir: \u0026#34;/app commandTests command that runs the binary and args adds the -i option to output info to STDOUT. The output must match the \u0026ldquo;expectedOutput\u0026rdquo; value in order to pass the test.\nmetadataTest Checks the Docker file to make sure variables, commands, and working directory data matches.\nskaffold run subcommand Build\u0026rsquo;s, tests, and deploys the application. Does not watch for new code changes.\nskaffold dev subcommand Same as run but watches for changes in source code and automatically runs the tests on new changes.\nPods will quit running after you press ^C so add the \u0026ndash;cleanup=false argument if you want it to keep running.\nRan while in the code directory you are running the test on.\n$ skaffold dev --cleanup=false Listing files to watch... - dftd/telnet-server Generating tags... # Sets the container tag number, based on current git commit hash. - dftd/telnet-server -\u0026gt; dftd/telnet-server:4622725 - Checking cache... - dftd/telnet-server: Not found. Building - Found [minikube] context, using local docker daemon. Building [dftd/telnet-server] ... Successfully tagged dftd/telnet-server:4622725 # skaffold starts the test section Starting test... Testing images... ======================================================= ====== Test file: command-and-metadata-test.yaml ====== ======================================================= === RUN: Command Test: telnet-server --- PASS duration: 571.602755ms stdout: telnet port :2323 Metrics Port: :9000 === RUN: Metadata Test --- PASS duration: 0s ======================================================= ======================= RESULTS ======================= ======================================================= Passes: 2 Failures: 0 Duration: 571.602755ms Total tests: 2 PASS Running custom test command: \u0026#34;go test ./... -v\u0026#34; ? telnet-server [no test files] ? telnet-server/metrics [no test files] === RUN TestServerRun Mocked charge notification function TestServerRun: server_test.go:23: PASS: Run() --- PASS: TestServerRun (0.00s) PASS ok telnet-server/telnet (cached) Command finished successfully. # Starts deploy Starting deploy... - deployment.apps/telnet-server created - service/telnet-server created - service/telnet-server-metrics created Waiting for deployments to stabilize... - deployment/telnet-server: waiting for rollout to finish: 0 of 2 updated replicas are available... - pod/telnet-server-6497d64d7f-j8jq5: creating container telnet-server - pod/telnet-server-6497d64d7f-sx5ll: creating container telnet-server - deployment/telnet-server: waiting for rollout to finish: 1 of 2 updated replicas are available... - deployment/telnet-server is ready. Deployments stabilized in 2.140948622s **Press Ctrl+C to exit** Watching for changes... Can use the debug flag if you have any errors for troubleshooting.\nskaffold will skip the build and test phase if there are no changes detected. -dirty is added to the end of the tag if the repo has uncommited changes.\nOnce you make a code change, the process will be re-run automatically if the dev option is used.\nYou can use the minicube tunnel command to jump to the server to test.\nGrab the ip of the server:\n$ minikube kubectl -- get services telnet-server NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE telnet-server LoadBalancer 10.105.161.160 **10.105.161.160** 2323:30488/TCP 6m40s Verify that new pods are running: minikube kubectl\nRolling back\nIf there is no immediate disruption in service then you can deploy a hot fix and run it through the CI/CD pipeline again. If your service is down and you just need to bring it up, then just use Kubernetes to roll back.\nCheck the rollout history: Kubernetes tracks deployments and saves states of previous versions. To check deployment history:\nminikube kubectl -- rollout history deployment telnet-server deployment.apps/telnet-server REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; Revision 2 is the current active.\nTo rollback to revision 1:\nminikube kubectl -- rollout undo deployment telnet-server --to-revision=1 deployment.apps/telnet-server rolled back Or leave off the --to-revision option to just roll back to the previous version.\nVerify:\nminikube kubectl -- get pods NAME READY STATUS RESTARTS AGE telnet-server-7fb57bd65f-qc8rg 1/1 Running 0 28s telnet-server-7fb57bd65f-wv4t9 1/1 Running 0 29s Enter the app to verify the change\nThe new revision is version 3 as listed with the rollout history command.\nIf your company focuses on Mean Time to Recovery (MTTR), this technique helps the service go back up as fast as possible from a customer\u0026rsquo;s point of view.\n","externalUrl":null,"permalink":"/tech/tools/skaffold/","section":"Teches","summary":"\u003cp\u003eSkaffold watches for code changes in a repo and builds, tests, and deployes and application in Kubernetes\u003c/p\u003e\n\n\u003ch3 class=\"relative group\"\u003eWhat questions are they asking? Headline melding.\n    \u003cdiv id=\"what-questions-are-they-asking-headline-melding\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#what-questions-are-they-asking-headline-melding\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eHow can I validate the structure of a container image?\u003c/p\u003e","title":"","type":"tech"},{"content":"Links: https://git.jharmison.com/james/dotfiles/src/branch/master/.tmux.conf https://www.youtube.com/watch?v=nTqu6w2wc68 https://www.youtube.com/watch?v=DzNmUNvnB04 https://www.youtube.com/watch?v=Yl7NFenTgIo https://github.com/alacritty/alacritty https://github.com/rothgar/awesome-tmux https://www.youtube.com/watch?v=GH3kpsbbERo https://git.jharmison.com/james/dotfiles/src/branch/master/.config/alacritty/alacritty.toml#L41\ntmux 2 # Productive Mouse-Free Development # easily manage a text editor, a database console, and a local web server within a single environment.\nsplit tmux windows into sections, so multiple apps can run side by side.\nQuickly move between these windows and panes using only the keyboard.\nkeystrokes you use to manage your environment will become second nature to you, which will greatly increase both your concentration and your productivity.\nconfigure, use, and customize tmux.\nmanage multiple programs simultaneously,\nwrite scripts to create custom environments\nuse tmux to work remotely with others.\ncreate a work environment that keeps almost everything you need at your fingertips.\nterminal multiplexer It\nlets you use a single environment to launch multiple terminals, or windows, each running its own process or program.\nYou can divide your terminal windows into horizontal or vertical panes, and run two or more programs on the same screen side by side.\ndetach from a session,\nleave your environment running in the background. If you\u0026rsquo;ve used uses a client-server model\nlets you control windows and panes from a central location, or even jump between multiple sessions from a single terminal window. lets you create scripts and interact with tmux from other windows or applications. Who Should Read This Book # Whether you\u0026rsquo;re a system administrator or a software developer who spends a good part of your time using the terminal and command-line tools, this book aims to help you work faster.\nuse tmux to build a development environment that can make working with multiple terminal sessions a breeze.\nif you\u0026rsquo;re already comfortable using Vim or Emacs, you\u0026rsquo;ll see how tmux can accelerate your workflow even more.\nIf you spend some time working with remote servers, you\u0026rsquo;ll be interested in how you can leverage tmux to create a persistent dashboard for managing or monitoring servers.\nWhat\u0026rsquo;s in This Book # create sessions, panes, and windows and learn how to perform basic navigation. redefine default keybindings and customize how tmux looks. script your own development environment using the command-line interface, configuration files, and the tmuxinator program. work with text and use the keyboard to move backwards through the buffer, select and copy text, and work with multiple paste buffers. Set up tmux so that you and a coworker can work together on the same codebase from different computers using tmux. Advanced ways to manage windows, panes, and sessions, and how to be even more productive with tmux. Changes in the Second Edition # This new edition has some notable changes from the first edition. tmux 2.1 and 2.2 introduced several backwards-incompatible changes that this edition addresses; this edition also introduces some new options. And tmux is now more popular than it was, so there are more tools and tricks you can use to improve your workflow. Here\u0026rsquo;s what\u0026rsquo;s new:\nConventions # tmux is a tool that\u0026rsquo;s driven by the keyboard. You\u0026rsquo;ll encounter many keyboard shortcuts throughout the book. Since tmux supports both lowercase and uppercase keyboard shortcuts, it may sometimes be unclear which key the book is referencing.\nTo keep it simple, these are the conventions I\u0026rsquo;ve used.\nCtrl-bmeans \u0026ldquo;press the Ctrl``b keys simultaneously.\u0026rdquo;\nCtrl-R means you\u0026rsquo;ll press the Ctrland rkeys simultaneously, but you\u0026rsquo;ll need to use the Shift key to produce the capital \u0026ldquo;R.\u0026rdquo; I won\u0026rsquo;t explicitly show the Shift key in any of these keystrokes.\nCtrl-b d means \u0026ldquo;press the Ctrl and b keys simultaneously, then release, and then press d In Chapter 1, , you\u0026rsquo;ll learn about the [command prefix], which will use this notation, but shortened to Prefix d\nI\u0026rsquo;ll show some terminal commands throughout the book, like\ntmux ​​new-session\nyou can configure tmux with a configuration file called tmux.conf\nconfig/tmux.conf\n# Setting the prefix from C-b to C-a ​set -g prefix C-a To make it easy for you to find the file in the source code download, I\u0026rsquo;ve named the example file [tmux.conf] , without the leading period. The headers above the code listing reference that file.\nChapter 1 Learning the Basics # incredible productivity booster manage applications within sessions, windows, and panes. Installing tmux # You can install tmux in one of two ways: using a package manager for your operating system, or building tmux from source.\nyou install tmux version 2.2 or higher. View tmux version: tmux -V\nInstalling on Linux # sudo dnf install tmux\nStarting tmux # Start tmux: ​tmux​\nfrom a terminal window. You\u0026rsquo;ll see something that looks like the following image appear on your screen.\nThis is a tmux \u0026ldquo;session,\u0026rdquo; and it works just like your normal terminal session.\nYou can issue any terminal command you\u0026rsquo;d like, and everything will work as expected.\nTo close the tmux session: exit​\nYou can instead create \u0026ldquo;named sessions\u0026rdquo; that you can then identify and work with later.\nCreating Named Sessions # You can have multiple sessions on a single computer, and you\u0026rsquo;ll want to be able to keep them organized. For example, you might have\none session for each application you\u0026rsquo;re developing, or a session for work and a session for your cool side project. You can keep these sessions organized by giving each session you create its own unique name. Create a named session called \u0026ldquo;basic\u0026rdquo;: tmux​​ new-session -s ​​basic​\nShortened version: tmux new -s basic​\nNamed sessions come in handy when you want to leave tmux running in the background\nThe Command Prefix # use a command prefix to tell tmux that the command we\u0026rsquo;re typing is for tmux and not for the underlying application. Ctrl-b is the default command prefix. Prefix tmux commands with the command prefix Hit the prefix keys, release, then press the command key. Open the tmux clock: (press enter to dismiss the clock) prefix-t\nDetach from a session: Prefix-d\nyou can remap the prefix to an easier combination Detaching and Attaching Sessions # you can run programs then detach and the programs will run in the background. when you detach from a tmux session, you\u0026rsquo;re not actually closing tmux. You can then \u0026ldquo;attach\u0026rdquo; to the session and pick up where you left off. Within a tmux session, start top: top​\nDetach from the tmux session: prefix-d\nClose your terminal window.\nReattaching to Existing Sessions # Open a terminal window.\nList existing tmux sessions: tmux list-sessions​\nShortened version of above: tmux​​ ​​ls​\nAttach to the session: tmux attach​\nDetach from the session again (prefix-d) and create a new session in the background:\ntmux​​ ​​new ​​-s​​ ​​second_session​​ ​​-d\nList sections again and you will see both: tmux ​​ls\nAttach to a specific section (-t) tmux ​​attach ​​-t ​​second_session\nyou can detach then attach the other session this way. Or switch between sessions using a faster way described later. Killing Sessions # type exit or use the kill-session command tmux ​​kill-session​ ​​-t ​​basic​ ​​kill-session​​ -t​​ ​second_session​`\nWorking with Windows # run multiple, simultaneous commands within a tmux session. keep these organized with windows Can create many windows and they will persist when session is detached Create a new session that has two windows. one with a normal prompt and the other with top.\nCreate a named session called \u0026ldquo;windows,\u0026rdquo; like this: (-n names the first window)\ntmux ​​new ​​-s ​​windows ​​-n ​​shell\nAdd a window to this session.\nCreating and Naming Windows # Create a window in a current session: Prefix c\nCreating a window like this automatically brings the new window into focus. From here, you can start up another application. Let\u0026rsquo;s\nstart top in this new window: top\nsecond window now appears to have the name \u0026ldquo;top.\u0026rdquo; window name defaults to program running if a name wasn\u0026rsquo;t specified. To rename a window, press Prefix ,, type the name and press enter. Rename the window to \u0026ldquo;Processes.\u0026rdquo;\nMoving Between Windows # Prefix n (next window) cycles through the windows you have open.\nPrefix p (previous) go to the previous window.\nWindow numbers start at 0 by default (You can change this)\nPrefix 0 Jump to a specific window. \u0026lsquo;0\u0026rsquo; in this case\nPrefix w display a visual menu of windows to select one.\nPrefix f find a window that contains a string of text.\nclose a window with exit or Prefix \u0026amp; (previous windows becomes active)\nclose all the windows in the session to completely kill the session.\nWorking with Panes # divide a single session into panes. Create a new tmux session called \u0026ldquo;panes\u0026rdquo;: ​​tmux​​ ​​new​​ ​​-s​ ​​panes​\ncan split windows vertically or horizontally.\nPrefix % window will divide down the middle and start up a second terminal session in the new pane.\nthe focus will move to this new pane\nPrefix \u0026quot; split this new pane in half horizontally.\nBy default, new panes split the existing pane in half evenly.\nPrefix o Cycle through panes.\nPrefix Up down left or right Move around panes using arrow keys\nSplit the window in half vertically first, and then horizontally, creating one large pane on the left and two smaller panes on the right: Prefix % Prefix \u0026quot;\nPane Layouts # resize a pane using incremental resizing or templates. Resizing panes incrementally using the default keybindings is quite awkward. Later we\u0026rsquo;ll define some shortcuts to make resizing panes easier. For now, we\u0026rsquo;ll use one of tmux\u0026rsquo;s several default pane layouts:\neven-horizontal\nstacks all panes horizontally, left to right. even-vertical\nstacks all panes vertically, top to bottom. main-horizontal\ncreates one larger pane on the top and smaller panes underneath. main-vertical\ncreates one large pane on the left side of the screen, and stacks the rest of the panes vertically on the right. tiled a - arranges all panes evenly on the screen.\nPrefix Spacebar cycle through the layouts\nClosing Panes # Type exit while in a pane to close it Prefix x Kill a pane and closes the window if there\u0026rsquo;s only one pane in that window. Working with Command Mode # execute tmux commands two ways: from the terminal itself from the command area in the tmux status line.\ntmux\u0026rsquo;s Command mode\nPrefix : enter command mode from within a running tmux session.\nCreate a new window with the name \u0026ldquo;console\u0026rdquo; while in command mode: new-window -n console\nlaunch a new window named \u0026ldquo;processes\u0026rdquo; that starts top while in command mode: new-window -n processes \u0026quot;top\u0026quot;\nif you exit the top by pressing q the tmux window you created will also close.\nYou can use configuration settings to get around this\nif you want the window to persist, create it without specifying an initial command, and then execute your command in the new window.\nYou can use Command mode to create new windows, new panes, or new sessions, or even set other environmental options.\nWhat\u0026rsquo;s Next? # Prefix ? get a list of predefined tmux keybindings and associated commands.\nthink about how you can create different environments for your work. If you\u0026rsquo;re monitoring servers, you could use tmux panes to create a dashboard that shows your various monitoring scripts and log files. For Future Reference # Creating Sessions # Command Description\ntmux new-session Creates a new session without a name. Can be shortened to tmux new or simply tmux tmux new -s development Creates a new session called \u0026ldquo;development.\u0026rdquo; tmux new -s development -n editor\nCreates a session named \u0026ldquo;development\u0026rdquo; and names the first window \u0026ldquo;editor.\u0026rdquo; tmux attach -t development Attaches to a session named \u0026ldquo;development.\u0026rdquo;\nDefault Commands for Sessions, Windows, and Panes # Command Description\nPrefix d Detaches from the session, leaving the session running in the background. Prefix : Enters Command mode. Prefix c Creates a new window from within an existing tmux session. Shortcut for [new-window]. Prefix n Moves to the next window. Prefix p Moves to the previous window. Prefix 0 \u0026hellip;9 Selects windows by number. Prefix w Displays a selectable list of windows in the current session. Prefix f Searches for a window that contains the text you specify. Displays a selectable list of windows containing that text in the current session. Prefix , Displays a prompt to rename a window. Prefix \u0026amp; Closes the current window after prompting for confirmation. Prefix % Divides the current window in half vertically. Prefix \u0026quot; Divides the current window in half horizontally. Prefix o Cycles through open panes. Prefix q Momentarily displays pane numbers in each pane. Prefix x Closes the current pane after prompting for confirmation. Prefix Space Cycles through the various pane layouts.\nChapter 2 Configuring tmux # build a basic configuration file for your environment customizing how you navigate around the screen and how you create and resize panes, and more advanced settings. learn how to make sure your terminal is properly configured so that some of the settings you\u0026rsquo;ll make to tmux\u0026rsquo;s appearance look good on your screen. .tmux.conf # tmux looks for configuration settings in two places. /etc/tmux.conf for a system-wide configuration. It then ~/.tmux.conf (current user\u0026rsquo;s home directory). If these files don\u0026rsquo;t exist, tmux uses its default settings. touch​​ ​​~/.tmux.conf​\ndefine new key shortcuts to setting up a default environment with multiple windows, panes, and running programs. Defining an Easier Prefix # Ctrl -a is an excellent choice for a prefix because it\u0026rsquo;s easier to trigger, especially if you remap your computer\u0026rsquo;s Caps Lock key to Ctrl This keeps your hands on the home row of your keyboard. Remapping the Caps Lock Key\nCan do this in the keyboard section of gnome-tweaks. Or Google it fool!\nset-option Set options in the .tmux.conf file,\nset Same as above but shortened.\nYou define the tmux prefix by adding this to the .tmux.conf file:\n# Setting the prefix from C-b to C-a set -g prefix C-a -g (global) sets the option for all tmux sessions we create.\nunbind-key, or unbind command, to remove a defined keybinding for later use\nFree up ctrl-b by unbinding it in the configuration file:\n​# Free the original Ctrl-b prefix keybinding unbind C-b Must restart tmux for changes to take effect or enter command mode in tmux and run the source-file command: source-file ~/.tmux.conf\nChanging the Default Delay # tmux adds a very small delay when sending commands, and this delay can interfere with other programs such as the Vim text editor. Set this delay so it\u0026rsquo;s much more responsive by adding this line to your configuration file: #setting the delay between prefix and command*​ set -s escape-time 1 Setting the Window and Panes Index # Set window index numbers to start at 1 instead of 0:\n# Set the base index for windows to 1 instead of 0 set -g base-index 1 set-window-option or setw\nconfigure options that affect how you interact with windows pane-base-index Set the starting index number for panes:\n# Set the base index for panes to 1 instead of 0*​ setw -g pane-base-index 1 Customizing Keys, Commands, and User Input # Creating a Shortcut to Reload the Configuration # create a custom keybinding to reload the configuration file.\nbind\nDefines a new keybinding. Specify the key you want to use, followed by the command you want to perform. Set Prefix r so it reloads .tmux.conf (Add too config file):\n# Remap reloading of config file to prefix + r bind r source-file ~/.tmux.conf Reload the config file in command mode with the with Prefix : and type source-file \\~/.tmux.conf\nThen you can use Prefix r to reload the config file.\ndisplay\nput a message in the status line. \\; Combine a series of commands Modify your reload command to display the text \u0026ldquo;Reloaded!\u0026rdquo; when the configuration file loads:\n# Reload the file with Prefix r bind r source-file ~/.tmux.conf \\; display \u0026#34;Reloaded!\u0026#34; Defining Keybindings That Don\u0026rsquo;t Require a Prefix # bind -n\ntells tmux that the keybinding doesn\u0026rsquo;t require pressing the prefix. Use this with care. This will mess up this key combo for any application running within a tmux session. Make ctrl-r reload the tmux session: (Comment out the previous bind for source-file this to work)\n# Make is so ctrl+r reloads tmux instead of having to press prefix first bind-key -n C-r source-file ~/.tmux.conf With this keybinding in place, you can make additional changes to the configuration file and then immediately activate them by pressing Prefix r .\nSending the Prefix to Other Applications # send-prefix\nSend tmux bound prefix keys to another app. Make it so you can use ctrl-a in other apps like vim: (Then you would just press ctrl-a twice)\n# Ensure that we can send Ctrl-A to other apps bind C-a send-prefix Splitting Panes # set the horizontal split to Prefix | and the vertical split to Prefix -:\n# splitting panes with | and - bind | split-window -h bind - split-window -v At first glance, this may look backwards. The [-v] and [-h] .ic} flags on [split-window] stand for \u0026ldquo;vertical\u0026rdquo; and \u0026ldquo;horizontal\u0026rdquo; splits, but to tmux, a vertical split means creating a new pane below the existing pane so the panes are stacked vertically on top of each other. A horizontal split means creating a new pane [next]{.emph} to the existing one so the panes are stacked horizontally across the screen. So, in order to divide the window vertically, we use a \u0026ldquo;horizontal\u0026rdquo; split, and to divide it horizontally, we use a \u0026ldquo;vertical\u0026rdquo; split.\nThese new shortcuts give us a nice visual association. If we want our windows split, we simply press the key that looks like the split we want to create.\nRemapping Movement Keys # Moving from pane to pane with Prefix o is cumbersome, and using the arrow keys means you have to take your fingers off the home row. If you use the Vim text editor, you\u0026rsquo;re probably familiar with its use of h , j , k , and l for movement keys. You can remap the movement keys in tmux to these same keys.\n​[ ] ​*# moving between panes with Prefix h,j,k,l*​ ​[ ] bind h ​select​-pane -L ​[ ] bind j ​select​-pane -D ​[ ] bind k ​select​-pane -U ​[ ] bind l ​select​-pane -R\nIn addition, you can use Prefix Ctrl -h and Prefix .keystroke Ctrl -l to cycle through the windows by binding those keystrokes to the respective commands:\n​[ ] ​*# Quick window selection*​ ​[ ] bind -r C-h ​select​-window -t :- ​[ ] bind -r C-l ​select​-window -t :+\nProvided you\u0026rsquo;ve mapped your Caps Lock key to the Ctrl key, you can now move between panes without moving your hands off the home row.\nResizing Panes # To resize a pane, you can enter Command mode and type [resize-pane -D] to resize a pane downward one row at a time. You can increase the resizing increment by passing a number after the direction, such as [resize-pane -D 5]. The command itself is pretty verbose, but you can make some keybindings to make resizing panes easier.\nLet\u0026rsquo;s use a variation of the Vim movement keys to resize windows. We\u0026rsquo;ll use Prefix H .keystroke , Prefix J , Prefix K , and Prefix L to change the size of the panes. Add these lines to your configuration file:\n​[ ] bind H resize-pane -L 5 ​[ ] bind J resize-pane -D 5 ​[ ] bind K resize-pane -U 5 ​[ ] bind L resize-pane -R 5\nNotice that we\u0026rsquo;re using uppercase letters in the configuration file. tmux allows both lowercase and uppercase letters for keystrokes. You\u0026rsquo;ll need to use the Shift key to trigger the uppercase keystroke.\nUsing these movement keys will help us keep track of which way we want the window size to change. For example, if we have a window divided into two panes stacked vertically, like this\n​[ ] ------------------- ​[ ] | | ​[ ] | Pane 1 | ​[ ] | | ​[ ] ------------------- ​[ ] | | ​[ ] | Pane 2 | ​[ ] | | ​[ ] -------------------\nand we want to increase the size of Pane 1, then we\u0026rsquo;d place our cursor inside Pane 1 and then press Prefix J , which moves the horizontal divider [downward]{.emph}. If we pressed Prefix K , we would move the horizontal divider up.\nWith the configuration we just used, you have to use the Prefix .keystroke each time you want to resize the pane. But if you use the [-r] flag with the [bind] command, you can specify that you want the key to be [repeatable]{.emph}, meaning you can press the prefix key only once and then continuously press the defined key within a given window of time, called the repeat limit.\nRedefine the window resizing commands by adding the [-r] option:\n​[ ] ​*# Pane resizing panes with Prefix H,J,K,L*​ ​[ ] bind -r H resize-pane -L 5 ​[ ] bind -r J resize-pane -D 5 ​[ ] bind -r K resize-pane -U 5 ​[ ] bind -r L resize-pane -R 5\nNow you can resize the panes by pressing Prefix J once, and then press J until the window is the size you want. The default repeat limit is 500 milliseconds, and can be changed by setting the [repeat-time] option to a higher value.\nNow let\u0026rsquo;s turn our attention to how tmux can work with the mouse.\nHandling the Mouse # While tmux is meant to be completely keyboard-driven, there are times when you may find it easier to use the mouse. If your terminal is set up to forward mouse clicks and movement through to programs in the terminal, then you can tell tmux how to handle certain mouse events.\nSometimes it\u0026rsquo;s nice to be able to scroll up through the terminal buffer with the mouse wheel, or to select windows and panes, especially when you\u0026rsquo;re just getting started with tmux. To configure tmux so we can use the mouse, we need to enable mouse mode.\n​[ ] set -g mouse on\nThis setting configures tmux so it will let us use the mouse to select a pane or resize a pane, let us click the window list to select a window, or even let us use the mouse to scroll backwards through the buffer if your terminal supports it.\nThis can be a handy addition to your configuration, but remember that using the mouse with tmux will slow you down. Even though being able to scroll and click might seem like a neat idea, you should learn the keyboard equivalents for switching panes and moving forward and backward through the buffers. So, for our configuration file, we\u0026rsquo;re going to disable the mouse.\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# mouse support - set to on if you want to use the mouse*​ ​[ ] set -g mouse off\nSetting this option prevents us from accidentally doing things when we select the terminal window with our mouse, and it forces us to get more comfortable with the keyboard.\nThe flexible configuration system tmux provides lets you customize the way you interact with the interface, but you can also configure its appearance to make its interface easier to see, and in some cases, more informative.\n[]{#part0026.xhtml}\nVisual Styling # tmux provides quite a few ways to customize your environment\u0026rsquo;s appearance. In this section, we\u0026rsquo;ll walk through configuring some of these options, as we customize the status line and other components. We\u0026rsquo;ll start by configuring the colors for various elements, then we\u0026rsquo;ll turn our bland status line into something that will provide us with some vital information about our environment.\nConfiguring Colors # To get the best visual experience out of tmux, make sure that both your terminal and tmux are configured for 256 colors.\nUsing the [tput] command, you can quickly determine the number of colors supported by your terminal session. Enter the command\n​[ ] ​[$ ]​​tput​​ ​​colors​ ​[ ] 256\ninto your terminal. If you don\u0026rsquo;t see [256] as the result, you\u0026rsquo;ll need to do a little configuration.\nYou may need to configure your terminal to use [xterm]\u0026rsquo;s 256 mode. On the Mac, you can configure this in the Terminal app by editing the profile as shown in the following figure:\n::: ss {#d24e2463 style=\u0026ldquo;width: 90%\u0026rdquo;} :::\nIf you\u0026rsquo;re using iTerm2,^\\[5\\]{#part0026.xhtml#FNPTR-5 .footnote}^ you can find this by editing the default profile and changing the terminal mode to [xterm-256color], as shown in the following figure:\n::: ss {#d24e2474 style=\u0026ldquo;width: 90%\u0026rdquo;} :::\nIf you\u0026rsquo;re using Linux, you might need to add\n​[ ] \\[ -z ​*\\\"*​\\$TMUX​*\\\"*​ \\] \u0026amp;\u0026amp; export TERM=xterm-256color\nto your [.bashrc] file to enable a 256-color terminal. This conditional statement ensures that the [TERM] variable is only set outside of tmux, since tmux sets its own terminal.\nAlso, ensure that your terminal emulator supports displaying UTF-8 characters so that visual elements such as the pane dividers appear as dashed lines.\nTo make tmux display things in 256 colors, add this line to our [.tmux.conf] file:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# Set the default terminal mode to 256color mode​ ​[ ] set -g default-terminal ​\u0026quot;screen-256color\u0026quot;*​\nOnce the right color mode is set, you\u0026rsquo;ll find it much easier to use programs such as Vim, Emacs, and other full-color programs from within tmux, especially when you are using more complex color schemes for syntax highlighting. Just take a look at this figure to see the difference.\n::: ss {#d24e2514} :::\nNow let\u0026rsquo;s configure the appearance of tmux\u0026rsquo;s components, starting with colors.\nChanging Colors # You can change the colors of several parts of the tmux interface, including the status line, window list, message area, and even the pane borders.\ntmux provides variables you can use to specify colors, including [black] .variable}, [red] .variable}, [green] .variable}, [yellow] .variable}, [blue] .variable}, [magenta] .variable}, [cyan] .variable}, or [white] .variable}. You can also use [colour0] .variable} to [colour255] .variable} to reference more specific colors on the 256 color palette.\nTo find the numbers for those colors, you can run this simple shell script to get the color variable you\u0026rsquo;d like to use:^\\[6\\]{#part0026.xhtml#FNPTR-6 .footnote}^\n​[ ] ​for ​i ​in​ {0..255} ; ​do​ ​[ ] ​​printf ​*\u0026quot;​​\\x​​1b[38;5;​​${​i​}​​m*​​${​i​}​​ *\u0026quot;*​ ​[ ] ​done​\nWhen you execute this command, you\u0026rsquo;ll see the following output in your terminal, displaying the colors:\n::: ss {#d24e2605 style=\u0026ldquo;width: 65%\u0026rdquo;} :::\ntmux has specific configuration options to change foreground and background colors for each of its components. Let\u0026rsquo;s start exploring these by customizing the colors of the status line.\nChanging the Status Line Colors # The default status line has black text on a bright green background. It\u0026rsquo;s pretty bland, and depending on your terminal configuration, it can be hard to read. Let\u0026rsquo;s make it have white text on a black background by default, so it looks like this:\n::: ss {#d24e2613} :::\nThe [status-style] option sets the foreground and background colors of the status line, as well as the style. Add the following line to your configuration to set the status line colors:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# set the status line's colors*​ ​[ ] set -g status-style fg=white,bg=black\nYou can set the foreground color and the background color, and you can control the appearance of the text, depending on whether or not your terminal supports it. As you can probably guess, the [fg] option sets the foreground color, and the [bg] option sets the background color.\nThis command supports the options [dim], [bright] (or [bold]), [reverse], and [blink] in addition to colors. For example, to make the status line\u0026rsquo;s text white and bold, you\u0026rsquo;d use the following configuration:\n​[ ] set -g status-style fg=white,bold,bg=black\nYou can also customize the colors of the items within the status line. Let\u0026rsquo;s start by customizing the window list.\nChanging the Window List Colors # tmux displays a list of windows in the status line. Let\u0026rsquo;s make it more apparent which window is active by styling the active window red and the inactive windows cyan. The option [window-status-style] controls how regular windows look, and the [window-status-current-style] option controls how the active window looks. To configure the colors, you use the same syntax you used for the [status-style] option.\nLet\u0026rsquo;s make the names of the windows cyan, like this:\n::: ss {#d24e2674} :::\nAdd this to your configuration file:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# set the color of the window list*​ ​[ ] setw -g window-status-style fg=cyan,bg=black\nYou can use [default] for a value so it inherits from the color of the status line.\nTo style the active window with a red background and bold white text, add this to your configuration:\n​[ ] ​*# set colors for the active window*​ ​[ ] setw -g window-status-current-style fg=white,bold,bg=red\nNow inactive windows are cyan, and the active window is easily identifiable:\n::: ss {#d24e2701} :::\nThis takes care of the window list. Let\u0026rsquo;s look at how we can customize how panes within a window appear.\nChanging the Appearance of Panes # We have a few options to control how panes look. We can control the color of the pane dividers, we can define colors to make the active pane more apparent, and we can even \u0026ldquo;dim out\u0026rdquo; the inactive panes.\nPanes have both foreground and background colors. The foreground color of a pane is the actual dashed line that makes up the border. The background color, by default, is black, but if we color it when the pane is active, we can make the active pane extremely noticeable, as shown in the following figure:\n::: ss {#d24e2713} :::\nAdd this to your configuration file to add this effect to your environment:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# colors for pane borders*​ ​[ ] setw -g pane-border-style fg=green,bg=black ​[ ] setw -g pane-active-border-style fg=white,bg=yellow\nFinally, you may want to be able to more easily determine what the active pane is by changing the color of the foreground or background of the current pane. Or, you might want to fade out panes that are not in use. The [set-window-style] and [set-window-active-style] .ic} options let you control the foreground and background colors, although you have to specify both the foreground and background colors as part of the value you set for the option.\nLet\u0026rsquo;s dim out any pane that\u0026rsquo;s not active. We\u0026rsquo;ll achieve this by actually dimming all of the panes, and then making the active pane look normal. Add these lines to your configuration:\n​[ ] ​*# active pane normal, other shaded out*​ ​[ ] setw -g window-style fg=colour240,bg=colour235 ​[ ] setw -g window-active-style fg=white,bg=black\nTo create the dimming effect, we set the foreground text color to a lighter grey, and then use a darker grey for the background color. Then for the active window, we use black and white.\nWith this change and the active pane borders, it should be pretty clear which pane is active. Now let\u0026rsquo;s touch up the area of tmux where we issue commands.\nCustomizing the tmux Command Line # We can also customize the command line, where we enter tmux commands and see alert messages. The approach is almost identical to the way we styled the status line itself. Let\u0026rsquo;s change the background color to black and the text color to white. We\u0026rsquo;ll use a bright white so the message stands out in more detail. Add this to your configuration:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# Command / message line*​ ​[ ] set -g message-style fg=white,bold,bg=black\nThat was easy. Now let\u0026rsquo;s change the areas of the status line on both sides of the window list.\n[]{#part0027.xhtml}\nCustomizing the Status Line\u0026rsquo;s Content # The tmux status line can display nearly any information we want. We can use some predefined components or create our own by executing shell commands.\nThe status line consists of three components: a left panel, the window list, and a right panel. By default, it looks like this:\n​[ ] \\[development\\] 0:bash* \u0026quot;example.local\u0026quot; 13:37 31-Oct-16\nOn the left side, we have the name of the tmux session followed by the list of windows. The list of windows shows the numerical index of the current window and its name. On the right side, we have the hostname of our server followed by the date and time. Let\u0026rsquo;s customize the content of our status line.\nConfiguring Status Line Items # You can change the content in the left or right panels of the status bar using a combination of text and variables. The following table shows the possible variables we can use in our status line.\n::: figurecaption Table 1. Status Line Variables :::\nVariable Description\n[#H] Hostname of local host [#h] Hostname of local host without the domain name [#F] Current window flag [#I] Current window index [#P] Current pane index [#S] Current session name [#T] Current window title [#W] Current window name [##] A literal # [#(shell-command)] First line of the shell command\u0026rsquo;s output [#\n\\[attributes\\]] Color or attribute change\nFor example, if you wanted to show just the name of the current tmux session on the left, you\u0026rsquo;d use the [set-option -g status-left] option with the [#S] value, like this:\n​[ ] set -g status-left ​*\u0026quot;#S\u0026quot;*​\nBut you can also make it stand out more by using an attribute to set the foreground color, like this:\n​[ ] set -g status-left ​*\u0026quot;#\n\\[fg=green\\]#S\u0026quot;*​\nYou can add as many attributes and items to the status line as you want. To demonstrate, let\u0026rsquo;s alter the left side of the status line so it shows the session name in green, the current window number in yellow, and the current pane in cyan. Add this line to your configuration file:\n​[ ] set -g status-left ​*\u0026quot;#\n\\[fg=green\\]#S #\n\\[fg=yellow\\]#I #\n\\[fg=cyan\\]#P\u0026quot;*​\nYou can add any arbitrary text into the status line, too. Let\u0026rsquo;s add text to make the session, window, and pane more noticeable, like this:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# Status line left side to show Session\u0026#x1fa9f;pane​ ​[ ] set -g status-left-length 40 ​[ ] set -g status-left ​\u0026quot;#\n\\[fg=green\\]Session: #S #\n\\[fg=yellow\\]#I #\n\\[fg=cyan\\]#P\u0026quot;*​\nWe set the [status-left-length] option because the output we\u0026rsquo;ve specified is too long for the default length, so we have to make that region wider.\nYou can configure the right side of the status line too. Add the current date and time, like this:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# Status line right side - 31-Oct 13:37​ ​[ ] set -g status-right ​\u0026quot;#\n\\[fg=cyan\\]%d %b %R\u0026quot;*​\nThis formats the date as \u0026ldquo;31-Oct 13:37,\u0026rdquo; but you can format it however you\u0026rsquo;d like, using the standard [strftime] .methodname} time formatting mechanism used in many programming languages.^\\[7\\]{#part0027.xhtml#FNPTR-7 .footnote}^ Your status line should now look like this:\n::: ss {#d24e2944} :::\nYou can take things a step further by incorporating shell commands into the mix by using the [#(shell-command)] variable to return the result of any external command-line program into the status line. We\u0026rsquo;ll go into this in detail in ​Adding Battery Life to the Status Line​.\nKeeping Status Line Info Up to Date # We\u0026rsquo;ve added the current time and some other dynamic information to our status line, but we need to tell tmux how often to refresh that information periodically. By default, tmux refreshes the status line every 15 seconds. We can specify exactly how quickly tmux refreshes its status line with [set-option] [-g status-interval] followed by the refresh interval in seconds, like this:\n​[ ] ​*# Update the status line every sixty seconds*​ ​[ ] set -g status-interval 60\nThis would refresh the status line every 60 seconds. Keep in mind that if you\u0026rsquo;re firing off shell commands, those will be executed once per interval, so be careful not to load too many resource-intensive scripts.\nCentering the Window List # We can also control the placement of the window list. By default, the window list is left-aligned, but we can center the window list in between the left and right status areas with a single configuration change:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# Center the window list in the status line*​ ​[ ] set -g status-justify centre\nWith this in place, the window list appears centered:\n::: ss {#d24e2987} :::\nAs you create new windows, the window list will shift accordingly, staying in the center of the status line.\nIdentifying Activity in Other Windows # When you\u0026rsquo;re working with more than one window, you\u0026rsquo;ll want to be notified when something happens in one of the other windows in your session so you can react to it. You can do that by adding a visual notification, like this:\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# enable activity alerts*​ ​[ ] setw -g monitor-activity on ​[ ] set -g visual-activity on\nThe [monitor-activity on] command highlights the window name in the status line when there\u0026rsquo;s activity in that window. The [visual-activity on] line tells tmux to show a message in the status line as well.\nNow when one of the other windows has some activity, it\u0026rsquo;ll stand out with a cyan background, like the \u0026ldquo;top\u0026rdquo; window shown here:\n::: ss {#d24e3017} :::\nOnce you switch to that window, the colors will revert back to normal. If you want to configure different colors, you can do so with [setw -g window-status-activity-style] and the colors of your choice.\n[]{#part0028.xhtml}\nWhat\u0026rsquo;s Next? # We\u0026rsquo;ve built up a pretty solid configuration file throughout this chapter. Look at Appendix 1, ​Our Configuration​ to see the whole [.tmux.conf] file.\nYou can define additional options in your [.tmux.conf] file. For example, in Chapter 3, ​Scripting Customized tmux Environments​, you\u0026rsquo;ll set up a custom default work environment using project-specific configuration files.\nIn addition, you can configure a default configuration for your system in [/etc/tmux.conf] . This is great for situations where you\u0026rsquo;ve set up a shared server so members of your team can collaborate, or if you just want to ensure that every user on the system has some sensible defaults.\nNow that you have a configuration defined, let\u0026rsquo;s look at creating your own custom development environments with scripts so you can take advantage of tmux\u0026rsquo;s panes and windows without having to set them up every day.\n[]{#part0029.xhtml}\nFor Future Reference # Keybindings defined in this chapter # Command Description\nCtrl -a The new Prefix . Prefix a Sends Ctrl -a to the program running in a tmux window or pane. Prefix r Reloads the tmux configuration file. Prefix | Splits the window horizontally. Prefix - Splits the window vertically. Prefix h , Prefix j , Prefix k , and Prefix l Moves between panes. Prefix H , Prefix J , Prefix K , and Prefix L Resizes the current pane. Prefix Ctrl -h and Prefix Ctrl -l Moves forward and backward through windows.\nCommands to control tmux\u0026rsquo;s behavior # Command Description\n[set -g prefix C-a] Sets the key combination for the Prefix key. [set -sg escape-time n] Sets the amount of time (in milliseconds) tmux waits for a keystroke after pressing Prefix . [set -g base-index 1] Sets the base index for windows to 1 instead of 0. [setw -g pane-base-index 1] Sets the base index for panes to 1 instead of 0. [source-file \\[file\\]] Loads a configuration file. Use this to reload the existing configuration or bring in additional configuration options later. [bind C-a send-prefix] Configures tmux to send the prefix when pressing the Prefix combination twice consecutively. [bind-key \\[key\\] \\[command\\]] Creates a keybinding that executes the specified command. Can be shortened to [bind]. [bind-key -r \\[key\\] \\[command\\]] Creates a keybinding that is repeatable, meaning you only need to press the Prefix key once, and you can press the assigned key repeatedly afterwards. This is useful for commands where you want to cycle through elements or resize panes. Can be shortened to [bind]. [unbind-key \\[key\\]] Removes a defined keybinding so it can be bound to a different command. Can be shortened to [unbind]. [display-message] or [display] Displays the given text in the status message. [set-option \\[flags\\] \\[option\\] \\[value\\]] Sets options for sessions. Using the [-g] flag sets the option for all sessions. [set-window-option \\[option\\] \\[value\\]] Sets options for windows, such as activity notifications, cursor movement, or other elements related to windows and panes. [set -a] Appends values onto existing options rather than replacing the option\u0026rsquo;s value. [set -g mouse off] Disables mouse support in tmux. Set to [on] if you wish to use the mouse. [set -g default-terminal] [\u0026quot;screen-256color\u0026quot;] Defines the terminal type for windows. Sets the value of [TERM], which other programs will use. [screen-256color] ensures the widest compatibility with programs originally written for the [screen] program.\nCommands to control tmux\u0026rsquo;s appearance # +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | Command | Description | +===================================+===================================+ | [set -g status-style] | Sets the foreground and | | | background color for the status | | | line. Supports the options | | | [dim], [bright] | | | (or [bold]), | | | [reverse], and | | | [blink] in addition to | | | colors. | | | | | | Example: [set -g status-style | | | fg=white,bold,bg=black] | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g window-status-style] | Sets the foreground and | | .ic} | background color of the window | | | list in the status line. Uses the | | | same options as | | | [status-style]. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g | Sets the foreground and | | window-status-current-style] | background color of the active | | .ic} | window in the window list in the | | | status line. Uses the same | | | options as [status-style] | | | .ic}. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g | Sets the foreground and | | window-status-activity-style] | background color of any window | | .ic} | with background activity. Uses | | | the same options as | | | [status-style]. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g pane-border-style] | Sets the foreground and | | .ic} | background color of the pane | | | borders. Uses the same options as | | | [status-style]. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g | Sets the foreground and | | pane-active-border-style] | background color of the active | | .ic} | pane\u0026rsquo;s border. Uses the same | | | options as [status-style] | | | .ic}. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g window-style] | Sets the foreground and | | | background color of the window. | | | Uses the same options as | | | [status-style]. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g window-active-style] | Sets the foreground and | | .ic} | background color of the active | | | window. Uses the same options as | | | [status-style]. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g message-style] | Sets the foreground and | | | background color of the message | | | area and tmux command line. Uses | | | the same options as | | | [status-style]. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [set -g status-length-left] | Controls the number of visible | | .ic} and [set -g | characters in the left and right | | status-length-right] | sides of the status line. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [set -g status-left ] | Configures the items that appear | | and [set -g status-right] | in the left and right sides of | | .ic} | the status line. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [set -g status-interval n] | Defines the refresh interval for | | .ic} | the status line, where [n]{.emph} | | | is the number of seconds between | | | refreshes. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [set -g status-justify centre | Centers the window list in the | | ] | status line. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [setw -g monitor-activity on | Looks for activity in other | | ] | windows and highlights the name | | | of the window with background | | | activity. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | [set -g visual-activity on ] | Displays a message in the message | | .ic} | area when there is activity in | | | another window. | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\n::: footnotes\nFootnotes # \\[4\\]{#part0029.xhtml#FOOTNOTE-4} http://www.emacswiki.org/emacs/MovingTheCtrlKey\n\\[5\\]{#part0029.xhtml#FOOTNOTE-5} http://www.iterm2.com\n\\[6\\]{#part0029.xhtml#FOOTNOTE-6} http://superuser.com/questions/285381/how-does-the-tmux-color-palette-work\n\\[7\\]{#part0029.xhtml#FOOTNOTE-7} See http://www.foragoodstrftime.com/ for a handy tool to help you find the perfect time format. :::\n::: {.copyright style=\u0026quot;;color:black;\u0026rdquo;} Copyright © 2016, The Pragmatic Bookshelf. :::\n[]{#part0030.xhtml}\n[ Chapter 3]{.chapter-number} [Scripting Customized tmux Environments]{.chapter-name} # You probably run a wide collection of tools and programs as you work on your projects. If you\u0026rsquo;re working on a web application, you most likely need to have a command shell, a text editor, a database console, and another window dedicated to running your automated test suite for your application. That\u0026rsquo;s a lot of windows to manage, and a lot of commands to type to get it all fired up.\nImagine being able to come to your workstation, ready to tackle that new feature, and being able to bring every one of those programs up, each in its own pane or window in a single tmux session, using a single command. We can use tmux\u0026rsquo;s client-server model to create custom scripts that build up our development environments, splitting windows and launching programs for us automatically. We\u0026rsquo;ll explore how to do this manually first, and then we\u0026rsquo;ll look at more advanced automatic tools.\n[]{#part0031.xhtml}\nCreating a Custom Setup with tmux Commands # We\u0026rsquo;ve already explored how we use the [tmux] command to create new tmux sessions, but the [tmux] command takes many other options. We can take an existing session and split its windows into panes, change layouts, or even start up applications within the session.\nThe key to this is the [-t] switch, or the \u0026ldquo;target.\u0026rdquo; When you have a named tmux session, you can attach to it like this:\n​[ ] ​[$ ]​​tmux​​ ​​attach​​ ​​-t​​ ​​\n\\[session_name\\]​\nYou can use this target switch to direct a tmux command to the appropriate tmux session. Create a new tmux session called \u0026ldquo;development,\u0026rdquo; like this:\n​[ ] ​[$ ]​​tmux​​ ​​new-session​​ ​​-s​​ ​​development​\nThen detach from the session with Prefix d . Even though you\u0026rsquo;re no longer connected, you can split the window in the tmux session horizontally by issuing this command:\n​[ ] ​[$ ]​​tmux​​ ​​split-window​​ ​​-h​​ ​​-t​​ ​​development​\nWhen you attach to the session again, the window will split into two panes. Attach to your session again to see for yourself.\n​[ ] ​[$ ]​​tmux​​ ​​attach​​ ​​-t​​ ​​development​\nIn fact, you don\u0026rsquo;t even have to detach from a tmux session to send commands. You can open another terminal and split the window again, but this time with a vertical split. Try it out. Open a second terminal window or tab, and enter this command:\n​[ ] ​[$ ]​​tmux​​ ​​split-window​​ ​​-v​​ ​​-t​​ ​​development​\nUsing this approach, you can customize your environment easily. Let\u0026rsquo;s explore this concept by creating our own development environment.\nScripting a Project Configuration # In Chapter 1, ​Learning the Basics​, we discussed tmux commands such as [new-session] and [new-window]. Let\u0026rsquo;s write a simple script using these and similar commands that creates a new tmux session and creates a window with a couple panes and two additional windows with one pane each. To top it off, we\u0026rsquo;ll launch applications in each of the panes.\nLet\u0026rsquo;s start by creating a new script called [development] in our home directory. We\u0026rsquo;ll make this script executable too, so we can run it like any other executable program from our shell. Execute these commands in your terminal:\n​[ ] ​[$ ]​​touch​​ ​​~/development​ ​[ ] ​[$ ]​​chmod​​ ​​+x​​ ​​~/development​\nWhen we start up our session, we want to change to the directory for our project. We\u0026rsquo;ll call that directory [devproject] . And before we can change to that directory, we\u0026rsquo;d better create it first.\n​[ ] ​[$ ]​​mkdir​​ ​​~/devproject​\nNow, open the [~/development] script in your text editor and add this line to create a new tmux session called \u0026ldquo;development\u0026rdquo;:\n::: livecodelozenge scripting/development :::\n​[ ] tmux new-session -s development -n editor -d\nWe\u0026rsquo;re passing a couple additional parameters when we create this new session. First, we\u0026rsquo;re creating this session and naming it with the [-s] flag like we\u0026rsquo;ve done before. Then we give the initial window a name of \u0026ldquo;editor,\u0026rdquo; and then immediately detach from this new session with the [-d] flag.\nNext, add a line to our configuration that uses tmux\u0026rsquo;s [send-keys] .ic} command to change the current directory to the one we\u0026rsquo;re using for our project:\n​[ ] tmux send-keys -t development ​*'cd ~/devproject'*​ C-m\nWe place [C-m] at the end of the line to send the Carriage Return sequence, represented by Ctrl-M.^\\[8\\]{#part0031.xhtml#FNPTR-8 .footnote}^ This is how we tell tmux to press the Enter key.\nWe\u0026rsquo;ll use the same approach to open the Vim text editor in that window. Add this line to your script:\n​[ ] tmux send-keys -t development ​*'vim'*​ C-m\nWith these three commands, we\u0026rsquo;ve created a new session, changed to a directory, and opened a text editor, but our environment isn\u0026rsquo;t yet complete. Let\u0026rsquo;s split the main editor window so we have a small terminal window on the bottom. We do this with the [split-window] command. Add this line to your script:\n​[ ] tmux split-window -v -t development\nThis splits the main window in half horizontally. You could have specified a percentage using something like\n​[ ] tmux split-window -v -p 10 -t development\nbut for now, just leave the [split-window] command as is and then select one of the default tmux layouts\u0026mdash;the [main-horizontal] .ic} one\u0026mdash;by adding this to your script:\n​[ ] tmux ​select​-layout -t development main-horizontal\nWe\u0026rsquo;ve created our first window and split it into two panes, but the bottom pane needs to open in the project folder. We already know how we send commands to tmux instances, but now we have to target those commands at specific panes and windows.\nTargeting Specific Panes and Windows # With commands such as [send-keys], you can specify not only the target session, but also the target window and pane. In the configuration file you created back in Chapter 2, ​Configuring tmux​, you specified a [base-index] of [1], meaning that your window numbering starts at 1. This base index doesn\u0026rsquo;t affect the panes, though, which is why you also set the [pane-base-index] to [1]. In our case, we have two panes in our current setup, like the following example:\n​[ ] -------------------------- ​[ ] | | ​[ ] | Pane 1 | ​[ ] | | ​[ ] -------------------------- ​[ ] | Pane 2 | ​[ ] --------------------------\nWe have the Vim text editor open in Pane 1, and we want to send a command to Pane 2 that changes to our project directory. We target a pane using the format [\n\\[session\\]:\n\\[window\\].\n\\[pane\\]], so to target Pane 2, we\u0026rsquo;d use [development:1.2]. So, add this line to your script, and you\u0026rsquo;ll get exactly what you want:\n​[ ] tmux send-keys -t development:1.2 ​*'cd ~/devproject'*​ C-m\nWe\u0026rsquo;re almost there. Let\u0026rsquo;s finish up this configuration by adding a couple more windows to the session.\nCreating and Selecting Windows # We want a second window in our session that will be a full-screen console. We can create that new window using the [new-window] command. Add these lines to your script:\n​[ ] tmux new-window -n console -t development ​[ ] tmux send-keys -t development:2 ​*'cd ~/devproject'*​ C-m\nAfter we create the window, we use [send-keys] to once again change into our project directory. We only have one pane in our new window, so we only have to specify the window number in the target.\nWhen we start up our session, we want our first window to be displayed, and we do that with the [select-window] command:\n​[ ] tmux ​select​-window -t development:1 ​[ ] tmux attach -t development\nWe could continue to add to this script, creating additional windows and panes, starting up remote connections to our servers, tailing log files, connecting to database consoles, or even running commands that pull down the latest version of our code when we start working. But we\u0026rsquo;ll stop here, and simply end our script by finally attaching to the session so it shows up on the screen, ready for us to begin working. Our entire script looks like this:\n​[ ] tmux new-session -s development -n editor -d ​[ ] tmux send-keys -t development ​*'cd ~/devproject'​ C-m ​[ ] tmux send-keys -t development ​'vim'​ C-m ​[ ] tmux split-window -v -t development ​[ ] tmux ​select​-layout -t development main-horizontal ​[ ] tmux send-keys -t development:1.2 ​'cd ~/devproject'​ C-m ​[ ] tmux new-window -n console -t development ​[ ] tmux send-keys -t development:2 ​'cd ~/devproject'*​ C-m ​[ ] tmux ​select​-window -t development:1 ​[ ] tmux attach -t development\nWhen you run it with\n​[ ] ​[$ ]​​~/development​\nyour environment will look like this:\n::: ss {#d24e4058} :::\nOne drawback to this approach is that this script creates a brand-new session. It won\u0026rsquo;t work properly if you run it a second time while the [development] session is currently running. You could modify the script to check if a session with that name already exists by using the [tmux has-session] command and only create the session if it\u0026rsquo;s not there, like this:\n::: livecodelozenge scripting/reattach/development :::\n​[ ] tmux has-session -t development ​[ ] ​if​ \\[ \\$? != 0 \\] ​[ ] ​then​ ​[ ] ​​tmux new-session -s development -n editor -d ​[ ] tmux send-keys -t development ​*'cd ~/devproject'​ C-m ​[ ] tmux send-keys -t development ​'vim'​ C-m ​[ ] tmux split-window -v -t development ​[ ] tmux ​select​-layout -t development main-horizontal ​[ ] tmux send-keys -t development:1.2 ​'cd ~/devproject'​ C-m ​[ ] tmux new-window -n console -t development ​[ ] tmux send-keys -t development:2 ​'cd ~/devproject'*​ C-m ​[ ] tmux ​select​-window -t development:1 ​[ ] ​fi​ ​[ ] tmux attach -t development\nThis approach works well for a single project setup. You could modify this further by using a variable for the project name to make the script more generic, but let\u0026rsquo;s look at a couple other ways we can configure things to manage multiple projects.\n[]{#part0032.xhtml}\nUsing tmux Configuration for Setup # The [.tmux.conf] file itself can include commands that set up a default environment. If you wanted every tmux session to start in the same default folder, or automatically open a split window, you could bake that right in to your default configuration, simply by using the appropriate commands.\nBut you can also specify a configuration file when you start up an instance of tmux, by using the [-f] flag. This way you don\u0026rsquo;t have to change your original default configuration file, and you can check your configuration file in with your project\u0026rsquo;s source code. You can also set up your own per-project configuration options, such as new keyboard shortcuts to run commands or start your test suite.\nLet\u0026rsquo;s try this out. Create a new file called [app.conf] .\n​[ ] ​[$ ]​​touch​​ ​​app.conf​\nInside this file, you can use the same commands you just learned about in the previous section, but since you\u0026rsquo;re inside the configuration file rather than a shell script, you don\u0026rsquo;t have to explicitly prefix each command with [tmux]. Add this code to your [app.conf] .filename} file:\n::: livecodelozenge scripting/app.conf :::\n​[ ] source-file ~/.tmux.conf ​[ ] new-session -s development -n editor -d ​[ ] send-keys -t development ​*'cd ~/devproject'​ C-m ​[ ] send-keys -t development ​'vim'​ C-m ​[ ] split-window -v -t development ​[ ] ​select​-layout -t development main-horizontal ​[ ] send-keys -t development:1.2 ​'cd ~/devproject'​ C-m ​[ ] new-window -n console -t development ​[ ] send-keys -t development:2 ​'cd ~/devproject'*​ C-m ​[ ] ​select​-window -t development:1\nThis code first loads your existing [.tmux.conf] file. This way you\u0026rsquo;ll have all your environment settings you previously defined, including your keybindings and status bar settings. This isn\u0026rsquo;t mandatory, but if you left this off, you\u0026rsquo;d have to use all the default keybindings and options, or you\u0026rsquo;d have to define your own options in this file.\nTo use this configuration file, pass the [-f] flag followed by the path to the config file. You also have to start tmux with the [attach] command, like this:\n​[ ] ​[$ ]​​tmux​​ ​​-f​​ ​​app.conf​​ ​​attach​\nThis is because, by default, tmux always calls the [new-session] .ic} command when it starts. This file creates a new session already, so you\u0026rsquo;d have [two]{.emph} tmux sessions running if you left off [attach].\nThis approach gives you a lot of flexibility, but you can gain even more by using a command-line tool called tmuxinator.\n[]{#part0033.xhtml}\nManaging Configuration with tmuxinator # tmuxinator is a simple tool you can use to define and manage different tmux configurations. You define window layouts and commands in a simple YAML format, and then launch them with the [tmuxinator] command. Unlike the other approaches, tmuxinator offers a central location for your configurations and a much easier dialect for creating complex layouts. It also lets you specify commands that should always run before each window gets created.\ntmuxinator requires the Ruby interpreter, so you\u0026rsquo;ll need to have that on your system. Mac OS X users already have Ruby installed, and Linux users can usually install Ruby through a package manager. However, if you plan to use Ruby for anything beyond tmuxinator, I strongly encourage you to install Ruby through RVM by following along with the instructions on the RVM website.^\\[9\\]{#part0033.xhtml#FNPTR-9 .footnote}^\nInstall tmuxinator by using Rubygems, which is the package management system for Ruby.\n​[ ] ​[$ ]​​gem​​ ​​install​​ ​​tmuxinator​\nIf you are not using RVM, you will need to run this as root or with the [sudo] command.\ntmuxinator needs the [$EDITOR] shell environment to be defined, so if you haven\u0026rsquo;t set yours yet, you\u0026rsquo;ll want to do that in your [.bashrc] file on Linux, or [.bash_profile] .filename} on OS X. For example, to define Vim as the default editor, you\u0026rsquo;d add this line to your Bash configuration:\n​[ ] export EDITOR=vim\nNow we can create a new tmuxinator project. Let\u0026rsquo;s call it \u0026ldquo;development.\u0026rdquo; Execute this command:\n​[ ] ​[$ ]​​tmuxinator​​ ​​open​​ ​​development​\nThis pops open the editor you assigned to the [$EDITOR] environment variable and displays the default project configuration, which looks like this:\n::: livecodelozenge scripting/default.yaml :::\n​[ ] ​*# ~/.tmuxinator/development.yml​ ​[ ]\n​[ ] ​name​: ​development​ ​[ ] ​root​: ​~/​ ​[ ]\n​[ ] ​# a bunch of comments.\u0026hellip;​ ​[ ]\n​[ ] ​windows​: ​[ ] - ​editor​: ​[ ] ​layout​: ​main-vertical​ ​[ ] ​panes​: ​[ ] - ​vim​ ​[ ] - ​guard​ ​[ ] - ​server​: ​bundle exec rails s​ ​[ ] - ​logs​: ​tail -f log/development.log*​\nThis is an environment that a Ruby on Rails developer who works with Git might really appreciate. This creates a tmux session with three windows. The first window is divided into two panes, using the [main-vertical] layout scheme. The left pane opens Vim, and the right pane opens Guard, a Ruby program that watches files for changes and executes tasks, like test runners. The second window launches Rails\u0026rsquo; built-in web server, and the third window uses the [tail] command to follow the application\u0026rsquo;s development log file.\nAs you can see, tmuxinator makes it trivial to define not only the windows and panes, but also what commands we want to execute in each one. Let\u0026rsquo;s use Tmuxinator to construct our development environment, with Vim in the top pane and a terminal on the bottom, starting in the [~/devproject] folder. Remove the contents of this file and replace it with the following code:\n::: livecodelozenge scripting/development.yaml :::\n​[ ] ​name​: ​development​ ​[ ] ​root​: ​*~/devproject​ ​[ ] ​windows​: ​[ ] - ​editor​: ​[ ] ​layout​: ​main-horizontal​ ​[ ] ​panes​: ​[ ] - ​vim​ ​[ ] - ​#empty, will just run plain bash​ ​[ ] - ​console​: ​# empty*​\nThe [yml] file format uses two spaces for indenting, so it\u0026rsquo;s really important to ensure you format the file correctly and that you don\u0026rsquo;t accidentally use tabs when you write the file.\nTo fire up the new environment, save the config file and then execute the following command:\n​[ ] ​[$ ]​​tmuxinator​​ ​​development​\ntmuxinator automatically loads up your original [.tmux.conf] .filename} file, applies the settings, and then arranges the windows and panes for you, just like you specified. If you want to make more changes to your environment, just use\n​[ ] ​[$ ]​​tmuxinator​​ ​​open​​ ​​development​\nagain and edit the configuration.\nBy default, the configuration files for tmuxinator are located in [~/.tmuxinator/] , so you can find those and back them up, or share them with others.\nUnder the hood, tmuxinator is just constructing a script that executes the individual tmux commands just like we did when we wrote our own script. However, it\u0026rsquo;s a nicer syntax that\u0026rsquo;s pretty easy to follow. It does require a Ruby interpreter on your machine, though, so it may not be something you\u0026rsquo;ll set up on every environment where you\u0026rsquo;d like to use tmux. However, you can use Tmuxinator to generate a configuration you can use anywhere. The [tmuxinator debug] command can display the script that Tmuxinator will use:\n​[ ] ​[$ ]​​tmuxinator​​ ​​debug​​ ​​development​\nHere\u0026rsquo;s what the output looks like:\n​[ ] ​*#!/bin/bash​ ​[ ]\n​[ ] ​# Clear rbenv variables before starting tmux​ ​[ ] unset RBENV_VERSION ​[ ] unset RBENV_DIR ​[ ]\n​[ ] tmux start-server; ​[ ]\n​[ ] cd /home/brianhogan/devproject ​[ ]\n​[ ] ​# Run pre command.​ ​[ ]\n​[ ] ​# Create the session and the first window. Manually switch to root​ ​[ ] ​# directory if required to support tmux \u0026lt; 1.9​ ​[ ] TMUX= tmux new-session -d -s development -n editor ​[ ] tmux send-keys -t development:1 cd​\\* ​/home/brianhogan/devproject C-m ​[ ]\n​[ ] ​*# Create other windows.​ ​[ ] tmux new-window -t development:2 -n console ​[ ]\n​[ ] ​# Window \u0026quot;editor\u0026quot;​ ​[ ] tmux send-keys -t development:1.1 vim C-m ​[ ]\n​[ ] tmux splitw -c /home/brianhogan/devproject -t development:1 ​[ ] tmux ​select​-layout -t development:1 tiled ​[ ]\n​[ ] tmux ​select​-layout -t development:1 tiled ​[ ]\n​[ ] tmux ​select​-layout -t development:1 main-horizontal ​[ ] tmux ​select​-pane -t development:1.1 ​[ ]\n​[ ] ​# Window \u0026quot;console\u0026quot;*​ ​[ ]\n​[ ] tmux ​select​-window -t 1 ​[ ]\n​[ ] ​if​ \\[ -z ​*\\\"*​\\$TMUX​*\\\"*​ \\]; ​then​ ​[ ] ​​tmux -u attach-session -t development ​[ ] ​else​ ​[ ] ​​tmux -u switch-client -t development ​[ ] ​fi​\nYou could save the output of [tmuxinator debug] to a script you can run on any machine. You can also use this option to troubleshoot any issues you might be having as you develop your configuration file.\n[]{#part0034.xhtml}\nWhat\u0026rsquo;s Next? # You can use every tmux command through the shell, which means you can write scripts to automate nearly every aspect of tmux, including running sessions. For example, you could create a keyboard binding that sources a shell script that divides the current window into two panes and logs you into your production web and database servers.\nWe\u0026rsquo;ve covered a lot so far. You know how to set up projects, move around panes and windows, and launch your consoles. You\u0026rsquo;ve tinkered around with your configuration enough to understand how to customize things to your liking. And you\u0026rsquo;ve experimented with three separate ways to script out your tmux environment. But as you start to integrate tmux into your workflow, you\u0026rsquo;ll start to notice some new issues crop up. For example, the results of tests or application logs start to scroll off the screen, and you\u0026rsquo;ll want to be able to scroll up to read things. And you\u0026rsquo;ll probably want to copy and paste text between panes, windows, or other applications. So let\u0026rsquo;s learn how to work with tmux\u0026rsquo;s output buffers next.\n[]{#part0035.xhtml}\nFor Future Reference # Scriptable tmux commands # Command Description\n[tmux new-session -s ][development][ -n editor] Creates a session named \u0026ldquo;development\u0026rdquo; and names the first window \u0026ldquo;editor.\u0026rdquo; [tmux attach -t development] Attaches to a session named \u0026ldquo;development.\u0026rdquo; [tmux send-keys -t development \u0026lsquo;\n\\[keys\\]\u0026rsquo; C-m] Sends the keystrokes to the \u0026ldquo;development\u0026rdquo; session\u0026rsquo;s active window or pane. [C-m] is equivalent to pressing the Enter key. [tmux send-keys -t ][development:1.1][ \u0026lsquo;\n\\[keys\\]\u0026rsquo; C-m] Sends the keystrokes to the \u0026ldquo;development\u0026rdquo; session\u0026rsquo;s first window and first pane, provided the window and pane indexes are set to 1. [C-m] is equivalent to pressing the Enter key. [tmux select-window -t ][development:1] Selects the first window of \u0026ldquo;development,\u0026rdquo; making it the active window. [tmux split-window -v -p 10 -t development] Splits the current window in the \u0026ldquo;development\u0026rdquo; session vertically, dividing it in half [horizontally]{.emph}, and sets its height to 10% of the total window size. [tmux select-layout -t ][development][ main-horizontal] Sets the layout for the \u0026ldquo;development\u0026rdquo; session to [main-horizontal]. [tmux source-file \\[file\\]] Loads the specified tmux configuration file. [tmux -f app.conf attach] Loads the [app.conf] configuration file and attaches to a session created within the [app.conf] file.\ntmuxinator commands # Command Description\n[tmuxinator open \\[name\\]] Opens the configuration file for the project [name] in the default text editor. Creates the configuration if it doesn\u0026rsquo;t exist. [tmuxinator \\[name\\]] Loads the tmux session for the given project. Creates the session from the contents of the project\u0026rsquo;s configuration file if no session currently exists, or attaches to the session. [tmuxinator list] Lists all current projects. [tmuxinator copy \\[source\\] \\[destination\\]] Copies a project configuration. [tmuxinator delete \\[name\\]] Deletes the specified project. [tmuxinator implode] Deletes all current projects. [tmuxinator doctor] Looks for problems with the tmuxinator and system configuration. [tmuxinator debug] Shows the script that tmuxinator will run, helping you figure out what\u0026rsquo;s going wrong.\n::: footnotes\nFootnotes # \\[8\\]{#part0035.xhtml#FOOTNOTE-8} http://en.wikipedia.org/wiki/Carriage_return\n\\[9\\]{#part0035.xhtml#FOOTNOTE-9} https://rvm.io/ :::\n::: {.copyright style=\u0026quot;;color:black;\u0026quot;} Copyright © 2016, The Pragmatic Bookshelf. :::\n[]{#part0036.xhtml}\n[ Chapter 4]{.chapter-number} [Working With Text and Buffers]{.chapter-name} # Throughout the course of your average day, you\u0026rsquo;ll copy and paste text more times than you can keep track of. When you\u0026rsquo;re working with tmux, you will eventually come to the point where you need to scroll backwards through the terminal\u0026rsquo;s output buffer to see something that scrolled off the screen. You might also need to copy some text and paste it into a file or into another program. This chapter is all about how to manage the text inside your sessions. You\u0026rsquo;ll see how to use the keyboard to scroll through tmux\u0026rsquo;s output buffer, how to work with multiple paste buffers, and how to work with the system clipboard.\n[]{#part0037.xhtml}\nScrolling Through Output with Copy Mode # When you work with programs in the terminal, it\u0026rsquo;s common that the output from these programs scrolls off the screen. But when you use tmux, you can use the keyboard to move backwards through the output buffer so you can see what you missed. This is especially useful for those times when you\u0026rsquo;re running tests or watching log files and you can\u0026rsquo;t just rely on the [less] command or your text editor.\nPressing Prefix [ places you in Copy mode. You can then use your movement keys to move the cursor around the screen. By default, the arrow keys work. But in Chapter 2, ​Configuring tmux​, you configured tmux to use Vim keys for moving between windows and resizing panes so you wouldn\u0026rsquo;t have to take your hands off the home row. tmux has a [vi] mode for working with the buffer as well. To enable it, add this line to [.tmux.conf] :\n::: livecodelozenge config/tmux.conf :::\n​[ ] ​*# enable vi keys.*​ ​[ ] setw -g mode-keys vi\nWith this option set, you can use h , j , k , and l to move around your buffer.\nTo get out of Copy mode, press the Enter key. Moving around one character at a time isn\u0026rsquo;t very efficient. Since you enabled vi mode, you can also use some other visible shortcuts to move around the buffer.\nFor example, you can use w to jump to the next word and b to jump back one word. And you can use f , followed by any character, to jump to that character on the same line, and F to jump backwards on the line.\nMoving Quickly Through the Buffer # When you have several pages of buffered output, moving the cursor around to scroll isn\u0026rsquo;t going to be that useful. Instead of moving word by word or character by character, you can scroll through the buffer page by page, or jump to the beginning or end of the buffer.\nYou can move up one page with Ctrl -b and down one page with Ctrl -f . You can jump all the way to the top of the buffer\u0026rsquo;s history with g , and then jump all the way to the bottom with G .\nSearching Through the Buffer # You don\u0026rsquo;t have to browse through the hundreds of lines of content page by page if you know what you\u0026rsquo;re looking for. By pressing ? .keystroke in Copy mode, you can search upwards for phrases or keywords. Simply press ? , type in the search phrase, and press Enter to jump to the first occurrence of the phrase. Then press n to jump to the next occurrence, or N to move to the previous.\nTo search downward, press / instead of ? . Pressing n then jumps to the next occurrence, and N jumps to the previous occurrence.\nLearning to move around the buffer this way will dramatically speed you up. It\u0026rsquo;s faster to type the word you want to move to instead of using the arrows to move around, especially if you\u0026rsquo;re looking through the output of log files.\nNow let\u0026rsquo;s explore how to copy text from one pane and paste it to another. This is Copy mode, after all.\n[]{#part0038.xhtml}\nCopying and Pasting Text # Moving around and looking for things in the output buffer is usually only half the equation. We often need to copy some text so we can do something useful with it. tmux\u0026rsquo;s Copy mode gives us the opportunity to select and copy text to a paste buffer so we can dump that text elsewhere.\nTo copy text, enter Copy mode and move the cursor to where you want to start selecting text. Then press Space and move the cursor to the end of the text. When you press Enter , the selected text gets copied into a paste buffer.\nTo paste the contents you just captured, press Prefix ] .\nLet\u0026rsquo;s look at a few ways to copy and paste text from our main output buffer.\nCapturing a Pane # tmux has a handy shortcut that copies the entire visible contents of a pane to a paste buffer. Enter tmux\u0026rsquo;s Command mode with Prefix .keystroke : and type\n[capture-pane]\nThe contents of the pane will be in a paste buffer. You can then paste that content into another pane or window by pressing Prefix .keystroke ] .\nShowing and Saving the Buffer # You can display the contents of your paste buffer by using the [show-buffer] command in Command mode, or from a terminal session with\n​[ ] ​[$ ]​​tmux​​ ​​show-buffer​\nHowever, by using the [save-buffer] command, you can save the buffer to a file, which can often be a real time saver. In fact, you can capture the contents of the current pane to a text file.\nIn Command mode, execute the command [capture-pane; save-buffer buffer.txt]. You could easily map that command to a keystroke if you wanted.\nUsing Multiple Paste Buffers # tmux maintains a stack of paste buffers, which means you can copy text without replacing the buffer\u0026rsquo;s existing content. This is much more flexible than the traditional clipboard offered by the operating system.\nEvery time you copy some new text, tmux creates a new paste buffer, putting the new buffer at the top of the stack. To demonstrate, fire up a new tmux session and load up a text editor such as Vim or Nano. In the editor, type the following sentences, one per line:\n​[ ] First sentence is first. ​[ ] Next sentence is next. ​[ ] Last sentence is last.\nNow copy some text to the paste buffer using tmux. Enter Copy mode with Prefix [ .keystroke . Move to the start of the first sentence, press Space to start selecting text, move to the end of the first sentence, and press Enter to copy the selection. Repeat this with the second and third sentences.\nEach time you copied text, tmux created a new buffer. You can see these buffers with the [list-buffers] command.\n​[ ] 0: 22 bytes: \u0026quot;Last sentence is last.\u0026quot; ​[ ] 1: 22 bytes: \u0026quot;Next sentence is next.\u0026quot; ​[ ] 2: 24 bytes: \u0026quot;First sentence is first.\u0026quot;\nPressing Prefix ] always pastes buffer 0, but you can issue the command [choose-buffer] to select a buffer and paste the contents into the focused pane.\nSplit the current window in half and launch Nano in the second pane, then enter Command mode and type this:\n[choose-buffer]\nYou\u0026rsquo;ll be presented with a list that looks like this:\n::: ss {#d24e5219} :::\nYou can select any entry in the list, press Enter , and the text will be inserted into the selected pane.\nThis is an excellent way to manage multiple bits of text, especially in text-based environments where you don\u0026rsquo;t have access to an OS-level clipboard.\nThese buffers are shared across [all]{.emph} running tmux sessions, too, so you can take content from one session and paste it into another.\nRemapping Copy and Paste Keys # If you use Vim and you\u0026rsquo;d like to make the copy and paste command keys a little more familiar, you can remap the keys in your configuration. For example, you can use Prefix Escape to enter Copy mode, then use v to start Visual mode to select your text, use y to \u0026ldquo;yank\u0026rdquo; text into the buffer, and use p to paste the text:\n​[ ] bind Escape copy-mode ​[ ] bind -t vi-copy ​*'v'​ begin-selection ​[ ] bind -t vi-copy ​'y'*​ copy-selection ​[ ] unbind p ​[ ] bind p paste-buffer\nThis can be a real productivity boost if you happen to do a lot of copying and pasting between windows and panes and are already comfortable with the keys that Vim uses.\n[]{#part0039.xhtml}\nWorking with the Clipboard on Linux # Using the [xclip] utility,^\\[10\\]{#part0039.xhtml#FNPTR-10 .footnote}^ you can integrate your buffers with the Linux system clipboard so you can more easily copy and paste between programs.\nFirst, you have to install [xclip]. On Ubuntu, use this command:\n​[ ] ​[$ ]​​sudo​​ ​​apt-get​​ ​​install​​ ​​xclip​\nThen we use tmux\u0026rsquo;s [save-buffer] and [set-buffer] commands with [xclip].\nTo copy the current buffer to the system clipboard, we add this command to our [.tmux.conf] file:\n​[ ] bind C-c run ​*\u0026quot;tmux save-buffer - | xclip -i -sel clipboard\u0026quot;*​\nThis configures Prefix Ctrl -c so it pipes the current buffer to [xclip].\nSo, you enter Copy mode, select your text, press y , and then press Prefix Ctrl -c to get your text on the clipboard. You can speed up the process by binding the y key to send the output to [xclip] directly:\n​[ ] bind -t vi-copy y copy-pipe ​*\u0026quot;xclip -sel clip -i\u0026quot;*​\nNow text you select and copy in Copy mode will be on your system clipboard.\nTo paste text from the system clipboard into a tmux session, add this line to your configuration:\n​[ ] bind C-v run ​*\u0026quot;tmux set-buffer* ​​*\\\u0026quot;​​$(​xclip -o -sel clipboard​)​​\\\u0026quot;​​; tmux paste-buffer\u0026quot;*​\nThis configures tmux to pull the content from [xclip] into a new tmux buffer and then pastes it into the selected tmux window or pane when you press Prefix Ctrl -v .\n[]{#part0040.xhtml}\nUsing OS X Clipboard Commands # If you\u0026rsquo;re a Mac user, you may be familiar with OS X\u0026rsquo;s command-line clipboard utilities [pbcopy] and [pbpaste]. These simple utilities make it a snap to work with the clipboard. The [pbcopy] command captures text to the system clipboard, and the [pbpaste] command pastes content out. For example, you can use [pbcopy] and [cat] together to easily put the contents of your [.tmux.conf] file into the clipboard so you can paste it in an email or on the web, like this:\n​[ ] ​[$ ]​​cat​​ ​​~/.tmux.conf​​ ​​|​​ ​​pbcopy​\nThis is a pretty handy way to work with text, but tmux doesn\u0026rsquo;t have access to these utilities, so we can\u0026rsquo;t use them while running inside a tmux session. We can use a wrapper program written by Chris Johnsen to get around this limitation.^\\[11\\]{#part0040.xhtml#FNPTR-11 .footnote}^\nTo use this wrapper script, we first install the script with Homebrew. While you could install this from source, using Homebrew simplifies the process:\n​[ ] ​[$ ]​​brew​​ ​​install​​ ​​reattach-to-user-namespace​\nThen configure tmux to use the wrapper by adding this line to your [.tmux.conf] :\n​[ ] set -g default-command \u0026quot;reattach-to-user-namespace -l /bin/bash\u0026quot;\nThis configures the default command that tmux uses for new windows, so it loads the Bash shell through the wrapper script. If you use a shell other than Bash, like Fish or zsh, you\u0026rsquo;d specify its path or command instead.\nOnce you reload the configuration file, you\u0026rsquo;ll be able to use the [pbcopy] command again. And as an added bonus, you can send the contents of the current tmux buffer to the system clipboard:\n​[ ] ​[$ ]​​tmux​​ ​​show-buffer​​ ​​|​​ ​​pbcopy​\nOr you can paste the clipboard contents with this:\n​[ ] ​[$ ]​​tmux​​ ​​set-buffer​​ ​​$(pbpaste);​​ ​​tmux​​ ​​paste-buffer​\nThis means that you can also create keyboard shortcuts to do this, just like you did in ​Working with the Clipboard on Linux​. Unfortunately, the wrapper program we\u0026rsquo;re using doesn\u0026rsquo;t work with tmux\u0026rsquo;s [run] command. The workaround is to explicitly prefix [pbpaste] and [pbcopy] .ic} with the wrapper script. So, to support copying the buffer to the system clipboard, add this line to your [.tmux.conf] file:\n​[ ] bind C-c run ​*\u0026quot;tmux save-buffer - | reattach-to-user-namespace pbcopy\u0026quot;*​\nJust like with Linux, you can also configure tmux\u0026rsquo;s Copy mode to send the text you copy directly to the system clipboard by adding this keybinding to your configuration:\n​[ ] bind -t vi-copy y copy-pipe ​*\u0026quot;reattach-to-user-namespace pbcopy\u0026quot;*​\nNow when you select text in Copy mode and press y , the text will be sent to [pbcopy] and will be on your system clipboard, ready for use in other programs.\nTo support pasting from the system clipboard, we\u0026rsquo;d add this longer command, which must be [all on one line]{.emph}.\n​[ ] bind C-v run ​*\\​ ​[ ] ​\u0026quot;tmux set-buffer* ​​*\\\u0026quot;​​$(​reattach-to-user-namespace pbpaste​)​​\\\u0026quot;​​; tmux paste-buffer\u0026quot;*​\nThis provides a simple solution to an otherwise fairly complex problem.\n[]{#part0041.xhtml}\nWhat\u0026rsquo;s Next? # By using tmux paste buffers to move text around, you gain the ability to have a clipboard in situations where you might not have one, such as when you\u0026rsquo;re logged into the console of a server or without a graphical terminal. Being able to scroll back through the history of a long console output can be a huge help. It\u0026rsquo;s worth installing tmux directly on your servers for that reason alone.\nNow that you have a good understanding of how to find, copy, and paste text, you can start working tmux into your daily routine. For many developers, pair programming is often part of that routine. Let\u0026rsquo;s take a look at how to use tmux to work with another developer.\n[]{#part0042.xhtml}\nFor Future Reference # Shortcut keys # Shortcut Description\nPrefix [ Enters Copy mode. Prefix ] Pastes current buffer contents. Prefix = Lists all paste buffers and pastes selected buffer contents.\nCopy mode movement keys (vi mode) # Command Description\nh , j , k , and l Moves the cursor left, down, up, and right, respectively. w Moves the cursor forward one word at a time. b Moves the cursor backward one word at a time. f followed by any character Moves to the next occurrence of the specified character. F followed by any character Moves to the previous occurrence of the specified character. Ctrl -b Scrolls up one page. Ctrl -f Scrolls down one page. g Jumps to the top of the buffer. G Jumps to the bottom of the buffer. ? Starts a search backward through the buffer. / Starts a search forward through the buffer.\nCommands # Command Description\n[show-buffer] Displays current buffer contents. [capture-pane] Captures the selected pane\u0026rsquo;s visible contents to a new buffer. [list-buffers] Lists all paste buffers. [choose-buffer] Shows paste buffers and pastes the contents of the one you select. [save-buffer \\[filename\\]] Saves the buffer\u0026rsquo;s contents to the specified file.\n::: footnotes\nFootnotes # \\[10\\]{#part0042.xhtml#FOOTNOTE-10} http://sourceforge.net/projects/xclip/\n\\[11\\]{#part0042.xhtml#FOOTNOTE-11} https://github.com/ChrisJohnsen/tmux-MacOSX-pasteboard :::\n::: {.copyright style=\u0026quot;;color:black;\u0026quot;} Copyright © 2016, The Pragmatic Bookshelf. :::\n[]{#part0043.xhtml}\n[ Chapter 5]{.chapter-number} [Pair Programming with tmux]{.chapter-name} # Up until now, you\u0026rsquo;ve been making configuration changes and learning how to work within tmux on your own machine. But one of the most popular uses of tmux by developers is pair programming. It was actually my first introduction to tmux, and I immediately saw the potential as my friend walked me through using its various features.\nPair programming has a lot of great benefits. Working with another developer can help you see things you might not have seen on your own, but unless you\u0026rsquo;re physically in the same location, pair programming can be somewhat difficult. Screen-sharing through iChat, Skype, or even GoToMeeting takes up a lot of bandwidth and can be dodgy when you\u0026rsquo;re not using the best network connection. In this chapter, we\u0026rsquo;ll explore using tmux for pair programming, so you can work remotely with another developer on even the slowest hotel Wi-Fi connection.\nThere are two ways to work with remote users. The first method involves creating a new user account that you and others share. You set up tmux and your development environment under that account and use it as a shared workspace. The second approach uses tmux\u0026rsquo;s sockets so you can have a second user connect to your tmux session without having to share your user account.\nBoth of these methods have an inherent security flaw: they let someone else see things on your screen and in your account. You\u0026rsquo;re inviting someone in to potentially look at your files. To get around this, it\u0026rsquo;s wise to use an intermediate server for pairing. Using a low-cost VPS or a virtual machine with VirtualBox^\\[12\\]{#part0043.xhtml#FNPTR-12 .footnote}^ and Vagrant^\\[13\\]{#part0043.xhtml#FNPTR-13 .footnote}^, you can quickly create a development environment for pairing. In this chapter, we\u0026rsquo;ll be working with a remote server as we explore both of these approaches.\n[]{#part0044.xhtml}\nPairing with a Shared Account # Using a shared account is the simplest way to work with another user. In a nutshell, you enable SSH access on the machine that will act as the host, install and configure tmux on that machine, and then create a tmux session there. The second user logs into that machine with the same user account and attaches to the session. By using SSH public keys, you can make the login process somewhat transparent. Let\u0026rsquo;s walk through the setup. For this example, we\u0026rsquo;ll use a server called [puzzles] running Ubuntu that has the SSH daemon installed.\nFirst, create a \u0026ldquo;tmux\u0026rdquo; user on the server. This is the user everyone will use to connect to the pairing session. On the remote server, execute this command:\n​[ ] ​[tmux@puzzles$ ]​​adduser​​ ​​tmux​\nWe want to configure the account so we can take SSH keys from other developers and use them to log into this account. We do this by creating the file [~/.ssh/authorized_keys] under the [tmux] .ic} account. So, use the [su] command to switch to the user:\n​[ ] ​[tmux@puzzles$ ]​​su​​ ​​tmux​\nThen create the [.ssh] folder and the [.ssh/authorized_keys] file, setting the appropriate permissions. Only the [tmux] user should be allowed to read, write, or execute the folder and file.\n​[ ] ​[tmux@puzzles$ ]​​mkdir​​ ​​~/.ssh​ ​[ ] ​[tmux@puzzles$ ]​​touch​​ ​​~/.ssh/authorized_keys​ ​[ ] ​[tmux@puzzles$ ]​​chmod​​ ​​700​​ ​​~/.ssh​ ​[ ] ​[tmux@puzzles$ ]​​chmod​​ ​​600​​ ​​~/.ssh/authorized_keys​\nEach user you\u0026rsquo;d like to connect needs a public key, which they would generate on their local machine. To generate a key, use the command\n​[ ] ​[$ ]​​ssh-keygen​\nand follow the prompts on the screen.\nThen each user would transfer their public key over to the server and add it to the [authorized_keys] file. There are a number of ways to do this, but the most universal approach would be to use [cat] and [ssh] to transfer the key and append it to [authorized_keys] at the same time, like this:\n​[ ] ​[$ ]​​cat​​ ​​~/.ssh/id_rsa.pub​​ ​​|​​ ​​ssh​​ ​​tmux@your_server​​ ​​*'cat \u0026gt;\u0026gt; .ssh/authorized_keys'*​\nYou\u0026rsquo;ll be prompted for the [tmux] user\u0026rsquo;s password before you can connect.\nThe command [ssh-copy-id] makes this process slightly easier. If you install this command using your package manager on your client, then you can transfer the key like this:\n​[ ] ​[$ ]​​ssh-copy-id​​ ​​tmux@your_server​\nThis copies the [.id_rsa.pub] file automatically.\nYou would repeat this process for any other users you wanted to share this account with.\nThen on the remote server, you\u0026rsquo;d set up tmux, text editors, compilers, programming languages, and version control systems just like you would on any other development environment. Then you create a new tmux session on the server:\n​[ ] ​[tmux@puzzles$ ]​​tmux​​ ​​new-session​​ ​​-s​​ ​​Pairing​\nAnother member of your team can log in to the same machine and attach to the session with this:\n​[ ] ​[tmux@puzzles$ ]​​tmux​​ ​​attach​​ ​​-t​​ ​​Pairing​\nYou can then work collaboratively on the project. What\u0026rsquo;s more, you can detach from the session and reattach to it later, which means you can leave your environment running for days or even weeks at a time. You\u0026rsquo;d have a persistent development environment you can log into from anywhere that has a terminal with SSH support.\n[]{#part0045.xhtml}\nUsing a Shared Account and Grouped Sessions # When two people are attached to the same tmux session, they usually both see the same thing and interact with the same windows. But there are times when it\u0026rsquo;s helpful if one person can work in a different window without completely taking over control.\nUsing \u0026ldquo;grouped sessions,\u0026rdquo; you can do just that. Let\u0026rsquo;s demonstrate by creating a new session on our remote server called [groupedsession] .ic}.\n​[ ] ​[tmux@puzzles$ ]​​tmux​​ ​​new-session​​ ​​-s​​ ​​groupedsession​\nThen, instead of attaching to the session, another user can join that session by [creating a new session]{.emph} by specifying the target of the original session [groupedsession] and then specifying their [own]{.emph} session name, like this:\n​[ ] ​[tmux@puzzles$ ]​​tmux​​ ​​new-session​​ ​​-t​​ ​​groupedsession​​ ​​-s​​ ​​mysession​\nWhen the second session launches, both users can interact with the session at the same time, just as if the second user had attached to the session. However, the users can create windows independent of each other. So, if our new user creates a window, you\u0026rsquo;ll both see the new window show up in the status line, but you\u0026rsquo;ll stay on the window you\u0026rsquo;re currently working in! This is great for those \u0026ldquo;Hey, let me just try something\u0026rdquo; moments, or when one person wants to use Emacs and the other person prefers Vim:\n::: ss {#d24e6162} :::\nThe second user can kill off their session with [kill-session], and the original will still exist. However, both sessions will be killed if all windows are closed, so be careful!\nThat\u0026rsquo;s a lot of work to go through if you just want someone to jump in and help you out with some code. So let\u0026rsquo;s look at a simple alternative that takes almost no time to set up.\n[]{#part0046.xhtml}\nQuickly Pairing with tmate # tmate^\\[14\\]{#part0046.xhtml#FNPTR-14 .footnote}^ is a fork of tmux designed to make pair programming painless. Using tmate, you can quickly invite another developer to collaborate. When you launch tmate, it generates an address that your pair can use to make the connection. You don\u0026rsquo;t have to set up any keys or use any intermediate services. Instead, tmate\u0026rsquo;s servers handle tunneling the connection for you.\nThe catch is that you have to install tmate and use it instead of tmux. But don\u0026rsquo;t worry; it completely supports the configuration you\u0026rsquo;ve already built. Let\u0026rsquo;s look at how to get it installed.\nOn Ubuntu, you can install it by adding the tmate PPA to your package manager:\n​[ ] ​[$ ]​​sudo​​ ​​apt-get​​ ​​install​​ ​​software-properties-common​ ​[ ] ​[$ ]​​sudo​​ ​​add-apt-repository​​ ​​ppa:tmate.io/archive​ ​[ ] ​[$ ]​​sudo​​ ​​apt-get​​ ​​update​​ ​​\u0026amp;\u0026amp;​​ ​​sudo​​ ​​apt-get​​ ​​install​​ ​​tmate​\nOn the Mac, you can install it with Homebrew:\n​[ ] ​[$ ]​​brew​​ ​​install​​ ​​tmate​\nOnce tmate is installed, fire it up with\n​[ ] ​[$ ]​​tmate​\nand tmate will launch, displaying the connection address in the bottom of the window where your status line would be.\n::: ss {#d24e6280} :::\nCopy that address and send it to your pair, and they\u0026rsquo;ll be able to join you instantly. If the address disappears before you can copy it, or you\u0026rsquo;d like to see it again, execute the command\n​[ ] ​[$ ]​​tmate​​ ​​show-messages​\nto view the address again, along with some other interesting details, including a read-only address you can send to someone if you just want to demonstrate something and don\u0026rsquo;t want them to have any control:\n​[ ] Sun Sep 25 17:46:13 2016 \\[tmate\\] Connecting to ssh.tmate.io... ​[ ] Sun Sep 25 17:46:13 2016 \\[tmate\\] Note: clear your terminal before sharing readonly ​[ ] access ​[ ] Sun Sep 25 17:46:13 2016 \\[tmate\\] web session read only: https://... ​[ ] Sun Sep 25 17:46:13 2016 \\[tmate\\] ssh session read only: ssh ... ​[ ] Sun Sep 25 17:46:13 2016 \\[tmate\\] web session: https://... ​[ ] Sun Sep 25 17:46:13 2016 \\[tmate\\] ssh session: ssh ...\ntmate supports the same commands that tmux supports, so you can create named sessions and even script up your configurations. You can even use it with Tmuxinator by adding the following to your Tmuxinator YAML file:\n​[ ] ​tmux_options​: ​*-S /tmp/your_project_name_tmate_socket​ ​[ ] ​tmux_command​: ​tmate*​\nSince tmate creates a randomly named socket file, we just tell it not to do that by passing the [-S] switch. Then we tell Tmuxinator that it should use [tmate] instead of [tmux].\n::: sidebar ::: sidebar-title Using tmate with Your Own Servers :::\n::: sidebar-content If you feel uncomfortable going through http://ssh.tmate.io to connect to other sessions, you can find instructions for setting up your own server at the tmate website.^\\[15\\]{#part0046.xhtml#FNPTR-15 .footnote}^ It provides you with the server, which you compile and install on your own Linux server. Then you run the server and configure client machines to use that server instead of the default service. This may add more security, but you\u0026rsquo;ll want to think about redundancy and availability. For example, the [tmate.io] address resolves to multiple backend servers, ensuring high availability. If you want to ensure continuity, you\u0026rsquo;ll want to configure your environment in a similar way. ::: :::\nUsing shared accounts or tmate is easy, but it\u0026rsquo;s not always desirable to share user accounts with team members or let someone remotely access your development machine. Let\u0026rsquo;s look at an alternative approach.\n[]{#part0047.xhtml}\nPairing with Separate Accounts and Sockets # Using tmux\u0026rsquo;s support for sockets, you can create sessions that multiple users can connect to with ease.\nTo test this out, create two new user accounts for the session: one called \u0026ldquo;ted\u0026rdquo; and another named \u0026ldquo;barney.\u0026rdquo;\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​adduser​​ ​​ted​\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​adduser​​ ​​barney​\nNext, create the \u0026ldquo;tmux\u0026rdquo; group and the [/var/tmux] folder that will hold the shared sessions.\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​addgroup​​ ​​tmux​\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​mkdir​​ ​​/var/tmux​\nNext, change the group ownership of the [/var/tmux] folder so that the [tmux] group has access:\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​chgrp​​ ​​tmux​​ ​​/var/tmux​\nThen alter the permissions on the folder so that new files will be accessible for all members of the [tmux] group:\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​chmod​​ ​​g+ws​​ ​​/var/tmux​\nFinally, add [ted] and [barney] to the tmux group.\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​usermod​​ ​​-aG​​ ​​tmux​​ ​​ted​\n​[ ] ​[tmux@puzzles$ ]​​sudo​​ ​​usermod​​ ​​-aG​​ ​​tmux​​ ​​barney​\nNow let\u0026rsquo;s look at how these users can work together on a project.\nCreating and Sharing Sessions # So far, you\u0026rsquo;ve used the [new-session] command to create these sessions, but that uses the default socket location, which won\u0026rsquo;t be reachable by every user. Instead of creating named sessions, we create our sessions using the [-S] switch.\nLog in to your server as [ted] and create a new tmux session using a socket file in the [/var/tmux/] folder:\n​[ ] ​[ted@puzzles$ ]​​tmux​​ ​​-S​​ ​​/var/tmux/pairing​\nIn another terminal window, log in as [barney] and then attach to the session. But instead of specifying the target with the [-t] .ic} switch, specify the location of the socket file, like this:\n​[ ] ​[barney@puzzles$ ]​​tmux​​ ​​-S​​ ​​/var/tmux/pairing​​ ​​attach​\nThe [barney] user now attaches to the tmux session and sees everything that the [ted] user sees.\nIt\u0026rsquo;s important to note that when using this approach, the [.tmux.conf] file used is the one that started up the session. Having two separate accounts doesn\u0026rsquo;t mean that each account gets to use its own configuration files within the tmux session, but it does mean they can customize their accounts for other purposes, and can each initiate their own tmux session as needed. More importantly, it keeps [barney] out of [ted]\u0026rsquo;s home directory.\n[]{#part0048.xhtml}\nWhat\u0026rsquo;s Next? # Now that you know how to use tmux to share your screen with others, you can use it for remote training, impromptu collaboration on open source projects, or even presentations.\nIn addition, you could use this technique to fire up a tmux session on one of your production servers, load up monitoring tools or consoles, and then detach from it, leaving those tools running in the background. Then you simply connect to your machine, reattach to the session, and everything is back where you left it. I do something similar with my development environment. I set up tmux on a VPS, which lets me use nothing more than an iPad, an SSH client, and a Bluetooth keyboard to hack on code when I\u0026rsquo;m away from home. It even works brilliantly over the 3G network.\nPair programming and working remotely are just two examples of how incorporating tmux into your workflow can make you more productive. In the next chapter, we\u0026rsquo;ll look at other enhancements we can make to our environment as we explore advanced ways to work with windows, panes, and our system in general.\n[]{#part0049.xhtml}\nFor Future Reference # Command Description\n[tmux new-session -t ][\n\\[existing session\\]][ -s \\[new session\\]] Creates a connection to a grouped session. [tmux show-messages] Displays a log of messages in the current window, useful for debugging. [tmux -S \\[socket\\]] Creates a new session using a socket instead of a name. [tmux -S \\[socket\\] attach] Attaches to an existing session using a socket instead of a name.\n::: footnotes\nFootnotes # \\[12\\]{#part0049.xhtml#FOOTNOTE-12} https://www.virtualbox.org/\n\\[13\\]{#part0049.xhtml#FOOTNOTE-13} https://www.vagrantup.com/docs/getting-started/\n\\[14\\]{#part0049.xhtml#FOOTNOTE-14} https://tmate.io/\n\\[15\\]{#part0049.xhtml#FOOTNOTE-15} https://tmate.io/ :::\n::: {.copyright style=\u0026quot;;color:black;\u0026quot;} Copyright © 2016, The Pragmatic Bookshelf. :::\n[]{#part0050.xhtml}\n[ Chapter 6]{.chapter-number} [Workflows]{.chapter-name} # By itself, tmux is just another terminal with a few bells and whistles that let us display...more terminal sessions. But tmux makes it easier to work with the programs we run in those sessions, so this chapter will explore some common, and uncommon, configurations and commands that you may find useful in your day-to-day work. You\u0026rsquo;ll see some advanced ways to manage your panes and sessions, how to make tmux work with your shell of choice, how to extend tmux commands with external scripts, and how to create keybindings that execute several commands. Let\u0026rsquo;s start with windows and panes.\n[]{#part0051.xhtml}\nWorking Effectively with Panes and Windows # Throughout this book, you\u0026rsquo;ve seen ways to divide up your tmux sessions into panes and windows. In this section, we\u0026rsquo;ll look at more advanced ways to work with those panes and windows.\nTurning a Pane into a Window # Panes are great for dividing up a workspace, but sometimes you might want to \u0026ldquo;pop out\u0026rdquo; a pane into its own window. tmux has a command to do just that.\nInside any pane, press Prefix ! and tmux will create a new window from your pane, removing the original pane.\nTurning a Window into a Pane # Occasionally, it\u0026rsquo;s nice to consolidate a workspace. You can easily take a window and turn it into a pane. To do this, issue the [join-pane] .ic} command.\nTry it out. Create a new tmux session with two windows.\n​[ ] ​[$ ]​​tmux​​ ​​new-session​​ ​​-s​​ ​​panes​​ ​​-n​​ ​​first​​ ​​-d​ ​[ ] ​[$ ]​​tmux​​ ​​new-window​​ ​​-t​​ ​​panes​​ ​​-n​​ ​​second​ ​[ ] ​[$ ]​​tmux​​ ​​attach​​ ​​-t​​ ​​panes​\nNow, to move the first window into a pane in the second window, press Prefix : .keystroke to enter Command mode, and type this:\n[join-pane -s panes:1]\nThis means \u0026ldquo;Take window 1 of the [panes] session and join it to the current window,\u0026rdquo; since we did not specify a target. When you \u0026ldquo;join\u0026rdquo; a pane, you\u0026rsquo;re essentially moving a pane from one session to another. You specify the source window and pane, followed by the target window and pane. If you leave the target off, the current focused window becomes the target.\nYou can use this technique to move panes around as well. If your first window had two panes, you could specify the source pane like this, keeping in mind that you set the window and pane base indexes to [1] .ic} instead of [0] back in Chapter 2, ​Configuring tmux​.\n[join-pane -s panes:1.1]\nThis command grabs the first pane of the first window and joins it to the current window.\nTo take it a step further, you can specify a different source session, using the notation [\n\\[session_name\\]:\n\\[window\\].\n\\[pane\\]], and you can specify a target window using the [-t] flag using the same notation. This lets you pull panes from one session into another.\nMaximizing and Restoring Panes # Sometimes you just want a pane to go full-screen for a bit so you can see its contents or work in a more focused way. You could use the [break-pane] command. But then you\u0026rsquo;d have to use [join-pane] to put it back where it was. But there\u0026rsquo;s a better way. The [resize-pane] command accepts the [-Z] option for zooming a pane. Best of all, it\u0026rsquo;s already mapped to Prefix .keystroke z , and pressing it again restores the pane to its original size.\nLaunching Commands in Panes # In Chapter 3, ​Scripting Customized tmux Environments​, we explored how to use shell commands and [send-keys] to launch programs in our panes, but we can execute commands automatically when we launch a window or a pane.\nWe have two servers, [burns] and [smithers], which run our web server and database server, respectively. When we start up tmux, we want to connect to these servers using a single window with two panes.\nLet\u0026rsquo;s create a new script called [servers.sh] and create one session connecting to two servers:\n​[ ] ​[$ ]​​tmux​​ ​​new-session​​ ​​-s​​ ​​servers​​ ​​-d​​ ​​*\u0026quot;ssh deploy@burns\u0026quot;​ ​[ ] ​[$ ]​​tmux​​ ​​split-window​​ ​​-v​​ ​​\u0026quot;ssh dba@smithers\u0026quot;*​ ​[ ] ​[$ ]​​tmux​​ ​​attach​​ ​​-t​​ ​​servers​\nWhen we create a new session, we can pass the command we want to execute as the last argument. In our case, we fire off the new session and connect to [burns] in the first window, and we detach the session. Then we divide the window using a vertical split and then connect to [smithers].\nThis configuration has a handy side effect: when we log off of our remote servers, the pane or window will close.\nOpening a Pane in the Current Directory # When you open a new pane, tmux places you in the directory where you originally launched tmux. Sometimes that\u0026rsquo;s exactly what you want, but if you navigated into another directory, you might want to create a new pane that starts in that directory instead.\nYou can use the [pane_current_path] variable provided by tmux when creating a new pane. Open Command mode and execute\n​[ ] split-window -v -c ​*\u0026quot;#{pane_current_path}\u0026quot;*​\nThis splits the window horizontally, but opens the new terminal session in the same working directory as the current pane or window.\nYou can add this to your configuration file to make this easy. Instead of changing the existing bindings for splits, add new ones so you can choose the behavior you\u0026rsquo;d like:\n​[ ] ​*# split pane and retain the current directory of existing pane​ ​[ ] bind _ split-window -v -c ​\u0026quot;#{pane_current_path}\u0026quot;​ ​[ ] bind ​\\* ​split-window -h -c ​*\u0026quot;#{pane_current_path}\u0026quot;*​\nThis configures things so that Prefix _ splits the window horizontally and Prefix / .keystroke splits the window vertically.\nIssuing Commands in Many Panes Simultaneously # Every once in a while, you might need to execute the same command in multiple panes. You might need to run the same update script on two servers, for example. You can do this easily with tmux.\nUsing the command [set-window-option synchronize-panes on], anything you type in one pane will be immediately broadcast to the other panes in the current session. Once you\u0026rsquo;ve issued the command, you can turn it off with [set-window-option synchronize-panes off].\nTo make this easier to do, you can map this to Prefix Ctrl -s , like this:\n​[ ] ​*# shortcut for synchronize-panes toggle*​ ​[ ] bind C-s set-window-option synchronize-panes\nBy not specifying the [off] or [on] option, the [synchronize-panes] command acts as a toggle. While this isn\u0026rsquo;t something you\u0026rsquo;ll use very often, it\u0026rsquo;s amazingly handy when you need it.\nManaging Sessions # As you get more comfortable with tmux, you may find yourself using more than one tmux session simultaneously. For example, you may fire up unique tmux sessions for each application you\u0026rsquo;re working on so you can keep the environments contained. There are some great tmux features to make managing these sessions painless.\nMoving Between Sessions # All tmux sessions on a single machine route through a single server. That means you can move effortlessly between your sessions from a single client.\nLet\u0026rsquo;s try this out. Start two detached tmux sessions, one named \u0026ldquo;editor,\u0026rdquo; which launches Vim, and the other running the [top] command, called \u0026ldquo;processes\u0026rdquo;:\n​[ ] ​[$ ]​​tmux​​ ​​new​​ ​​-s​​ ​​editor​​ ​​-d​​ ​​vim​ ​[ ] ​[$ ]​​tmux​​ ​​new​​ ​​-s​​ ​​processes​​ ​​-d​​ ​​top​\nConnect to the \u0026ldquo;editor\u0026rdquo; session with\n​[ ] ​[$ ]​​tmux​​ ​​attach​​ ​​-t​​ ​​editor​\nand then press Prefix ( to go to the previous session and Prefix ) to move to the next session.\nYou can also use Prefix s to display a list of sessions, so you can quickly navigate between sessions:\nYou can use the j and k keys to move up and down if you\u0026rsquo;ve configured tmux to use Vim-like movement, and you can press Space to expand a session so you can jump to a specific window or pane.\nYou can add custom keybindings for this to your [.tmux.conf] .filename} file by binding keys to the [switch-client] command. The default configuration looks like this:\n​[ ] bind ( switch-client -p ​[ ] bind ) switch-client -n\nIf you\u0026rsquo;ve set up multiple workspaces, this is an extremely efficient way to move around your environments, without detaching and reattaching.\nMoving Windows Between Sessions # You can move a window from one session to another. This is handy in case you\u0026rsquo;ve started up a process in one environment and want to move it around or want to consolidate your workspaces.\nThe [move-window] command is mapped to Prefix . (the period), so you can bring up the window you want to move, press the key combination, and then type the name of the target session.\nTo try this out, create two sessions, with the names \u0026ldquo;editor\u0026rdquo; and \u0026ldquo;processes,\u0026rdquo; running [vim] and [top] respectively:\n​[ ] ​[$ ]​​tmux​​ ​​new​​ ​​-s​​ ​​editor​​ ​​-d​​ ​​vim​ ​[ ] ​[$ ]​​tmux​​ ​​new​​ ​​-s​​ ​​processes​​ ​​-d​​ ​​top​\nLet\u0026rsquo;s move the window in the \u0026ldquo;processes\u0026rdquo; session into the \u0026ldquo;editor\u0026rdquo; session.\nFirst, attach to the \u0026ldquo;processes\u0026rdquo; session with this:\n​[ ] ​[$ ]​​tmux​​ ​​attach​​ ​​-t​​ ​​processes​\nThen, press Prefix . and type \u0026ldquo;editor\u0026rdquo; in the command line that appears.\nThis removes the only window in the \u0026ldquo;processes\u0026rdquo; session, causing it to close. If you attach to the \u0026ldquo;editor\u0026rdquo; session, you\u0026rsquo;ll see both windows.\nYou can use shell commands to do this, too, so you don\u0026rsquo;t need to consolidate things by opening sessions. To do that, use [move-window], like this:\n​[ ] ​[$ ]​​tmux​​ ​​move-window​​ ​​-s​​ ​​processes:1​​ ​​-t​​ ​​editor​\nThis moves the first window of the \u0026ldquo;processes\u0026rdquo; session to the \u0026ldquo;editor\u0026rdquo; session.\nCreating or Attaching to Existing Sessions # So far, we\u0026rsquo;ve always taken the approach of creating new tmux sessions whenever we want to work. However, we can actually detect if a tmux session exists and connect to it if it does.\nThe [has-session] command returns a Boolean value that we can use in a shell script. That means we can do something like this in a Bash script:\n​[ ] ​if​ ! tmux has-session -t development; ​then​ ​[ ] ​​exec tmux new-session -s development -d ​[ ] ​*# other setup commands before attaching.\u0026hellip;*​ ​[ ] ​fi​ ​[ ] exec tmux attach -t development\nIf you modify the script to take an argument, you can use this to create a single script that you can use to connect to or create any tmux session.\ntmux and Your Operating System # As tmux becomes part of your workflow, you may want to integrate it more tightly with your operating system. In this section, you\u0026rsquo;ll discover ways to make tmux and your system work well together.\nUsing a Different Shell # In this book, we\u0026rsquo;ve used the Bash shell, but if you\u0026rsquo;re a fan of [zsh], you can still get all the tmux goodness.\nJust explicitly set the default shell in [.tmux.conf] like this:\n​[ ] set -g default-shell /bin/zsh\nSince tmux is just a terminal multiplexer and not a shell of its own, you just specify exactly what to run when it starts.\nLaunching tmux by Default # You can configure your system to launch tmux automatically when you open a terminal. And using what you know about session names, you can create a new session if one doesn\u0026rsquo;t exist, or attach to one that does.\nWhen tmux is running, it sets the [TERM] variable to \u0026ldquo;screen\u0026rdquo; or the value of the [default-terminal] setting in the configuration file. You can use this value in your [.bashrc] .filename} (or [.bash_profile] on macOS) file to determine whether or not you\u0026rsquo;re currently in a tmux session. You set your tmux terminal to \u0026ldquo;screen-256color\u0026rdquo; back in Chapter 2, ​Configuring tmux​, so you could use that to detect if tmux is actually running.\nFor example, you could add these lines to the end of your [.bashrc] .ic} file:\n​[ ] ​if​ \\[\\[ ​*\\\"*​\\$TERM​*\\\"*​ != ​*\\\"screen-256color\\\"*​ \\]] ​[ ] ​then​ ​[ ] ​​tmux attach-session -t ​*\u0026quot;​$USER​\u0026quot;​ || tmux new-session -s ​\u0026quot;​$USER​\u0026quot;*​ ​[ ] ​fi​\nThis first checks that you\u0026rsquo;re not already in a tmux session. If that\u0026rsquo;s the case, it attempts to attach to a session with a session name of [$USER], which is your username. You can replace this with any value you want, but using the username helps avoid conflicts.\nIf the session doesn\u0026rsquo;t exist, tmux will throw an error that the shell script can interpret as a [false] value. It can then run the right side of the expression, which creates a new session with your username as the session\u0026rsquo;s name. It then exits out of the script.\nWhen the tmux session starts up, it will run through our [.bashrc] .filename} or [.bash_profile] file again, but this time it will see that we\u0026rsquo;re in a tmux session, skip over this chunk of code, and execute the rest of the commands in our configuration file, ensuring that all our environment variables are set for us.\nNow every time you open a new terminal, you\u0026rsquo;ll be in a tmux session. Be careful, though, since each time you open a new terminal session on your machine, it will be attached to the same session. Exiting tmux in one terminal will exit tmux in all of them.\nKeeping Specific Configuration Separate # In Chapter 4, ​Working With Text and Buffers​, you learned how to make tmux work with the OS X and Linux system clipboards, and this involved adding some specific configuration options to your [.tmux.conf] file. But if you wanted your configuration to work on both operating systems, you\u0026rsquo;d run into some conflicts.\nThe solution is to move your OS-specific configuration into a separate file and then tell tmux to load it up by using tmux\u0026rsquo;s [if-shell] .ic} command and the [source] command.\nTry it out. Create a new file called [.tmux.mac.conf] in your home directory:\n​[ ] ​[$ ]​​touch​​ ​​~/.tmux.mac.conf​\nIn that file, put all the code to make the Mac\u0026rsquo;s clipboard work with tmux:\n​[ ] ​*# Setting the namespace​ ​[ ] set -g default-command ​\u0026quot;reattach-to-user-namespace -l /bin/bash\u0026quot;​ ​[ ]\n​[ ] ​# Prefix C-c copy buffer to system clipboard​ ​[ ] bind C-c run ​\u0026quot;tmux save-buffer - | reattach-to-user-namespace pbcopy\u0026quot;​ ​[ ]\n​[ ] ​# Prefix C-v paste system clipboard into tmux​ ​[ ] bind C-v run \\ ​[ ] ​\u0026quot;tmux set-buffer \\\u0026quot;​$(reattach-to-user-namespace pbpaste)\\​\u0026quot;; tmux paste-buffer\u0026quot;​ ​[ ]\n​[ ] ​# use y in visual mode to copy text to system clipboard​ ​[ ] bind -t vi-copy y copy-pipe ​\u0026quot;reattach-to-user-namespace pbcopy\u0026quot;*​\nThen open [.tmux.conf] and remove any lines related to OS X if you\u0026rsquo;ve put them in. Then add this to the end of the file:\n​[ ] ​*# Load mac-specific settings​ ​[ ] ​if​-shell ​\u0026quot;uname | grep -q Darwin\u0026quot;​ ​\u0026quot;source-file ~/.tmux.mac.conf\u0026quot;*​\nThe [if-shell] command runs a shell command, and if it was successful, it executes the step. In this case, we tell tmux to run the [uname] command and use [grep] to see if it contains the word \u0026ldquo;Darwin.\u0026rdquo; If it does, it\u0026rsquo;s a safe bet we\u0026rsquo;re on a Mac, so we load the configuration file.\nYou could use a similar approach to load an additional bit of configuration only if it exists. For example, you may want to share your main [.tmux.conf] file with the world on GitHub, but you may want to keep some of your own secret sauce private. So move all of those tricks into [.tmux.private] , and add this to your [.tmux.conf] file:\n​[ ] ​*# load private settings if they exist​ ​[ ] ​if​-shell ​\u0026quot;\n\\[ -f \\~/.tmux.private\\]\u0026quot;​ ​\u0026quot;source ~/.tmux.private\u0026quot;*​\nThis will only load the file if it exists.\nRecording Program Output to a Log # Sometimes it\u0026rsquo;s useful to be able to capture the output of a terminal session to a log. You already learned how to use [capture-pane] and [save-buffer] to do this, but tmux can actually record the activity in a pane right to a text file with the [pipe-pane] command. This is similar to the [script] command available in many shells, except that with [pipe-pane], you can toggle it on and off at will, and you can start it after a program is already running.\nTo activate this, enter Command mode and type [pipe-pane -o \u0026quot;cat \u0026gt;\u0026gt; mylog.txt\u0026quot;].\nYou can use the [-o] flag to toggle the output, which means if you send the exact command again, you can turn the logging off. To make it easier to execute this command, add this to your configuration script as a shortcut key.\n​[ ] ​*# Log output to a text file on demand​ ​[ ] bind P pipe-pane -o ​\u0026quot;cat \u0026gt;\u0026gt;~/#W.log\u0026quot;​ ​\\;​ display ​\u0026quot;Toggled logging to ~/#W.log\u0026quot;*​\nNow you can press Prefix P to toggle logging. Thanks to the [display] command (short for [display-message]), you\u0026rsquo;ll see the name of the log file displayed in the status line. The [display] command has access to the same variables as the status line, which you learned about in Table 1, ​Status Line Variables​.\nAdding Battery Life to the Status Line # If you use tmux on a laptop, you may want to show the remaining battery life in your status line, especially if you run your terminal in full-screen mode. It turns out that this is a simple thing to add thanks to the [#(shell-command)] variable.\nLet\u0026rsquo;s add the battery status to our configuration file. Grab a shell script that can fetch the remaining battery charge and display it to the screen. We\u0026rsquo;ll place this in a file called [battery] in our home folder and tell tmux to run it for us.\nFirst, download the file:\n​[ ] ​[$ ]​​wget​​ ​​--no-check-certificate​​ ​​\\​ ​[ ] ​https://raw.github.com/richo/battery/master/bin/battery​\nYou can also find the [battery] script in the book\u0026rsquo;s source code downloads.\nNow make it executable so tmux can use it:\n​[ ] ​[$ ]​​chmod​​ ​​+x​​ ​​~/battery​\nTest it out by running\n​[ ] ​[$ ]​​~/battery​​ ​​Discharging​\nIf you\u0026rsquo;re running this on a laptop without the power cord plugged in, you\u0026rsquo;ll see the percentage left on the battery.\nWe can get tmux to display the output of any command-line program in its status bar by using [#(\u0026lt;command\u0026gt;)]. So, to display the battery in front of the clock, change the [status-right] line in [.tmux.conf] to this:\n​[ ] ​*# Status line right side - 50% | 31 Oct 13:37​ ​[ ] set -g status-right ​\u0026quot;#(~/battery Discharging) | #\n\\[fg=cyan\\]%d %b %R\u0026quot;*​\nNow, when you reload the [.tmux.conf] file, the battery status indicator will appear.\nTo get battery status when it\u0026rsquo;s charging, you\u0026rsquo;ll need to execute the command\n​[ ] ​[$ ]​​~/battery​​ ​​Charging​\nand work that into the status line. I\u0026rsquo;ll leave that up to you.\nYou can use this approach to customize your status line further. You\u0026rsquo;d simply need to write your own script that returns the value you want to display, and then drop it into the status line.\nIntegrating Seamlessly with Vim # The Vim text editor works pretty well with tmux, but developer Mislav Marohni[ć]{style=\u0026quot;\u0026quot;} developed a solution that lets you move between tmux panes and Vim splits seamlessly. To make this work, you\u0026rsquo;ll need to install Chris Toomey\u0026rsquo;s vim-tmux-navigator plugin for Vim^\\[16\\]{#part0053.xhtml#FNPTR-16 .footnote}^ and add some keybindings to your [.tmux.conf] file.\nThis setup will create the following keybindings:\nCtrl -j moves up\nCtrl -k moves down\nCtrl -h moves left\nCtrl -l moves right\nIf you\u0026rsquo;re in tmux and you move into Vim, then the Vim plugin will take over. If you\u0026rsquo;re in Vim and you move to tmux, then tmux will take over. Instead of having to learn two sets of commands to navigate, you just have one. To set this up, install the Vim plugin using Vundle by adding this to your [.vimrc] file:\n​[ ] Plugin 'christoomey/vim-tmux-navigator'\nThen save your [.vimrc] file and run\n​[ ] :PluginInstall\nin Vim to install the plugin.\nThen in [.tmux.conf] , add these lines:\n​[ ] is_vim=​*\u0026quot;ps -o state= -o comm= -t '#{pane_tty}'* ​​*\\​ ​[ ] ​| grep -iqE '^\n\\[\\^TXZ \\]+ +(​​\\\\​​S+​​\\\\​​/)?g?(view|n?vim?x?)(diff)?​$'​\u0026quot;​ ​[ ] bind-key -n C-h ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-h\u0026quot;​ ​\u0026quot;select-pane -L\u0026quot;​ ​[ ] bind-key -n C-j ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-j\u0026quot;​ ​\u0026quot;select-pane -D\u0026quot;​ ​[ ] bind-key -n C-k ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-k\u0026quot;​ ​\u0026quot;select-pane -U\u0026quot;​ ​[ ] bind-key -n C-l ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-l\u0026quot;​ ​\u0026quot;select-pane -R\u0026quot;​ ​[ ] bind-key -n C-​\\* ​​if​-shell ​*\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-​​\\\\​​\u0026quot;​ ​\u0026quot;select-pane -l\u0026quot;​ ​[ ]\n​[ ] bind C-l send-keys ​'C-l'*​\nCtrl -l is the keybinding used by the [readline] library in many shells for clearing the screen. The last line of this configuration sets up Prefix Ctrl -l to issue that command instead.\nExtending tmux with Plugins # So far, we\u0026rsquo;ve made modifications directly to the tmux configuration file. While that works, it can be a little awkward when doing something more complex. Bruno Sutic developed a solution to this called TPM, the tmux plugin manager. Since then, more and more people have come together to build plugins to extend tmux. Let\u0026rsquo;s use TPM to install the incredibly useful [tmux-resurrect] .ic}^\\[17\\]{#part0054.xhtml#FNPTR-17 .footnote}^ plugin, which can restore tmux sessions even after a reboot!\nTo set it up, first clone the repository into a folder called [~/.tmux/plugins/tpm] :\n​[ ] ​[$ ]​​git​​ ​​clone​​ ​​https://github.com/tmux-plugins/tpm​​ ​​~/.tmux/plugins/tpm​\nThen add these lines to your [.tmux.conf] file:\n​[ ] set -g @plugin ​*'tmux-plugins/tpm'​ ​[ ] set -g @plugin ​'tmux-plugins/tmux-resurrect'​ ​[ ] run ​'~/.tmux/plugins/tpm/tpm'*​\nFirst we list TPM itself, followed by the [tmux-resurrect] plugin. Then we load TPM so it can load other plugins. Save this file and reload your configuration. Then press Prefix I to install the plugin. You\u0026rsquo;ll see this output in tmux:\n​[ ] Already installed \u0026quot;tpm\u0026quot; ​[ ]\n​[ ] Installing \u0026quot;tmux-resurrect\u0026quot; ​[ ] \u0026quot;tmux-resurrect\u0026quot; download success ​[ ]\n​[ ] TMUX environment reloaded. ​[ ]\n​[ ] Done, press ENTER to continue.\nNow test out the [tmux-resurrect] program. Open a couple more panes, and then press Prefix Ctrl -s to save the state of the tmux session. Then close all of the panes and exit tmux. Finally, reload tmux and press Prefix Ctrl -r to restore the session you saved. All of your panes will come back!\nVisit the list of tmux plugins^ and find one you\u0026rsquo;d like to install. You\u0026rsquo;ll find one for the batter meter we set up, another for OS-specific clipboards, and even one with sensible configuration options.\nWhat\u0026rsquo;s Next? # There\u0026rsquo;s so much more you can do with tmux now that you know the basics and you\u0026rsquo;ve had some experience playing around with various configurations. The tmux manual, which you can access from your terminal with\n​[ ] ​[$ ]​​man​​ ​​tmux​\nhas the complete list of configuration options and available commands.\nAnd don\u0026rsquo;t forget that tmux itself is rapidly evolving. The next version will bring new configuration options, which will give you even more flexibility.\nAs you integrate tmux into your workflow, you may discover other techniques you start to rely on. For example, you can use tmux and a text-based editor on a remote server to create an incredibly effective development environment that you can use to collaborate with another developer. You can even use irssi (a terminal-based IRC client) and Alpine (a terminal-based email app) within your tmux sessions, either alongside of your text editor in a pane, or in background windows. Then you can detach from the session and come back to it later, with your entire environment ready to go.\nKeep working with tmux and before you know it, it\u0026rsquo;ll be an indispensable part of your workflow.\nFor Future Reference # Command Description\nPrefix ! Converts the currently selected pane into a new window. [join-pane -s \\[session\\]:\n\\[window\\].\n\\[pane\\]] Converts the specified session\u0026rsquo;s window or pane into a pane in the current window. [join-pane -s \\[session\\]:\n\\[window\\].\n\\[pane\\] -t \\[other session\\]] Converts the specified session\u0026rsquo;s window or pane into a pane in the target session. Prefix z Zooms the current pane, making it full screen. Pressing it again restores the pane to its original size. [tmux new-session \u0026quot;\n\\[command\\]\u0026quot;] Launches tmux and executes a command. When the command completes, the tmux session closes. [split-pane \u0026quot;\n\\[command\\]\u0026quot;] Splits the current window and executes the specified command in the new pane. When the command completes, the pane closes. [split-window -c ][\u0026quot;#{pane_current_path}\u0026quot;] Splits the pane and sets the working directory of the new pane to the current working directory of the focused pane. [set-window-option synchronize-panes] Toggles pane synchronization, where keystrokes are issued to all panes simultaneously instead of only the current pane. Prefix ( Moves to the next tmux session. Prefix ) Moves to the previous tmux session. Prefix s Shows the session selection list. [move-window -s \\[source session\\]:][\n\\[window\\] -t \\[target session\\] ] Moves a window from one session to another. Also available with Prefix . , followed by the target session name. [set -g default-shell \\[shell\\]] Sets the default shell that tmux uses when creating new windows. [set -g default-command \\[command\\]] Sets the default command that tmux uses when creating new windows. Blank by default. [if-shell \u0026quot;\n\\[condition\\]\u0026quot; \u0026quot;\n\\[command\\]\u0026quot;] Performs a given [command]{.emph} if the [condition]{.emph} evaluates to [true]. [ pipe-pane -o \u0026quot;cat \u0026gt;\u0026gt;~/#W.log\u0026quot;] Records the current pane to a text file.\nhttps://github.com/christoomey/vim-tmux-navigator https://github.com/tmux-plugins/tmux-resurrect https://github.com/tmux-plugins\nAppendix 1 Our Configuration # Throughout the book, we\u0026rsquo;ve built up a somewhat complex [.tmux.conf] .filename} file. Here\u0026rsquo;s the entire file for your reference.\n​[ ] ​*# Setting the prefix from C-b to C-a​ ​[ ] set -g prefix C-a ​[ ] ​#​ ​[ ] ​# Free the original Ctrl-b prefix keybinding​ ​[ ] unbind C-b ​[ ] ​#​ ​[ ] ​#setting the delay between prefix and command​ ​[ ] set -s escape-time 1 ​[ ] ​#​ ​[ ] ​# Ensure that we can send Ctrl-A to other apps​ ​[ ] bind C-a send-prefix ​[ ]\n​[ ] ​# Set the base index for windows to 1 instead of 0​ ​[ ] set -g base-index 1 ​[ ]\n​[ ] ​# Set the base index for panes to 1 instead of 0​ ​[ ] setw -g pane-base-index 1 ​[ ]\n​[ ] ​# Reload the file with Prefix r​ ​[ ] bind r source-file ~/.tmux.conf ​\\;​ display ​\u0026quot;Reloaded!\u0026quot;​ ​[ ]\n​[ ] ​# splitting panes with | and -​ ​[ ] bind | split-window -h ​[ ] bind - split-window -v ​[ ]\n​[ ] ​# moving between panes with Prefix h,j,k,l*​ ​[ ] bind h ​select​-pane -L ​[ ] bind j ​select​-pane -D ​[ ] bind k ​select​-pane -U ​[ ] bind l ​select​-pane -R\n​[ ] ​*# Quick window selection​ ​[ ] bind -r C-h ​select​-window -t :- ​[ ] bind -r C-l ​select​-window -t :+ ​[ ]\n​[ ] ​# Pane resizing panes with Prefix H,J,K,L​ ​[ ] bind -r H resize-pane -L 5 ​[ ] bind -r J resize-pane -D 5 ​[ ] bind -r K resize-pane -U 5 ​[ ] bind -r L resize-pane -R 5 ​[ ]\n​[ ] ​# mouse support - set to on if you want to use the mouse​ ​[ ] set -g mouse off ​[ ]\n​[ ] ​# Set the default terminal mode to 256color mode​ ​[ ] set -g default-terminal ​\u0026quot;screen-256color\u0026quot;​ ​[ ]\n​[ ] ​# set the status line's colors​ ​[ ] set -g status-style fg=white,bg=black ​[ ]\n​[ ] ​# set the color of the window list​ ​[ ] setw -g window-status-style fg=cyan,bg=black ​[ ]\n​[ ] ​# set colors for the active window​ ​[ ] setw -g window-status-current-style fg=white,bold,bg=red ​[ ]\n​[ ] ​# colors for pane borders​ ​[ ] setw -g pane-border-style fg=green,bg=black ​[ ] setw -g pane-border-active-style fg=white,bg=yellow ​[ ]\n​[ ] ​# active pane normal, other shaded out​ ​[ ] setw -g window-style fg=colour240,bg=colour235 ​[ ] setw -g window-active-style fg=white,bg=black ​[ ]\n​[ ] ​# Command / message line​ ​[ ] setw -g message-style fg=white,bold,bg=black ​[ ]\n​[ ] ​# Status line left side to show Session\u0026#x1fa9f;pane​ ​[ ] set -g status-left-length 40 ​[ ] set -g status-left ​\u0026quot;#\n\\[fg=green\\]Session: #S #\n\\[fg=yellow\\]#I #\n\\[fg=cyan\\]#P\u0026quot;​ ​[ ]\n​[ ] ​# Status line right side - 50% | 31 Oct 13:37​ ​[ ] set -g status-right ​\u0026quot;#(~/battery Discharging) | #\n\\[fg=cyan\\]%d %b %R\u0026quot;​ ​[ ]\n​[ ] ​# Update the status line every sixty seconds​ ​[ ] set -g status-interval 60 ​[ ]\n​[ ] ​# Center the window list in the status line​ ​[ ] set -g status-justify centre ​[ ]\n​[ ] ​# enable activity alerts​ ​[ ] setw -g monitor-activity on ​[ ] set -g visual-activity on ​[ ]\n​[ ] ​# enable vi keys.​ ​[ ] setw -g mode-keys vi ​[ ]\n​[ ] ​# shortcut for synchronize-panes toggle​ ​[ ] bind C-s set-window-option synchronize-panes ​[ ]\n​[ ] ​# split pane and retain the current directory of existing pane​ ​[ ] bind _ split-window -v -c ​\u0026quot;#{pane_current_path}\u0026quot;​ ​[ ] bind ​\\* ​split-window -h -c ​*\u0026quot;#{pane_current_path}\u0026quot;​ ​[ ]\n​[ ] ​# Log output to a text file on demand​ ​[ ] bind P pipe-pane -o ​\u0026quot;cat \u0026gt;\u0026gt;~/#W.log\u0026quot;​ ​\\;​ display ​\u0026quot;Toggled logging to ~/#W.log\u0026quot;​ ​[ ] ​#​ ​[ ] ​# Load mac-specific settings​ ​[ ] ​if​-shell ​\u0026quot;uname | grep -q Darwin\u0026quot;​ ​\u0026quot;source-file ~/.tmux.mac.conf\u0026quot;​ ​[ ]\n​[ ] ​# load private settings if they exist​ ​[ ] ​if​-shell ​\u0026quot;\n\\[ -f \\~/.tmux.private\\]\u0026quot;​ ​\u0026quot;source ~/.tmux.private\u0026quot;​ ​[ ]\n​[ ] is_vim=​\u0026quot;ps -o state= -o comm= -t '#{pane_tty}'* ​​*\\​ ​[ ] ​| grep -iqE '^\n\\[\\^TXZ \\]+ +(​​\\\\​​S+​​\\\\​​/)?g?(view|n?vim?x?)(diff)?​$'​\u0026quot;​ ​[ ] bind-key -n C-h ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-h\u0026quot;​ ​\u0026quot;select-pane -L\u0026quot;​ ​[ ] bind-key -n C-j ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-j\u0026quot;​ ​\u0026quot;select-pane -D\u0026quot;​ ​[ ] bind-key -n C-k ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-k\u0026quot;​ ​\u0026quot;select-pane -U\u0026quot;​ ​[ ] bind-key -n C-l ​if​-shell ​\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-l\u0026quot;​ ​\u0026quot;select-pane -R\u0026quot;​ ​[ ] bind-key -n C-​\\* ​​if​-shell ​*\u0026quot;​$is_vim​\u0026quot;​ ​\u0026quot;send-keys C-​​\\\\​​\u0026quot;​ ​\u0026quot;select-pane -l\u0026quot;​ ​[ ]\n​[ ] bind C-l send-keys ​'C-l'​ ​[ ]\n​[ ] set -g @plugin ​'tmux-plugins/tpm'​ ​[ ] set -g @plugin ​'tmux-plugins/tmux-resurrect'​ ​[ ] run ​'~/.tmux/plugins/tpm/tpm'*​\n","externalUrl":null,"permalink":"/tech/tools/tmuxbook/","section":"Teches","summary":"\u003cp\u003eLinks:\n\u003ca\n  href=\"https://git.jharmison.com/james/dotfiles/src/branch/master/.tmux.conf\"\n    target=\"_blank\"\n  \u003ehttps://git.jharmison.com/james/dotfiles/src/branch/master/.tmux.conf\u003c/a\u003e\n\u003ca\n  href=\"https://www.youtube.com/watch?v=nTqu6w2wc68\"\n    target=\"_blank\"\n  \u003ehttps://www.youtube.com/watch?v=nTqu6w2wc68\u003c/a\u003e\n\u003ca\n  href=\"https://www.youtube.com/watch?v=DzNmUNvnB04\"\n    target=\"_blank\"\n  \u003ehttps://www.youtube.com/watch?v=DzNmUNvnB04\u003c/a\u003e\n\u003ca\n  href=\"https://www.youtube.com/watch?v=Yl7NFenTgIo\"\n    target=\"_blank\"\n  \u003ehttps://www.youtube.com/watch?v=Yl7NFenTgIo\u003c/a\u003e\n\u003ca\n  href=\"https://github.com/alacritty/alacritty\"\n    target=\"_blank\"\n  \u003ehttps://github.com/alacritty/alacritty\u003c/a\u003e\n\u003ca\n  href=\"https://github.com/rothgar/awesome-tmux\"\n    target=\"_blank\"\n  \u003ehttps://github.com/rothgar/awesome-tmux\u003c/a\u003e\n\u003ca\n  href=\"https://www.youtube.com/watch?v=GH3kpsbbERo\"\n    target=\"_blank\"\n  \u003ehttps://www.youtube.com/watch?v=GH3kpsbbERo\u003c/a\u003e\n\u003ca\n  href=\"https://git.jharmison.com/james/dotfiles/src/branch/master/.config/alacritty/alacritty.toml#L41\"\n    target=\"_blank\"\n  \u003ehttps://git.jharmison.com/james/dotfiles/src/branch/master/.config/alacritty/alacritty.toml#L41\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1 class=\"relative group\"\u003etmux 2\n    \u003cdiv id=\"tmux-2\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#tmux-2\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\n\u003ch3 class=\"relative group\"\u003eProductive Mouse-Free Development\n    \u003cdiv id=\"productive-mouse-free-development\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#productive-mouse-free-development\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eeasily manage a text editor, a database console, and a local web server within a single environment.\u003c/p\u003e","title":"","type":"tech"},{"content":"We are sick of manually setting up virtual machines and configuring them. You can use Ansible to automate the configuration of a server. But what about setting up a server before it\u0026rsquo;s configured? Let\u0026rsquo;s stop doing that manually as well by using Vagrant.\nAlso, by setting up your infrastructure as code, you get to set up machines quickly and consistently manage and deploy your servers. This reduces the time it takes, operating costs, and minimizes the chance of configuration errors.\nI just spent a full hour setting up and Ansible lab by manually attaching an installation disk, configuring IPs, selectng software, setting up accounts, etc.\nVagrant uses a configuration file called a vagrantfile. This file has all of the information needed to deploy a new virtual machine. Let\u0026rsquo;s install Vagrant on Fedora:\nsudo dnf install -y dnf-plugins-core sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo sudo dnf -y install vagrant Install remaining dependencies and plugin:\nsudo dnf install --assumeyes libvirt libguestfs-tools \\ gcc libvirt-devel libxml2-devel make ruby-devel vagrant plugin install vagrant-libvirt You will also need virt-manager, libvirt, and qemu for this tutorial. First, we need a plugin that will let Vagrant interact with our VM provider. Install libvirt plugin to do this: vagrant plugin install vagrant-libvirt\nLibvirt documenttation can be found here.\nAnd another plugin to manage VM disk sizes: vagrant plugin install vagrant-disksize\ncd ~/Documents/projects\nSetting up a Git repository for version control and sharing. # In Github, make a new repository, I will call mine \u0026ldquo;vagrant-vms\u0026rdquo;. Then clone the repo to your local host:\ncd ~/Documents/projects git clone https://github.com/linuxreader/vagrant-vms.git cd vagrant-vms Vagrantfile # A vagrantfile descibes how to build and provision a VM. A vagrant file is written in Ruby. To make a vagrant file, you need to add a box using the vagrant command, then initialize the box. Vagrant boxes are OS images that Vagrant provides here.\ndavid@fedora:~/Documents/projects/vagrant-vms$mkdir ansible-lab david@fedora:~/Documents/projects/vagrant-vms$ cd ansible-lab david@fedora:~/Documents/projects/vagrant-vms/ansible-lab$ vagrant box add almalinux/9 ==\u0026gt; box: Loading metadata for box \u0026#39;almalinux/9\u0026#39; box: URL: https://vagrantcloud.com/api/v2/vagrant/almalinux/9 This box can work with multiple providers! The providers that it can work with are listed below. Please review the list and choose the provider you will be working with. 1) hyperv 2) libvirt 3) parallels 4) virtualbox 5) vmware_desktop Enter your choice: 2 ==\u0026gt; box: Adding box \u0026#39;almalinux/9\u0026#39; (v9.5.20241203) for provider: libvirt box: Downloading: https://vagrantcloud.com/almalinux/boxes/9/versions/9.5.20241203/providers/libvirt/amd64/vagrant.box box: Calculating and comparing box checksum... ==\u0026gt; box: Successfully added box \u0026#39;almalinux/9\u0026#39; (v9.5.20241203) for \u0026#39;libvirt\u0026#39;! Then, initialize the box:\n$ vagrant init almalinux/9 A `Vagrantfile` has been placed in this directory. You are now ready to `vagrant up` your first virtual environment! Please read the comments in the Vagrantfile as well as documentation on `vagrantup.com` for more information on using Vagrant. This will place a vagrant file into the current directory:\n$ ls Vagrantfile $ cat Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below. The \u0026#34;2\u0026#34; in Vagrant.configure # configures the configuration version (we support older styles for # backwards compatibility). Please don\u0026#39;t change it unless you know what # you\u0026#39;re doing. Vagrant.configure(\u0026#34;2\u0026#34;) do |config| # The most common configuration options are documented and commented below. # For a complete reference, please see the online documentation at # https://docs.vagrantup.com. # Every Vagrant development environment requires a box. You can search for # boxes at https://vagrantcloud.com/search. config.vm.box = \u0026#34;almalinux/9\u0026#34; # Disable automatic box update checking. If you disable this, then # boxes will only be checked for updates when the user runs # `vagrant box outdated`. This is not recommended. # config.vm.box_check_update = false # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine. In the example below, # accessing \u0026#34;localhost:8080\u0026#34; will access port 80 on the guest machine. # NOTE: This will enable public access to the opened port # config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 80, host: 8080 # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine and only allow access # via 127.0.0.1 to disable public access # config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 80, host: 8080, host_ip: \u0026#34;127.0.0.1\u0026#34; # Create a private network, which allows host-only access to the machine # using a specific IP. # config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.10\u0026#34; # Create a public network, which generally matched to bridged network. # Bridged networks make the machine appear as another physical device on # your network. # config.vm.network \u0026#34;public_network\u0026#34; # Share an additional folder to the guest VM. The first argument is # the path on the host to the actual folder. The second argument is # the path on the guest to mount the folder. And the optional third # argument is a set of non-required options. # config.vm.synced_folder \u0026#34;../data\u0026#34;, \u0026#34;/vagrant_data\u0026#34; # Provider-specific configuration so you can fine-tune various # backing providers for Vagrant. These expose provider-specific options. # Example for VirtualBox: # # config.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| # # Display the VirtualBox GUI when booting the machine # vb.gui = true # # # Customize the amount of memory on the VM: # vb.memory = \u0026#34;1024\u0026#34; # end # # View the documentation for the provider you are using for more # information on available options. # Enable provisioning with a shell script. Additional provisioners such as # Ansible, Chef, Docker, Puppet and Salt are also available. Please see the # documentation for more information about their specific syntax and use. # config.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL # apt-get update # apt-get install -y apache2 # SHELL end For this project, we are going to set up an Ansible lab. With 1 control VM, and 2 host VMs.\nControl # name: control Ip: 192.168.124.200 RAM: 2048 Storage: 20GB\nI could not find a way to increase the initial disk capacity but there is a guide on changing the disk size after it\u0026rsquo;s been made: https://nullr0ute.com/2018/08/increasing-a-libvirt-kvm-virtual-machine-disk-capacity/\nAdd an additional disk\nlibvirt.storage :file, :size =\u0026gt; \u0026#39;20G\u0026#39; You can use either the command line option --provider=kvm or you can set the VAGRANT_DEFAULT_PROVIDER environment variable:\nexport VAGRANT_DEFAULT_PROVIDER=libvirt # \u0026lt;-- may be in ~/.profile, /etc/profile, or elsewhere vagrant up ","externalUrl":null,"permalink":"/tech/tools/vagrant-what-is-it-for/","section":"Teches","summary":"\u003cp\u003eWe are sick of manually setting up virtual machines and configuring them. You can use Ansible to automate the configuration of a server. But what about setting up a server before it\u0026rsquo;s configured? Let\u0026rsquo;s stop doing that manually as well by using Vagrant.\u003c/p\u003e","title":"","type":"tech"},{"content":"","externalUrl":null,"permalink":"/tech/tools/vm-creation-using-ansible/","section":"Teches","summary":"","title":"","type":"tech"},{"content":"https://github.com/VSCodium VScode but without the Microsoft Telemetry\nSee section about migrating from VScod to VS codium https://github.com/VSCodium/vscodium/blob/a02839b466d8d01b8a61ea9611f0b74039538eae/DOCS.md#migrating-from-visual-studio-code-to-vscodium.\nIt pulls from this server rather than Microsoft\u0026rsquo;s: https://open-vsx.org/. And you can manually download the extensions from microsoft https://marketplace.visualstudio.com/vscode if they haven\u0026rsquo;t been added to open-vsx.org yet.\n","externalUrl":null,"permalink":"/tech/tools/vs_codium/","section":"Teches","summary":"\u003cp\u003e\u003ca\n  href=\"https://github.com/VSCodium\"title=\"https://github.com/VSCodium\"\n    target=\"_blank\"\n  \u003ehttps://github.com/VSCodium\u003c/a\u003e VScode but without the Microsoft Telemetry\u003c/p\u003e\n\u003cp\u003eSee section about migrating from VScod to VS codium\n\u003ca\n  href=\"https://github.com/VSCodium/vscodium/blob/a02839b466d8d01b8a61ea9611f0b74039538eae/DOCS.md#migrating-from-visual-studio-code-to-vscodium\"title=\"https://github.com/VSCodium/vscodium/blob/a02839b466d8d01b8a61ea9611f0b74039538eae/DOCS.md#migrating-from-visual-studio-code-to-vscodium\"\n    target=\"_blank\"\n  \u003ehttps://github.com/VSCodium/vscodium/blob/a02839b466d8d01b8a61ea9611f0b74039538eae/DOCS.md#migrating-from-visual-studio-code-to-vscodium\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIt pulls from this server rather than Microsoft\u0026rsquo;s: \u003ca\n  href=\"https://open-vsx.org/\"title=\"https://open-vsx.org/\"\n    target=\"_blank\"\n  \u003ehttps://open-vsx.org/\u003c/a\u003e. And you can manually download the extensions from microsoft \u003ca\n  href=\"https://marketplace.visualstudio.com/vscode\"title=\"https://marketplace.visualstudio.com/vscode\"\n    target=\"_blank\"\n  \u003ehttps://marketplace.visualstudio.com/vscode\u003c/a\u003e if they haven\u0026rsquo;t been added to \u003ca\n  href=\"http://open-vsx.org/\"title=\"http://open-vsx.org/\"\n    target=\"_blank\"\n  \u003eopen-vsx.org\u003c/a\u003e yet.\u003c/p\u003e","title":"","type":"tech"},{"content":"https://awx.wiki/installation\nhttps://github.com/MrMEEE/awx-rpm-v2\nError: fatal: [localhost]: FAILED! =\u0026gt; {\u0026ldquo;msg\u0026rdquo;: \u0026ldquo;Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this. Please add this host\u0026rsquo;s fingerprint to your known_hosts file to manage this host.\u0026rdquo;}\nCreate a file ansible/ansible.cfg in your project directory (i.e. ansible.cfg in the provisioning_path on the target) with the following contents:\n[defaults] host_key_checking = false Error:\nTASK [awx-rpm : Install AWX-RPM] ****************************************************************************************** fatal: [localhost]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;failures\u0026#34;: [\u0026#34;No package awx-rpm available.\u0026#34;], \u0026#34;msg\u0026#34;: \u0026#34;Failed to install some of the specified packages\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;results\u0026#34;: []} ","externalUrl":null,"permalink":"/tech/ansible/awx-rpm-install/","section":"Teches","summary":"\u003cp\u003e\u003ca\n  href=\"https://awx.wiki/installation\"\n    target=\"_blank\"\n  \u003ehttps://awx.wiki/installation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca\n  href=\"https://github.com/MrMEEE/awx-rpm-v2\"\n    target=\"_blank\"\n  \u003ehttps://github.com/MrMEEE/awx-rpm-v2\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eError: fatal: [localhost]: FAILED! =\u0026gt; {\u0026ldquo;msg\u0026rdquo;: \u0026ldquo;Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this.  Please add this host\u0026rsquo;s fingerprint to your known_hosts file to manage this host.\u0026rdquo;}\u003c/p\u003e","title":" AWX RPM Install","type":"tech"},{"content":"Ad hoc commands are ansible tasks you can run against managed hosts without the need of a playbook or script. These are used for bringing nodes to their desired states, verifying playbook results, and verifying nodes meet any needed criteria/pre-requisites. These must be ran as the ansible user (whatever your remote_user directive is set to under \\[defaults\\] in ansible.cfg)\nRun the user module with the argument name=lisa on all hosts to make sure the user \u0026ldquo;lisa\u0026rdquo; exists. If the user doesn\u0026rsquo;t exist, it will be created on the remote system: ansible all -m user -a \u0026quot;name=lisa\u0026quot;\n{command} {host} -m {module} -a {\u0026quot;argument1 argument2 argument3\u0026quot;}\nIn our lab:\nansible all -m user -a \u0026#34;name=lisa\u0026#34; This Ad Hoc command created user \u0026ldquo;Lisa\u0026rdquo; on ansible1 and ansible2. If we run the command again, we get \u0026ldquo;SUCCESS\u0026rdquo; on the first line instead of \u0026ldquo;CHANGED\u0026rdquo;. Which means the hosts already meet the requirements:\n[ansible@control base]$ ansible all -m user -a \u0026#34;name=lisa\u0026#34; indempotent Regardless of current condition, the host is brought to the desired state. Even if you run the command multiple times.\nRun the command id lisa on all managed hosts:\n[ansible@control base]$ ansible all -m command -a \u0026#34;id lisa\u0026#34; Here, the command module is used to run a command on the specified hosts. And the output is displayed on screen. To note, this does not show up in our ansible user\u0026rsquo;s command history on the host:\n[ansible@ansible1 ~]$ history Remove the user lisa from all managed hosts:\n[ansible@control base]$ ansible all -m user -a \u0026#34;name=lisa state=absent\u0026#34; You can also use the -u option to specify the Ansible user that Ansible will use to run the command. Remember, with no modules specified, ansible uses the command module: ansible all -a \u0026quot;free -m\u0026quot; -u david `\nAd hoc commands in Scripts # Follow normal bash scripting guidelines to run ansible commands in a script:\n[ansible@control base]$ vim httpd-ansible.sh Let\u0026rsquo;s set up a script that installs and starts/enables httpd, creates a user called \u0026ldquo;anna\u0026rdquo;, and copies the ansible control node\u0026rsquo;s /etc/hosts file to /tmp/ on the managed nodes:\n#!/bin/bash ansible all -m yum -a \u0026#34;name=httpd state=latest\u0026#34; ansible all -m service -a \u0026#34;name=httpd state=started enabled=yes\u0026#34; ansible all -m user -a \u0026#34;name=anna\u0026#34; ansible all -m copy -a \u0026#34;src=/etc/hosts dest=/tmp/hosts\u0026#34; [ansible@control base]$ chmod +x httpd-ansible.sh [ansible@control base]$ ./httpd-ansible.sh web2 | UNREACHABLE! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Failed to connect to the host via ssh: ssh: Could not resolve hostname web2: Name or service not known\u0026#34;, \u0026#34;unreachable\u0026#34;: true } web1 | UNREACHABLE! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Failed to connect to the host via ssh: ssh: Could not resolve hostname web1: Name or service not known\u0026#34;, \u0026#34;unreachable\u0026#34;: true } ansible1 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python3\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Nothing to do\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;results\u0026#34;: [] } ansible2 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python3\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Nothing to do\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;results\u0026#34;: [] } ... \u0026lt;-- Results truncated And from the ansible1 node we can verify:\n[ansible@ansible1 ~]$ cat /etc/passwd | grep anna anna:x:1001:1001::/home/anna:/bin/bash [ansible@ansible1 ~]$ cat /tmp/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.124.201 ansible1 192.168.124.202 ansible2 View a file from a managed node: ansible ansible1 -a \u0026quot;cat /somfile.txt\u0026quot;\n","externalUrl":null,"permalink":"/tech/ansible/ad-hoc-ansible-commands/","section":"Teches","summary":"How to use Ad-Hoc Ansible Commands","title":"Ad Hoc Ansible Commands","type":"tech"},{"content":" Permission Classes and Types # Permission classes\nuser (u) group (g) other (o) (public) all (a) \u0026lt;- all combined Permission types\nr,w,x works differently on files and directories hyphen (-) represents no permissions set ls results permissions groupings # rwx rw- r\u0026ndash; user (owner), group, and other (public) ls results first character meaning # regular file d directory l symbolic link c character device file b block device file p named pipe s socket Modifying Access Permission Bits # chmod command # Modify permissions using symbolic or octal notation. Used by root or the file owner. Flags chmod -v ::: Verbose.\nSymbolic notation # Letters (ugo/rwx) and symbols (+, -, =) used to add, revoke, or assign permission bits. Octal Notation # Three-digit numbering system ranging from 0 to 7. 0 \u0026mdash; 1 \u0026ndash;x 2 -w- 3 -wx 4 r\u0026ndash; 5 r-x 6 rw- 7 rwx\nDefault Permissions # Calculated based on the umask (user mask) value subtracted from the initial permissions value. umask # Three-digit value (octal or symbolic) that refers to read, write, and execute permissions for owner, group, and public. Default umask value is 0022 for the root user and 0002 normal users. The left-most 0 has no significance. If umask is set to 000 files will get max of 666 If the initial permissions are 666 and the umask is 002 then the default permissions are 664. (666-002) Any new files or directories created after changing the umask will have the new default permissions set. umask settings are lost when you log off. Add it to the appropriate startup file to make it permanent. Defaults\nfiles 666 rw-rw-rw- directories 777 rwxrwxrwx umask command # Options\n-S symbolic form Special Permission Bits # 3 types of special permission bits for executable files or directories for non root users setuid setgid sticky setuid set on exe\u0026rsquo;s to provide non-owners the ability to run them with the privileges of the owning user may be set on directories and files but will have no effect. example: the su command shows an \u0026rsquo;s\u0026rsquo; in ls -l listing at the end of owners permissions If the file already has the “x” bit set for the user, the long listing will show a lowercase “s”, otherwise it will list it with an uppercase “S”. setgid set on exe\u0026rsquo;s to provide non-group members the ability to run them with the privileges of the owning group. May be set on shared directories allow files and subdirectories created underneath to automatically inherit the directory’s owning group. saves group members who are sharing the directory contents from changing the group ID for every new file and subdirectory that they add. write command has this set by default so a member of the tty group can run it. If the file already has the “x” bit set for the group, the long listing will show a lowercase “s”, otherwise it will list it with an uppercase “S”. Sticky bit may be set on public directories for inhibiting file deletion by non-owners may be set on directories and files but will have no effect. Set on /tmp and /var/tmp by default Letter \u0026ldquo;t\u0026rdquo; in other permission feild If the directory already has the “x” bit set for public, the long listing will show a lowercase “t”, otherwise it will list it with an uppercase “T”. Access Control Lists (ACLs) # Setting a default ACL on a directory allows content sharing among user\u0026rsquo;s without having to modify access on each new file and subdirectory.\nExtra permissions that can be set on files and directories.\nDefine permissions for named user and named groups.\nConfigured the same way on both files and directories.\nNamed Users\nMay or may not be a part of the same group. 2 different groups of ACLs. Default ACLs and Access ACLs.\nAccess ACLs Set on individual files and directories Default ACLs Applied on directories files and subdirectories inherit the ACL Execute bit must be set on the directory for public. Files receive the shared directory\u0026rsquo;s default ACLs as their access ACLs - what the mask limits. Subdirectories receive both default ACLs and access ACLs as they are. A \u0026ldquo;+\u0026rdquo; at the end of ls -l listing indicates ACL is set\n-rw-rw-r\u0026ndash;+ ACL Commands # getfacl\nDisplay ACL settings Displays: name of file owner owning group Permissions colon characters save space for named user/group (or UID/GID) when extended Permissions are set. Example: user:1000:r\u0026ndash; the named user with UID 1000, who is neither the file owner nor a member of the owning group, is allowed read-only access to this file. Example: group:dba:rw- give the named group (dba) read and write access to the file. setfacl set, modify, substitute, or delete ACL settings If you want to give read and write permissions to a specific user (user1) and change the mask to read-only at the same time, the setfacl command will allocate the permissions as mentioned; however, the effective permissions for the named user will only be read-only. u:UID:perms\nnamed user must exist in /etc/passwd if no user specified, permissions are given to the owner of the file/directory g:GID:perms\nNamed group must exist in /etc/group If no group specified, permissions are given to the owning group of the file/directory o:perms\nNeither owner or owning group m:perms\nMaximum permissions for named user or named group Switches\nSwitch Description -b Remove all Access ACLs -d Applies to default ACLs -k Removes all default ACLs -m Sets or modifies ACLs -n Prevent auto mask recalculation -R Apply Recursively to directory -x Remove Access ACL -c Display output without header Mask Value # Determine maximum allowable permissions for named user or named group Mask value displayed on separate line in getfacl output Mask is recalculated every time an ACL is modified unless value is manually entered. Overrides the set ACL value. Find Command # Search files and display the full path. Execute command on search results. Different search criteria name part name ownership owning group permissions inode number last access modification time in days or minutes size file type Command syntax {find} + {path} + {search option} + {action} Options -name / -iname (search by name) -user / -group (UID / GID) -perm (permissions) -inum (inode) -atime/amin (access time) -mtime/amin (modification time) -size / -type (size / type) Action copy, erase, rename, change ownership, modify permissions -exec {} \\; replaces {} for each filename as it is found. The semicolon character (;) marks the termination of the command and it is escaped with the backslash character (\\). -ok {} \\; same as exec but requires confirmation. -delete -print \u0026lt;- default Advanced File Management Labs # Lab: find stuff # Create file 10 and search for it. [vagrant@server1 ~]$ sudo touch /root/file10 [vagrant@server1 ~]$ sudo find / -name file10 -print /root/file10 Perform a case insensitive search for files and directories in /dev that begin with \u0026ldquo;usb\u0026rdquo; followed by any characters. [vagrant@server1 ~]$ find /dev -iname usb* /dev/usbmon0 Find files smaller than 1MB (-1M) in size (-size) in the root user’s home directory (~). [vagrant@server1 etc]$ find ~ -size -1M Search for files larger than 40MB (+40M) in size (-size) in the /usr directory: [vagrant@server1 etc]$ sudo find /usr -size +40M /usr/share/GeoIP/GeoLite2-City.b Find files in the entire root file system (/) with ownership (-user) set to user daemon and owning group (-group) set to any group other than (-not or ! for negation) user1: [vagrant@server1 etc]$ sudo find / -user daemon -not -group user1 Search for directories (-type) by the name “src” (-name) in /usr at a maximum of two subdirectory levels below (-maxdepth): [vagrant@server1 etc]$ sudo find /usr -maxdepth 2 -type d -name src /usr/local/src /usr/src Run the above search but at least three subdirectory levels beneath /usr, substitute -maxdepth 2 with -mindepth 3. [vagrant@server1 etc]$ sudo find /usr -mindepth 3 -type d -name src /usr/src/kernels/4.18.0-425.3.1.el8.x86_64/drivers/gpu/drm//display/dmub/src /usr/src/kernels/4.18.0-425.3.1.el8.x86_64/tools/usb/usbip/src Find files in the /etc directory that were modified (-mtime) more than (the + sign) 2000 days ago: [vagrant@server1 etc]$ sudo find /etc -mtime +2000 /etc/libuser.conf /etc/xattr.conf /etc/whois.conf Run the above search for files that were modified exactly 12 days ago, replace “+2000” with “12”. [vagrant@server1 etc]$ sudo find /etc -mtime 12 To find files in the /var/log directory that have been modified (-mmin) in the past (the - sign) 100 minutes: [vagrant@server1 etc]$ sudo find /var/log -mmin -100 /var/log/rhsm/rhsmcertd.log /var/log/rhsm/rhsm.log /var/log/audit/audit.log /var/log/dnf.librepo.log /var/log/dnf.rpm.log /var/log/sa /var/log/sa/sa16 /var/log/sa/sar15 /var/log/dnf.log /var/log/hawkey.log /var/log/cron /var/log/messages /var/log/secure Run the above search for files that have been modified exactly 25 minutes ago, replace “-100” with “25”. [vagrant@server1 etc]$ sudo find /var/log -mmin 25 To search for block device files (-type) in the /dev directory with permissions (-perm) set to exactly 660: [vagrant@server1 etc]$ sudo find /dev -type b -perm 660 /dev/dm-1 /dev/dm-0 /dev/sda2 /dev/sda1 /dev/sda Search for character device files (-type) in the /dev directory with at least (-222) world writable permissions (this example would ignore checking the write and execute permissions): [vagrant@server1 etc]$ sudo find /dev -type c -perm -222 Find files in the /etc/systemd directory that are executable by at least their owner or group members: [vagrant@server1 etc]$ sudo find /etc/systemd -perm /110 Search for symlinked files (-type) in /usr with permissions (-perm) set to read and write for the owner and owning group: sudo find /usr -type l -perm -ug=rw Search for directories in the entire directory tree (/) by the name “core” (-name) and list them (ls-ld) as they are discovered without prompting for user confirmation (-exec): [vagrant@server1 etc]$ sudo find / -name core -exec ls -ld {} \\; Use the -ok switch to prompt for confirmation before it copies each matched file (-name) in /etc/sysconfig to /tmp: sudo find /etc/sysconfig -name \u0026#39;*.conf\u0026#39; -ok cp {} /tmp \\; Lab: Display ACL and give permissions # Create and empty file aclfile1 in /tmp and display ACLs on it: cd /tmp touch aclfile1 getfacl aclfile1 Give rw permission to user 1 but with a mask of read only and view the results. setfacl -m u:user1:rw,m:r aclfile1 Promote the mask value to include write bit and verify: setfacl -m m:rw aclfile1 getfacl -c aclfile1 Lab: Identify, Apply, and Erase Access ACLs # Switch to user1 and create file acluser1 in /tmp: su - user1 cd /tmp touch acluser1 Use ls and getfacl to check existing acl entries: ls -l acluser1 getfacl acluser1 -c Allocate rw permissions to user100 with setfacl in octal form: setfacl -m u:user100:6 acluser1 Run ls (+) and getfacl to verify: ls -l acluser1 getfacl -c acluser1 Open another terminal as user100 and open the file and edit it.\nAdd user200 with full rwx permissions to acluser1 using the symbolic notation and then show the updated ACL settings:\nsetfacl -m u:user200:rwx acluser1 getfacl -c acluser1 Delete the ACL entries set for user200 and validate: setfacl -x u:user200 acluser1 getfacl acluser1 -c Delete the rest of the ACLs: setfacl -b acluser1 Use the ls and getfacl commands and confirm for the ACLs removal: ls -l acluser1 getfacl acluser1 -c create group aclgroup1 groupadd -g 8000 aclgroup1 add this group as a named group along with the two named users (user100 and user200). Lab: Apply, Identify, and erase default ACLs # Switch or log in as user1 and create a directory projects in /tmp: su - user1 cd /tmp mkdir projects Use the getfacl command for an initial look at the permissions on the directory: getfacl -c projects Allocate default read, write, and execute permissions to user100 and user200 on the directory. Use both octal and symbolic notations and the -d (default) option with the setfacl command. setfacl -dm u:user100:7,u:user200:rwx projects/ getfacl -c projects/ Create a subdirectory prjdir1 under projects and observe the ACL inheritance: mkdir prjdir1 getfacl -c prjdir1 Create a file prjfile1 under projects and observe the ACL inheritance: touch prjfile1 getfacl -c prjfilel log in as one of the named users, change directory into /tmp/projects, and edit prjfile1 (add some random text). Then change into the prjdir1 and create file file100. su - user100 cd /tmp/projects vim prjfile1 ls -l prjfile1 cd prjdir1 touch file100 pwd Delete all the default ACLs from the projects directory as user1 and confirm: exit su - user1 cd /tmp setfacl -k projects getfacl -c projects create a group such as aclgroup2 by running groupadd -g 9000 aclgroup2 as the root user and repeat this exercise by adding this group as a named group along with the two named users (user100 and user200). Lab: Modify Permission Bits Using Symbolic Form # Add an execute bit for the owner and a write bit for group and public [vagrant@server1 ~]$ chmod u+x permfile1 -v mode of \u0026#39;permfile1\u0026#39; changed from 0444 (r--r--r--) to 0544 (r-xr--r--) [vagrant@server1 ~]$ chmod -v go+w permfile1 mode of \u0026#39;permfile1\u0026#39; changed from 0544 (r-xr--r--) to 0566 (r-xrw-rw-) Revoke the write bit from public [vagrant@server1 ~]$ chmod -v o-w permfile1 mode of \u0026#39;permfile1\u0026#39; changed from 0566 (r-xrw-rw-) to 0564 (r-xrw-r--) [vagrant@server1 ~]$ chmod -v a=rwx permfile1 mode of \u0026#39;permfile1\u0026#39; changed from 0564 (r-xrw-r--) to 0777 (rwxrwxrwx) Revoke write from the owning group and write and execute bits from public. [vagrant@server1 ~]$ chmod g-w,o-wx permfile1 -v mode of \u0026#39;permfile1\u0026#39; changed from 0777 (rwxrwxrwx) to 0754 (rwxr-xr--) Lab: Modify Permission Bits Using Octal Form # Read only for user, group, and other: [vagrant@server1 ~]$ touch permfile2 [vagrant@server1 ~]$ chmod 444 permfile2 [vagrant@server1 ~]$ ls -l permfile2 -r--r--r--. 1 vagrant vagrant 0 Feb 4 12:22 permfile2 Add an execute bit for the owner: [vagrant@server1 ~]$ chmod -v 544 permfile2 mode of \u0026#39;permfile2\u0026#39; changed from 0444 (r--r--r--) to 0544 (r-xr--r--) Add a write permission bit for group and public: [vagrant@server1 ~]$ chmod -v 566 permfile2 mode of \u0026#39;permfile2\u0026#39; changed from 0544 (r-xr--r--) to 0566 (r-xrw-rw-) Revoke the write bit for public: [vagrant@server1 ~]$ chmod -v 564 permfile2 mode of \u0026#39;permfile2\u0026#39; changed from 0566 (r-xrw-rw-) to 0564 (r-xrw-r--) Assign read, write, and execute permission bits to all three user categories: [vagrant@server1 ~]$ chmod -v 777 permfile2 mode of \u0026#39;permfile2\u0026#39; changed from 0564 (r-xrw-r--) to 0777 (rwxrwxrwx) Run the umask command without any options and it will display the current umask value in octal notation: [vagrant@server1 ~]$ umask 0002 Symbolic form [vagrant@server1 ~]$ umask -S u=rwx,g=rwx,o=rx Set all new files and directories to get 640 and 750 permissions, umask 027 umask u=rwx,g=rx,o= Test new umask (666-027=640) (777-027=750) [vagrant@server1 ~]$ touch tempfile1 [vagrant@server1 ~]$ ls -l tempfile1 -rw-r-----. 1 vagrant vagrant 0 Feb 5 12:09 tempfile1 [vagrant@server1 ~]$ mkdir tempdir1 [vagrant@server1 ~]$ ls -ld tempdir1 drwxr-x---. 2 vagrant vagrant 6 Feb 5 12:10 tempdir1 Lab: View suid bit on su command # [vagrant@server1 ~]$ ls -l /usr/bin/su -rwsr-xr-x. 1 root root 50152 Aug 22 10:08 /usr/bin/su Lab: Test the Effect of setuid Bit on Executable Files # Open 2 terminal windows. Switch to user1 in terminal1 [vagrant@server1 ~]$ su - user1 Password: Last login: Sun Feb 5 12:37:12 UTC 2023 on pts/1 Switch to root on terminal2 sudo su - root T1 Revoke the setuid bit from /usr/bin/su chmod -v u-s /usr/bin/su T2 log off as root ctrl+d Try to log in has root from both terminals [user1@server1 ~]$ su - root Password: su: Authentication failure T1 restore the setuid bit [vagrant@server1 ~]$ sudo chmod -v +4000 /usr/bin/su mode of \u0026#39;/usr/bin/su\u0026#39; changed from 0755 (rwxr-xr-x) to 4755 (rwsr-xr-x) Lab: Test the Effect of setgid Bit on Executable Files # Log into two terminals T1 root T2 user1 Opened with ssh\nT2 list users currently logged in\nwho T2 send a message to root write root T1 revoke setgid from /usr/bin/write chmod g-s /usr/bin/write -v Try to write root [user1@server1 ~]$ write root write: effective gid does not match group of /dev/pts/0 Restore the setgid bit on /usr/bin/write: [root@server1 ~]# sudo chmod -v +2000 /usr/bin/write mode of \u0026#39;/usr/bin/write\u0026#39; changed from 0755 (rwxr-xr-x) to 2755 (rwxr-sr-x) Test write root Lab: Set up Shared Directory for Group Collaboration # set up 2 test users [root@server1 ~]# adduser user100 [root@server1 ~]# adduser user200 Add group sgrp with GID 9999 with the groupadd command: [root@server1 ~]# groupadd -g 9999 sgrp Add user100 and user200 as members to sgrp using the usermod command: [root@server1 ~]# usermod -aG sgrp user100 [root@server1 ~]# usermod -aG sgrp user200 Create /sdir directory [root@server1 ~]# mkdir /sdir Set ownership and owning group on /sdir to root and sgrp, using the chown command: [root@server1 ~]# chown root:sgrp /sdir Set the setgid bit on /sdir using the chmod command: [vagrant@server1 ~]$ sudo chmod g+s /sdir Add write permission to the group members on /sdir and revoke all permissions from public: [root@server1 ~]# chmod g+w,o-rx /sdir Verify [root@server1 ~]# ls -ld /sdir drwxrws---. 2 root sgrp 6 Feb 13 15:49 /sdir Switch or log in as user100 and change to the /sdir directory: [root@server1 ~]# su - user100 [user100@server1 ~]$ cd /sdir Create a file and check the owner and owning group on it: [user100@server1 sdir]$ touch file100 [user100@server1 sdir]$ ls -l file100 -rw-rw-r--. 1 user100 sgrp 0 Feb 10 22:41 file100 Log out as user100, and switch or log in as user200 and change to the /sdir directory: [root@server1 ~]# su - user200 [user200@server1 ~]$ cd /sdir Create a file and check the owner and owning group on it: [user200@server1 sdir]$ touch file200 [user200@server1 sdir]$ ls -l file200 -rw-rw-r--. 1 user200 sgrp 0 Feb 13 16:01 file200 Lab: View \u0026ldquo;t\u0026rdquo; in permissions for sticky bit # [user200@server1 sdir]$ ls -l /tmp /var/tmp -d drwxrwxrwt. 8 root root 185 Feb 13 16:12 /tmp drwxrwxrwt. 4 root root 113 Feb 13 16:00 /var/tmp Lab: Test the effect of Sticky Bit # Switch to user100 and change to the /tmp directory [user100@server1 sdir]$ cd /tmp Create file called stckyfile [user100@server1 tmp]$ touch stickyfile Try to delete the file as user200 [user200@server1 tmp]$ rm stickyfile rm: remove write-protected regular empty file \u0026#39;stickyfile\u0026#39;? y rm: cannot remove \u0026#39;stickyfile\u0026#39;: Operation not permitted Revoke the /tmp stickybit and confirm [vagrant@server1 ~]$ sudo chmod o-t /tmp [vagrant@server1 ~]$ ls -ld /tmp drwxrwxrwx. 8 root root 4096 Feb 13 22:00 /tmp Retry the removal as user200 rm stickyfile Restore the sticky bit on /tmp sudo chmod -v +1000 /tmp Lab: Manipulate File Permissions (user1) # Create file file11 and directory dir11 in the home directory. Make a note of the permissions on them. touch file11 mkdir dir11 Run the umask command to determine the current umask. umask Change the umask value to 0035 using symbolic notation. umask g=r,0=w Create file22 and directory dir22 in the home directory. touch file22 mkdir dir22 Observe the permissions on file22 and dir22, and compare them with the permissions on file11 and dir11. ls -l Use the chmod command and modify the permissions on file11 to match those on file22. chmod g-w,o-r,o+w file11 Use the chmod command and modify the permissions on dir22 to match those on dir11. Do not remove file11, file22, dir11, and dir22 yet. chmod g-wx,o-rx,o+w dir11 Lab: Configure Group Collaboration and Prevent File Deletion (root) # create directory /sdir. Create group sgrp and create user1000 and user2000 and add them to the group: mkdir /sdir groupadd sgrp adduser user1000 \u0026amp;\u0026amp; adduser user2000 usermod -a -G sgrp user1000 usermod -a -G sgrp user2000 Set up appropriate ownership (root), owning group (sgrp), and permissions (rwx for group, \u0026mdash; for public, s for group, and t for public) on the directory to support group collaboration and ensure non-owners cannot delete files. chgrp sgrp sdir chmod g=rwx,o=--- sdir chmod o+t sdir chmod g+s sdir Log on as user1000 and create a file under /sdir. su - user1000 cd /sdir touch testfile Log on as user2000 and try to edit that file. You should be able to edit the file successfully. su - user200 cd /sdir vim testfile cat testfile As user2000 try to delete the file. You should not be able to. rm testfile Lab: Find Files (root) # Search for all files in the entire directory structure that have been modified in the last 300 minutes and display their type. find /sdir -mtime -300 -exec file {} \\; Search for named pipe and socket files. find / -type p find / -type s Lab: Find Files Using Different Criteria (root) # Search for regular files under /usr that were accessed more than 100 days ago, are not bigger than 5MB in size, and are owned by the user root. find /usr -type f -mtime +100 -size -5M -user root Lab: Apply ACL Settings (root) # Create file testfile under /tmp. touch /tmp/testfile Create users. adduser user2000 adduser user3000 adduser user4000 Apply ACL settings on the file so that user2000 gets 7, user3000 gets 6, and user4000 gets 4 permissions. setfacl -m u:user2000:7 testfile setfacl -m u:user3000:6 testfile setfacl -m u:user4000:4 testfile Remove the ACLs for user2000, and verify. setfacl -x user2000 testfile getfacl testfile Erase all remaining ACLs at once, and confirm. setfacl -b testfile getfacl testfile ","externalUrl":null,"permalink":"/tech/linux/advanced-file-management/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003ePermission Classes and Types\n    \u003cdiv id=\"permission-classes-and-types\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#permission-classes-and-types\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePermission classes\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003euser (u)\u003c/li\u003e\n\u003cli\u003egroup (g)\u003c/li\u003e\n\u003cli\u003eother (o) (public)\u003c/li\u003e\n\u003cli\u003eall (a) \u0026lt;- all combined\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePermission types\u003c/strong\u003e\u003c/p\u003e","title":"Advanced File Management","type":"tech"},{"content":" Package Groups # package group # Group of packages that serve a common purpose. Can query, install, and delete as a single unit rather than dealing with packages individually. Two types of package groups: environment groups and package groups. environment groups available in RHEL 9:\nserver, server with GUI, minimal install, workstation, virtualization host, and custom operating system. Listed on the software selection window during RHEL 9 installation. Package groups include:\ncontainer management, smart card support, security tools, system tools, network servers, etc. Individual packages, package groups, and modules: # Individual Package Management # List, install, query, and remove packages.\nListing Available and Installed Packages # dnf lists available packages as well as installed packages. Lab: list all packages available for installation from all enabled repos, # sudo dnf repoquery Lab: list of packages that are available only from a specific repo: # sudo dnf repoquery --repo \u0026#34;BaseOS\u0026#34; Lab: grep for an expression to narrow down your search. # For example, to find whether the BaseOS repo includes the zsh package.\nsudo dnf repoquery --repo BaseOS | grep zsh Lab: list all installed packages on the system: # sudo dnf list installed Three columns: - package name - package version - repo it was installed from. - @anaconda means the package was installed at the time of RHEL installation.\nList all installed packages and all packages available for installation from all enabled repositories:\nsudo dnf list @ sign identifies the package as installed. List all packages available from all enabled repositories that should be able to update:\nsudo dnf list updates List whether a package (bc, for instance) is installed or available for installation from any enabled repository:\nsudo dnf list bc List all installed packages whose names begin with the string \u0026ldquo;gnome\u0026rdquo; followed by any number of characters:\nsudo dnf list installed ^gnome* List recently added packages:\nsudo dnf list recent Refer to the repoquery and list subsections of the dnf command manual pages for more options and examples.\nInstalling and Updating Packages # Installing a package:\ncreates the necessary directory structure installs the required files runs any post-installation steps. If already installed, dnf command updates it to the latest available version. Attempt to install a package called ypbind, proceed to update if it detects the presence of an older version:\nsudo dnf install ypbind Install or update a package called dcraw located locally at /mnt/AppStream/Packages/\nsudo dnf localinstall /mnt/AppStream/Packages/dcraw* Update an installed package (autofs, for example) to the latest available version. Dnf will fail if the specified package is not already installed:\nsudo dnf update autofs Update all installed packages to the latest available versions:\nsudo dnf -y update Refer to the install and update subsections of the dnf command manual pages for more options and examples.\nExhibiting Package Information # Show:\nrelease size whether it is installed or available for installation repo name it was installed or is available from short and long descriptions license so on dnf info subcommand\nView information about a package called autofs:\ndnf info autofs Determines whether the specified package is installed or not. Refer to the info subsection of the dnf command manual pages.\nRemoving Packages # Removing a package:\nuninstalls it and removes all associated files and directory structure. erases any dependencies as part of the deletion process. Remove a package called ypbind:\nsudo dnf remove ypbind Output\nResolved dependencies List of the packages that it would remove. Disk space that their removal would free up. After confirmation, it erased the identified packages and verified their removal. List of the removed packages Refer to the remove subsection of the dnf command manual pages for more options and examples available for removing packages.\nLab: Manipulate Individual Packages # Perform management operations on a package called cifs-utils. Determine if this package is already installed and if it is available for installation. Display its information before installing it. Install the package and exhibit its information. Erase the package along with its dependencies and confirm the removal.\nCheck whether the cifs-utils package is already installed: dnf list installed | grep cifs-utils Determine if the cifs-utils package is available for installation: dnf repoquery cifs-utils Display detailed information about the package: dnf info cifs-utils Install the package: dnf install -y cifs-utils Display the package information again: dnf info cifs-utils Remove the package: dnf remove -y cifs-utils Confirm the removal: dnf list installed | grep cif Determining Provider and Searching Package Metadata # You can determine what package a specific file belongs to or which package comprises a certain string. Search for packages that contain a specific file such as /etc/passwd/, use the provides or the whatprovides subcommand with dnf:\ndnf provides /etc/passwd Indicates file is part of a package called setup, installed during RHEL installation.\nSecond instance, setup package is part of the BaseOS repository.\nCan also use a wildcard character for filename expansion.\nList all packages that contain filenames beginning with \u0026ldquo;system-config\u0026rdquo; followed by any number of characters:\ndnf whatprovides /usr/bin/system-config* To search for all the packages that match the specified string in their name or summary:\ndnf search system-config Package Group Management # group subcommand list, install, query, and remove groups of packages. Listing Available and Installed Package Groups # group list subcommand:\nlist the package groups available for installation from either or both repos list the package groups that are already installed on the system. List all available and installed package groups from all repositories:\ndnf group list output:\ntwo categories of package groups: Environment group Package groups Environment group:\nLarger collection of RHEL packages that provides all necessary software to build the operating system foundation for a desired purpose. Package group\nSmall bunch of RHEL packages that serve a common purpose. Saves time on the deployment of individual and dependent packages. Output shows installed and available package groups. Display the number of installed and available package groups:\nsudo dnf group summary List all installed and available package groups including those that are hidden:\nsudo dnf group list hidden Try group list with --installed and --available options to narrow down the output list.\nsudo dnf group list --installed List all packages that a specific package group such as Base contains:\nsudo dnf group info Base -v option with the group info subcommand for more information.\nReview group list and group info subsections of the dnf man pages.\nInstalling and Updating Package Groups # Creates the necessary directory structure for all the packages included in the group and all dependent packages. Installs the required files. Runs any post-installation steps. Attempts to update all the packages included in the group to the latest available versions. Install a package group called Emacs. Update if it detects an older version.\nsudo dnf -y groupinstall emacs Update the smart card support package group to the latest version:\ndnf groupupdate \u0026#34;Smart Card Support\u0026#34; Refer to the group install and group update subsections of the dnf command manual pages for more details.\nRemoving Package Groups # Uninstalls all the included packages and deletes all associated files and directory structure. Erases any dependencies Erase the smart card support package group that was installed:\nsudo dnf -y groupremove \u0026#39;smart card support\u0026#39; Refer to the remove subsection of the dnf command manual pages for more details.\nLab: Manipulate Package Groups # Perform management operations on a package group called system tools. Determine if this group is already installed and if it is available for installation. List the packages it contains and install it. Remove the group along with its dependencies and confirm the removal.\nCheck whether the system tools package group is already installed: dnf group list installed Determine if the system tools group is available for installation: dnf group list available The group name is exhibited at the bottom of the list under the available groups.\nDisplay the list of packages this group contains: dnf group info \u0026#39;system tools\u0026#39; All of the packages will be installed as part of the group installation. Install the group: sudo dnf group install \u0026#39;system tools\u0026#39; Remove the group: sudo dnf group remove \u0026#39;system tools\u0026#39; -y Confirm the removal: dnf group list installed Application Streams and Modules # Application Streams\nIntroduced in RHEL 8. Employs a modular approach to organize multiple versions of a software application alongside its dependencies to be available for installation from a single repository. module\nLogical set of application packages that includes everything required to install it, including the executables, libraries, documentation, tools, and utilities as well as dependent components. Modularity gives the flexibility to choose the version of software based on need. In older RHEL releases, each version of a package would have to come from a separate repository. (This has changed in RHEL 8.) Now modules of a single application with different versions can be stored and made available for installation from a common repository. The package management tool has also been enhanced to manipulate modules. RHEL 9 is shipped with two core repositories called BaseOS and Application Stream (AppStream). BaseOS repository\nIncludes the core set of RHEL 9 components kernel, modules, bootloader, and other foundational software packages. Lays the foundation to install and run software applications and programs. Available in the traditional rpm format. AppStream repository\nComes standard with core applications, Plus several add-on applications Rpm and modular format Include web server software, development languages, database software, etc. Benefits of Segregation # Why separate BaseOS components from other applications?\n(1) Separates application components from the core operating system elements.\n(2) Allows publishers to deliver and administrators to apply application updates more frequently.\nIn previous RHEL versions, an OS update would update all installed components including the kernel, service, and application components to the latest versions by default.\nThis could result in an unstable system or a misbehaving application due to an unwanted upgrade of one or more packages.\nBy detaching the base OS components from the applications, either of the two can be updated independent of the other.\nThis provides enhanced flexibility in tailoring the system components and application workloads without impacting the underlying stability of the system.\nModule Streams # Collection of packages organized by version Each module can have multiple streams Each stream receives updates independent of the other streams Stream can be enabled or disabled. enabled stream\nAllows the packages it contains to be queried or installed Only one stream of a specific module can be enabled at a time Each module has a default stream, which provides the latest or the recommended version. Module Profiles # List of recommended packages organized for purpose-built, convenient deployments to support a variety of use cases such as: Minimal, development, common, client, server, etc. A profile may also include packages from the BaseOS repository or the dependencies of the stream Each module stream can have zero, one, or more profiles associated with it with only one of them marked as the default. Module Management # Modules are special package groups usually representing an application, a language runtime, or a set of tools. They are available in one or multiple streams which usually represent a major version of a piece of software, They are available in one or multiple streams which give you an option to choose what versions of packages you want to consume. https://docs.fedoraproject.org/en-US/modularity/using-modules/\nModules are a way to deliver different versions of software (such as programming languages, databases, or web servers) independently of the base operating system\u0026rsquo;s release cycle.\nEach module can contain multiple streams, representing different versions or configurations of the software. For example, a module for Python might have streams for Python 2 and Python 3.\nmodule dnf subcommand\nlist, enable, install, query, remove, and disable modules. Listing Available and Installed Modules # List all modules along with their stream, profile, and summary information available from all configured repos:\ndnf module list Limit the output to a list of modules available from a specific repo such as AppStream by adding --repo AppStream:\ndnf module list --repo AppStream Output:\ndefault (d) enabled (e) disabled (x) installed (i) List all the streams for a specific module such as ruby and display their status:\ndnf module list ruby Modify the above and list only the specified stream 3.3 for the module ruby\ndnf module list ruby:3.3 List all enabled module streams:\ndnf module list --enabled Similarly, you can use the --installed and --disabled options with dnf module list to output only the installed or the disabled streams.\nRefer to the module list subsection of the dnf command manual pages.\nInstalling and Updating Modules # Installing a module\nCreates directory tree for all packages included in the module and all dependent packages. Installs required files for the selected profile. Runs any post-installation steps. If module being loaded or a part of it is already present, the command attempts to update all the packages included in the profile to the latest available versions. Install the perl module using its default stream and default profile:\nsudo dnf -y module install perl Update a module called squid to the latest version:\nsudo dnf module update squid -y Install the profile \u0026ldquo;common\u0026rdquo; with stream \u0026ldquo;rhel9\u0026rdquo; for the container-tools module: (module:stream/profile)\nsudo dnf module install container-tools:rhel9/common Displaying Module Information # Shows Name, stream, version, list of profiles, default profile, repo name module was installed or is available from Summary, description, and artifacts. Can be viewed by supplying module info with dnf. List all profiles available for the module ruby:\ndnf module info --profile ruby Limit the output to a particular stream such as 3.1:\ndnf module info --profile ruby:3.1 Refer to the module info subsection of the dnf command manual pages for more details.\nRemoving Modules # Removing a module will:\nUninstall all the included packages and Delete all associated files and directory structure. Erases any dependencies as part of the deletion process. Remove the ruby module with \u0026ldquo;3.1\u0026rdquo; stream:\nsudo dnf module remove ruby:3.1 Refer to the module remove subsection of the dnf command manual pages:\nLab: Manipulate Modules # Perform management operations on a module called postgresql. Determine if this module is already installed and if it is available for installation. Show its information and install the default profile for stream \u0026ldquo;10\u0026rdquo;. Remove the module profile along with any dependencies confirm the removal. Check whether the postgresql module is already installed(i): dnf module list postgresql Display detailed information about the default stream of the module: dnf module info postgresql:15 Install the module with default profile for stream \u0026ldquo;15\u0026rdquo;: sudo dnf -y module install --profile postgresql:15 Display the module information again: dnf module info postgresql:15 Erase the module profile for the stream: dnf module remove -y postgresql:15 Confirm the removal (back to (d)): dnf module info postgresql:15 Switching Module Streams # Typically performed to upgrade or downgrade the version of an installed module. process:\nuninstall the existing version provided by a stream alongside any dependencies that it has,\nswitch to the other stream\ninstall the desired version.\nInstalling a module from a stream automatically enables the stream if it was previously disabled\nyou can manually enable or disable it with the dnf command.\nOnly one stream of a given module enabled at a time.\nAttempting to enable another one for the same module automatically disables the current enabled stream.\ndnf module list and dnf module info expose the enable/disable status of the module stream.\nLab: Install a Module from an Alternative Stream # Downgrade a module to a lower version. Remove the stream ruby 3.3 and Confirm its removal. manually enable the stream perl 5.24 and confirm its new status. install the new version of the module and display its information. Check the current state of all perl streams: dnf module list perl Remove perl 5.26: sudo dnf module remove perl -y Confirm the removal: dnf module list ruby Reset the module so that neither stream is enabled or disabled. This will remove the enabled (e) indication from ruby 3.3 sudo dnf module reset ruby Install the non-default profile \u0026ldquo;minimal\u0026rdquo; for ruby stream 3.1. This will auto-enable the stream. \u0026ndash;allowerasing\nWill instruct the command to remove installed packages for dependency resolution. sudo dnf module install ruby:3.1 --allowerasing Check the status of the module: dnf module list perl The dnf Command # Introduced in RHEL 8 Can use interchangeably with yum in RHEL yum is a soft link to the dnf utility. Requires the system to have access to either: a local or remote software repository a local installable package file. Subscription Management* (RHSM) service\nAvailable in the Red Hat Customer Portal\nOffers access to official Red Hat software repositories.\nOther web-based repositories that host packages are available\nYou can also set up a local, custom repository on your system and add packages of your choice to it.\nPrimary benefit of using dnf over rpm:\nResolve dependencies automatically\nBy identifying and installing any additional required packages With multiple repositories set up, dnf extracts the software from wherever it finds it.\nPerform abundant software administration tasks.\nInvokes the rpm utility in the background\nCan perform a number of operations on individual packages, package groups, and modules:\nlisting querying installing removing enabling and disabling specific module streams. Software handling tasks that dnf can perform on packages:\nClean and repolist are specific to repositories. Refer to the manual pages of dnf for additional subcommands, operators, options, examples, and other details. Subcommand Description check-update Checks if updates are available for installed packages clean Removes cached data history Display previous dnf activities as recorded in /var/lib/dnf/history/ info Show details for a package install Install or update a package list List installed and available packages provides Search for packages that contain the specified file or feature reinstall Reinstall the exact version of an installed package remove Remove a package and its dependencies repolist List enabled repositories repoquery Runs queries on available packages search Searches package metadata for the specified string upgrade Updates each installed package to the latest version dnf subcommands that are intended for operations on package groups and modules:\nSubcommand Description group install Install or updates a package group group info Return details for a package group group list List available package groups group remove Remove a package group module disable Disable a module along with all the streams it contains module enable Enable a module along with all the streams it contains module install Install a module profile including its packages module info Show details for a module module list Lists all available module streams along with their profiles and status module remove Removes a module profile including its packages module reset Resets a module so that it is neither in enable nor in disable state module update Updates packages in a module profile For labs, you\u0026rsquo;ll need to create a definition file and configure access to the two repositories available on the RHEL 8 ISO image.\nLab: Configure Access to Pre-Built Repositories # Set up access to the two dnf repositories that are available on RHEL 9 image. (You should have already configured an automatic mounting of RHEL 9 image on /mnt.) Create a definition file for the repositories and confirm.\nVerify that the image is currently mounted: df -h | grep mnt Create a definition file called local.repo in /etc/yum.repos.d/ using the vim editor and define the following data for both repositories in it: [BaseOS] name=BaseOS baseurl=file:///mnt/BaseOS gpgcheck=0 [AppStream] name=AppStream baseurl=file:///mnt/AppStream gpgcheck=0 Confirm access to the repositories: sudo dnf repolist Ignore lines 1-4 in the output that are related to subscription and system registration. Lines 5 and 6 show the rate at which the command read the repo data. Line 7 displays the timestamp of the last metadata check. last two lines show the repo IDs, repo names, and a count of packages they hold. AppStream repo consists of 4,672 packages BaseOS repo contains 1,658 packages. Both repos are enabled by default and are ready for use. dnf yum Repository # dnf repository (yum repository or a repo)\nDigital library for storing software packages\nRepository is accessed for package retrieval, query, update, and installation\nThe two repositories\nBaseOS and AppStream come preconfigured with the RHEL 9 ISO image. Number of other repositories available on the Internet that are maintained by software publishers such as Red Hat and CentOS.\nCan build private custom repositories for internal IT use for stocking and delivering software.\nGood practice for an organization with a large Linux server base, as it manages dependencies automatically and aids in maintaining software consistency across the board. Can also be used to store in-house developed packages.\nIt is important to obtain software packages from authentic and reliable sources such as Red Hat to prevent potential damage to your system and to circumvent possible software corruption.\nThere is a process to create repositories and to access preconfigured repositories.\nThere are two pre-set repositories available on the RHEL 9 image. You will configure access to them via a definition file to support the exercises and lab environment.\nRepository Definition File # Repo definition files are located in /etc/yum.repos.d/ Can create local.repo file in this directory to specify local repos See dnf.conf man page Sample repo definition file and key directives:\n[BaseOS_RHEL_9] name= RHEL 9 base operating system components baseurl=file://*mnt*BaseOS enabled=1 gpgcheck=0 EXAM TIP:\nKnowing how to configure a dnf/yum repository using a URL plays an important role in completing some of the RHCSA exam tasks successfully. Use two forward slash characters (//) with the baseurl directive for an FTP, HTTP, or HTTPS source. Five lines from a sample repo file: Line 1 defines an exclusive ID within the square brackets. Line 2 is a brief description of the repo with the \u0026ldquo;name\u0026rdquo; directive. Line 3 is the location of the repodata directory with the \u0026ldquo;baseurl\u0026rdquo; directive. Line 4 shows whether this repository is active. Line 5 shows if packages are to be GPGchecked for authenticity.\nEach repository definition file must have:\nUnique ID Description Baseurl directive defined Other directives are set as required. The baseurl directive for a local directory path is defined as file:///local_path\nThe first two forward slash characters represent the URL convention, and the third forward slash is for the absolute path to the destination directory) FTP and \\ftp://hostname/network_path HTTP(S) http(s)://hostname/network_path network path must include a resolvable hostname or an IP address. Software Management with dnf # Tools are available to work with individual packages as well as package groups and modules. rpm command is limited to managing one package at a time. dnf has an associated configuration file that can define settings to control its behavior. dnf Configuration File # Key configuration file: /etc/dnf/dnf.conf \u0026ldquo;main\u0026rdquo; section - Sets directives that have a global effect on dnf operations. Can define separate sections for each custom repository that you plan to set up on the system. Preferred location to store configuration for each custom repository in their own definition files is in /etc/yum.repos.d default location created for this purpose. Default content of this configuration file:\ncat /etc/dnf/dnf.conf [main] gpgcheck=1 installonly_limit=3 clean_requirements_on_remove=True best=True skip_if_unavailable=False The above and a few other directives that you may define in the file:\nDirective Description best Whether to install (or upgrade to) the latest available version. clean_requirements_on_remove Whether to remove dependencies during a package removal process that are no longer in use. debuglevel Sets debug from 1 (minimum) and 10 (maximum). Default is 2. A value of 0 disables this feature. gpgcheck Whether to check the GPG signature for package authenticity. Default is 1 (enabled). installonly_limit Count of packages that can be installed concurrently. Default is 3. keepcache Defines whether to store the package and header cache following a successful installation. Default is 0 (disabled). logdir Sets the directory location to store the log files. Default is /var/log/ obsoletes Checks and removes any obsolete dependent packages during installs and updates. Default is 1 (enabled). For other directives: man 5 dnf.conf\nAdvanced Package Management DIY Labs # Configure Access to RHEL 8 Repositories (Make sure the RHEL 8 ISO image is attached to the VM and mounted.) Create a definition file under /etc/yum.repos.d/, and define two blocks (one for BaseOS and another for AppStream). vim /etc/yum.repos.d/local.repo [BaseOS] name=BaseOS baseurl=file:///mnt/BaseOS gpgcheck=0 [AppStrean] name=AppStream baseurl=file:///mnt/AppStream gpgcheck=0 Verify the configuration with dnf repolist. You should see numbers in thousands under the Status column for both repositories. dnf repolist -v Lab: Install and Manage Individual Packages # List all installed and available packages separately. dnf list --available \u0026amp;\u0026amp; dnf list --installed Show which package contains the /etc/group file. dnf provides /etc/group Install the package httpd. dnf -y install httpd Review /var/log/yum.log/ for confirmation. (/var/lib/dnf/history) dnf history Perform the following on the httpd package: Show information dnf info httpd List dependencies dnf repoquery --requires httpd Remove it dnf remove httpd Lab Install and Manage Package Groups # List all installed and available package groups separately. dnf group list available \u0026amp;\u0026amp; dnf group list installed Install package groups Security Tools and Scientific Support. dnf group install \u0026#39;Security Tools\u0026#39; Review /var/log/yum.log for confirmation. dnf history Show the packages included in the Scientific Support package group, and delete this group. dnf group info \u0026#39;Scientific Support\u0026#39; \u0026amp;\u0026amp; dnf group remove \u0026#39;Scientific Support\u0026#39; Lab: Install and Manage Modules # List all modules. Identify which modules, streams and profiles are installed, default, disabled, and enabled from the output. dnf module list Install the default stream of the development profile for module php, and verify. dnf module install php \u0026amp;\u0026amp; dnf module list Remove the module. dnf module remove php Lab Switch Module Streams and Install Software # List postgresql module. This will display the streams and profiles, and their status. dnf module list postgresql Reset both streams dnf module reset postgresql enable the stream for the older version, and install its client profile. dnf module install postgresql:15 ","externalUrl":null,"permalink":"/tech/linux/advanced-package-management/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003ePackage Groups\n    \u003cdiv id=\"packagegroups\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#packagegroups\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\n\u003ch3 class=\"relative group\"\u003epackage group\n    \u003cdiv id=\"package-group\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#package-group\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGroup of packages that serve a common purpose.\u003c/li\u003e\n\u003cli\u003eCan query, install, and delete as a single unit rather than dealing with packages individually.\u003c/li\u003e\n\u003cli\u003eTwo types of package groups: \u003cem\u003eenvironment groups\u003c/em\u003e and \u003cem\u003epackage groups\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eenvironment groups\u003c/strong\u003e available in RHEL 9:\u003c/p\u003e","title":"Advanced Package Management","type":"tech"},{"content":" Local User Authentication Files # Three supported account types: root, normal, service root has full access to all services and administrative functions on the system. created by default during installation. Normal user-level privileges cannot perform any administrative functions can run applications and programs that have been authorized. Service take care of their respective services, which include apache, ftp, mail, and chrony. User account information for local users is stored in four files that are located in the /etc directory. passwd, shadow, group, and gshadow (user authentication files) updated when a user or group account is created, modified, or deleted. referenced to check and validate the credentials for a user at the time of their login attempt, system creates their automatic backups by default as passwd-, shadow-, group-, and gshadow- in the /etc directory. /etc/passwd # vital user login data each row hold info for one user 644 permissions by default 7 feilds per row login name up to 255 characters _ and - characters are supported not recommended to include special characters and uppercase letters in login names. password \u0026ldquo;x\u0026rdquo; in this field points to /etc/shadow for actual password. \u0026ldquo;*\u0026rdquo; identifies disabled account Can also include a hashed password (RHEL uses SHA-512 by default) UID Number between 0 and 4.2 billion UID 0 is reserved for root account UIDs 1-200 are used by Red Hat for core service accounts UIDs 201-999 are reserved for non-core service accounts UIDs 1000 \u0026lt; are for normal user accounts (starts at 1000 by default) GID GID that matches entry in /etc/group (primary group) Group for every user by default that matches UID Comments (GECOS) or (GCOS) general comments about the user Home Directory absolute path to the user home directory. Shell absolute path of the shell file for the user\u0026rsquo;s primary shell after logging in. (default = (/bin/bash)) /etc/shadow # no access permissions for any user (even root) (but owned by root) secure password control (shadow password) user passwords are hashed and stored in a more secure file /etc/shadow limits on user passwords in terms of expiration, warning period, etc. applied on per-user basis limits and other settings are defined in /etc/login.defs user is initially checked in the passwd file for existence and then in the shadow file for authenticity. contains user authentication and password aging information. Each row in the file corresponds to one entry in the passwd file. login names are used as a common key between the shadow and passwd files. nine colon-separated fields per line entry. 1 Login name 2 Encrypted password ! at the beginning of this field shows that the user account is locked if field is empty then user has passwordless entry 3 last change Number of days (lastchg) since the UNIX epoch, (UNIX time (January 01, 1970 00:00:00 UTC) when the password was last modified. Empty field represents the passiveness of password aging features. 0 forces the user to change their password upon next login. 4 minimum number of days (mindays) that must elapse before the user is allowed to change their password can be altered using the chage command with the -m option or the passwd command with the -n option. 0 or null in this field disables this feature. 5 (Maximum) maximum number of days (maxdays) before the user password expires and must be changed. may be altered using the chage command with the -M option or the passwd command with the -x option. null value here disables this feature along with other features such as the maximum password age, warning alerts, and the user inactivity period. 6 Field 6 (Warning) number of days (warndays) the user gets warnings for changing their password before it expires. may be altered using the chage command with the -W option or the passwd command with the -w option. 0 or null in this field disables this feature. 7 Password Expiry) maximum allowable number of days for the user to be able to log in with the expired password. (inactivity period). may be altered using the chage command with the -I option or the passwd command with the -i option. empty field disables this feature. 8 (Account Expiry) number of days since the UNIX time when the user account will expire and no longer be available. may be altered using the chage command with the -E option. empty field disables this feature. 9 (Reserved): Reserved for future use. /etc/group # plaintext file and contains critical group information. 644 permissions by default and owned by root. Each row in the file stores information for one group entry. Every user on the system must be a member of at least one group (User Private Group (UPG)). a group name matches the username it is associated with by default four colon-separated fields per line entry. Field 1 (Group Name): Holds a group name that must begin with a letter. Group names with up to 255 characters, including the uppercase, underscore (_) and hyphen (-) characters, are also supported. (not recommended) Field 2 (Encrypted Password): Can be empty or contain an “x” (points to the /etc/gshadow file for the actual password), or a hashed group-level password. can set a password on a group for non-members to be able to change their group identity temporarily using the newgrp command. non-members must enter the correct password in order to do so. Field 3 (GID): Holds a GID, that is also placed in the GID field of the passwd file. By default, groups are created with GIDs starting at 1000 and with the same name as the username. system allows several users to belong to a single group also allows a single user to be a member of multiple groups at the same time. Field 4 (Group Members): Lists the membership for the group. (user’s primary group is always defined in the GID field of the passwd file.) /etc/gshadow # no access permissions for any user (even root) group passwords are hashed and stored group names are used as a common key between the gshadow and group files. 000 permissions and owned by root four colon-separated fields Field 1 (Group Name): Consists of a group name as appeared in the group file. Field 2 (Encrypted Password): Can contain a hashed password, which may be set with the gpasswd command for non-group members to access the group temporarily using the newgrp command. single exclamation mark (!) or a null value in this field allows group members password-less access and restricts non-members from switching into this group. Field 3 (Group Administrators): Lists usernames of group administrators that are authorized to add or remove members with the gpasswd command. Field 4 (Members): comma-separated list of members. gpasswd command: # add group administrators. add or delete group members. assign or revoke a group-level password. disable the ability of the newgrp command to access a group. picks up the default values from the /etc/login.defs file. useradd and login.defs configuration files # useradd command # picks up the default values from the /etc/default/useradd and /etc/login.defs files for any options that are not specified at the command line when executing it. login.defs file is also consulted by the usermod, userdel, chage, and passwd commands Both files store several defaults including those that affect the password length and password lifecycle. /etc/default/useradd Default Directives: starting GID (GROUP) (provided the USERGROUPS_ENAB directive in the login.defs file is set to no) home directory location (HOME) number of inactivity days between password expiry and permanent account disablement (INACTIVE) account expiry date (EXPIRE), login shell (SHELL), skeleton directory location to copy user initialization files from (SKEL) whether to create mail spool directory (CREATE_MAIL_SPOOL) /etc/login.defs default directives: # MAIL_DIR\nmail directory location PASS_MAX_DAYS, PASS_MIN_DAYS, PASS_MIN_LEN, and PASS_WARN_AGE\npassword aging attributes. UID_MIN, UID_MAX, GID_MIN, and GID_MAX\nranges of UIDs and GIDs to be allocated to new users and groups SYS_UID_MIN, SYS_UID_MAX, SYS_GID_MIN, and SYS_GID_MAX\nranges of UIDs and GIDs to be allocated to new service users and groups CREATE_HOME\nwhether to create a home directory UMASK\npermissions to be set on the user home directory at creation based on this umask value USERGROUPS_ENAB\nwhether to delete a user’s group (at the time of user deletion) if it contains no more members ENCRYPT_METHOD\nencryption method for user passwords Password Aging attributes # Can be done for an individual user or applied to all users. Can prevent users from logging in to the system by locking their access for a period of time or permanently. Must be performed by a user with elevated privileges of the root user. Normal users may be allowed access to privileged commands by defining them appropriately in a configuration file. Each file that exists on the system regardless of its type has an owning user and an owning group. every file that a user creates is in the ownership of that user. ownership may be changed and given to another user by a super user. Password Aging and management # Setting restrictions on password expiry, account disablement, locking and unlocking users, and password change frequency. Can choose to inactivate it completely for an individual user. Stored in the /etc/shadow file (fields 4 to 8) and its default policies in the /etc/login.defs configuration file. aging management tools—chage and passwd— usermod command can be used to implement two aging attributes (user expiry and password expiry) and lock and unlock user accounts. chage command # Set or alter password aging parameters on a user account. Changes various fields in the shadow file Switches -d (\u0026ndash;lastday) Specifies an explicit date in the YYYY-MM-DD format, or the number of days since the UNIX time when the password was last modified. With -d 0, the user is forced to change the password at next login. It corresponds to field 3 in the shadow file. -E (\u0026ndash;expiredate) Sets an explicit date in the YYYY-MM-DD format, or the number of days since the UNIX time on which the user account is deactivated. This feature can be disabled with -E -1. It corresponds to the eighth field in the shadow file. -I (\u0026ndash;inactive) Defines the number of days of inactivity after the password expiry and before the account is locked. The user may be able to log in during this period with their expired password. This feature can be disabled with -I -1. It corresponds to field 7 in the shadow file. -l Lists password aging attributes set on a user account. -m (\u0026ndash;mindays) Indicates the minimum number of days that must elapse before the password can be changed. A value of 0 allows the user to change their password at any time. It corresponds to field 4 in the shadow file. -M (\u0026ndash;maxdays) Denotes the maximum number of days of password validity before the user password expires and it must be changed. This feature can be disabled with -M -1. It corresponds to field 5 in the shadow file. -W (\u0026ndash;warndays) Designates the number of days for which the user gets alerts to change their password before it expires. It corresponds to field 6 in the shadow file. passwd command # set or modify a user’s password modify the password aging attributes and lock or unlock account Switches -d (\u0026ndash;delete) Deletes a user password does not expire the user account. -e (\u0026ndash;expire) Forces a user to change their password upon next logon. sets date to prior to Unix time -i (\u0026ndash;inactive) Defines the number of days of inactivity after the password expiry and before the account is locked. (field 7 in shadow file) -l (\u0026ndash;lock) Locks a user account. -n (\u0026ndash;minimum) Specifies the number of days that must elapse before the password can be changed. (field 4 in shadow file) -S (\u0026ndash;status) Displays the status information for a user. -u (\u0026ndash;unlock) Unlocks a locked user account. -w (\u0026ndash;warning) Designates the number of days for which the user gets alerts to change their password before it actually expires. (field 6 in shadow file) -x (\u0026ndash;maximum) Denotes the maximum number of days of password validity before the user password expires and it must be changed. (field 5 in shadow file) usermod command # Modify a user’s attribute Lock or unlock their account Switches -L (\u0026ndash;lock) Locks a user account by placing a single exclamation mark (!) at the beginning of the password field and before the hashed password string. -U (\u0026ndash;unlock) Unlocks a user’s account by removing the exclamation mark (!) from the beginning of the password field. Linux Groups and their Management # /etc/group group info /etc/login.defs default policies /etc/gshadow group administrator information and group-level passwords group management tools groupadd, groupmod, and groupdel create, alter, and erase groups groupadd command # adds entries to the group and gshadow files for each group added to the system picks up default values from /etc/login.defs Switches -g (\u0026ndash;gid) Specifies the GID to be assigned to the group -o (\u0026ndash;non-unique) Creates a group with a matching GID of an existing group. When two groups have an identical GID, members of both groups get identical rights on each other’s files. This should only be done in specific situations. -r Creates a system group with a GID below 1000 groupname Specifies a group name groupmod command # syntax of this command is very similar to the groupadd with most options identical. Additional flags -n change name of existing group User Management # Switching Users # su command # Ctrl-d - return to previous user su - - switch user with startup scripts -c - issue a command as a user without switching to them.\nroot user can switch into any user account that exists on the system without being prompted for that user’s password. switching into the root account to execute privileged actions is not recommended. whoami command # show current user logname command # Identity of the user who originally logged in. groupdel command # removes entries for the specified group from both group and gshadow files. Doing as Superuser (substitute user) # Any normal user that requires privileged access to administrative commands or non-owning files is defined in the sudoers file. File may be edited with a command called visudo Creates a copy of the file as sudoers.tmp and applies the changes there. After the visudo session is over, the updated updated file overwrites the original sudoers file and sudoers.tmp is deleted. syntax user1 ALL=(ALL) ALL %dba ALL=(ALL) ALL group is prefixed by % Make it so members are not prompted for password user1 ALL=(ALL) NOPASSWD:ALL %dba ALL=(ALL) NOPASSWD:ALL Limit access to a single command user1 ALL=/usr/bin/cat %dba ALL=/usr/bin/cat too many entries can clutter sudoers file. Use aliases instead: User_Alias you can define a User_Alias called PKGADM for user1, user100, and user200. These users may or may not belong to the same Linux group. Cmnd_Alias you can define a Cmnd_Alias called PKGCMD containing yum and rpm package management commands sudo command # /etc/sudoers /etc/sudoers.d/ drop-in directory /var/log/secure Sudo logs successful authentication and command data to here under the name of the user using the command. Owning User and Owning Group # Every file and directory has an owner. Creator assumes ownership by default. Every user is a member of one or more groups. Owners group is also assigned to file or directory by default. chown command # alter the ownership for files and directories Must have root privileges. Can also change owning group. chgrp command # alter the owning group for files and directories Must have root privileges. Advanced User Management Labs # Lab: Set and Confirm Password Aging with chage (root) # Set password aging parameters for user100 to mindays (-m) 7, maxdays (-M) 28, and warndays (-W) 5: chage -m 7 -M 28 -W 5 user100 Confirm chage -l user100 Set the account expiry to January 31, 2020 chage -E 2020-01-31 user100 Verify the new account expiry setting chage -l user100 Lab: Set and Confirm Password Aging with passwd (root) # Set password aging attributes for user200 to mindays 10, maxdays 90, and warndays 14: passwd -n 10 -x 90 -w 14 user200 Confirm: passwd -S user200 Set the number of inactivity days to 5: passwd -i 5 user200 Confirm: passwd -S user200 Ensure that the user is forced to change their password at next login: passwd -e user200 Confirm: passwd -S user200 Lab: Lock and Unlock a User Account with usermod and passwd (root) # Obtain the current password information for user200 from the shadow file: grep user200 /etc/shadow Lock the account for user200: usermod -L user200 Confirm: grep user200 /etc/shadow Unlock the account with either of the following: usermod -U user200 or passwd -u user200 confirm grep user200 /etc/shadow Lab: Create a Group and Add Members (root) # Create the group linuxadm with GID 5000: groupadd -g 5000 linuxadm Create a group called dba with the same GID as that of group linuxadm: groupadd -o -g 5000 dba Confirm: grep linuxadm /etc/group grep dba /etc/group Add user1 as a secondary member of group dba using the usermod command. The existing membership for the user must remain intact. usermod -aG dba user1 Verify the updated group membership information for user1 by extracting the relevant entry from the group file, and running the id and groups command for user1: grep dba /etc/group id user1 groups user1 Lab: Modify and Delete a Group Account (root) # Alter the name of linuxadm to sysadm: groupmod -n sysadm linuxadm Change the GID of sysadm to 6000: groupmod -g 6000 sysadm Confirm: grep sysadm /etc/group grep linuxadm /etc/group Delete sysadm group and confirm: groupdel sysadm grep sysadm /etc/group Lab: To switch from user1 (assuming you are logged in as user1) into root without executing the startup scripts # su switch to user100 su - user100 See what whoami and logname reports now: whoami logname use su as follows and execute this privileged command to obtain desired results: su -c \u0026#39;firewall-cmd --list-services\u0026#39; Lab: Add user1 to sudo file but only for the cat command. # Open up /etc/sudoers and add the following: user1 ALL=/usr/bin/cat run cat as user1 with and without sudo: cat /etc/sudoers sudo cat /etc/sudoers Lab: Add user and command aliases to the sudoer file. # Add the following to the bottom of the sudoers file: Cmnd_Alias PKGCMD = /usr/bin/yum, /usr/bin/rpm User_Alias PKGADM = user1, user100, user200 PKGADM ALL=PKGCMD Run rpm or yum with sudo as one of the users. sudo yum Lab: Take a look at examples in the sudoers file. # cat /etc/sudoers Lab: Viewing owner and group information # Create a file file1 as user1 in their home directory and exhibit the file’s long listing: touch file1 ls -l file1 View the corresponding UID and GID instead, you can specify the -n option with the command: ls -ln file1 Lab: Modify File Owner and Owning Group # Change into the /tmp directory and create file10 and dir10: cd /tmp touch file10 mkdir dir10 Check and validate that both attributes are set to user1: ls -l file10 ls -ld dir10 Set the ownership of file10 to user100 and confirm: sudo chown user100 file10 ls -l file10 Alter the owning group to dba and verify: sudo chgrp dba file10 ls -l file10 Change the ownership to user200 and owning group to user100 and confirm: sudo chown user200:user100 file10 Modify the ownership to user200 and owning group to dba recursively on dir10 and validate: sudo chown -R user200:dba dir10 ls -ld dir10 Lab: Create User and Configure Password Aging (root) # Create group lnxgrp with GID 6000. groupadd lnxgrp -g 6000 Create user user5000 with UID 5000 and GID 6000. Assign this user a password. useradd -u 5000 -g 6000 user5000 Establish password aging attributes so that this user cannot change their password within 4 days after setting it and with a password validity of 30 days. This user should start getting warning messages for changing password 10 days prior to account lock down. chage -m 4 -M 30 -W 10 user5000 This user account needs to expire on the 20th of December, 2021. chage -E 2021-12-20 user5000 Lab 6-2: Lock and Unlock User (root) # Lock the user account for user5000 using the passwd command, and passwd -l user5000 confirm by examining the change in the /etc/shadow file. cat /etc/shadow Try to log in with user5000 and observe what happens. su - user1 su - user5000 Use the usermod command and unlock ","externalUrl":null,"permalink":"/tech/linux/advanced-user-management/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eLocal User Authentication Files\n    \u003cdiv id=\"local-user-authentication-files\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#local-user-authentication-files\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThree supported account types: root, normal, service\u003c/li\u003e\n\u003cli\u003eroot\n\u003cul\u003e\n\u003cli\u003ehas full access to all services and administrative functions on the system.\u003c/li\u003e\n\u003cli\u003ecreated by default during installation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNormal\n\u003cul\u003e\n\u003cli\u003euser-level privileges\u003c/li\u003e\n\u003cli\u003ecannot perform any administrative functions\u003c/li\u003e\n\u003cli\u003ecan run applications and programs that have been authorized.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eService\n\u003cul\u003e\n\u003cli\u003etake care of their respective services, which include apache, ftp, mail, and chrony.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUser account information for local users is stored in four files that are located in the /etc directory.\n\u003cul\u003e\n\u003cli\u003epasswd, shadow, group, and gshadow (user authentication files)\u003c/li\u003e\n\u003cli\u003eupdated when a user or group account is created, modified, or deleted.\u003c/li\u003e\n\u003cli\u003ereferenced to check and validate the credentials for a user at the time of their login attempt,\u003c/li\u003e\n\u003cli\u003esystem creates their automatic backups by default as passwd-, shadow-, group-, and gshadow- in the /etc directory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e/etc/passwd\n    \u003cdiv id=\"etcpasswd\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#etcpasswd\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003evital user login data\u003c/li\u003e\n\u003cli\u003eeach row hold info for one user\u003c/li\u003e\n\u003cli\u003e644 permissions by default\u003c/li\u003e\n\u003cli\u003e7 feilds per row\n\u003cul\u003e\n\u003cli\u003elogin name\n\u003cul\u003e\n\u003cli\u003eup to 255 characters\u003c/li\u003e\n\u003cli\u003e_ and - characters are supported\u003c/li\u003e\n\u003cli\u003enot recommended to include special characters and uppercase letters in login names.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003epassword\n\u003cul\u003e\n\u003cli\u003e\u0026ldquo;x\u0026rdquo; in this field points to /etc/shadow for actual password.\u003c/li\u003e\n\u003cli\u003e\u0026ldquo;*\u0026rdquo; identifies disabled account\u003c/li\u003e\n\u003cli\u003eCan also include a hashed password (RHEL uses SHA-512 by default)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUID\n\u003cul\u003e\n\u003cli\u003eNumber between 0 and 4.2 billion\u003c/li\u003e\n\u003cli\u003eUID 0 is reserved for root account\u003c/li\u003e\n\u003cli\u003eUIDs 1-200 are used by Red Hat for core service accounts\u003c/li\u003e\n\u003cli\u003eUIDs 201-999 are reserved for non-core service accounts\u003c/li\u003e\n\u003cli\u003eUIDs 1000 \u0026lt; are for normal user accounts (starts at 1000 by default)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eGID\n\u003cul\u003e\n\u003cli\u003eGID that matches entry in /etc/group (primary group)\u003c/li\u003e\n\u003cli\u003eGroup for every user by default that matches UID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eComments (GECOS) or (GCOS)\n\u003cul\u003e\n\u003cli\u003egeneral comments about the user\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHome Directory\n\u003cul\u003e\n\u003cli\u003eabsolute path to the user home directory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eShell\n\u003cul\u003e\n\u003cli\u003eabsolute path of the shell file for the user\u0026rsquo;s primary shell after logging in. (default = (/bin/bash))\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e/etc/shadow\n    \u003cdiv id=\"etcshadow\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#etcshadow\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eno access permissions for any user (even root) (but owned by root)\u003c/li\u003e\n\u003cli\u003esecure password control (shadow password)\u003c/li\u003e\n\u003cli\u003euser passwords are hashed and stored in a more secure file /etc/shadow\u003c/li\u003e\n\u003cli\u003elimits on user passwords in terms of expiration, warning period, etc. applied on per-user basis\u003c/li\u003e\n\u003cli\u003elimits and other settings are defined in /etc/login.defs\u003c/li\u003e\n\u003cli\u003euser is initially checked in the passwd file for existence and then in the shadow file for authenticity.\u003c/li\u003e\n\u003cli\u003econtains user authentication and password aging information.\u003c/li\u003e\n\u003cli\u003eEach row in the file corresponds to one entry in the passwd file.\u003c/li\u003e\n\u003cli\u003elogin names are used as a common key between the shadow and passwd files.\u003c/li\u003e\n\u003cli\u003enine colon-separated fields per line entry.\n\u003cul\u003e\n\u003cli\u003e1 Login name\u003c/li\u003e\n\u003cli\u003e2 Encrypted password\n\u003cul\u003e\n\u003cli\u003e! at the beginning of this field shows that the user account is locked\u003c/li\u003e\n\u003cli\u003eif field is empty then user has passwordless entry\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e3 last change\n\u003cul\u003e\n\u003cli\u003eNumber of days (lastchg) since the UNIX epoch, (UNIX time (January 01, 1970 00:00:00 UTC) when the password was last modified.\u003c/li\u003e\n\u003cli\u003eEmpty field represents the passiveness of password aging features.\u003c/li\u003e\n\u003cli\u003e0 forces the user to change their password upon next login.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e4 minimum\n\u003cul\u003e\n\u003cli\u003enumber of days (mindays) that must elapse before the user is allowed to change their password\u003c/li\u003e\n\u003cli\u003ecan be altered using the \u003ccode\u003echage\u003c/code\u003e command with the \u003ccode\u003e-m\u003c/code\u003e option or the \u003ccode\u003epasswd\u003c/code\u003e command with the \u003ccode\u003e-n\u003c/code\u003e option.\u003c/li\u003e\n\u003cli\u003e0 or null in this field disables this feature.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e5 (Maximum)\n\u003cul\u003e\n\u003cli\u003emaximum number of days (maxdays) before the user password expires and must be changed.\u003c/li\u003e\n\u003cli\u003emay be altered using the \u003ccode\u003echage\u003c/code\u003e command with the \u003ccode\u003e-M \u003c/code\u003eoption or the \u003ccode\u003epasswd\u003c/code\u003e command with the \u003ccode\u003e-x\u003c/code\u003e option.\u003c/li\u003e\n\u003cli\u003enull value here disables this feature along with other features such as the maximum password age, warning alerts, and the user inactivity period.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e6 Field 6 (Warning)\n\u003cul\u003e\n\u003cli\u003enumber of days (warndays) the user gets warnings for changing their password before it expires.\u003c/li\u003e\n\u003cli\u003emay be altered using the \u003ccode\u003echage\u003c/code\u003e command with the \u003ccode\u003e-W\u003c/code\u003e option or the \u003ccode\u003epasswd\u003c/code\u003e command with the \u003ccode\u003e-w\u003c/code\u003e option.\u003c/li\u003e\n\u003cli\u003e0 or null in this field disables this feature.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e7 Password Expiry)\n\u003cul\u003e\n\u003cli\u003emaximum allowable number of days for the user to be able to log in with the expired password. (inactivity period).\u003c/li\u003e\n\u003cli\u003emay be altered using the \u003ccode\u003echage\u003c/code\u003e command with the \u003ccode\u003e-I\u003c/code\u003e option or the \u003ccode\u003epasswd\u003c/code\u003e command with the \u003ccode\u003e-i\u003c/code\u003e option.\u003c/li\u003e\n\u003cli\u003eempty field disables this feature.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e8 (Account Expiry)\n\u003cul\u003e\n\u003cli\u003enumber of days since the UNIX time when the user account will expire and no longer be available.\u003c/li\u003e\n\u003cli\u003emay be altered using the chage command with the \u003ccode\u003e-E\u003c/code\u003e option.\u003c/li\u003e\n\u003cli\u003eempty field disables this feature.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e9 (Reserved): Reserved for future use.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e/etc/group\n    \u003cdiv id=\"etcgroup\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#etcgroup\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eplaintext file and contains critical group information.\u003c/li\u003e\n\u003cli\u003e644 permissions by default and owned by root.\u003c/li\u003e\n\u003cli\u003eEach row in the file stores information for one group entry.\u003c/li\u003e\n\u003cli\u003eEvery user on the system must be a member of at least one group (User Private Group (UPG)).\u003c/li\u003e\n\u003cli\u003ea group name matches the username it is associated with by default\u003c/li\u003e\n\u003cli\u003efour colon-separated fields per line entry.\n\u003cul\u003e\n\u003cli\u003eField 1 (Group Name):\n\u003cul\u003e\n\u003cli\u003eHolds a group name that must begin with a letter. Group names with up to 255 characters, including the\u003c/li\u003e\n\u003cli\u003euppercase, underscore (_) and hyphen (-) characters, are also supported. (not recommended)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eField 2 (Encrypted Password):\n\u003cul\u003e\n\u003cli\u003eCan be empty or contain an “x” (points to the /etc/gshadow file for the actual password), or a hashed group-level password.\u003c/li\u003e\n\u003cli\u003ecan set a password on a group for non-members to be able to change their group identity temporarily using the \u003ccode\u003enewgrp \u003c/code\u003ecommand.\u003c/li\u003e\n\u003cli\u003enon-members must enter the correct password in order to do so.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eField 3 (GID):\n\u003cul\u003e\n\u003cli\u003eHolds a GID, that is also placed in the GID field of the passwd file.\u003c/li\u003e\n\u003cli\u003eBy default, groups are created with GIDs starting at 1000 and with the same name as the username.\u003c/li\u003e\n\u003cli\u003esystem allows several users to belong to a single group\u003c/li\u003e\n\u003cli\u003ealso allows a single user to be a member of multiple groups at the same time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eField 4 (Group Members):\n\u003cul\u003e\n\u003cli\u003eLists the membership for the group. (user’s primary group is always defined in the GID field of the passwd file.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e/etc/gshadow\n    \u003cdiv id=\"etcgshadow\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#etcgshadow\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eno access permissions for any user (even root)\u003c/li\u003e\n\u003cli\u003egroup passwords are hashed and stored\u003c/li\u003e\n\u003cli\u003egroup names are used as a common key between the gshadow and group files.\u003c/li\u003e\n\u003cli\u003e000 permissions and owned by root\u003c/li\u003e\n\u003cli\u003efour colon-separated fields\n\u003cul\u003e\n\u003cli\u003eField 1 (Group Name):\n\u003cul\u003e\n\u003cli\u003eConsists of a group name as appeared in the group file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eField 2 (Encrypted Password):\n\u003cul\u003e\n\u003cli\u003eCan contain a hashed password, which may be set with the gpasswd command for non-group members to access the group temporarily using the \u003ccode\u003enewgrp\u003c/code\u003e command.\u003c/li\u003e\n\u003cli\u003esingle exclamation mark (!) or a null value in this field allows group members password-less access and restricts non-members from switching into this group.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eField 3 (Group Administrators):\n\u003cul\u003e\n\u003cli\u003eLists usernames of group administrators that are authorized to add or remove members with the gpasswd command.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eField 4 (Members):\n\u003cul\u003e\n\u003cli\u003ecomma-separated list of members.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003egpasswd\u003c/code\u003e command:\n    \u003cdiv id=\"gpasswd-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#gpasswd-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eadd group administrators.\u003c/li\u003e\n\u003cli\u003eadd or delete group members.\n\u003cul\u003e\n\u003cli\u003eassign or revoke a group-level password.\u003c/li\u003e\n\u003cli\u003edisable the ability of the newgrp command to access a group.\u003c/li\u003e\n\u003cli\u003epicks up the default values from the /etc/login.defs file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003euseradd and login.defs configuration files\n    \u003cdiv id=\"useradd-and-logindefs-configuration-files\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#useradd-and-logindefs-configuration-files\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003euseradd\u003c/code\u003e command\n    \u003cdiv id=\"useradd-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#useradd-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003epicks up the default values from the /etc/default/useradd and /etc/login.defs files for any options that are not specified at the command line when executing it.\u003c/li\u003e\n\u003cli\u003elogin.defs file is also consulted by the usermod, userdel, chage, and passwd commands\u003c/li\u003e\n\u003cli\u003eBoth files store several defaults including those that affect the password length and password lifecycle.\n/etc/default/useradd Default Directives:\u003c/li\u003e\n\u003cli\u003estarting GID (GROUP) (provided the USERGROUPS_ENAB directive in the login.defs file is set to no)\u003c/li\u003e\n\u003cli\u003ehome directory location (HOME)\u003c/li\u003e\n\u003cli\u003enumber of inactivity days between password expiry and permanent account disablement (INACTIVE)\u003c/li\u003e\n\u003cli\u003eaccount expiry date (EXPIRE),\u003c/li\u003e\n\u003cli\u003elogin shell (SHELL),\u003c/li\u003e\n\u003cli\u003eskeleton directory location to copy user initialization files from (SKEL)\u003c/li\u003e\n\u003cli\u003ewhether to create mail spool directory (CREATE_MAIL_SPOOL)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e/etc/login.defs default directives:\n    \u003cdiv id=\"etclogindefs-default-directives\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#etclogindefs-default-directives\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eMAIL_DIR\u003c/p\u003e","title":"Advanced User Management","type":"tech"},{"content":"","externalUrl":null,"permalink":"/tech/ansible/","section":"Teches","summary":"","title":"Ansible","type":"tech"},{"content":"If you\u0026rsquo;re used to managing packages installed on VMs, it\u0026rsquo;s definitely a step up to run containers in podman via quadlet units\nansible + podman is pretty close to k8s in terms of overall \u0026ldquo;how easy is this to manage\u0026rdquo;-ness, just not in terms of your RTO times or your overall scalability or resiliency\nYou would totally use ansible to generate the necessary configuration file(s) via jinja templates, confirm the desired state of your runtime/selinux/firewalls/OS/whatever, and then containers.podman.podman_container or containers.podman.podman_play or whatever\n","externalUrl":null,"permalink":"/tech/podman/ansible-and-podman/","section":"Teches","summary":"\u003cp\u003eIf you\u0026rsquo;re used to managing packages installed on VMs, it\u0026rsquo;s definitely a step up to run containers in podman via quadlet units\u003c/p\u003e\n\u003cp\u003eansible + podman is pretty close to k8s in terms of overall \u0026ldquo;how easy is this to manage\u0026rdquo;-ness, just not in terms of your RTO times or your overall scalability or resiliency\u003c/p\u003e","title":"Ansible and Podman","type":"tech"},{"content":"Build portable control nodes packaged as containers. (Execution environments)\nWorks with AWX and Ansible Navigator for playbook development and testing. Able to choose specific Python and Ansible-core version Also package with Python packages, system packages, and Ansible collections. Steps needed:\nInstall ansible-builder Make sure podman is installed Make an execution-environment.yml file that includes: Base container image Python version Ansible-core version ansible-runner version collections with version restrictions system packages with version restrictions Python packages with version restrictions other items to download, intsall, or configure If base image includes Python you omit that. Ansible builder execute\u0026rsquo;s two steps:\ncreate containerfile for podman or Dockerfile for docker based on the definition file run containerization tool to build an image based on the build instruction file and build context ansible-builder build\nruns both steps ansible-builder create\nruns first step only Building images with ansible-builder # Four stages to build a container image:\nBase: pull the base image, installPython version, pip, ansible-runner, and ansible-core Galaxy: download collections and store them locally as files Builder: download python/system packages and store them locally as files Final: install downloaded files on the output of the base stage and generating a new image that includes all the content. Ansible Builder injects hooks at each stage of the container build process so you can add custom steps before and after every build stage.\nYou may need to install certain packages or utilities before the Galaxy and Builder stages. For example, if you need to install a collection from GitHub, you must install git after the Base stage to make it available during the Galaxy stage.\nTo add custom build steps, add an additional_build_steps section to your execution environment definition.\nInstall: pip3 install ansible-builder\n","externalUrl":null,"permalink":"/tech/ansible/ansible-builder/","section":"Teches","summary":"Using Ansible Builder to build an Execution Environment","title":"Ansible Builder","type":"tech"},{"content":" ansible-navigator # Was advised to start using this tools for Ansible because it is available during the RHCE exam. https://ansible.readthedocs.io/projects/navigator/\nAnsible Docs # https://docs.ansible.com/\nansible-doc # https://docs.ansible.com/ansible/latest/cli/ansible-doc.html\n","externalUrl":null,"permalink":"/tech/ansible/ansible-documentation/","section":"Teches","summary":"Ansible Documentation resources","title":"Ansible Documentation","type":"tech"},{"content":" Ansible projects # For small companies, you can use a single Ansible configuration. But for larger ones, it\u0026rsquo;s a good idea to use different project directories. A project directory contains everything you need to work on a single project. Including:\nplaybooks variable files task files inventory files ansible.cfg playbook An Ansible script written in YAML that enforce the desired configuration on manage hosts.\nInventory # A file that Identifies hosts that Ansible has to manage. You can also use this to list and group hosts and specify host variables. Each project should have it\u0026rsquo;s own inventory file.\n/etc/ansible/hosts\ncan be used for system wide inventory. default if no inventory file is specified. has some basic inventory formatting info if you forget) Ansible will also target localhosts if no hosts are found in the inventory file. It\u0026rsquo;s a good idea to store inventory files in large environments in their own project folders. localhost is not defined in inventory. It is an implicit host that is usable and refers to the Ansible control machine. Using localhost can be a good way to verify the accessibility of services on managed hosts.\nListing hosts # List hosts by IP address or hostname. You can list a range of hosts in an inventory file as well such as web-server[1:10].example.com\nansible1:2222 \u0026lt; specify ssh port if the host is not using the default port 22 ansible2 10.0.10.55 web-server[1:10].example.com Listing groups # You can list groups and groups of groups. See the groups web and db are included in the group \u0026ldquo;servers:children\u0026rdquo;\nansible1 ansible2 10.0.10.55 web-server[1:10].example.com [web] web-server[1:10].example.com [db] db1 db2 [servers:children] \u0026lt;-- servers is the group of groups and children is the parameter that specifies child groups web db There are 3 general approaches to using groups:\nFunctional groups Address a specific group of hosts according to use. Such as web servers or database servers.\nRegional host groups Used when working with region oriented infrastructure. Such as USA, Canada.\nStaging host groups Used to address different hosts according to the staging phase that the current environment is in. Such as testing, development, production.\nUndefined host groups are called implicit host groups. These are all, ungrouped, and localhost. Names making the meaning obvious.\nHost variables # In older versions of Ansible you could define variables for hosts. This is no longer used. Example:\n[groupname:vars] ansible=ansible_user Variables are now set using host_vars and group_vars directories instead.\nMultiple inventory files # Put all inventory files in a directory and specify the directory as the inventory to be used. For dynamic directories you also need to set the execution bit on the inventory file.\nWorking with Multiple Inventory Files # Ansible supports working with multiple inventory files. One way of using multiple inventory files is to enter multiple -i parameters with the ansible or ansible-playbook commands to specify the name of the files to be used. ansible-inventory -i inventory -i listing101.py --list Would produce an output list based on the static inventory in the inventory file, as well as the dynamic inventory that is generated by the listing101.py Python script. You can also specify the name of a directory using the -i option. Uses all files in the directory as inventory files. When using an inventory directory, dynamic inventory files still must be executable for this approach to work. Lab: Using Multiple Inventories # Open a shell as the ansible user and create a directory with the name inventories. Copy the file listing101.py to the directory inventories. Also copy the inventory file to the directory inventories. To make sure both inventories have some unique contents, add the following lines to the file inventories/inventory: webserver1 webserver2 Add the following lines to the Linux /etc/hosts file: 192.168.4.203 ansible3.example.com ansible3 192.168.4.204 ansible4.example.com ansible4 Use the command ansible-inventory -i inventories --list. Inventory commands: # To view the inventory, specify the inventory file such as ~/base/inventory in the command line. You can name the inventory file anything you want. You can also set the default in the ansible.cfg file.\nView the current inventory: ansible -i inventory \u0026lt;pattern\u0026gt; --list-hosts\nList inventory hosts in JSON format: ansible-inventory -i inventory --list\nDisplay overview of hosts as a graph: ansible-inventory -i inventory --graph\nIn our lab example:\n[ansible@control base]$ pwd /home/ansible/base [ansible@control base]$ ls inventory [ansible@control base]$ cat inventory ansible1 ansible2 [web] web1 web2 [ansible@control base]$ ansible-inventory -i inventory --graph @all: |--@ungrouped: | |--ansible1 | |--ansible2 |--@web: | |--web1 | |--web2 [ansible@control base]$ ansible-inventory -i inventory --list { \u0026#34;_meta\u0026#34;: { \u0026#34;hostvars\u0026#34;: {} }, \u0026#34;all\u0026#34;: { \u0026#34;children\u0026#34;: [ \u0026#34;ungrouped\u0026#34;, \u0026#34;web\u0026#34; ] }, \u0026#34;ungrouped\u0026#34;: { \u0026#34;hosts\u0026#34;: [ \u0026#34;ansible1\u0026#34;, \u0026#34;ansible2\u0026#34; ] }, \u0026#34;web\u0026#34;: { \u0026#34;hosts\u0026#34;: [ \u0026#34;web1\u0026#34;, \u0026#34;web2\u0026#34; ] } } [ansible@control base]$ ansible -i inventory all --list-hosts hosts (4): ansible1 ansible2 web1 web2 [ansible@control base]$ ansible -i inventory ungrouped --list-hosts hosts (2): ansible1 ansible2 Using the ansible-inventory Command # default output of a dynamic inventory script is unformatted. To show formatted JSON output of the scripts, you can use the ansible-inventory command. Apart from the --list and --host options, this command also uses the --graph option to show a list of hosts, including the host groups they are a member of. [ansible@control rhce8-book]$ ansible-inventory -i listing101.py --graph [WARNING]: A duplicate localhost-like entry was found (localhost). First found localhost was 127.0.0.1 @all: |--@ungrouped: | |--127.0.0.1 | |--192.168.4.200 | |--192.168.4.201 | |--192.168.4.202 | |--ansible1 | |--ansible1.example.com | |--ansible2 | |--ansible2.example.com | |--control | |--control.example.com | |--localhost | |--localhost.localdomain | |--localhost4 | |--localhost4.localdomain4 | |--localhost6 | |--localhost6.localdomain6 Dynamic inventory # A script is used to detect inventory hosts so that you do not have to manually enter them. This is good for larger environments. You can find community provided dynamic inventory scripts that come with an .ini file that provides information on how to connect to a resource.\nInventory scripts must include \u0026ndash;list and \u0026ndash;host options and output must be JSON formatted. Here is an example from sandervanvught that generates an inventory script using /etc/hosts:\n[ansible@control base]$ cat inventory-helper.py #!/usr/bin/python from subprocess import Popen,PIPE import sys try: import json except ImportError: import simplejson as json result = {} result[\u0026#39;all\u0026#39;] = {} pipe = Popen([\u0026#39;getent\u0026#39;, \u0026#39;hosts\u0026#39;], stdout=PIPE, universal_newlines=True) result[\u0026#39;all\u0026#39;][\u0026#39;hosts\u0026#39;] = [] for line in pipe.stdout.readlines(): s = line.split() result[\u0026#39;all\u0026#39;][\u0026#39;hosts\u0026#39;]=result[\u0026#39;all\u0026#39;][\u0026#39;hosts\u0026#39;]+s result[\u0026#39;all\u0026#39;][\u0026#39;vars\u0026#39;] = {} if len(sys.argv) == 2 and sys.argv[1] == \u0026#39;--list\u0026#39;: print(json.dumps(result)) elif len(sys.argv) == 3 and sys.argv[1] == \u0026#39;--host\u0026#39;: print(json.dumps({})) else: print(\u0026#34;Requires an argument, please use --list or --host \u0026lt;host\u0026gt;\u0026#34;) When ran on our sample lab:\n[ansible@control base]$sudo python3 ./inventory-helper.py Requires an argument, please use --list or --host \u0026lt;host\u0026gt; [ansible@control base]$ sudo python3 ./inventory-helper.py --list {\u0026#34;all\u0026#34;: {\u0026#34;hosts\u0026#34;: [\u0026#34;127.0.0.1\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;localhost.localdomain\u0026#34;, \u0026#34;localhost4\u0026#34;, \u0026#34;localhost4.localdomain4\u0026#34;, \u0026#34;127.0.0.1\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;localhost.localdomain\u0026#34;, \u0026#34;localhost6\u0026#34;, \u0026#34;localhost6.localdomain6\u0026#34;, \u0026#34;192.168.124.201\u0026#34;, \u0026#34;ansible1\u0026#34;, \u0026#34;192.168.124.202\u0026#34;, \u0026#34;ansible2\u0026#34;], \u0026#34;vars\u0026#34;: {}}} To use a dynamic inventory script:\n[ansible@control base]$ chmod u+x inventory-helper.py [ansible@control base]$ sudo ansible -i inventory-helper.py all --list-hosts [WARNING]: A duplicate localhost-like entry was found (localhost). First found localhost was 127.0.0.1 hosts (11): 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 localhost6 localhost6.localdomain6 192.168.124.201 ansible1 192.168.124.202 ansible2 Configuring Dynamic Inventory # dynamic inventory\nscript that can be used to detect whether new hosts have been added to the managed environment.\nDynamic inventory scripts are provided by the community and exist for many different environments.\neasy to write your own dynamic inventory script.\nThe main requirement is that the dynamic inventory script works with a --list and a --host \u0026lt;hostname\u0026gt; option and produces its output in JSON format.\nScript must have the Linux execute permission set.\nMany dynamic inventory scripts are written in Python, but this is not a requirement.\nWriting dynamic inventory scripts is not an exam requirement\n#!/usr/bin/python from subprocess import Popen,PIPE import sys try: import json except ImportError: import simplejson as json result = {} result[\u0026#39;all\u0026#39;] = {} pipe = Popen([\u0026#39;getent\u0026#39;, \u0026#39;hosts\u0026#39;], stdout=PIPE, universal_newlines=True) result[\u0026#39;all\u0026#39;][\u0026#39;hosts\u0026#39;] = [] for line in pipe.stdout.readlines(): s = line.split() result[\u0026#39;all\u0026#39;][\u0026#39;hosts\u0026#39;]=result[\u0026#39;all\u0026#39;][\u0026#39;hosts\u0026#39;]+s result[\u0026#39;all\u0026#39;][\u0026#39;vars\u0026#39;] = {} if len(sys.argv) == 2 and sys.argv[1] == \u0026#39;--list\u0026#39;: print(json.dumps(result)) elif len(sys.argv) == 3 and sys.argv[1] == \u0026#39;--host\u0026#39;: print(json.dumps({})) else: print(\u0026#34;Requires an argument, please use --list or --host \u0026lt;host\u0026gt;\u0026#34;) pipe = Popen(\\['getent', 'hosts'\\], stdout=PIPE, universal_newline=True)\ngets a list of hosts using the getent function. This queries all hosts in /etc/hosts and other mechanisms where host name resolving is enabled. To show the resulting host list, you can use the \\--list command To show details for a specific host, you can use the option \\--host hostname. [ansible@control rhce8-book]$ ./listing101.py --list {\u0026#34;all\u0026#34;: {\u0026#34;hosts\u0026#34;: [\u0026#34;127.0.0.1\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;localhost.localdomain\u0026#34;, \u0026#34;localhost4\u0026#34;, \u0026#34;localhost4.localdomain4\u0026#34;, \u0026#34;127.0.0.1\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;localhost.localdomain\u0026#34;, \u0026#34;localhost6\u0026#34;, \u0026#34;localhost6.localdomain6\u0026#34;, \u0026#34;192.168.4.200\u0026#34;, \u0026#34;control.example.com\u0026#34;, \u0026#34;control\u0026#34;, \u0026#34;192.168.4.201\u0026#34;, \u0026#34;ansible1.example.com\u0026#34;, \u0026#34;ansible1\u0026#34;, \u0026#34;192.168.4.202\u0026#34;, \u0026#34;ansible2.example.com\u0026#34;, \u0026#34;ansible2\u0026#34;], \u0026#34;vars\u0026#34;: {}}} Dynamic inventory scripts are activated in the same way as regular inventory scripts: you use the -i option to either the ansible or the ansible-playbook command to pass the name of the inventory script as an argument. External directory service can be based on a wide range of solutions:\nFreeIPA\nActive Directory\nRed Hat Satellite\netc.\nAlso are available for virtual machine-based infrastructures such as VMware of Red Hat Enterprise Virtualization, where virtual machines can be discovered dynamically.\nCan be found in cloud environments, where scripts are available for many solutions, including AWS, GCE, Azure, and OpenStack.\nWhen you are working with dynamic inventory, additional parameters are normally required:\nTo get an inventory from an EC2 cloud environment, you need to enter your web keys. To pass these parameters, many inventory scripts come with an additional configuration file that is formatted in .ini style. The community-provided ec2.py script, for instance, comes with an ec2.ini parameter file. Another feature that is seen in many inventory scripts is cache management:\nCan use a cache to store names and parameters of recently discovered hosts. If a cache is provided, options exist to manage the cache, allowing you, for instance, to make sure that the inventory information really is recently discovered. ","externalUrl":null,"permalink":"/tech/ansible/ansible-inventory/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eAnsible projects\n    \u003cdiv id=\"ansible-projects\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#ansible-projects\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eFor small companies, you can use a single Ansible configuration. But for larger ones, it\u0026rsquo;s a good idea to use different project directories. A project directory contains everything you need to work on a single project. Including:\u003c/p\u003e","title":"Ansible Inventory and Ansible.cfg","type":"tech"},{"content":" Exploring playbooks YAML Managing Multiplay Playbooks Lets create our first playbook:\n[ansible@control base]$ vim playbook.yaml\n--- - name: install start and enable httpd \u0026lt;-- play is at the highest level hosts: all tasks: \u0026lt;-- play has a list of tasks - name: install package \u0026lt;-- name of task 1 yum: \u0026lt;-- module name: httpd \u0026lt;-- argument 1 state: installed \u0026lt;-- argument 2 - name: start and enable service \u0026lt;-- task 2 service: name: httpd state: started enabled: yes There are thee dashes at the top of the playbook. And sometimes you\u0026rsquo;ll find three dots at the end of a playbook. These make it easy to isolate the playbook and embed the playbook code into other projects.\nPlaybooks are written in YAML format and saved as either .yml or .yaml. YAML specifies objects as key-value pairs (dictionaries). Key value pairs can be listed in either key: value (preferred) or key=value. And dashes specify lists of embedded objects.\nThere is a collection of one or more plays in a playbook. Each play targets specific hosts and lists tasks to perform on those hosts. There is one play here with the name \u0026ldquo;install start and enable httpd\u0026rdquo;. You target the host names to target at the top of the play, not in the individual tasks performed.\nEach task is identified by \u0026ldquo;- name\u0026rdquo; (not required but recommended for troubleshooting and identifying tasks). Then the module is listed with arguments and their values under that.\nIndentation is important here. It identifies the relationships between different elements. Data elements at the same level must have the same indentation. And items that are children or properties of another element must be indented more than their parent elements.\nIndentation is created using spaces. Usually two spaces is used, but not required. You cannot use tabs for indentation.\nYou can also edit your .vimrc file to help with indentation when it detects that you are working with a YAML file: vim ~/.vimrc\nautocmd FileType yaml setlocal ai ts=2 sw=2 et Required elements:\nhosts - name of host(s) to perform play on name - name of the play tasks - one or more tasks to execute for this play To run a playbook:\n[ansible@control base]$ ansible-playbook playbook.yaml # Name of the play PLAY [install start and enable http+userd] *********************************************** # Overview of tasks and the hosts it was successful on TASK [Gathering Facts] ************************************************************** fatal: [web1]: UNREACHABLE! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Failed to connect to the host via ssh: ssh: Could not resolve hostname web1: Name or service not known\u0026#34;, \u0026#34;unreachable\u0026#34;: true} fatal: [web2]: UNREACHABLE! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Failed to connect to the host via ssh: ssh: Could not resolve hostname web2: Name or service not known\u0026#34;, \u0026#34;unreachable\u0026#34;: true} ok: [ansible1] ok: [ansible2] TASK [install package] ************************************************************** ok: [ansible1] ok: [ansible2] TASK [start and enable service] ***************************************************** ok: [ansible2] ok: [ansible1] # overview of the status of each task PLAY RECAP ************************************************************************** ansible1 : ok=3 (no changes required) changed=0 (indicates the task was successful and target node was modified.) unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible2 : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 web1 : ok=0 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 web2 : ok=0 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 Before running tasks, the ansible-playbook command gathers facts (current configuration and settings) about managed nodes.\nHow to undo playbook modifications # Ansible does not have a built in feature to undo a playbook that you ran. So to undo changes, you need to make another playbook that defines the new desired state of the host.\nWorking with YAML # Key value pairs can also be listed as:\ntasks: - name: install vsftpd yum: name=vsftpd - name: enable vsftpd service: name=vsftpd enabled=true - name: create readme file But better to list them as such for better readability:\ncopy: content: \u0026#34;welcome to the FTP server\\n\u0026#34; dest: /var/ftp/pub/README force: no mode: 0444 Some modules support multiple values for a single key:\n--- - name: install multiple packages hosts: all tasks: - name: install packages yum: name: \u0026lt;-- key with multiple values - nmap - httpd - vsftpd state: latest \u0026lt;-- will install and/or update to latest version YAML Strings # Valid fomats for a string in YAML:\nsuper string \u0026quot;super string\u0026quot; 'super string' When inserting text into a file, you may have to deal with spacing. You can either preserve newline characters with a pipe | such as:\n- name: Using | to preserve newlines copy: dest: /tmp/rendezvous-with-death.txt content: | I have a rendezvous with Death At some disputed barricade, When Spring comes back with rustling shade And apple-blossoms fill the air— Output:\nI have a rendezvous with Death At some disputed barricade, When Spring comes back with rustling shade And apple-blossoms fill the air— Or chose not to with a carrot \u0026gt;\n- name: Using \u0026gt; to fold lines into one copy: dest: /tmp/rendezvous-with-death.txt content: \u0026gt; I have a rendezvous with Death At some disputed barricade, When Spring comes back with rustling shade And apple-blossoms fill the air— Output:\nI have a rendezvous with Death At some disputed barricade, When Spring comes back with rustling shade And apple-blossoms fill the air— Checking syntax with --syntax-check # You can use the --syntax-check flag to check a playbook for errors. The ansible-playbook command does check syntax by default though, and will throw the same error messages. The syntax check stops after detecting a single error. So you will need to fix the first errors in order to see errors further in the file. I\u0026rsquo;ve added a tab in front of the host key to demonstrate:\n[ansible@control base]$ cat playbook.yaml --- - name: install start and enable httpd hosts: all tasks: - name: install package yum: name: httpd state: installed - name: start and enable service service: name: httpd state: started enabled: yes [ansible@control base]$ ansible-playbook --syntax-check playbook.yaml ERROR! We were unable to read either as JSON nor YAML, these are the errors we got from each: JSON: Expecting value: line 1 column 1 (char 0) Syntax Error while loading YAML. mapping values are not allowed in this context The error appears to be in \u0026#39;/home/ansible/base/playbook.yaml\u0026#39;: line 3, column 10, but may be elsewhere in the file depending on the exact syntax problem. The offending line appears to be: - name: install start and enable httpd hosts: all ^ here And here it is again, after fixing the syntax error:\n[ansible@control base]$ vim playbook.yaml [ansible@control base]$ cat playbook.yaml --- - name: install start and enable httpd hosts: all tasks: - name: install package yum: name: httpd state: installed - name: start and enable service service: name: httpd state: started enabled: yes [ansible@control base]$ ansible-playbook --syntax-check playbook.yaml playbook: playbook.yaml Doing a dry run # Use the -C flag to perform a dry run. This will check the success status of all of the tasks without actually making any changes. ansible-playbook -C playbook.yaml\nMultiple play playbooks # Using multiple plays in a playbook lets you set up one group of servers with one configuration and another group with a different configuration. Each play has it\u0026rsquo;s own list of hosts to address.\nYou can also specify different parameters in each play such as become: or the remote_user: parameters.\nTry to keep playbooks small. As bigger playbooks will be harder to troubleshoot. You can use include: to include other playbooks. Other than troubleshooting, using smaller playbooks lets you use your playbooks in a flexible way to perform a wider range of tasks.\nHere is an example of a playbook with two plays:\n--- - name: install start and enable httpd \u0026lt;--- play 1 hosts: all tasks: - name: install package yum: name: httpd state: installed - name: start and enable service service: name: httpd state: started enabled: yes - name: test httpd accessibility \u0026lt;-- play 2 hosts: localhost tasks: - name: test httpd access uri: url: http://ansible1 Verbose output options # You can increase the output of verbosity to an amount hitherto undreamt of. This can be useful for troubleshooting.\nVerbose output of the playbook above showing task results: [ansible@control base]$ ansible-playbook -v playbook.yaml Verbose output of the playbook above showing task results and task configuration: [ansible@control base]$ ansible-playbook -vv playbook.yaml Verbose output of the playbook above showing task results, task configuration, and info about connections to managed hosts: [ansible@control base]$ ansible-playbook -vvv playbook.yaml Verbose output of the playbook above showing task results, task configuration, and info about connections to managed hosts, plug-ins, user accounts, and executed scripts: [ansible@control base]$ ansible-playbook -vvvv playbook.yaml Lab playbook # Now we know enough to create and enable a simple webserver. Here is a playbook example. Just make sure to download the posix collection or you won\u0026rsquo;t be able to use the firewalld module: [ansible@control base]$ ansible-galaxy collection install ansible.posix\n[ansible@control base]$ cat playbook.yaml --- - name: Enable web server hosts: ansible1 tasks: - name: install package yum: name: - httpd - firewalld state: installed - name: Create welcome page copy: content: \u0026#34;Welcome to the webserver!\\n\u0026#34; dest: /var/www/html/index.html - name: start and enable service service: name: httpd state: started enabled: yes - name: enable firewall service: name: firewalld state: started enabled: true - name: Open service in firewall firewalld: service: http permanent: true state: enabled immediate: yes - name: test webserver accessibility hosts: localhost become: no tasks: - name: test webserver access uri: url: http://ansible1 return_content: yes \u0026lt;-- Return the body of the response as a content key in the dictionary result status_code: 200 \u0026lt;-- After running this playbook, you should be able to reach the webserver at http://ansible1\nWith return content and status code\nok: [localhost] =\u0026gt; {\u0026#34;accept_ranges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;changed\u0026#34;: false, \u0026#34;connection\u0026#34;: \u0026#34;close\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Welcome to the webserver!\\n\u0026#34;, \u0026#34;content_length\u0026#34;: \u0026#34;26\u0026#34;, \u0026#34;content_type\u0026#34;: \u0026#34;text/html; charset=UTF-8\u0026#34;, \u0026#34;cookies\u0026#34;: {}, \u0026#34;cookies_string\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Thu, 10 Apr 2025 12:12:37 GMT\u0026#34;, \u0026#34;elapsed\u0026#34;: 0, \u0026#34;etag\u0026#34;: \u0026#34;\\\u0026#34;1a-6326b4cfb4042\\\u0026#34;\u0026#34;, \u0026#34;last_modified\u0026#34;: \u0026#34;Thu, 10 Apr 2025 11:58:14 GMT\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;OK (26 bytes)\u0026#34;, \u0026#34;redirected\u0026#34;: false, \u0026#34;server\u0026#34;: \u0026#34;Apache/2.4.62 (Red Hat Enterprise Linux)\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;url\u0026#34;: \u0026#34;http://ansible1\u0026#34;} Adds this: \u0026quot;content\u0026quot;: \u0026quot;Welcome to the webserver!\\n\u0026quot; and this: \u0026quot;status\u0026quot;: 200, \u0026quot;url\u0026quot;: \u0026quot;http://ansible1\u0026quot;} to verbose output for that task.\n","externalUrl":null,"permalink":"/tech/ansible/ansible-playbooks/","section":"Teches","summary":"\u003cul\u003e\n\u003cli\u003eExploring playbooks\u003c/li\u003e\n\u003cli\u003eYAML\u003c/li\u003e\n\u003cli\u003eManaging Multiplay Playbooks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLets create our first playbook:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e[ansible@control base]$ vim playbook.yaml\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nn\"\u003e---\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003einstall start and enable httpd \u0026lt;-- play is at the highest level\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003ehosts\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eall\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"nt\"\u003etasks\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e\u0026lt;-- play has a list of tasks\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003einstall package \u0026lt;-- name of task 1\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003eyum\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e\u0026lt;-- module\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ehttpd \u0026lt;-- argument 1\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003einstalled \u0026lt;-- argument 2\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003estart and enable service \u0026lt;-- task 2\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003eservice\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ehttpd\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003estarted\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eenabled\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003eyes\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThere are thee dashes at the top of the playbook. And sometimes you\u0026rsquo;ll find three dots at the end of a playbook. These make it easy to isolate the playbook and embed the playbook code into other projects.\u003c/p\u003e","title":"Ansible Playbooks","type":"tech"},{"content":" Ansible Vault # For webkeys, passwords, and other types of sensitive data that you really shouldn\u0026rsquo;t store as plain text in a playbook. Can use Ansible Vault to encrypt and decrypt sensitive data to make it unreadable, and only while accessing data does it ask for a password so that it is decrypted. 1. Sensitive data is stored as values in variables in a separate variable file. 2. The variable file is encrypted, using the ansible-vault command. 3. While accessing the variable file from a playbook, you enter a password to decrypt.\nManaging Encrypted Files # ansible-vault create secret.yaml\nAnsible Vault prompts for a password and then opens the file using the default editor. The password can be provided in a password file.(must be really well protected (for example, by putting it in the user root home directory)) If a password file is used, the encrypted variable file can be created using ansible-vault create \\--vault-password-file=passfile secret.yaml ansible-vault encrypt\nencrypt one or more existing files. The encrypted file can next be used from a playbook, where a password needs to be entered to decrypt. ansible-vault decrypt\nused to decrypt the file. Commonly used ansible-vault commands: create\nCreates new encrypted file encrypt Encrypts an existing file encrypt_string Encrypts a string decrypt Decrypts an existing file rekey Changes password on an existing file view Shows contents of an existing file edit Edits an existing encrypted file Using Vault in Playbooks # --vault-id @prompt\nWhen a Vault-encrypted file is accessed from a playbook, a password must be entered. Has the ansible-playbook command prompt for a password for each of the Vault-encrypted files that may be used Enables a playbook to work with multiple Vault-encrypted files where these files are allowed to have different passwords set. ansible-playbook --ask-vault-pass\nUsed if all Vault-encrypted files a playbook refers to have the same password set. ansible-playbook --vault-password-file=secret\nObtain the Vault password from a password file. Password file should contain a string that is stored as a single line in the file. Make sure the vault password file is protected through file permissions, such that it is not accessible by unauthorized users! Managing Files with Sensitive Variables # You should separate files containing unencrypted variables from files that contain encrypted variables.\nUse group_vars and host_vars variable inclusion for this.\nYou may create a directory (instead of a file) with the name of the host or host group.\nWithin that directory you can create a file with the name vars, which contains unencrypted variables, and a file with the name vault, which contains Vault-encrypted variables.\nVault-encrypted variables can be included from a file using the vars_files parameter.\nLab: Working with Ansible Vault # 1. Create a secret file containing encrypted values for a variable user and a variable password by using ansible-vault create secrets.yaml\nSet the password to password and enter the following lines:\nusername: bob pwhash: password When creating users, you cannot provide the password in plain text; it needs to be provided as a hashed value. Because this exercise focuses on the use of Vault, the password is not provided as a hashed value, and as a result, a warning is displayed. You may ignore this warning.\n2. Create the file create-users.yaml and provide the following contents:\n--- - name: create a user with vaulted variables hosts: ansible1 vars_files: - secrets.yaml tasks: - name: creating user user: name: \u0026#34;{{ username }}\u0026#34; password: \u0026#34;{{ pwhash }}\u0026#34; 3. Run the playbook by using ansible-playbook --ask-vault-pass create-users.yaml\n4. Change the current password on secrets.yaml by using ansible-vault rekey secrets.yaml and set the new password to secretpassword.\n5. To automate the process of entering the password, use echo secretpassword \u0026gt; vault-pass\n6. Use chmod 400 vault-pass to ensure the file is readable for the ansible user only; this is about as much as you can do to secure the file.\n7. Verify that it\u0026rsquo;s working by using ansible-playbook --vault-password-file=vault-pass create-users.yaml JunctionScallopPoise\n","externalUrl":null,"permalink":"/tech/ansible/ansible-vault/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eAnsible Vault\n    \u003cdiv id=\"ansible-vault\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#ansible-vault\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFor webkeys, passwords, and other types of sensitive data that you really shouldn\u0026rsquo;t store as plain text in a playbook.\u003c/li\u003e\n\u003cli\u003eCan use Ansible Vault to encrypt and decrypt sensitive data to make it unreadable, and only while accessing data does it ask for a password so that it is decrypted.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e1. Sensitive data is stored as values in variables in a separate variable file.\n2. The variable file is encrypted, using the \u003cstrong\u003eansible-vault\u003c/strong\u003e command.\n3. While accessing the variable file from a playbook, you enter a password to decrypt.\u003c/p\u003e","title":"Ansible Vault","type":"tech"},{"content":"6.0 Automation and Programmability 6.6 Recognize the capabilities of configuration mechanisms Puppet, Chef, and Ansible\nConfiguration Drift\nThe on-device manual configuration process does not track change history: which lines changed, what changed on each line, what old configuration was removed, who changed the configuration, when each change was made. External systems used by good systems management processes, like trouble ticketing and change management software, may record details. However, those sit outside the configuration and require analysis to figure out what changed. They also rely on humans to follow the operational processes consistently and correctly; otherwise, an engineer cannot find the entire history of changes to a configuration. Referring to historical data in change management systems works poorly if a device has gone through multiple configuration changes over a period of time. Centralized Configuration Files and Version Control\nconfiguration management tools use version control software to track the changes to centralized configuration files, noting who changes a file, what lines and specific characters changed, when the change occurred, and so on. also allow you to compare the differences between versions of the files over time\nConfiguration Monitoring and Enforcement With this new modelin the centralized repository. The configuration management tool can then be directed to copy or , engineers should make changes by editing the associated configuration files apply the configuration to the device the central config file and the device’s running-config (and startup-config) should be identical. eventually someone will change the devices directly eventually, some configuration drift can occur. Configuration management toolsconfiguration differs from the intended ideal configuration, and then either reconfigure the can monitor device configurations to discover when the device device or notify the network engineering staff to make the changeconfiguration monitoring or configuration enforcement, particularly if the tool automatically. This feature might be called changes the device configuration. The automated configuration management software asks for a copy of the device’s runningconfig file, as shown in steps 1 and 2. At step 3, the config management software compares the - ideal config file with the just-arrived running-config file to check whether they have any differences (configuration drift). Per the configuration of the tool, it either fixes the configuration or notifies the staff about the configuration drift.\nConfiguration Provisioning how to provision or deploy changes to the configuration once made by changing files in the configuration management system. some of the majorfeatures included in all of the configuration management tools The core function toimplement configuration changes in one device after someone has edited the device’s centralized configuration file The ability to attribute (such as those of a particular role), or just one device, based on attributes and choose which subset of devices to configure: all devices, types with a given attribute (such as those of a particular role), or just one device, based on attributes and logic The ability to determine if each change was accepted or rejected, and to use logic to react differently in each case depending on the result For each change, the ability to revert to the original configuration if even one configuration command is rejected on a device Thewhether the change will work or not when attemptedability to validate the change now (without actually making the change) to determine The ability to configuration management tool’s intended configuration does match the device’s check the configuration after the process completes to confirm that the configuration The ability tonot use logic to choose whether to save the running-config to startup-config or The ability to similar roles can use the same template but with different valuesrepresent configuration files as templates and variables so that devices with The ability to store the logic steps in a file, scheduled to execute, so that the changes can be implemented by the automation tool without the engineer being present Configuration Templates and Variables Configuration management tools can separate the components of a configuration into the parts in common to all devices in that role (the variables) (the template)versus the parts unique to any one device\nAnsible using the Jinja2 language for templates\nTo supply the values for a device,Ansible calls for defining variable files using YAML The file shows the Example 19-1, but now defined as variables.syntax for defining the variables shown in the complete configuration in the engineer would create and edit one template filethat looks like Example 19- 2 the engineer would create and edit one template filethat looks like Example 19- 2 then create and edit one variable file like Example 19would process the files to create complete configuration files like the text shown in Example 19-3 for each branch office router. Ansible -1. using templates has some big advantages. Templates helping to avoid snowflakes (uniquely configured devices).increase the focus on having a standard configuration for each device role, New devices with an existing role can be deployed easily by simply copying an existing per- device variable file and changing the values. Templatesallow for easier troubleshooting because troubleshooting issues with one standard template should find and fix issues with all devices that use the same template. Tracking the file versions for the template versus the variables files allows for easier troubleshooting as well. Issues with a device can be investigated to find changes in the device’s settings separately from the standard configuration template. Files That Control Configuration Automation Configuration management tools provide methods that tell the tool what changes to make, to which devices, and when the files you could see in any of the configuration management tools. Ansible, Puppet, and Chef Basics some of the tools do not run in a Windows OS you need to install Ansible on some computer: Mac, Linux, or a Linux VM free open-source version or use the paid Ansible Tower serverversion Once it is installed, you create several text files, such as the following:\nAnsible\nPlaybooks:These files provide actions and logic about what Ansible should do. Inventory:These files provide device hostnames along with information about each device, like device roles, so Ansible can perform functions for subsets of the inventory Templates: Using Jinja2 language, the templates represent a device’s configuration but with variables(see Example 19-2). Variablestemplates: Using YAML, a file can list variables that Ansible will substitute into (see Example 19-3). uses an agentlessarchitecture Ansible does not rely on any code (agent) running on the network device Ansiblemake changes and extract information.relies on features typical in network devices, namely SSH and/or NETCONF, to using apush model(per Figure 19-8) rather than a pull model (like Puppet and Chef)\nAnsible can do both configuration provisioning (configuring devices after changes are made in the files) and configuration monitoring (checking to find out whether the device config matches the ideal configuration on the control node). To do configuration monitoring, Ansible uses logic modules that detect and list configuration differences, after which the playbook defines what action to take (reconfigure or notify). Puppet installing Puppet on aLinuxhost install it on aLinux server called a Puppet master free open-source version with paid versions available Puppet also uses several important text files with different components Manifest: This is a human-readable text file on the Puppet master, using a language defined by Puppet, used to define the desired configuration state of a device. Resource, Class, Modulelargest component (module) being composed of smaller classes, which are in turn : These terms refer to components of the manifest, with the composed of resources. Templates: Using a Puppet domain-specific language, these files allow Puppet to generate manifests (and modules, classes, and resources) by substituting variables into the template.\nAnsible’s playbooks use an imperative language, whereas Puppet uses a declarative language with Ansible, the playbook will list tasks and choices based on those results, like “Configure all branch routers in these locations, and if errors occur for any device, do these extra tasks for that device.” Puppet manifests instead declare the end state that a device should have: “This branch router should have the configuration in this file by the end of the process.” The manifest, built by the engineer, defines the end state, and Puppet has the job to cause the device to have that configuration, without being told the specific set of steps to take. uses an agent-based architecture for network device support. Some network devices enable Puppet support via an on-device agent not every Cisco OS supports Puppet agents, so Puppet solves that problem usinga proxy agent running on some external hostuses SSH to communicate with the network device(calledagentless operation). The external agent then uses a pull model to make that configuration appear in the device Step 1. The engineer creates and edits all the files on the Puppet server. Step 2. The engineereach device. configures and enables the on-device agent or a proxy agent for Step 3. The agent pulls manifest details from the server, which tells the agent what its configuration should be. configuration should be. Step 4. performs additional pulls to get all required detail, with the agent updating the device If the agent device’s configuration should be updated, the Puppet agent configuration.\nChef Chef Automatebeing the product that most people refer to simply as Chef you probablyrun Chef as a server (called server-client mode Chef files that are stored on the Chef server you can also run Chef in standalone mode(calledChef Zero), which is helpful when you’re just getting started and learning in the lab. you create several text files with different components, Resource:The configuration objects whose state is managed by Chef; for instance, a set of configuration commands for a network devicea recipe in a cookbook —analogous to the ingredients in Recipe:TheChef logic applied to resources to determine when, how, and whether to act against the resources—analogous to a recipe in a cookbook Cookbooks: A set of recipes about the same kinds of work, grouped together for easier management and sharing Runlist: Anordered list of recipes that should be run against a given device each managed device (called a Chef node or Chef client) runs an agent. configuration monitoring in that the client pulls recipes and resources from the Chef server The agent performs and then adjusts its configuration to stay in sync with the details in those recipes and runlists Chef requires on-device Chef client code, and many Cisco devices do not support a Chef Chef requires onclient, so you will likely see more use of Ansible and Puppet for Cisco device configuration -device Chef client code, and many Cisco devices do not support a Chef management. Summary of Configuration Management Tools Ansible appears to have the most interest, then Puppet, and then Chef. Ansible’s agentless architecture and the use of SSH provides support for a wide range of Cisco devices. Puppet’s agentless model also creates wide support for Cisco devices. the column for Puppet assumes an on-device agent. ###### R2 G0/1 2001:DB8:0:1:201:63FF:FEb0:b802 R1 G0/1 2001:DB8::230:f2FF:FE36:4502 ###### 0 1 ###### 2 3 ###### 4 5 ###### 6 7 Wireless It is not possible to configure Layer 2 security on a Guest WLAN - - WPA + WPA2802.1X - - Static WEPStatic WEP + 802.1X - - CKIPNone + EAP Passthrough Layer 2 security\n- - IPSecVPN Pass-Through - - Web Authentication (guest)Web Passthrough (guest) Layer 3 security\nNot all layer 2 is compatible with layer 3 so configure layer 2 first FlexConnect ACLs\n- - Applied per VLAN and per APConfigured on lightweight AP VLAN interfaces-(if the AP is in flexconnect mode) - - Name flexconnect ACLs differently than other ACLs on the WLANSupported on the native vlan - - Applied at engress or igress as a whole (each rule cannot be configured differently.Support Implicit deny - - management framescontain SSID - Can be disabled to hide the presence of a wireless network Beacons\nManagement Frames\nAssociation Requests\u0026ndash; Sent from wireless client to AP to request wireless accessComes after the client has been authenticated by AP or authentication server\n## Exam Missed Tuesday, November 30, 2021 8:14 PM Association Response- Provides answer to Association request\n- - not management frames that contain the SSID of a networkSent by AP or wireless client to terminate an authorized connection - Can also end connections between rogue clients and rogue Aps Deauthentication Frame\n- - Not management frames that contain an SSIDSent by wireless clients to request network information from any ap within transmission range - AP the can provide a probe response with the info Probe Requests\n□□ IP and default gatewayDNS server address □□ Subnetmask configured on the APAP Identifier □ MAC Address\nShow ap config general MyLAP - produces general AP configuration output\nshow ap config global - syslog server settings\nCommands\nCisco Autonomous WLAN Solution\n- - Simplifies management and deployment of WAPs in a Cisco Autonomous WLAN solutionDynamic RF management - - network securityintrusion detection - - selfMonitoring and reporting services-healing capabilities CiscoWorks Wireless LAN Solution Engine (WLSE)\nWireless Domain Services (WDS)\u0026ndash; IOS feature that can be installed on Aps to allow them to interact with a WLSECollects and aggregates radio info and forwards it to the WLSE\nCisco Unified Wireless Network WLC Cisco Wireless Services Module (WiSM)\u0026ndash; used on a cisco unified wireless networkInstalled on a certain series of router and/or switch\nIp arp inspection - DAI ports are untrusted by default Automation and Programmability - - installed on desktop workstationFree Java based application - Does not support SDA - Cisco Network assistant - DNA Center- Accessed via browser - Prime infastructure-- Does not support SDABrowser based GUI - - used to communicate with an SDN application planeEncode data in XML or JSON - Can be northbound or southbound APIs - REST APIs IP Connectivity ▪▪ cost = reference bandwidth / interface bandwidthdefault reference bandwidth is 100 mbps ▪ 100 mbps or higher will default to a cost of 1 because of this formula\n- OSPF- Formula to determine cost Five OSPF Network types\n- - Enabled on Fiber Distributed Data Interface (FDDI)Enabled on Ethernet Interfaces - - DR and BDR elections are performedMulticast updates are sent - - Manual configuration of neighbor routers with the **ip ospf network broadcast** command to configure **neighbor** command is not needed - Broadcast - - Enabled on frame Relay and X.25 InterfacesDR and BDR elections are performed - - No multicastsneeds manual configurations of neighbor routers with the **neighbor** command - - Hello 30 dead 120Non broadcast Multi Access network (NBMA) - Nonbroadcast - - Non broadcast Multi Access network (NBMA) **ip ospf network non-broadcast** to configure - - DR and BDR elections are not performed10 hello 40 dead - - **ip ospf network pointHDLC and PPP -to-point** - point-to-point - - DR and BDR elections are not performedMulticast updates are sent - - manual configurations with hello 30 dead 120 **neighbor** command are not required - **ip ospf network point-to-multipoint** - point-to-multipoint - - DR and BDR elections are not performedno multicasts - - manual configuration with the Hello 30 Dead 120 **neighbor** command is needed - ip ospf network point-to-multipoint nonbroadcast - Point-to-multipoint nonbroadcast FHRPs\n- - RFC 2281Multiple routers are assigned an HSRP group - - Routers function as a single gatewayOne active router (highest priority) and one standby router (second highest priority) - - Other routers in the group are in the listen state (new elections if active router fails)VMAC: 0000:5E00:0101 ###### HSRP - - Configure multiple routers as a GLBP groupRouters in group receive traffic that is sent to a virtual IP address configured for the group - - Each group contains a Active Virtual Gateway (AVG) (highest priority or highest IP)Other routers in group are configured as primary (up to 4) or secondary active virtual forwarders (AVFs) - - Primary AVfs can participate in forwarding traffic (load balancing)VMAC: 0007:B400:0102 ###### GLBP - - Virtual MAC: 0000.5E00.01xx (xx would be the group number)Supported by Cisco and non cisco devices - - Routers are assigned to a VRRP groupOne master router (highest priority)□ group functions as a single gateway for clients - - All other routers in group are backup routersVMAC: 0000:5E00:0101 ###### VRRP Network Access IP Phone Class of Service (CoS) value - Cisco IP phones assign CoS priority value of 0 (worst) to traffic received from a host on a access port (changing the value that the host sent the data with) - Voice traffic is vulnerable to degradation and deterioration if traffic is sent unevenly - - Voice traffic is vulnerable to degradation and deterioration if traffic is sent unevenly802.1p - - Ip Phones support QoSQos uses the CoS value to prioritize the forwarding of voice and data packets - if the data packets from the node connected to the phone have a higher priority than voice traffic on the phone, data packets could take precedence - - Voice data traffic on phones is 5 by defaultVoice signaling traffic is 3 by default - Can configure the phone so that it does not override CoS values coming from the host for mission critical data - any VTP advertisements received by transparentregardless of domain name configuration -mode switches will be forwarded on all trunk ports - **vtp version 2** ###### - VTP V2 ###### VTP Network Fundamentals - - Uses spine and leaf topologyNetwork application policies are defines on APIC and implemented by spine and leaf nodes - Spine and leaf- optimized for east-west data transfer ###### - ACI - **power inline police** □ when PD attempts to draw more that the cutoff power the interface will errdisable and a log message will be sent to the console.command - errdisable recovery cause inline-power - **power inline police action log** □ Interface will restart if there is a PD issue and send a log to the console PoE- Power policing\nSecurity Fundementals VPN IPSec encryption process (site-to-site) - - session key is added to data portion of packetVPN and IP headers are encapsulated onto the packet - - The packet is sentreceiving device decrypts the device with the session key ","externalUrl":null,"permalink":"/tech/networking/ansiblepuppetchef/","section":"Teches","summary":"\u003cp\u003e6.0 Automation and Programmability\n6.6 Recognize the capabilities of configuration mechanisms Puppet, Chef, and Ansible\u003c/p\u003e\n\u003cp\u003eConfiguration Drift\u003c/p\u003e\n\u003cp\u003eThe on-device manual configuration process does not track change history: which lines changed, what changed on each line, what old configuration was removed, who changed the configuration, when each change was made.\nExternal systems used by good systems management processes, like trouble ticketing and change management software, may record details. However, those sit outside the configuration and\nrequire analysis to figure out what changed. They also rely on humans to follow the operational processes consistently and correctly; otherwise, an engineer cannot find the entire history of\nchanges to a configuration.\nReferring to historical data in change management systems works poorly if a device has gone through multiple configuration changes over a period of time.\nCentralized Configuration Files and Version Control\u003c/p\u003e","title":"Ansible, Puppet, and Chef","type":"tech"},{"content":" ansible.cfg # You can store this in a project\u0026rsquo;s directory or a user\u0026rsquo;s home directory, in the case that multiple user\u0026rsquo;s want to have their own Ansible configuration. Or in /etc/ansible if the configuration will be the same for every user and every project. You can also specify these settings in Ansible playbooks. The settings in a playbook take precedence over the .cfg file.\nansible.cfg precedence (Ansible uses the first one it finds and ignores the rest.)\nANSIBLE_CONFIG environment variable ansible.cfg in current directory ~/.ansible.cfg /etc/ansible/ansible.cfg Generate an example config file in the current directory. All directive are commented out by default: [ansible@control base]$ ansible-config init --disabled \u0026gt; ansible.cfg\nInclude existing plugin to the file: ansible-config init --disabled -t all \u0026gt; ansible.cfg\nThis generates an extremely large file. So I\u0026rsquo;ll just show Van Vugt\u0026rsquo;s example in .ini format:\n[defaults] \u0026lt;-- General information remote_user = ansible \u0026lt;--Required host_key_checking = false \u0026lt;-- Disable SSH host key validity check inventory = inventory [privilege_escalation] \u0026lt;-- Define how ansible user requires admin rights to connect to hosts become = True \u0026lt;-- Escalation required become_method = sudo become_user = root \u0026lt;-- Escalated user become_ask_pass = False \u0026lt;-- Do not ask for escalation password Privilege escalation parameters can be specified in ansible.cfg, playbooks, and on the command line.\n","externalUrl":null,"permalink":"/tech/ansible/ansible.cfg/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eansible.cfg\n    \u003cdiv id=\"ansiblecfg\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#ansiblecfg\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eYou can store this in a project\u0026rsquo;s directory or a user\u0026rsquo;s home directory, in the case that multiple user\u0026rsquo;s want to have their own Ansible configuration. Or in /etc/ansible if the configuration will be the same for every user and every project. You can also specify these settings in Ansible playbooks. The settings in a playbook take precedence over the .cfg file.\u003c/p\u003e","title":"Ansible.cfg","type":"tech"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":" AutoFS # Automatically mount and unmount on clients during runtime and system reboots. Triggers mount or unmount action based on mount point activity. Client-side service Mount an NFS share on demand Entry placed in AutoFS config files. Automatically mounts share upon detecting activity in it\u0026rsquo;s mount point. (touch, ls, cd) unmounts share if the share hasn\u0026rsquo;t been accessed for a predefined period of time. Mounts managed with autofs should not be mounted manually via /etc/fstab to avoid inconsistencies. Saves Kernel from having to maintain unused NFS shares. (Improved performance!) NFS shares are defined in config files called maps (/etc/ or /etc/auto.master.d/) Does not use /etc/fstab. Does not require root to mount a share (fstab does). Prevents client from hanging if share is down. Share is unmounted if not accessed for 5 minutes (default) Supports wildcard characters or environment variables. Automount daemon in the userland mounts configured shares automatically upon access. invoked at system boot. Reads AutoFS master map and create initial mount point entries. (not mounting yet) Does not mount shares until user activity is detected. Unmounts after set timeframe of inactivity. Use the mount command on a share to verify the path of the AutoFS map, file system type, and options used during mount. /etc/autofs.conf/ preset Directives: master_map_name=auto.master timeout = 300 negative_timeout = 60 mount_nfs_default_protocol = 4 logging = none\nAdditional directives: # master_map_name\nName of the master map. Default is /etc/auto.master timeout\nTime in second to unmount a share. negative_timeout\nTimeout (in seconds) value for failed mount attempts. (1 minute default) mount_nfs_default_protocol\nSets the NFS version used to mount shares. logging\nLogging level (none, verbose, debug)\nDefault is none (disabled)\nNormally left to their default values.\nAutoFS Maps # Where AutoFS finds the shares to mount and their locations. Also tells Autofs what options to use. Map Types:\nmaster direct indirect Master Map\nDefine entries for indirect and direct maps.\n/etc/auto.master is default Default is defined in /etc/autofs.conf with master_map_name directive. May be used to define entries for indirect and direct maps. But it is recommended to store user-defined maps in /etc/auto.master.d/ AutoFS service parses this at startup. You can append an option to auto.master but it will apply globally to all subentries in the specified map file. Map entry format examples:\n/- /etc/auto.master.d/auto.direct \\# Line 1 /misc /etc/auto.misc \\# Line 2 Direct Map # /- /etc/auto.master.d/auto.direct \u0026lt;-- defines direct map and points to auto.direct for details Mount shares on unrelated mount points\nAlways visible to users Can exist with an indirect share under one parent directory Accessing a directory containing many direct mount points mounts all shares. Each direct map entry places a separate share entry to /etc/mtab /etc/mtab maintains a list of all mounted file systems whether they are local or remote. Updated whenever a local file system, removable file system, or a network share is mounted or unmounted. Indirect Map # /misc /etc/auto.misc \u0026lt;-- indirect map and points to auto.misc for details Automount removable filesystems\nMount point /misc precedes mount point entries in /etc/auto.miscq Used to automount removable file systems (CD, DVD, USB disks, etc.) Custom indirect map files should be located in /etc/auto.master.d/ Preferred over direct mount for mounting all shares under one common parent directory. Become visible only after they have been accessed. Local and indirect mounted shares cannot coexist under the same parent directory. One entry in /etc/mtab gets added for each indirect map. Usually better to use indirect map for automounting NFS shares. Lab: Access NFS Share Using Direct Map (server10) # Install Autofs sudo dnf install -y autofs Create mount point /autodir using mkdir sudo mkdir /autodir Add an entry to /etc/auto.master to point the AutoFS service to the auto.dir file for more information: /- /etc/auto.master.d/auto.dir Create /etc/auto.master.d/auto.dir and add the mount point, NFS server, and share info: /autodir server20:/common Start AutoFS service and enable it at startup: sudo systemctl enable --now autofs Make sure AUtoFS service is running. Use -l and \u0026ndash;no-pager options to show full details without piping the output to a pager program (pg) sudo systemctl status autofs -l --no-pager Run ls on the mount point then verify the share is automounted and accessible with mount. ls /autodir mount | grep autodir Wait 5 minutes and run the mount command again to see it has disappeared. mount | grep autodir Exercise 16-4: Access NFS Share Using Indirect Map # configure an indirect map to automount the NFS share /common that is available from server20. install the relevant software and set up AutoFS maps to support the automatic mounting. Observe that the specified mount point \u0026ldquo;autoindir\u0026rdquo; is created automatically under /misc. Note that /common is already mounted on the /local mount point via the fstab file and it is also configured via a direct map for automounting on /autodir. There should occur no conflict in configuration or functionality among the three.\n1. Install the autofs software package if it is not already there:\n2. Confirm the entry for the indirect map /misc in the /etc/auto.master file exists:\n[root@server30 common]# grep ^/misc /etc/auto.master /misc\t/etc/auto.misc 3. Edit the /etc/auto.misc file and add the mount point, NFS server, and share information to it:\nautoindir server30:/common 4. Start the AutoFS service now and set it to autostart at system reboots:\n[root@server40 /]# systemctl enable --now autofs 5. Verify the operational status of the AutoFS service. Use the -l and --no-pager options to show full details without piping the output to a pager program (the pg command in this case):\n[root@server40 /]# systemctl status autofs -l --no-pager 6. Run the ls command on the mount point /misc/autoindir and then grep for both auto.misc and autoindir on the mount command output to verify that the share is automounted and accessible:\n[root@server40 /]# ls /misc/autoindir test.text [root@server40 /]# mount | egrep \u0026#39;auto.misc|autoindir\u0026#39; /etc/auto.misc on /misc type autofs (rw,relatime,fd=7,pgrp=3321,timeout=300,minproto=5,maxproto=5,indirect,pipe_ino=31779) server30:/common on /misc/autoindir type nfs4 (rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.40,local_lock=none,addr=192.168.0.30) /misc/autoindir has been auto generated. You can use the umbrella mount point /misc to mount additional auto-generated mount points. Automounting User Home Directories \\ # AutoFS allows us to automount user home directories by exploiting two special characters in indirect maps.\nasterisk (*)\nReplaces the references to specific mount points ampersand (\u0026amp;)\nSubstitutes the references to NFS servers and shared subdirectories.\nWith user home directories located under /home, on one or more NFS servers, the AutoFS service will connect with all of them simultaneously when a user attempts to log on to a client.\nThe service will mount only that specific user\u0026rsquo;s home directory rather than the entire /home.\nThe indirect map entry for this type of substitution is defined in an indirect map, such as /etc/auto.master.d/auto.home.\n* -rw \u0026amp;:/home/\u0026amp;\nWith this entry in place, there is no need to update any AutoFS configuration files if additional NFS servers with /home shared are added or removed.\nIf user home directories are added or deleted, there will be no impact on the functionality of AutoFS.\nIf there is only one NFS server sharing the home directories, you can simply specify its name in lieu of the first \u0026amp; symbol in the above entry.\nExercise 16-5: Automount User Home Directories Using Indirect Map # There are two portions for this exercise. The first portion should be done on server20 (NFS server) and the second portion on server10 (NFS client) as user1 with sudo where required.\nfirst portion\ncreate a user account called user30 with UID 3000. add the /home directory to the list of NFS shares so that it becomes available for remote mount. second portion\ncreate a user account called user30 with UID 3000, base directory /nfshome, and no home directory. create an umbrella mount point called /nfshome for mounting the user home directory from the NFS server. install the relevant software and establish an indirect map to automount the remote home directory of user30 under /nfshome. observe that the home directory is automounted under /nfshome when you sign in as user30. On NFS server server20:\n1. Create a user account called user30 with UID 3000 (-u) and assign password \u0026ldquo;password1\u0026rdquo;:\n[root@server30 common]# useradd -u 3000 user30 [root@server30 common]# echo password1 | sudo passwd --stdin user30 Changing password for user user30. passwd: all authentication tokens updated successfully. 2. Edit the /etc/exports file and add an entry for /home (do not modify or remove the previous entry): /home server40(rw)\n3. Export all the shares listed in the /etc/exports file:\n[root@server30 common]# sudo exportfs -avr exporting server40.example.com:/home exporting server40.example.com:/common On NFS client server10:\n1. Install the autofs software package if it is not already there: dnf install autofs\n2. Create a user account called user30 with UID 3000 (-u), base home directory location /nfshome (-b), no home directory (-M), and password \u0026ldquo;password1\u0026rdquo;:\n[root@server40 misc]# sudo useradd -u 3000 -b /nfshome -M user30 [root@server40 misc]# echo password1 | sudo passwd --stdin user30 This is to ensure that the UID for the user is consistent on the server and the client to avoid access issues.\n3. Create the umbrella mount point /nfshome to automount the user\u0026rsquo;s home directory:\nsudo mkdir /nfshome 4. Edit the /etc/auto.master file and add the mount point and indirect map location to it: /nfshome /etc/auto.master.d/auto.home\n5. Create the /etc/auto.master.d/auto.home file and add the following information to it: * -rw server30:/home/\u0026amp;\nFor multiple user setup, you can replace \u0026ldquo;user30\u0026rdquo; with the \u0026amp; character, but ensure that those users exist on both the server and the client with consistent UIDs.\n6. Start the AutoFS service now and set it to autostart at system reboots. This step is not required if AutoFS is already running and enabled. systemctl enable --now autofs\n7. Verify the operational status of the AutoFS service. Use the -l and --no-pager options to show full details without piping the output to a pager program (the pg command): systemctl status autofs -l --no-pager\n8. Log in as user30 and run the pwd, ls, and df commands for verification:\n[root@server40 nfshome]# su - user30 [user30@server40 ~]$ ls user30.txt [user30@server40 ~]$ pwd /nfshome/user30 [user30@server40 ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 4.0M 0 4.0M 0% /dev tmpfs 888M 0 888M 0% /dev/shm tmpfs 356M 5.1M 351M 2% /run /dev/mapper/rhel-root 17G 2.2G 15G 13% / /dev/sda1 960M 344M 617M 36% /boot tmpfs 178M 0 178M 0% /run/user/0 server30:/common 17G 2.2G 15G 13% /local server30:/home/user30 17G 2.2G 15G 13% /nfshome/user30 EXAM TIP: You may need to configure AutoFS for mounting a remote user home directory.\nNFS DIY Labs # Lab: Configure NFS Share and Automount with Direct Map # As user1 with sudo on server30, share directory /sharenfs (create it) in read/write mode using NFS. [root@server30 /]# mkdir /sharenfs [root@server30 /]# chmod 777 /sharenfs [root@server30 /]# vim /etc/exports # Add -\u0026gt; /sharenfs server40(rw) [root@server30 /]# dnf -y install nfs-utils [root@server30 /]# firewall-cmd --permanent --add-service nfs [root@server30 /]# firewall-cmd --reload success [root@server30 /]# systemctl --now enable nfs-server [root@server30 /]# exportfs -av exporting server40.example.com:/sharenfs On server40 as user1 with sudo, install the AutoFS software and start the service. [root@server40 nfshome]# dnf -y install autofs Configure the master and a direct map to automount the share on /mntauto (create it). [root@server40 ~]# vim /etc/auto.master /- /etc/auto.master.d/auto.dir [root@server40 ~]# vim /etc/auto.master.d/auto.dir /mntauto server30:/sharenfs [root@server40 /]# mkdir /mntauto [root@server40 ~]# systemctl enable --now autofs Run ls on /mntauto to trigger the mount. [root@server40 /]# mount | grep mntauto /etc/auto.master.d/auto.dir on /mntauto type autofs (rw,relatime,fd=10,pgrp=6211,timeout=300,minproto=5,maxproto=5,direct,pipe_ino=40247) server30:/sharenfs on /mntauto type nfs4 (rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.40,local_lock=none,addr=192.168.0.30) Use df -h to confirm. [root@server40 /]# df -h | grep mntauto server30:/sharenfs 17G 2.2G 15G 13% /mntauto Lab: Automount NFS Share with Indirect Map # As user1 with sudo on server40, configure the master and an indirect map to automount the share under /autoindir (create it). [root@server40 /]# mkdir /autoindir [root@server40 etc]# vim /etc/auto.master /autoindir /etc/auto.misc [root@server40 etc]# vim /etc/auto.misc sharenfs server30:/common [root@server40 etc]# systemctl restart autofs Run ls on /autoindir/sharenfs to trigger the mount. [root@server40 etc]# ls /autoindir/sharenfs test.text Use df -h to confirm. [root@server40 etc]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 4.0M 0 4.0M 0% /dev tmpfs 888M 0 888M 0% /dev/shm tmpfs 356M 5.1M 351M 2% /run /dev/mapper/rhel-root 17G 2.2G 15G 13% / /dev/sda1 960M 344M 617M 36% /boot tmpfs 178M 0 178M 0% /run/user/0 server30:/common 17G 2.2G 15G 13% /autoindir/sharenfs ","externalUrl":null,"permalink":"/tech/linux/autofs/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eAutoFS\n    \u003cdiv id=\"autofs\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#autofs\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAutomatically mount and unmount on clients during runtime and system reboots.\u003c/li\u003e\n\u003cli\u003eTriggers mount or unmount action based on mount point activity.\u003c/li\u003e\n\u003cli\u003eClient-side service\u003c/li\u003e\n\u003cli\u003eMount an NFS share on demand\u003c/li\u003e\n\u003cli\u003eEntry placed in AutoFS config files.\u003c/li\u003e\n\u003cli\u003eAutomatically mounts share upon detecting activity in it\u0026rsquo;s mount point. (touch, ls, cd)\u003c/li\u003e\n\u003cli\u003eunmounts share if the share hasn\u0026rsquo;t been accessed for a predefined period of time.\u003c/li\u003e\n\u003cli\u003eMounts managed with autofs should not be mounted manually via /etc/fstab to avoid inconsistencies.\u003c/li\u003e\n\u003cli\u003eSaves Kernel from having to maintain unused NFS shares. (Improved performance!)\u003c/li\u003e\n\u003cli\u003eNFS shares are defined in config files called maps (\u003cem\u003e/etc/\u003c/em\u003e or \u003cem\u003e/etc/auto.master.d/\u003c/em\u003e)\u003c/li\u003e\n\u003cli\u003eDoes not use /etc/fstab.\u003c/li\u003e\n\u003cli\u003eDoes not require root to mount a share (fstab does).\u003c/li\u003e\n\u003cli\u003ePrevents client from hanging if share is down.\u003c/li\u003e\n\u003cli\u003eShare is unmounted if not accessed for 5 minutes (default)\u003c/li\u003e\n\u003cli\u003eSupports wildcard characters or environment variables.\u003c/li\u003e\n\u003cli\u003eAutomount daemon\n\u003cul\u003e\n\u003cli\u003ein the userland mounts configured shares automatically upon access.\u003c/li\u003e\n\u003cli\u003einvoked at system boot.\u003c/li\u003e\n\u003cli\u003eReads AutoFS master map and create initial mount point entries. (not mounting yet)\u003c/li\u003e\n\u003cli\u003eDoes not mount shares until user activity is detected.\u003c/li\u003e\n\u003cli\u003eUnmounts after set timeframe of inactivity.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUse the mount command on a share to verify the path of the AutoFS map, file system type, and options used during mount.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e/etc/autofs.conf/\u003c/em\u003e preset Directives:\nmaster_map_name=auto.master\ntimeout = 300\nnegative_timeout = 60\nmount_nfs_default_protocol = 4\nlogging = none\u003c/p\u003e","title":"AutoFS","type":"tech"},{"content":" My name is Marcel and I\u0026rsquo;m partially a shell.. # A shell is a program that takes commands and passes them to the operating system.^1^ This is done via terminal emulator with keyboard commands or by using scripts ran on the system. There are many shell programs that you can use on Linux. Almost all Linux distributions come with a shell called Bash. Some others include zsh, fsh, ksh, and Tcsh. (But not limited to)\nShells have different features such as built in commands, job control, alias definitions, history substitution, PATH searching, command completion, and more. Each shell has it\u0026rsquo;s own syntax, hotkeys, way of doing things. Most of them follow a standard called \u0026ldquo;POSIX\u0026rdquo; that help with script portability between shells.\nYou can see a list of more shells and a comparison of their features on this Wikipedia page.\nTerminator emulators.. # I meant Terminal Emulators! Silly me..\nThe Current shell\nWhere a program is executed. Sub-shell (child shell)\ncreated within a shell to run a program. There are two types of variables. Local variables are private variables to the shell that creates it. And they are only used by programs started in the shell that created them. Environment variables are passed to any sub-shells created by the current shell. As well as any programs ran in the current and sub shells.\n- Value stored in an environment variable is accessible to the program, as well as any sub-programs that it spawns during its lifecycle. - Any environment variable set in a sub-shell is lost when the sub-shell terminates. - `env` or the `printenv` command to view predefined environment variables. - Common predefined environment variables: - **DISPLAY** - Stores the hostname or IP address for graphical terminal sessions - **HISTFILE** - Defines the file for storing the history of executed commands - **HISTSIZE** - Defines the maximum size for the HISTFILE - **HOME** - Sets the home directory path LOGNAME Retains the login name - **MAIL** - Contains the path to the user mail directory - **PATH** - Directories to be searched when executing a command. Eliminates the need to specify the absolute path of a command to run it. - **PPID** - Holds the identifier number for the parent program - **PS1** - Defines the primary command prompt PS2 Defines the secondary command prompt - **PWD** - Stores the current directory location - **SHELL** - Holds the absolute path to the primary primary shell file - **TERM** - Holds the terminal type value - **UID** - Holds the logged-in user’s UID - **USER** - Retains the name of the logged-in user Setting and unsetting variables # export, unset, and echo to define and undefine environment variables Use uppercase for variables echo command # Restricted to showing the value of a specific variable env command # Displays the environment variables only. [root@localhost ~]# env SHELL=/bin/bash HISTCONTROL=ignoredups HISTSIZE=1000 HOSTNAME=localhost PWD=/root LOGNAME=root XDG_SESSION_TYPE=tty MOTD_SHOWN=pam HOME=/root LANG=en_US.UTF-8 LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36: SSH_CONNECTION=192.168.0.233 56990 192.168.0.169 22 XDG_SESSION_CLASS=user SELINUX_ROLE_REQUESTED= TERM=xterm-256color LESSOPEN=||/usr/bin/lesspipe.sh %s USER=root SELINUX_USE_CURRENT_RANGE= SHLVL=1 XDG_SESSION_ID=1 XDG_RUNTIME_DIR=/run/user/0 SSH_CLIENT=192.168.0.233 56990 22 which_declare=declare -f PATH=/root/.local/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin SELINUX_LEVEL_REQUESTED= DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/0/bus MAIL=/var/spool/mail/root SSH_TTY=/dev/pts/0 BASH_FUNC_which%%=() { ( alias; eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@ } _=/usr/bin/env OLDPWD=/dev/vg200 printenv command # Displays the environment variables only. set command # View both local and environment variables. Command and Variable substitutions # PS1 Environment variable sets what the prompt looks like. Default value is \\u@\\h \\W\\$ \\u logged-in user name \\h System hostname \\W Working directory \\$ End of command prompt Command whose output you want assigned to a variable must be encapsulated within either backticks hostname or parentheses $(hostname). Input, Output, and Error Redirections # Input, output, and error character streams: standard input (or stdin) Input redirection \u0026lt; standard output (or stdout) Append instead of overwrite \u0026amp;\u0026gt; Redirect standard error and output standard error (or stderr) 2\u0026gt; File descriptors: 0, 1, and 2 1 Represents standard output location Can use these to represent character streams instead of \u0026lt;, and \u0026gt; noclobber feature prevent overwriting of the output file set -o noclobber activates the feature set +o noclobber deactivate the feature [root@localhost ~]# vim test.txt [root@localhost ~]# set -o noclobber [root@localhost ~]# echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt -bash: test.txt: cannot overwrite existing file [root@localhost ~]# set +o noclobber [root@localhost ~]# echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt [root@localhost ~]# cat test.txt Hello History Substitution # Command history or history expansion. Can disable and re-enable if required. Values may be altered for individual users by editing .bashrc or .bash_profile in the user\u0026rsquo;s home directory. Three variables HISTFILE Defines the name and location of the history file to be used to store command history, Default is .bash_history in the user’s home directory. HISTSIZE Size of the history buffer for the current shell. HISTFILESIZE Sets the maximum number of commands allowed for storage in the history file at the beginning of the current session and are written to the HISTFILE from memory at the end of the current terminal session. Usually, HISTSIZE and HISTFILESIZE are set to a common value. history command # Displays or reruns previously executed commands. Gets the history data from the system memory as well as from the .bash_history file. Shows all entries by default. set +o history disable history expansion set -o history re-enable history expansion [root@localhost ~]# set +o history [root@localhost ~]# history | tail 126 ls 127 vim test.txt 128 set -o noclobber 129 echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt 130 set +o noclobber 131 echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt 132 cat test.txt 133 history | tail 134 set +0 history 135 set +o history [root@localhost ~]# vim test2.txt [root@localhost ~]# history | tail 126 ls 127 vim test.txt 128 set -o noclobber 129 echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt 130 set +o noclobber 131 echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt 132 cat test.txt 133 history | tail 134 set +0 history 135 set +o history [root@localhost ~]# set -o history [root@localhost ~]# vim test2.txt [root@localhost ~]# history | tail 128 set -o noclobber 129 echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt 130 set +o noclobber 131 echo \u0026#34;Hello\u0026#34; \u0026gt; test.txt 132 cat test.txt 133 history | tail 134 set +0 history 135 set +o history 136 vim test2.txt 137 history | tail Add timestamps to history output system wide: echo \u0026quot;export HISTTIMEFORMAT='%F %T '\u0026quot; \u0026gt;\u0026gt; /etc/profile \u0026amp;\u0026amp; source /etc/profile\nEditing at the Command line # Common key combinations: # Ctrl+a / Home Moves the cursor to the beginning of the command line Ctrl+e / End Moves the cursor to the end of the command line Ctrl+u Erase everything at and before cursor Ctrl+k Erase Everything at cursor and after Alt+f Moves the cursor to the right one word at a time Alt+b Moves the cursor to the left one word at a time Ctrl+f / Right arrow Moves the cursor to the right one character at a time Ctrl+b / Left arrow Moves the cursor to the left one character at a time Tab completion # Hitting the Tab key twice automatically completes the entire name. In case of multiple possibilities matching the entered characters, it completes up to the point they have in common and prints the rest of the possibilities on the screen. Tilde Substitution (tilde expansion) # Performed on words that begin with the tilde character (~). ~ refers to user\u0026rsquo;s home directory. ~+ - refers to current directory\n~- - Refers to previous working directory.\n~USER - Refers to specific user\u0026rsquo;s home directory.\nAlias Substitution (command aliasing or alias) # Define shortcuts for commands. Bash shell includes several predefined aliases that are set during user login. Shell gives precedent to an alias if an alias matches a command or program. can still run the command without using the alias but preceding command with a backslash. \\ alias command # Set an alias. Internal shell command. [root@localhost ~]# alias alias cp=\u0026#39;cp -i\u0026#39; alias egrep=\u0026#39;egrep --color=auto\u0026#39; alias fgrep=\u0026#39;fgrep --color=auto\u0026#39; alias grep=\u0026#39;grep --color=auto\u0026#39; alias l.=\u0026#39;ls -d .* --color=auto\u0026#39; alias ll=\u0026#39;ls -l --color=auto\u0026#39; alias ls=\u0026#39;ls --color=auto\u0026#39; alias mv=\u0026#39;mv -i\u0026#39; alias rm=\u0026#39;rm -i\u0026#39; alias xzegrep=\u0026#39;xzegrep --color=auto\u0026#39; alias xzfgrep=\u0026#39;xzfgrep --color=auto\u0026#39; alias xzgrep=\u0026#39;xzgrep --color=auto\u0026#39; alias zegrep=\u0026#39;zegrep --color=auto\u0026#39; alias zfgrep=\u0026#39;zfgrep --color=auto\u0026#39; alias zgrep=\u0026#39;zgrep --color=auto\u0026#39; [root@localhost ~]# alias frog=\u0026#39;pwd\u0026#39; [root@localhost ~]# frog /root unalias command # Unset an alias. Internal shell command. [root@localhost ~]# unalias frog [root@localhost ~]# frog -bash: frog: command not found Metacharacters and Wildcard # Metacharacters Special characters that possess special meaning to the shell. Used in pattern matching (a.k.a. filename expansion or file globbing) and regular expressions. dollar sign ($) mark the end of a line Used in regular expressions caret (^)\nmark the beginning of a line Used in regular expressions period (.)\nMatch a single position Used in regular expressions asterisk (*)\nused in pattern matching wildcard character Matches zero to an unlimited number of characters except for the leading period (.) in a hidden filename. Used in regular expressions question mark (?)\nUsed in pattern matching Wildcard character Matches exactly one character except for the leading period in a hidden filename. Used in regular expressions pipe (|)\nSend the output of one command as input to the next. Also used to define alternations in regular expressions. Can use as many times in a command as you need. (pipeline)\t- Can be used as an OR operator (alternation) this|that|other angle brackets (\u0026lt; \u0026gt;)\nRedirections curly brackets ({})\nUsed in regular expressions Match an element a specific number of times square brackets ([])\nUsed in pattern matching Wildcard character Match either a set of characters or a range of characters for a single character position. Order in which they are listed has no importance. Range of characters must be specified in a proper sequence such as [a-z] or [0-9]. Used in regular expressions parentheses (())\nCreate a sub shell plus (+)\nMatch a character one or more time exclamation mark (!)\ninverse matches semicolon (;)\nRun a second command after the ; Quoting Mechanisms # Disable special meaning of metacharacters Backslash (\\)\nEscape character Cancel out a special character\u0026rsquo;s meaning. single quotation (‘’)\nMask the meaning of all encapsulated special characters. double quotation (“”)\nMask the meaning of all but the backslash (), dollar sign ($), and single quotes (‘’). Regular expressions (regexp or regex) # Text pattern or an expression that is matched against a string of characters in a file or supplied input in a search operation. Pattern may include a single character, multiple random characters, a range of characters, word, phrase, or an entire sentence. Any pattern containing one or more white spaces must be surrounded by quotation marks. grep command # Searches the contents of one or more text files or input supplied for a match. Flags -i\ncase insensitive search -n\nnumber the lines -v\nexclude these lines -w\nfind an exact match for a word -E\nmatch one or the other -e\n-Use patterns for matching Jobs # job Process that is started in the background and controlled by the terminal where it is spawned. Assigned a PID (process identifier) by the kernel and, additionally, a job ID by the shell. Does not hold the terminal window where it is initiated. Can run multiple job simultaneously. Can be brought to foreground, returned to the background, suspended, or stopped. job control management of multiple jobs within a shell environment commands and control sequences for administering the jobs.\njobs Shell built-in command display jobs. bg Shell built-in command Move a job to the background or restart a job in the background that was suspended with Ctrl+z. fg Shell built-in command Move a job to the foreground Ctrl+z Suspends a foreground job and allows the terminal window to be used for other purposes jobs command # output plus sign (+) - indicates the current background job minus sign (-) - signifies the previous job. Stopped - currently suspended - can be signaled to continue their execution with bg or fg\nShell Startup Files # Sourced by the shell following user authentication at the time of logging in and before the command prompt appears. Aliases, functions, and scripts can be added to these files as well. two types of startup files: system-wide Set the general environment for all users at the time of their login to the system. Located in the /etc directory Maintained by the Linux admin. System-wide startup files for bash shell users: /etc/bashrc Defines functions and aliases, sets umask for user accounts with a non-login shell, establishes the command prompt, etc. May include settings from the shell scripts located in the /etc/profile.d directory. /etc/profile Sets common environment variables such as PATH, USER, LOGNAME, MAIL, HOSTNAME, HISTSIZE, and HISTCONTROL for all users, establishes umask for user accounts with a login shell, processes the shell scripts located in the /etc/profile.d directory, and so on. /etc/profile.d Contains scripts for bash shell users that are executed by the /etc/profile file. Files can be edited and updated. per-user Override or modify system default definitions set by the system-wide startup files. By default, two files, in addition to the .bash_logout file, are located in the skeleton directory /etc/skel and are copied into user home directories at the time of user creation. .bashrc Defines functions and aliases. This file sources global definitions from the /etc/bashrc file. .bash_profile Sets environment variables and sources the .bashrc file to set functions and aliases. .gnome2/ Directory that holds environment settings when GNOME desktop is started. Only available if GNOME is installed. .bash_logout Executed when the user leaves the shell or logs off. May be customized Startup file order: /etc/profile \u0026gt; .bash_profile \u0026gt; .bashrc \u0026gt; /etc/bashrc Per user settings must be added to the appropriate file for persistence. Bash Labs # Lab: View environment variables # env printenv Lab: Viewing Environment variable values # View the value for the PATH variable echo $PATH View the value for the HOME variable echo $HOME View the value for the SHELL variable echo $SHELL View the value for the TERM variable echo $TERM View the value for the PPID variable echo $PPID View the value for the PS1 variable echo $PS1 View the value for the USER variable echo $USER Lab: Setting and Unsetting Variables # Define a local variable called VR1: VR1=RHEL9 View the value of VR1: echo $VR1 Type bash at the command prompt to enter a sub-shell and then run echo $VR1 to check whether the variable is visible in the sub-shell. echo $VR1 Exit out of the subshell: exit Turn VR1 into an environment variable: export VR1 Type bash at the command prompt to enter a sub-shell and then run echo $VR1 to check whether the variable is visible in the sub-shell. echo $VR1 Undefine this variable and erase it from the shell environment: unset VR1 Define a local variable that contains a value with one or more white spaces: VR2=\u0026#34;I love RHEL 9\u0026#34; Define and make the variable an environment variable at the same time: export VR3=\u0026#34;I love RHEL 9\u0026#34; View local and environment variables: set Lab: Modify Primary Command Prompt # Change the value of the variable PS1 to reflect the desired information: export PS1=\u0026#34;\u0026lt; $LOGNAME on $HOSTNAME in \\$PWD \u0026gt; \u0026#34; Edit the .bash_profile file for user1 and define the value exactly as it was run in Step 1. vim .bash_profile Test by logging off as user1 and logging back in. The new command prompt will be displayed. Lab: Redirecting Standard Input # Have the cat command read the /etc/redhat-release file and display its content on the standard output (terminal screen): cat \u0026lt; /etc/redhat-release Lab: Redirecting Standard Output # Direct the ls command output to a file called ls.out: ls \u0026gt; ls.out Do the same thing but using file descriptors: ls 1\u0026gt; ls.out Activate the noclobber feature then try the redirect feature again: set -o noclobber ls \u0026gt; ls.out Deactivate noclobber set +o noclobber Direct the ls command to append the output to the ls.out file instead of overwriting it: ls \u0026gt;\u0026gt; ls.out or ls 1\u0026gt;\u0026gt; ls.out Lab: Redirecting Standard Error # Direct the find command issued as a normal user to search for all occurrences of files by the name core in the entire directory tree and sends any error messages produced to /dev/null find / -name core -print 2\u0026gt; /dev/null Redirect both standard output and error: ls /usr /cdr \u0026amp;\u0026gt; outerr.out or ls /usr /cdr 1\u0026gt; outerr.out 2\u0026gt;\u0026amp;1 \\# means to redirect file descriptor 1 to file outerr.out as well as to file descriptor 2. Same as above but append to file: ls /usr/cdr \u0026amp;\u0026gt;\u0026gt; outerr.out Lab: Show History Variables # View HISTFILE Variable: echo $HISTFILE View HISTSIZE variable: echo $HISTSIZE View HISTFILESIZE variable: echo $HISTFILESIZE Lab: History command # Rune history without any options: history Display 10 entries: history 10 Run the 15th command in history: !15 re-execute the most recent occurrence of a command that started with a particular letter or series of letters (ch for example): !ch Issue the most recent command that contained “grep”: !?grep? Remove entry 24 from history: history -d 24 Repeat the last command executed: !! Lab: Tilde expansion # Display user\u0026rsquo;s home directory: echo ~ Display the current directory: echo ~+ Display the previous working directory: echo ~- Display user1\u0026rsquo;s home directory: echo ~user1 cd into the home directory of user1 and confirm: cd ~user1 pwd cd into a subdirectory: cd ~/Documents/ View directory information of the root user\u0026rsquo;s desktop: ls -ld ~root/Desktop Lab: Command Aliasing # shows all aliases that are currently set for user1: su - user1 alias Run alias as root: alias Create an alias “search” to abbreviate the find command with several switches and arguments. Enclose the entire command within single quotation marks (‘’) to ensure white spaces are taken care of. Do not leave any spaces before and after the equal sign (=). alias search=\u0026#39;find / -name core -exec ls -l {} \\;\u0026#39; Search with the new alias: search Create and alias by the same name as rm command but adding the -i flag: alias rm=\u0026#39;rm -i\u0026#39; Run rm without using the alias: rm file1 Remove the two aliases we just created: unalias search rm Lab: Wildcards and Metacharacters # List all files in the /etc directory that begin with letters “ma” and followed by any characters: ls /etc/ma* List all hidden files and directories in /home/user1: ls -d .* List all files in the /var/log directory that end in “.log”: ls /var/log/*.log List all files and directories under /var/log with exactly four characters in their names: ls -d /var/log/???? Include all files and directories that begin with either of the two characters and followed by any number of characters. ls /usr/bin/[yw]* Match all directory names that begin with any letter between “m” and “o” in the /etc/systemd/system directory: ls -d /etc/systemd/system/[m-o]* Inverse results of the previous: ls -d /etc/systemd/system/[!m-o]* Lab: Piping # Pipe the output to the less command in order to view the directory listing one screenful at a time: ls -l /etc | less Run the last command and pipe the output to nl to number each line: last | nl Send the output of ls to grep for the lines that do not contain the pattern “root”. Piped again for a case-insensitive selection of all lines that exclude the pattern “dec”. Number the output, and show the last four lines on the display: ls -l /proc | grep -v root | grep -iv dec | nl | tail -4 Lab: Quoting Mechanisms # Remove a file called * without deleting everything in the directory: rm \\* Display $LOGNAME without expanding the LOGNAME variable: echo \u0026#39;$LOGNAME\u0026#39; Run the following with double quotes and without: echo \u0026#34;$SHELL\u0026#34; echo \u0026#34;\\$PWD\u0026#34; echo \u0026#34;\u0026#39;\\\u0026#39;\u0026#34; Lab: grep and regex # Search for the pattern “operator” in the /etc/passwd file: grep operator /etc/passwd Search for the space-separated pattern “aliases and functions” in the $HOME/.bashrc file: grep \u0026#39;aliases and functions\u0026#39; .bashrc Search for the pattern “nologin” in the passwd file and exclude (-v) the lines in the output that contain this pattern. Add the -n switch to show the line numbers associated with the matched lines. grep -nv nologin /etc/passwd Find any duplicate entries for the root user in the passwd file. Prepend the caret sign (^) to the pattern “root” to mark the beginning of a line. grep ^root /etc/passwd Identify all users in the passwd file with bash as their primary shell. grep bash$ /etc/passwd Show the entire login.defs file but exclude all the empty lines: grep -v ^$ /etc/login.defs Perform a case-insensitive search (-i) for all the lines in the /etc/bashrc file that match the pattern “path.” grep -i path /etc/bashrc Print all the lines from the /etc/lvm/lvm.conf file that contain an exact match for a word (-w) “acce” followed by exactly two characters: grep -w acce.. /etc/lvm/lvm.conf Print all the lines from the ls command output that include either (-E) the pattern “cron” or “ly”. ls -l /etc | grep -E \u0026#39;cron|ly\u0026#39; Show all the lines from the /etc/ssh/sshd_config file but exclude (-v) the empty lines and commented lines. Use the -e flag multiple times instead of | for either or. sudo grep -ve ^$ -ve ^# /etc/ssh/sshd_config Learn more about regex: man 7 regex Consult the grep man pages: man grep Lab: Managing jobs # Issue the jobs command with the -l switch to view all the jobs running in the background: jobs -l bring job ID 1 to the foreground and start running it: fg %1 Suspend job 1 with ctrl+z and then let it run in the background: bg %1 Terminate job ID 1, supply its PID (31726) to the kill command: kill 31726 Lab: Shell Startup Files # View the first 10 lines of /etc/bashrc: head /etc/bashrc View the first 10 lines of /etc/profile: head /etc/profile View the directory /etc/profile.d ls -l /etc/profile.d/ View .bashrc cat ~/.bashrc View .bash_profile cat ~/.bash_profile Lab: Customize the Command Prompt (user1) # Permanently customize the primary shell prompt to display “\u0026lt;user1@server1 in /etc \u0026gt;:” when this user switches into the /etc directory. The prompt should always reflect the current directory directory path. vim ~/.bash_profile export PS1=\u0026#39;$USERNAME $PWD\u0026#39; Lab 7-2: Redirect the Standard Input, Output, and Error (user1) # Run the ls command on /etc, /dvd, and /var. Have the output printed on the screen and the errors forwarded to file /tmp/ioerror. ls /etc /dvd /var 2\u0026gt; /tmp/ioerror Check the file after the command execution and analyze the results: cat /tmp/ioerror Notes # It is NOT, a hard, protective outer layer usually created by an animal or organism that lives in the sea. ","externalUrl":null,"permalink":"/tech/linux/bash-shell/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eMy name is Marcel and I\u0026rsquo;m partially a shell..\n    \u003cdiv id=\"my-name-is-marcel-and-im-partially-a-shell\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#my-name-is-marcel-and-im-partially-a-shell\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"\"\n    src=\"../../../img/Pasted%20image%2020250316051144.png\"\n    \u003e\u003c/figure\u003e\n\u003cp\u003eA shell is a program that takes commands and passes them to the operating system.^1^ This is done via \u003cstrong\u003eterminal emulator\u003c/strong\u003e with keyboard commands or by using scripts ran on the system. There are many shell programs that you can use on Linux. Almost all Linux distributions come with a shell called \u003cstrong\u003eBash\u003c/strong\u003e. Some others include \u003cstrong\u003ezsh\u003c/strong\u003e, \u003cstrong\u003efsh\u003c/strong\u003e, \u003cstrong\u003eksh\u003c/strong\u003e, and \u003cstrong\u003eTcsh\u003c/strong\u003e. (But not limited to)\u003c/p\u003e","title":"Bash","type":"tech"},{"content":" 7 File types # regular directory block special device character special device symbolic link named pipe socket Commands\nls stat file Regular files # Text or binary data. Represented by hyphen (-). Directory Files # Identified by the letter \u0026ldquo;d\u0026rdquo; in the beginning of ls output. Block and Character (raw) Special Device Files # All hardware has device file in /dev/. Used by system to communicate with device. Identified by \u0026ldquo;c\u0026rdquo; or \u0026ldquo;b\u0026rdquo; in ls listing. Each device driver is assigned a unique number called the major number Character device Reads and writes 8 bits at a time. Serial Block device Receives data in fixed block size determined by drivers 512 or 4096 bytes Major Number # Used by kernel to recognize device driver type. Column 5 of ls listing. ls -l /dev/sda Minor Number # Each device controlled by the same device driver gets a Minor Number Applies to disk partitions as well. The same driver can control multiple devices of the same type. Column 6 of ls listing ls -l /dev/sda Symbolic Links # Shortcut to another file or directory. Begins with \u0026ldquo;l\u0026rdquo; in ls listing. ls -l /usr/sbin/vigr lrwxrwxrwx. 1 root root 4 Jul 21 14:36 /usr/sbin/vigr -\u0026gt; vipw Compression and Archiving # Archiving # Preserves file attributes such as ownership, owning group, and timestamp. Preserves extended file attributes such as ACLs and SELinux contexts. Syntax of tar and star are identical. star command # tar (tape archive) command # Create, append, update, list, and extract files/directory tree to/from a file called a tarball(tarfile) Can compress a tarball after it\u0026rsquo;s been created. Automatically removes \u0026ldquo;/\u0026rdquo; so you do not have to specify the full pathname when restoring files at any location. flags tar -c :: Create tarball. tar -f :: Specify tarball name. tar -p :: Preserve file permissions. Default for the root user. Specify this if you create an archive as a normal user. tar -r :: Append files to the end of an existing uncompressed tarball. tar -t :: List contents of a tarball. tar -u :: Append files to the end of an existing uncompressed tarball provided the specified files being added are newer. -z -j -C\nArchive entire home directory: # tar -cvf /tmp/home.tar /home Archive two specific files: # tar -cvf /tmp/files.tar /etc/passwd /etc/yum.conf Append files in a directory to existing tarball: # tar -rvf /tmp/files.tar /etc/yum.repos.d List what is included in home.tar tarball: # tar -tvf /tmp/files.tar Restore single file and confirm: # tar -xf /tmp/files.tar etc/yum.conf ls -l etc/yum.conf Restore all files and confirm: # tar -xf /tmp/files.tar ls Create a gzip-compressed tarball under /tmp for /home: # tar -czf /tmp/home.tar.gz /home Create bzip2-compressed tarball under /tmp for /home: # sudo tar -cjf /tmp/home.tar.bz2 /home List content of gzip-compressed archive without uncompressing it: # tar -tf /tmp/home.tar.gz Extract files from gzip-compressed tarball in the current directory: # tar -xf /tmp/home.tar.gz Extract files from the bzip2-compressed tarball under /tmp: # tar -xf /tmp/home.tar.bz2 -C /tmp Compression tools # gzip (gunzip) command # Create a compressed file for each of the specified files. Adds .gz extension. Flags\nCopy /etc/fstab to the current directory and display filename when uncompressed: # cp /etc/fstab . ls -l fstab gzip fstab and view details: # gzip fstab ls -l fstab.gz Display compression info: # gzip -l fstab.gz Uncompress fstab.gz: # gunzip fstab.gz ls -l fstab bzip2 (bunzip2) command # Adds .bz2 extension. Better compression/ decompression ratio but is slower than gzip. Compress fstab using bzip and view details: # bzip2 fstab ls -l fstab.bz2 Unzip fstab.bz2 and view details: # bunzip2 fstab.bz2 ls -l fstab File Editing # Vim # vimguide\nFile and Directory Operations # touch command # File is created with 0 bytes in size. Run touch on it and it will get a new timestamp Flags\nSet date on file1 to 2019-09-20: # touch -d 2019-09-20 file1 Change modification time on file1 to current system time: # touch -m file1 mkdir command # Create a new directory. flags\nCreate dir1 verbosely: # mkdir dir1 -v Create dir2/perl/perl5: # mkdir -vp dir2/perl/perl5 Commands for displaying file contents # cat more less head tail cat command # Concatenate and print files to standard output. Flags\nRedirect output to specified file: # cat \u0026gt; catfile1 tac command # Display file contents in reverse more command # Display files on page-by-page basis. Forward text searching only. Navigation # less command # Display files on page-by-page basis. Forward and backwards searching. less /usr/bin/znew Navigation # head command # Displays first 10 lines of a file. head /etc/profile View top 3 lines of a file: # head -3 /etc/profile tail command # Display last 10 lines of a file. Flags\ntail /etc/profile View last 3 lines of /etc/profile: # tail -3 /etc/profile View updates to the system log file /varlog/messages in real time: # sudo tail -f /var/log/messages Counting Words, Lines, and Characters in Text Files # wc (word count) command # Display the number of lines, words, and characters (or bytes) contained in a text file or input supplied. Flags\nwc /etc/profile 85 294 2123 /etc/profile Display count of characters on /etc/profile: # wc -m /etc/profile Copying Files and Directories # cp command # Copy files or directories. Overwrites destination without warning. root has a custom alias in their .bashrc file that automatically adds the -i option. alias cp=\u0026#39;cp -i\u0026#39; Flags\ncp file1 newfile1 Copy file to new directory: # cp file1 dir1 Get confirmation before overwriting: # cp file1 dir1 -i cp: overwrite \u0026#39;dir1/file1\u0026#39;? y Copy a directory and view hierarchy: # cp -r dir1 dir2 ls -l dir2 -R Copy file while preserving attributes: # cp -p file1 /tmp Moving and renaming Files and Directories # mv command # Move or rename files and directories. Can move a directory into another directory. Target directory must exist otherwise you are just renaming the directory. Alias exists in root\u0026rsquo;s home directory for -i in the .bashrc file. alias—“alias mv=’mv -i’\u0026#34;\u0026#34; Flags\nmv -i file1 dir1 mv newfile1 newfile2 Move a dir into another dir (target exists): # mv dir1 dir2 Rename a directory (Target does not exist): # mv dir2 dir20 Removing files # rm command # Delete one or more specified files or directories. Alias—“alias rm=’rm -i’”— in the .bashrc file in the root user’s home directory. Remember to backslash \u0026ldquo;\u0026quot; any wildcard characters in filenames. Flags\nErase newfile2: # rm -i newfile2 rm a directory: # rm -dv emptydir rm a directory recursively: # rm -r dir20 rmdir command # Remove empty directories. Flags\nrmdir emptydir -v File Linking # inode (index node) # Contains metadata about a file (128 bytes) File type, Size, permissions, owner name, owning group, access times, link count, etc. Also shows number of allocated blocks and pointers to the data storage location. Assigned a unique numeric identifier that is used by the kernel for accessing, tracking, and managing the file. Does not store the filename. Filename and corresponding inode number mapping is maintained in the directory’s metadata where the file resides. Links are not created between files and directories Hard links # Mapping between one or more filenames and an inode number. Hard-linked files are indistinguishable from one another. All hard-linked files will have identical metadata. Changes to the file metadata and content can be made by accessing any of the filenames. Cannot cross file system boundaries. Cannot link directories. ls -li output # Column 1 inode number. Column 3 link count. Soft Links # Symbolic (symlink). Like a Windows shortcut. Unique inode number for each symlink. Link count does not increase or decrease. Size of soft link is the number of character in pathname to target. Can cross file system boundaries. Can link directories. ls-l shows l at the beginning of the permissions for soft link if you remove the original file, the softlink will point to a file that doesn\u0026rsquo;t exist. RHEL 8 has four soft-linked directories under /. bin -\u0026gt; usr/bin lib -\u0026gt; usr/lib lib64 -\u0026gt;usr/lib64 sbin -\u0026gt; usr/sbin Same syntax for creating linked directories ln command # Create links between files. Creates hard link by default. Hard link file10 and file20 and verify the inode number: # touch file10 ln file10 file20 ls -li Create a soft link to file10 called soft10: # ln -s file10 soft10 Copying vs linking # Copying\nDuplicates source file. Each copy stores data at a unique location. Each copied file has a unique inode number and unique metadata. If a copy is moved, erased, or renamed, the source file will have no impact, and vice versa. Copy is used when the data needs to be edited independent of the other. Permissions on the source and the copy are managed independent of each other. Linking\nCreates a shortcut that points to the source file. Source can be accessed or modified using either the source file or the link. All linked files point to the same data. Hard Link: All hard-linked files share the same inode number, and hence the metadata. Symlink: Each symlinked file has a unique inode number, but the inode number stores only the pathname to the source. Hard Link: If the hard link is weeded out, the other file and the data will remain untouched. Symlink: If the source is deleted, the soft link will be broken and become meaningless. If the soft link is removed, the source will have no impact. Links are used when access to the same source is required from multiple locations. Permissions are managed on the source file. Labs # Lab: Viewing regular file information: # touch file1 ls -l file file1 stat file1 Lab Create and Manage Hard Links # Create an empty file /tmp/hard1, and display the long file listing including the inode number: touch /tmp/hard1 ls -li /tmp/hard1 Create two hard links called hard2 and hard3 under /tmp, and display the long listing: ln /tmp/hard1 /tmp/hard2 ln /tmp/hard1 /tmp/hard3 ls -li /tmp/hard* Edit file hard2 and add some random text. Display the long listing for all three files again: vim /tmp/hard2 ls -li /tmp/hard* Erase file hard1 and hard3, and display the long listing for the remaining file: rm -f /tmp/hard1 /tmp/hard3 ls -li /tmp/hard* Lab: Create and Manage Soft Links # Create soft link /root/soft1 pointing to /tmp/hard2, and display the long file listing for both: sudo ln -s /tmp/hard2 /root/soft1 ls -li /tmp/hard2 /root/soft1 sudo ls -li /tmp/hard2 /root/soft1 2.Edit soft1 and display the long listing again:\nsudo vim /root/soft1 sudo ls -li /tmp/hard2 /root/soft1 3.Remove hard2 and display the long listing:\nsudo ls -li /tmp/hard2 /root/soft1 remove the soft link\nrm -f /root/soft1. Lab: Archive, List, and Restore Files # Create a gzip-compressed archive of the /etc directory.\ntar -czf etc.tar.gz /etc Create a bzip2-compressed archive of the /etc directory.\nsudo tar -cjf etc.tar.bz2 /etc Compare the file sizes of the two archives.\nls -l etc* Run the tar command and uncompress and restore both archives without specifying the compression tool used.\nsudo tar -xf etc.tar.bz2 ; sudo tar -xf etc.tar.gz Lab: Practice the vim Editor # As user1 on server1, create a file called vipractice in the home directory using vim. Type (do not copy and paste) each sentence from Lab 3-1 on a separate line (do not worry about line wrapping). Save the file and quit the editor.\nOpen vipractice in vim again and reveal line numbering. Copy lines 2 and 3 to the end of the file to make the total number of lines in the file to 6. # :set number! #then yy and p Move line 3 to make it line 1. # 3m0 Go to the last line and append the contents of the .bash_profile. # :r ~/.bashrc Substitute all occurrences of the string “Profile” with “Pro File”, and all occurrences of the string “profile” with “pro file”. # :%s/profile/pro file/gi Erase lines 5 to 8. # :5,8d Provide a count of lines, words, and characters in the vipractice file using the wc command.\nwc vipractice Lab: File and Directory Operations # As user1 on server1, create one file and one directory in the home directory.\ntouch file3 mkdir dir5 List the file and directory and observe the permissions, ownership, and owning group.\nls -l file3 ls -l dir5 ls -ld dir5 Try to move the file and the directory to the /var/log directory and notice what happens.\nmv dir5 /var/log mv file3 /var/log Try again to move them to the /tmp directory.\nmv dir5 /tmp ls /tmp Duplicate the file with the cp command, and then rename the duplicated file using any name.\ncp /tmp/file3 file4 ls /tmp ls Erase the file and directory created for this lab.\nrm -d /tmp/dir5; rm file4 ","externalUrl":null,"permalink":"/tech/linux/file-management/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003e7 File types\n    \u003cdiv id=\"7-file-types\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#7-file-types\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eregular\u003c/li\u003e\n\u003cli\u003edirectory\u003c/li\u003e\n\u003cli\u003eblock special device\u003c/li\u003e\n\u003cli\u003echaracter special device\u003c/li\u003e\n\u003cli\u003esymbolic link\u003c/li\u003e\n\u003cli\u003enamed pipe\u003c/li\u003e\n\u003cli\u003esocket\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCommands\u003c/p\u003e","title":"Basic File Managment","type":"tech"},{"content":" RPM (Redhat Package Manager) # Specially formatted File(s) packaged together with the .rpm extension. Packages included or available for RHEL are in rpm format. Metadata info gets updated whenever a package is updated. rpm command # Install, Upgrade, remove, query, freshen, or decompress packages. Validate package authenticity and integrity. Packages # Two types of packages binary (or installable) and source. Binary packages\nInstallation ready Bundled for distribution. Have .rpm extension. Contain: install scripts (pre and post) Executables Configuration files Library files Dependency information Where to install files Documentation How to install/uninstall Man pages for config files/commands Other install and usage info Metadata Stored in central location Includes: Package version Install location Checksum values List of included files and their attributes Package intelligence Used by package administration toolset for successful completion of the package installation process. May include info on: prerequisites User account setup Needed directories/ soft links Includes reverse process for uninstall Package Naming # 5 parts to a package name: 1. Name 2. Version 3. release (revision or build) 4. Linux version 5. Processor Architecture - noarch - platform independant - src - Source code packages\nAlways has .rpm extension .rpm is removed after install Example: openssl-1.1.1-8.el8.x86_64.rpm, Package Dependency # Dependency info is in the metadata Read by package handling utilities Package Database # Metadata for installed packages and package files is stored in /var/lib/rpm/ Package database Referenced by package manipulation utilities to obtain: package name and version data Info about owerships, permissions, timestamps, and file sizes that are part of the package. Contain info on dependencies. Aids management commands in: listing and querying packages Verifying dependencies and file attributes. Installing new packages. Upgrading and uninstalling packages. Removes and replaces metadata when a package is replaced. Can maintain multiple version of a single package. Package Management Tools # rpm (redhat package manager) Does not automatically resolve dependencies. yum (yellowdog update, modified) Find, get, and install dependencies automatically. softlink to dnf now. dnf (dandified yum) Package management with rpm # rpm package management tasks: - query - install - upgrade - freshen - overwrite - remove - extract - validate - verify\nWorks with installed and installable packages. rpm command # Query options # Query and display packages\n-q (--query) List all installed packages\n-qa (--query --all)\nList config files in a package\n-qc (--query --config-files)\nList documentation files in a package\n-qd (--query --docfiles) Exhibit what package a file comes from\n-qf (--query --file) Show installed package info (Version, Size, Installation status, Date, Signature, Description, etc.) -qi (--query --info)\nShow installable package info (Version, Size, Installation status, Date, Signature, Description, etc.) -qip (--query --info --package)\nList all files in a package.\n-ql (--query --list)\nList files and packages a package depends on. -qR (--query --requires)\nList packages that provide the specified package or file.\n-q --whatprovides\nList packages that require the specified package or file.\n-q --whatrequires\nPackage installation options # Remove a package\n-e (--erase)\nUpgrades installed package. Or loads if not installed.\n-U (--upgrade)\nDisplay detailed information\n-v (--verbose or -vv)\nVerify integrity of a package or package files\n-V (--verify)\nQuerying packages # Query packages in the package database or at a specified location.\nInstalling a package # Creates directory structure needed Installs files Runs needed post installation steps Installing package will fail if missing dependencies. Error message will show missing dependencies. Upgrading a package # Installs the package if previous version does not exist. (-U) Makes backup of effected configuration files and adds .rpmsave extension. Freshening a package # Older version must exist. -F option Will only work if a newer version of a package is available. Overwriting a Package # Replaces existing files of a package with the same version. \u0026ndash;replacepkgs option. Useful when you suspect corruption. Removing a Package # Uninstalls package and associated files/ directories -e Option Checks to see if this package is a dependency for another program and fails if it is. Extracting Files from an Installable Package # rpm2cpio command -i (extract) -d create directory structure. Useful for: Examining package contents. Replacing corrupt or lost command. Replace critical configuration file to it\u0026rsquo;s original state Package Integrity and Credibility # MD5 Checksum for verifying package integrity GNU Privacy Guard Public Key (GNU Privacy Guard or GPG) for ensuring credibility of publisher. PGP (Pretty Good Privacy) - commercial version of GPG. --nosignature Don\u0026rsquo;t verify package or header signatures when reading. -K keep package files after installation rpmkeys command check credibility, import GPG key, and verify packages Redhat signs their products and updates with a GPG key. Files in installation media include public keys in the products for verification. Copied to /etc/pki/rpm-gpg during OS installation. RPM-GPG-KEY-redhat-release Used for packages shipped after November 2009 and their updates. RPM-GPG-KEY-redhat-beta For Beta products shipped after November 2009. Import the relevant GPG key and the verify the package to check the credibility of a package. Viewing GPG Keys # view with rpm command rpm -q gpg-pubkey -i option show info about a key. Verifying Package Attributes # Compare package file attributes with originals stored in package database at the time of installation. -V option compare owner, group, permission mode, size, modification time, digest, type, etc. Returns to prompt if no changes are detected -v or vv for verbose -Vf run the check directly on the file Three columns of output: Column 1 9 fields S = Different file size. M = Mode or permission or file type change. 5 = MD5 Checksum does not match. D = Device file and its major and minor number have changed. L = File is a symlink and it\u0026rsquo;s path has been altered. U = Ownership has changed. G = Group membership has been modified. T = Timestamp changed. P = Capabilities are altered. . = No modifications detected. Column 2 File type c = Configuration file d = Documentation File g = Ghost FIle l = License file r = Readme file Column 3 Full path of file Basic Package Management Labs # Lab: Mount RHEL 9 ISO Persistently # Go to the VirtualBox VM Manager and make sure that the RHEL 8 image is attached to RHEL9-VM1 as depicted below: Open the /etc/fstab file in the vim editor (or another editor of your choice) and add the following line entry at the end of the file to mount the DVD image (/dev/sr0) in read-only (ro) mode on the /mnt directory.\n/dev/sr0 /mnt iso9660 ro 0 0 Note: sr0 represents the first instance of the optical device and iso9660 is the standard format for optical file systems.\nMount the file system as per the configuration defined in the /etc/fstab file using the mount command with the -a (all) option:\nsudo mount -a Verify the mount using the df command:\ndf -h | grep mnt Note: The image and the packages therein can now be accessed via the /mnt directory just like any other local directory on the system.\nList the two directories—/mnt/BaseOS/Packages and /mnt/AppStream/Packages—that contain all the software packages (directory names are case sensitive):\nls -l /mnt/BaseOS/Packages | more Lab: Query Packages (RPM) # query all installed packages: rpm -qa\nquery whether the perl package is installed: rpm -q perl\nlist all files in a package: rpm -ql iproute\nlist only the documentation files in a package: rpm -qd audit\nlist only the configuration files in a package: rpm -qc cups\nidentify which package owns the specified file: rpm -qf /etc/passwd\ndisplay information about an installed package including version, release, installation status, installation date, size, signatures, description, and so on: rpm -qi setup\nlist all file and package dependencies for a given package: rpm -qR chrony\nquery an installable package for metadata information (version, release, architecture, description, size, signatures, etc.): rpm -qip /mnt/BaseOS/Packages/zsh-5.5.1-6.el8.x86_64.rpm\ndetermine what packages require the specified package in order to operate properly: rpm -q --whatrequires lvm2\nLab: Installing a Package (RPM) # Install zsh-5.5.1-6.el8.x86_64.rpm sudo rpm -ivh /mnt/BaseOS/Packages/zsh-5.5.1-6.el8.x86_64.rpm Lab: Upgrading a Package (RPM) # Upgrade sushi with the -U option: sudo rpm -Uvh /mnt/AppStream/Packages/sushi-3.28.3-1.el8.x86_64.rpm Lab: Freshening a Package # Freshen the sushi package: sudo rpm -Fvh /mnt/AppStream/Packages/sushi-3.28.3-1.el8.x86_64.rpm Lab: Overwriting a Package # Overwrite zsh-5.5.1-6.el8.x86_64 sudo rpm -ivh --replacepkgs /mnt/BaseOS/Packages/zsh-5.5.1-6.el8.x86_64 Lab: Removing a Package # Remove sushi sudo rpm sushi -ve Lab: Extracting Files from an Installable Package # You have lost /etc/crony.conf. Determine what package this file comes from: rpm -qf /etc/chrony.conf\nExtract all files from the crony package to /tmp and create the directory structure:\n[root@server30 mnt]# cd /tmp [sudo rpm2cpio /mnt/BaseOS/Packages/chrony-3.3-3.el8.x86_64.rpm | cpio -imd 1066 blocks](\u0026lt;[root@server30 tmp]# rpm2cpio /mnt/BaseOS/Packages/chrony-4.3-1.el9.x86_64.rpm | cpio -imd 1253 blocks\u0026gt;) Use find to locate the crony.conf file: sudo find . -name chrony.conf\nCopy the file to /etc:\nLab: Validating Package Integrity and Credibility # Check the integrity of zsh-5.5.1-6.el8.x86_64.rpm located in /mnt/BaseOS/Packages: rpm -K /mnt/BaseOS/Packages/zsh-5.5.1-6.el8.x86_64.rpm --nosignature Import the GPG key from the proper file and verify the signature for the zsh-5.5.1-6.el8.x86_64.rpm package. sudo rpmkeys --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release sudo rpmkeys -K /mnt/BaseOS/Packages/zsh-5.5.1-6.el8.x86_64.rpm Lab: Viewing GPG Keys # List the imported key: rpm -q gpg-pubkey View details for the first key: rpm -qi gpg-pubkey-fd431d51-4ae0493b Lab: Verifying Package Attributes # Run a check on the at program: sudo rpm -V at\nChange permissions of one of the files and run the check again:\nls -l /etc/sysconfig/atd sudo chmod -v 770 /etc/sysconfig/atd sudo rpm -V at Run the check directly on the file: sudo rpm -Vf /etc/sysconfig/atd\nReset the value and check the file again:\nsudo chmod -v 644 /etc/sysconfig/atd sudo rpm -V at Lab: Perform Package Management Using rpm # Run the ls command on the /mnt/AppStream/Packages directory to confirm that the rmt package is available: [root@server30 tmp]# ls -l /mnt/BaseOS/Packages/rmt* -r--r--r--. 1 root root 49582 Nov 20 2021 /mnt/BaseOS/Packages/rmt-1.6-6.el9.x86_64.rpm Run the rpm command and verify the integrity and credibility of the package: [root@server30 tmp]# rpmkeys -K /mnt/BaseOS/Packages/rmt-1.6-6.el9.x86_64.rpm /mnt/BaseOS/Packages/rmt-1.6-6.el9.x86_64.rpm: digests signatures OK Install the Package: [root@server30 tmp]# rpmkeys -K /mnt/BaseOS/Packages/rmt-1.6-6.el9.x86_64.rpm /mnt/BaseOS/Packages/rmt-1.6-6.el9.x86_64.rpm: digests signatures OK [root@server30 tmp]# rpm -ivh /mnt/BaseOS/Packages/rmt-1.6-6.el9.x86_64.rpm Verifying... ################################# [100%]) Preparing... ################################# [100%]) Updating / installing... 1:rmt-2:1.6-6.el9 ################################# [100%]) Show basic information about the package: [root@server30 tmp]# rpm -qi rmt Name : rmt Epoch : 2 Version : 1.6 Release : 6.el9 Architecture: x86_64 Install Date: Sat 13 Jul 2024 09:02:08 PM MST Group : Unspecified Size : 88810 License : CDDL Signature : RSA/SHA256, Sat 20 Nov 2021 08:46:44 AM MST, Key ID 199e2f91fd431d51 Source RPM : star-1.6-6.el9.src.rpm Build Date : Tue 10 Aug 2021 03:13:47 PM MST Build Host : x86-vm-55.build.eng.bos.redhat.com Packager : Red Hat, Inc. \u0026lt;http://bugzilla.redhat.com/bugzilla\u0026gt; Vendor : Red Hat, Inc. URL : http://freecode.com/projects/star Summary : Provides certain programs with access to remote tape devices Description : The rmt utility provides remote access to tape devices for programs like dump (a filesystem backup program), restore (a program for restoring files from a backup), and tar (an archiving program). Show all the files the package contains: [root@server30 tmp]# rpm -ql rmt /etc/default/rmt /etc/rmt /usr/lib/.build-id /usr/lib/.build-id/c2 /usr/lib/.build-id/c2/6a51ea96fc4b4367afe7d44d16f1405c3c7ec9 /usr/sbin/rmt /usr/share/doc/star /usr/share/doc/star/CDDL.Schily.txt /usr/share/doc/star/COPYING /usr/share/man/man1/rmt.1.gz List the documentation files the package has: [root@server30 tmp]# rpm -qd rmt /usr/share/doc/star/CDDL.Schily.txt /usr/share/doc/star/COPYING /usr/share/man/man1/rmt.1.gz Verify the attributes of each file in the package. Use verbose mode. [root@server30 tmp]# rpm -vV rmt ......... c /etc/default/rmt ......... /etc/rmt ......... a /usr/lib/.build-id ......... a /usr/lib/.build-id/c2 ......... a /usr/lib/.build-id/c2/6a51ea96fc4b4367afe7d44d16f1405c3c7ec9 ......... /usr/sbin/rmt ......... /usr/share/doc/star ......... d /usr/share/doc/star/CDDL.Schily.txt ......... d /usr/share/doc/star/COPYING ......... d /usr/share/man/man1/rmt.1.gz Remove the package: [root@server30 tmp]# rpm -ve rmt Preparing packages... rmt-2:1.6-6.el9.x86_64 Lab 9-1: Install and Verify Packages # As user1 with sudo on server3,\nmake sure the RHEL 9 ISO image is attached to the VM and mounted. Use the rpm command and install the zsh package by specifying its full path. [root@server30 Packages]# rpm -ivh /mnt/BaseOS/Packages/zsh-5.8-9.el9.x86_64.rpm Verifying... ################################# [100%]) Preparing... ################################# [100%]) package zsh-5.8-9.el9.x86_64 is already installed Run the rpm command again and perform the following on the zsh package: (1) show information [root@server30 Packages]# rpm -qi zsh Name : zsh Version : 5.8 Release : 9.el9 Architecture: x86_64 Install Date: Sat 13 Jul 2024 06:49:40 PM MST Group : Unspecified Size : 8018363 License : MIT Signature : RSA/SHA256, Thu 24 Feb 2022 08:59:15 AM MST, Key ID 199e2f91fd431d51 Source RPM : zsh-5.8-9.el9.src.rpm Build Date : Wed 23 Feb 2022 07:10:14 AM MST Build Host : x86-vm-56.build.eng.bos.redhat.com Packager : Red Hat, Inc. \u0026lt;http://bugzilla.redhat.com/bugzilla\u0026gt; Vendor : Red Hat, Inc. URL : http://zsh.sourceforge.net/ Summary : Powerful interactive shell Description : The zsh shell is a command interpreter usable as an interactive login shell and as a shell script command processor. Zsh resembles the ksh shell (the Korn shell), but includes many enhancements. Zsh supports command line editing, built-in spelling correction, programmable command completion, shell functions (with autoloading), a history mechanism, and more. (2) validate integrity [root@server30 Packages]# rpm -K zsh-5.8-9.el9.x86_64.rpm zsh-5.8-9.el9.x86_64.rpm: digests signatures OK (3) display attributes [root@server30 Packages]# rpm -V zsh Lab 9-2: Query and Erase Packages # As user1 with sudo on server3,\nmake sure the RHEL 9 ISO image is attached to the VM and mounted. Use the rpm command to perform the following: (1) check whether the setup package is installed [root@server30 Packages]# rpm -q setup setup-2.13.7-10.el9.noarch (2) display the list of configuration files in the setup package [root@server30 Packages]# rpm -qc setup /etc/aliases /etc/bashrc /etc/csh.cshrc /etc/csh.login /etc/environment /etc/ethertypes /etc/exports /etc/filesystems /etc/fstab /etc/group /etc/gshadow /etc/host.conf /etc/hosts /etc/inputrc /etc/motd /etc/networks /etc/passwd /etc/printcap /etc/profile /etc/profile.d/csh.local /etc/profile.d/sh.local /etc/protocols /etc/services /etc/shadow /etc/shells /etc/subgid /etc/subuid /run/motd /usr/lib/motd (3) show information for the zlib-devel package on the ISO image [root@server30 Packages]# rpm -qi ./zlib-devel-1.2.11-40.el9.x86_64.rpm Name : zlib-devel Version : 1.2.11 Release : 40.el9 Architecture: x86_64 Install Date: (not installed) Group : Unspecified Size : 141092 License : zlib and Boost Signature : RSA/SHA256, Tue 09 May 2023 05:31:02 AM MST, Key ID 199e2f91fd431d51 Source RPM : zlib-1.2.11-40.el9.src.rpm Build Date : Tue 09 May 2023 03:51:20 AM MST Build Host : x86-64-03.build.eng.rdu2.redhat.com Packager : Red Hat, Inc. \u0026lt;http://bugzilla.redhat.com/bugzilla\u0026gt; Vendor : Red Hat, Inc. URL : https://www.zlib.net/ Summary : Header files and libraries for Zlib development Description : The zlib-devel package contains the header files and libraries needed to develop programs that use the zlib compression and decompression library. (4) reinstall the zsh package (\u0026ndash;reinstall -vh), [root@server30 Packages]# rpm -hv --reinstall ./zsh-5.8-9.el9.x86_64.rpm Verifying... ################################# [100%]) Preparing... ################################# [100%]) Updating / installing... 1:zsh-5.8-9.el9 ################################# [ 50%]) Cleaning up / removing... 2:zsh-5.8-9.el9 ################################# [100%]) (5) remove the zsh package. [root@server30 Packages]# rpm -e zsh ","externalUrl":null,"permalink":"/tech/linux/basic-package-management/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eRPM (Redhat Package Manager)\n    \u003cdiv id=\"rpm-redhat-package-manager\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#rpm-redhat-package-manager\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSpecially formatted File(s) packaged together with the .rpm extension.\u003c/li\u003e\n\u003cli\u003ePackages included or available for RHEL are in rpm format.\u003c/li\u003e\n\u003cli\u003eMetadata info gets updated whenever a package is updated.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003erpm command\n    \u003cdiv id=\"rpm-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#rpm-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eInstall, Upgrade, remove, query, freshen, or decompress packages.\u003c/li\u003e\n\u003cli\u003eValidate package authenticity and integrity.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003ePackages\n    \u003cdiv id=\"packages\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#packages\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTwo types of packages binary (or installable) and source.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eBinary packages\u003c/strong\u003e\u003c/p\u003e","title":"Basic Package Management","type":"tech"},{"content":" Listing Logged-In Users # A list of the users who have successfully signed on to the system with valid credentials can be printed using who and w\nwho command # references the /run/utmp file and displays the information. displays login name of user shows terminal session device filename pts stands for pseudo terminal session shows data and time of user login Shows if terminal session is graphical(:0), remote(IP address), or textual on the console what command (w) # Shows length of time the user has been idle CPU time used by all processes including any existing background jobs attached to this terminal (JCPU), CPU time used by the current process (PCPU), current activity (WHAT). current system time system up duration number of users logged in cpu averages over last 1, 5, and 15 minutes load average (CPU load): 0.00 and 1.00 correspond to no load and full load, and a number greater than 1.00 signifies excess load (over 100%). last command # Reports the history of successful user login attempts and system boots Consults the wtmp file located in the /var/log directory. wtmp keeps a record of login/logout activities login time duration a user stayed logged in tty Output Login name Terminal name Terminal name or IP from where connection was established Day, Month, date, and time when the connection was established Log out time or (still logged in) Duration of session Action name (system reboots section) Activity name (system reboots section) Linux kernel version (system reboots section) Day, Month, date, and time when the reboot command was issued (system reboots section) System restart time (system reboots section) Duration the system remained down or (still running) (system reboots section) log filename (wtmp) (last line) lastb command # reports failed login attempts Consults /var/log/btmp record of failed login attempts login name time tty Must be root to run this command Columns name of user protocol used terminal name or ip address Day, Month, Date, and time of the attempt Duration the attempt was tried Duration the attempt last for log filename (btmp) (last line) lastlog command # most recent login evidence info for every user account that exists on the system Consults /var/log/lastlog record of most recent user attempts login name time port (or tty) Columns: Login name of user Terminal name assigned upon Logging in Terminal name or Ip address from where the session was initiated Timestamp for the latest login or \u0026ldquo;Never logged in\u0026rdquo; service accounts are used by their respective services, and they are not meant for logging. id (identifier) Command # displays the calling user’s: UID (User IDentifier) username GID (Group IDentifier) group name all secondary groups the user is a member of SELinux security context groups Command: # lists all groups the calling user is a member of: first group listed is the primary group for the user who executed this command other groups are secondary (or supplementary). can also view group membership information for a different user. User Account Management # useradd Command # add a new user to the system adds entries to the four user authentication files for each account added to the system creates a home directory for the user and copies the default user startup files from the skeleton directory /etc/skel into the user’s home directory used to update the default settings that are used at the time of new user creation for unspecified settings Options -b (\u0026ndash;base-dir) Defines the absolute path to the base directory for placing user home directories. The default is /home. -c (\u0026ndash;comment) Describes useful information about the user. -d (\u0026ndash;home-dir) Defines the absolute path to the user home directory. -D (\u0026ndash;defaults) Displays the default settings from the /etc/default/useradd file and modifies them. -e (\u0026ndash;expiredate) Specifies a date on which a user account is automatically disabled. The format for the date specification is YYYY-MM-DD. -f (\u0026ndash;inactive) Denotes maximum days of inactivity between password expiry and permanent account disablement. -g (\u0026ndash;gid) Specifies the primary GID. Without this option, a group account matching the username is created with the GID matching the UID. -G (\u0026ndash;groups) Specifies the membership to supplementary groups. -k (\u0026ndash;skel) location of the skeleton directory (default is /etc/skel) (stores default user startup files) These files are copied to the user’s home directory at the time of account creation. Three hidden bash shell files: (default) .bash_profile, .bashrc, and .bash_logout You can customize these files or add your own to be used for accounts created thereafter. -m (\u0026ndash;create-home) Creates a home directory if it does not already exist. -o (\u0026ndash;non-unique) Creates a user account sharing the UID of an existing user. When two users share a UID, both get identical rights on each other’s files. Should only be done in specific situations. -r (\u0026ndash;system) Creates a service account with a UID below 1000 and a never-expiring password. -s (\u0026ndash;shell) Defines the absolute path to the shell file. The default is /bin/bash. -u (\u0026ndash;uid) Indicates a unique UID. Without this option, the next available UID from the /etc/passwd file is used. login Specifies a login name to be assigned to the user account. usermod Command # modify the attributes of an existing user similar syntax to useradd and most switches identical. Options unique to usermod: -a (\u0026ndash;append) Adds a user to one or more supplementary groups -l (\u0026ndash;login) Specifies a new login name -m (\u0026ndash;move-home) Creates a home directory and moves the content over from the old location -G Add a list of groups a user is a member of. userdel Command # to remove a user from the system passwd Command # set or modify a user’s password No-Login (Non-Interactive) User Account # nologin command # /sbin/nologin special purpose program that can be employed for user accounts that do not require login access to the system. located in the /usr/sbin (or /sbin) directory user is refused with the message, “This account is currently not available.” If a custom message is required, you can create a file called nologin.txt in the /etc directory and add the desired text to it. If a no-login user is able to log in with their credentials, there is a problem. Use the grep command against the /etc/passwd file to ensure ‘/sbin/nologin’ is there in the shell field for that user. examples of user accounts that do not require login access are the service accounts such as ftp, apache, and sshd. Basic User Management Labs # Lab: who # who Lab: what # w Lab: last # List all user login, logout, and system reboot occurrences: last List system reboot info only: last reboot Lab: lastb # lastb Lab: lastlog # lastlog Lab: id # View info about currently active user: id View info about another user: id user1 Lab: groups # View current user\u0026rsquo;s groups: groups View groups of another user: groups user1 Lab: user authentication files # list of the four files and their backups from the /etc directory: ls -l /etc/passwd* /etc/group* /etc/shadow* /etc/gshadow* View first and last 3 lines of the passwd file head -3 /etc/passwd ; tail -3 /etc/passwd verify the permissions and ownership on the passwd file: ls -l /etc/passwd View first and last 3 lines of the shadow file: head -3 /etc/shadow ; tail -3 /etc/shadow verify the permissions and ownership on the shadow file: ls -l /etc/shadow View first and last 3 lines of the group file: head -3 /etc/group ; tail -3 /etc/group Verify the permissions and ownership on the group file: ls -l /etc/group View first and last 3 lines of the gshadow file: head -3 /etc/gshadow ; tail -3 /etc/gshadow Verify the permissions and ownership on the gshadow file: ls -l /etc/gshadow Lab: useradd and login.defs # use the cat or less command to view the useradd file content or display the settings with the useradd command: useradd -D grep on the/etc/login.defs with uncommented and non-empty lines: grep -v ^# /etc/login.defs | grep -v ^$ Lab: Create a User Account with Default Attributes (root) # Create user2 with all the default directives: useradd user2 Assign this user a password and enter it twice when prompted: passwd user2 grep for user2: on the authentication files to examine what the useradd command has added: cd /etc ; grep user2: passwd shadow group gshadow Test this new account by logging in as user2 and then run the id and groups commands to verify the UID, GID, and group membership information: su - user2 id groups Lab: Create a User Account with Custom Values # Create user3 with UID 1010, home directory /usr/user3a, and shell /bin/sh: useradd -u 1010 -d /usr/user3a -s /bin/sh user3 Assign user1234 as password (passwords assigned in the following way is not recommended; however, it is okay in a lab environment): echo user1234 | passwd --stdin user3 grep for user3: on the four authentication files to see what was added for this user: cd /etc ; grep user3: passwd shadow group gshadow Test this account by switching to or logging in as user3 and entering user1234 as the password. Run the id and groups commands for further verification. su - user3 id groups Lab: Modify and Delete a User Account # Modify the login name for user2 to user2new, UID to 2000, home directory to /home/user2new, and login shell to /sbin/nologin. usermod -l user2new -m -d /home/user2new -s /sbin/nologin -u 2000 user2 Obtain the information for user2new from the passwd file for confirmation: grep user2new /etc/passwd Remove user2new along with their home and mail spool directories: userdel -r user2new Confirm the user deletion: grep user2new /etc/passwd Lab: Create a User Account with No-Login Access (root) # Look at the current nologin users: grep nologin /etc/passwd Create user4 with non-interactive shell file /sbin/nologin: useradd -s /sbin/nologin user4 Assign user1234 as password: echo user1234 | passwd --stdin user4 grep for user4 on the passwd file and verify the shell field containing the nologin shell: grep user4 /etc/passwd Test this account by attempting to log in or switch: su - user4 Lab: Check User Login Attempts (root) # execute the last, lastb, and lastlog commands, and observe the outputs. last lastb lastlog List the timestamps when the system was last rebooted. last | grep reboot Lab 5-2: Verify User and Group Identity (user1) # run the who and w commands one at a time, and compare the outputs. who w Execute the id and groups commands, and compare the outcomes. Examine the extra information that the id command shows, but not the groups command. id groups Lab 5-3: Create Users (root) # create user account user4100 with UID 4100 and home directory under /usr. useradd -m -d /usr/user4100 -u 4100 user4100 Create another user account user4200 with default attributes. useradd user4200 Assign both users a password. passwd user4100 passwd user4200 View the contents of the passwd, shadow, group, and gshadow files, and observe what has been added for the two new users. cat /etc/passwd cat /etc/shadow cat /etc/group cat /etc/gshadow Lab: Create User with Non-Interactive Shell (root) # Create user account user4300 with the disability of logging in. useradd -s /sbin/nologin user4300 Assign this user a password. passwd user4300 Try to log on with this user and see is displayed on the screen. su - user4300 View the content of the passwd file, and see what is there that prevents this user from logging in. cat /etc/passwd ","externalUrl":null,"permalink":"/tech/linux/basic-user-management/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eListing Logged-In Users\n    \u003cdiv id=\"listing-logged-in-users\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#listing-logged-in-users\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eA list of the users who have successfully signed on to the system with valid credentials can be printed using \u003ccode\u003ewho\u003c/code\u003e and \u003ccode\u003ew\u003c/code\u003e\u003c/p\u003e","title":"Basic User Management","type":"tech"},{"content":"These are the summaries and notes of books I have read.\nThe Willpower Instinct ★★★★★ I picked up a bunch of new habits in the past couple months since reading this. Positive habits are self-perpetuating. So are negative ones. Dopamine Nation ★★★★★ Our brains are not adapted for a world of abundance. So the forces that kept us alive for thousands of years, are now making us miserable. Consider this a primer into understanding addiction and over consumption. And steps you can take to mitigate this. Good Inside ★★★★★ We must separate the child from the behavior. Often, when a child's behavior appears as \"bad\", it's really the child having a big emotion that they can't regulate in a controlled way. The Relationship Handbook ★★★★★ People can be mean if they are in a bad mood. Forgive them for what they say and have a fresh start. The Power of Positive Dog Training ★★★★★ Dog training without being a jerk. Any dog can be trained with progressive reinforcement techniques. The author even explains why aversive training not only doesn’t work, but it makes problems sprout up in other areas of your dogs life. Nonviolent Communication ★★★★★ Easy concepts to grasp, challenging to apply to the real world. This is how we communicate with other people. This is going to need a re-read at some point. Meditations ★★★★☆ This book makes me want to keep a better journaling habit. And study philosophy. And not worry about things that do not matter. Because we all become dust in the end anyway. Caffeine Blues ★★★★☆ Everyone should read this book. Caffeine does more than just block adenosine receptors so you don't feel tired. Caffeine = Stress. The Psychology of Money ★★★★☆ Holy cow was this eye opening. Not only in regards to money, but the principles apply to many aspects of life. Strong story telling, clear and concise. Worth reading multiple times. Million Dollar Weekend ★★★★☆ Motivational, inspiring, and practical. I will be coming back to these notes over and over as I embark on my business ventures. Don't be afraid to ask for what you want. In fact, develop your asking powers like a muscle. Also, don't try to figure out how to do something before you start. Just start and figure out how along the way. Anything You Want ★★★★☆ Quick read about creating the business (and life) that you want. Life isn't all about maximizing profits and accelerated growth. Do what makes you happy. Treat your customers like rock stars and they will tell everyone. The 12 Week Year ★★★☆☆ Solid advice on establishing a vision, settings goals, measuring the effectiveness of your actions, and other tidbits about being effective and taking action. I have been implementing most of this and needed to go back through to make sure I've got down the process. The Blog Startup ★★★☆☆ If you are wanting to blog as a business, you need a sound strategy and business model. Filled with practical tips on identifying your audience, email strategy, branding, marketing, things to watch out for, etc. Definitely read this if you are wanting to start a blog. 1-2-3 Magic ★★☆☆☆ Don't argue with your kids. Manage behavior without getting angry. Very easy to use \"breaks\" and rewards to try and manipulate your child instead of teaching lessons and trying to get activities to be enjoyable in and of themselves. ","externalUrl":null,"permalink":"/notes/","section":"","summary":"\u003cp\u003eThese are the summaries and notes of books I have read.\u003c/p\u003e\n\u003cdiv style=\"display:flex; gap:1rem; align-items:flex-start;\"\u003e\n  \u003cimg src=\"thewillpowerinstinct/willpowerinstinct.png\" style=\"width:19%; height:auto;\" /\u003e\n  \u003cdiv\u003e\n    \u003ch3 class=\"mb-1\"\u003e\n    \u003ca href=\"thewillpowerinstinct\" class=\"hover:underline\"\u003e\n     The Willpower Instinct\n    \u003c/a\u003e\n    \u003c/h3\u003e\n    \u003cdiv class=\"text-lg\"\u003e\n    ★★★★★\n    \u003c/div\u003e\n    \u003cp\u003e\n    I picked up a bunch of new habits in the past couple months since reading this. Positive habits are self-perpetuating. So are negative ones.\n    \u003c/p\u003e","title":"Book Notes \u0026 Reviews","type":"page"},{"content":" Managing the Boot Process # No modules for managing boot process.\nfile module\nmanage the systemd boot targets lineinfile module\nmanage the GRUB configuration. reboot module\nenables you to reboot a host and pick up after the reboot at the exact same location. Managing Systemd Targets # To manage the default systemd target:\n/etc/systemd/system/default.target file must exist as a symbolic link to the desired default target. ls -l /etc/systemd/system/default.target lrwxrwxrwx. 1 root root 37 Mar 23 05:33 /etc/systemd/system/default.target -\u0026gt; /lib/systemd/system/multi-user.target --- - name: set default boot target hosts: ansible2 tasks: - name: set boot target to graphical file: src: /usr/lib/systemd/system/graphical.target dest: /etc/systemd/system/default.target state: link Rebooting Managed Hosts # reboot module.\nRestart managed nodes. test_command argument\nVerify the renewed availability of the managed hosts Specifies an arbitrary command that Ansible should run successfully on the managed hosts after the reboot. The success of this command indicates that the rebooted host is available again. Equally useful while using the reboot module are the arguments that relate to timeouts. The reboot module uses no fewer than four of them:\n• connect_timeout: The maximum seconds to wait for a successful connection before trying again\n• post_reboot_delay: The number of seconds to wait after the reboot command before trying to validate the managed host is available again\n• pre_reboot_delay: The number of seconds to wait before actually issuing the reboot\n• reboot_timeout: The maximum seconds to wait for the rebooted machine to respond to the test command\nWhen the rebooted host is back, the current playbook continues its tasks. This scenario is shown in the example in Listing 14-7, where first all managed hosts are rebooted, and after a successful reboot is issued, the message \u0026ldquo;successfully rebooted\u0026rdquo; is shown. Listing 14-8 shows the result of running this playbook. In Exercise 14-2 you can practice rebooting hosts using the reboot module.\nListing 14-7 Rebooting Managed Hosts\n::: pre_1 \u0026mdash; - name: reboot all hosts hosts: all gather_facts: no tasks: - name: reboot hosts reboot: msg: reboot initiated by Ansible test_command: whoami - name: print message to show host is back debug: msg: successfully rebooted :::\nListing 14-8 Verifying the Success of the reboot Module\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing147.yaml\nPLAY [reboot all hosts] ************************************************************************************************* TASK [reboot hosts] ***************************************************************************************************** changed: [ansible2] changed: [ansible1] changed: [ansible3] changed: [ansible4] changed: [ansible5] TASK [print message to show host is back] ******************************************************************************* ok: [ansible1] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;successfully rebooted\u0026quot; } ok: [ansible2] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;successfully rebooted\u0026quot; } ok: [ansible3] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;successfully rebooted\u0026quot; } ok: [ansible4] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;successfully rebooted\u0026quot; } ok: [ansible5] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;successfully rebooted\u0026quot; } PLAY RECAP ************************************************************************************************************** ansible1 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible2 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible3 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible4 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible5 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 :::\n::: box Exercise 14-2 Managing Boot State\n1. As a preparation for this playbook, so that it actually changes the default boot target on the managed host, use ansible ansible2 -m file -a \u0026ldquo;state=link src=/usr/lib/systemd/system/graphical.target dest=/etc/systemd/system/default.target\u0026rdquo;.\n2. Use your editor to create the file exercise142.yaml and write the following playbook header:\n--- - name: set default boot target and reboot hosts: ansible2 tasks: 3. Now you set the default boot target to multi-user.target. Add the following task to do so:\n- name: set default boot target file: src: /usr/lib/systemd/system/multi-user.target dest: /etc/systemd/system/default.target state: link 4. Complete the playbook to reboot the managed hosts by including the following tasks:\n- name: reboot hosts reboot: msg: reboot initiated by Ansible test_command: whoami - name: print message to show host is back debug: msg: successfully rebooted 5. Run the playbook by using ansible-playbook exercise142.yaml.\n6. Test that the reboot was issued successfully by using ansible ansible2 -a \u0026ldquo;systemctl get-default\u0026rdquo;. :::\n","externalUrl":null,"permalink":"/tech/ansible/boot-process/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eManaging the Boot Process\n    \u003cdiv id=\"managing-the-boot-process\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#managing-the-boot-process\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eNo modules for managing boot process.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003efile\u003c/strong\u003e module\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emanage the systemd boot targets\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003elineinfile\u003c/strong\u003e module\u003c/p\u003e","title":"Boot Process","type":"tech"},{"content":" Linux Kernel # controls everything on the system. hardware enforces security and access controls runs, schedules, and manages processes and service daemons. comprised of several modules. new kernel must be installed or an existing kernel must be upgraded when the need arises from an application or functionality standpoint. core of the Linux system. manages hardware enforces security regulates access to the system handles\nprocesses\nservices\napplication workloads.\ncollection of software components called modules\nModules device drivers that control hardware devices processor memory storage controller cards peripheral equipment interact with software subsystems storage partitioning file systems networking virtualization Some modules are static to the kernel and are integral to system functionality,\nSome modules are loaded dynamically as needed\nRHEL 8.0 and RHEL 8.2 are shipped with kernel version 4.18.0 (4.18.0-80 and 4.18.0-193 to be specific) for the 64-bit Intel/AMD processor architecture computers with single, multi-core, and multi-processor configurations.\nuname -m shows the architecture of the system.\nKernel requires a rebuild when a new functionality is added or removed.\nfunctionality may be introduced by:\ninstalling a new kernel upgrading an existing one installing a new hardware device, or changing a critical system component. existing functionality that is no longer needed may be removed to make the overall footprint of the kernel smaller for improved performance and reduced memory utilization.\ntunable parameters are set that define a baseline for kernel functionality.\nSome parameters must be tuned for some applications and database software to be installed smoothly and operate properly.\nYou can generate and store several custom kernels with varied configuration and required modules\nonly one of them can be active at a time.\ndifferent kernel may be loaded by interacting with GRUB2.\nKernel Packages # set of core kernel packages that must be installed on the system at a minimum to make it work. Additional packages providing supplementary kernel support are also available. Core and some add-on kernel packages.\nKernel Package Description kernel Contains no files, but ensures other kernel packages are accurately installed kernel-core Includes a minimal number of modules to provide core functionality kernel-devel Includes support for building kernel modules kernel-modules Contains modules for common hardware devices kernel-modules-extra Contains modules for not-so-common hardware devices kernel-headers Includes files to support the interface between the kernel and userspace kernel-tools-libs Includes the libraries to support the kernel tools libraries and programs kernel-tools Includes tools to manipulate the kernel Kernel Packages # Packages containing the source code for RHEL 8 are also available for those who wish to customize and recompile the code List kernel packages installed on the system:\ndnf list installed kernel* Shows six kernel packages that were loaded during the OS installation. Analyzing Kernel Version # Check the version of the kernel running on the system to check for compatibility with an application or database:\nuname -r 5.14.0-362.24.1.el9_3.x86_64 5 - Major version 14 - Major revision 0 - Kernel patch version 362 - Red Hat version el9 - Enterprise Linux 9 x86_64 - Processor architecture\nKernel Directory Structure # Kernel and its support files (noteworthy locations)\n/boot /proc /usr/lib/modules /boot # Created at system installation. Linux kernel GRUB2 configuration other kernel and boot support files. View the /boot filesystem: ls -l /boot\nfour files are for the kernel and vmlinuz - main kernel file initramfs - main kernel\u0026rsquo;s boot image config - configuration System.map - mapping two files for kernel rescue version Have the current kernel version appended to their names. have the string \u0026ldquo;rescue\u0026rdquo; embedded within their names /boot/efi/ and /boot/grub2/\nhold bootloader information specific to firmware type used on the system: UEFI or BIOS. List /boot/Grub2:\n[root@localhost ~]# ls -l /boot/grub2 total 32 -rw-r--r--. 1 root root 64 Feb 25 05:13 device.map drwxr-xr-x. 2 root root 25 Feb 25 05:13 fonts -rw-------. 1 root root 7049 Mar 21 04:47 grub.cfg -rw-------. 1 root root 1024 Mar 21 05:12 grubenv drwxr-xr-x. 2 root root 8192 Feb 25 05:13 i386-pc drwxr-xr-x. 2 root root 4096 Feb 25 05:13 locale grub.cfg bootable kernel information grub.env environment information that the kernel uses. /boot/loader\nstorage location for configuration of the running and rescue kernels. Configuration is stored in files under the /boot/loader/entries/ [root@localhost ~]# ls -l /boot/loader/entries/ total 12 -rw-r--r--. 1 root root 484 Feb 25 05:13 8215ac7e45d34823b4dce2e258c3cc47-0-rescue.conf -rw-r--r--. 1 root root 460 Mar 16 06:17 8215ac7e45d34823b4dce2e258c3cc47-5.14.0-362.18.1.el9_3.x86_64.conf -rw-r--r--. 1 root root 459 Mar 16 06:17 8215ac7e45d34823b4dce2e258c3cc47-5.14.0-362.24.1.el9_3.x86_64.conf The files are named using the machine id of the system as stored in /etc/machine-id/ and the kernel version they are for. content of the kernel file:\n[root@localhost entries]# cat /boot/loader/entries/8215ac7e45d34823b4dce2e258c3cc47-5.14.0- 362.18.1.el9_3.x86_64.conf title Red Hat Enterprise Linux (5.14.0-362.18.1.el9_3.x86_64) 9.3 (Plow) version 5.14.0-362.18.1.el9_3.x86_64 linux /vmlinuz-5.14.0-362.18.1.el9_3.x86_64 initrd /initramfs-5.14.0-362.18.1.el9_3.x86_64.img $tuned_initrd options root=/dev/mapper/rhel-root ro crashkernel=1G-4G:192M,4G- 64G:256M,64G-:512M resume=/dev/mapper/rhel-swap rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet $tuned_params grub_users $grub_users grub_arg --unrestricted grub_class rhel \u0026ldquo;title\u0026rdquo; is displayed on the bootloader screen \u0026ldquo;kernelopts\u0026rdquo; and \u0026ldquo;tuned_params\u0026rdquo; supply values to the booting kernel to control its behavior. /proc # Virtual, memory-based file system contents are created and updated in memory at system boot and during runtime destroyed at system shutdown current state of the kernel, which includes hardware configuration status information processor memory storage file systems swap processes network interfaces connections routing etc. Data kept in tens of thousands of zero-byte files organized in a hierarchy. List /proc: ls -l /proc\nnumerical subdirectories contain information about a specific process process ID matches the subdirectory name. other files and subdirectories contain information, such as memory segments for processes and configuration data for system components. can view the configuration in vim Show selections from the cpuinfo and meminfo files that hold processor and memory information: cat/proc/cpuinfo \u0026amp;\u0026amp; cat /proc/meminfo\ndata used by top, ps, uname, free, uptime and w, to display information. /usr/lib/modules/ # holds information about kernel modules. subdirectories are specific to the kernels installed on the system. Long listing of /usr/lib/modules/ shows two installed kernels:\n[root@localhost entries]# ls -l /usr/lib/modules total 8 drwxr-xr-x. 7 root root 4096 Mar 16 06:18 5.14.0-362.18.1.el9_3.x86_64 drwxr-xr-x. 8 root root 4096 Mar 16 06:18 5.14.0-362.24.1.el9_3.x86_64 View /usr/lib/modules/5.14.0-362.18.1.el9_3.x86_64/:\nls -l /usr/lib/modules/5.14.0-362.18.1.el9_3.x86_64 Subdirectories hold module-specific information for the kernel version. /lib/modules/4.18.0-80.el8.x86_64/kernel/drivers/\nstores modules for a variety of hardware and software components in various subdirectories: ls -l /usr/lib/modules/5.14.0-362.18.1.el9_3.x86_64/kernel/drivers Additional modules may be installed on the system to support more components. Installing the Kernel # requires extra care\ncould leave your system in an unbootable or undesirable state.\nhave the bootable medium handy prior to starting the kernel install process.\nBy default, the dnf command adds a new kernel to the system, leaving the existing kernel(s) intact. It does not replace or overwrite existing kernel files.\nAlways install a new version of the kernel instead of upgrading it.\nThe upgrade process removes any existing kernel and replaces it with a new one.\nIn case of a post-installation issue, you will not be able to revert to the old working kernel.\nNewer version of the kernel is typically required:\nif an application needs to be deployed on the system that requires a different kernel to operate. When deficiencies or bugs are identified in the existing kernel, it can hamper the kernel\u0026rsquo;s smooth operation. new kernel\naddresses existing issues adds bug fixes security updates new features improved support for hardware devices. dnf is the preferred tool to install a kernel\nit resolves and installs any required dependencies automatically.\nrpm may be used but you must install any dependencies manually.\nKernel packages for RHEL are available to subscribers on Red Hat\u0026rsquo;s Customer Portal.\nLinux Boot Process # Multiple phases during the boot process.\nStarts selective services during its transition from one phase into another. Presents the administrator an opportunity to interact with a preboot program to boot the system into a non-default target. Pass an option to the kernel. Reset the lost or forgotten root user password. Launches a number of services during its transition to the default or specified target. boot process after the system has been powered up or restarted. lasts until all enabled services are started. login prompt will appear on the screen boot process is automatic, but you may need to interact with it to take a non-default action, such as booting an alternative kernel booting into a non-default operational state repairing the system recovering from an unbootable state boot process on an x86 computer may be split into four major phases: (1) the firmware phase (2) the bootloader phase (3) the kernel phase (4) the initialization phase. The system accomplishes these phases one after the other while performing and attempting to complete the tasks identified in each phase.\nThe Firmware Phase (BIOS and UEFI) # firmware:\nBIOS (Basic Input/Output System) or the UEFI (Unified Extensible Firmware Interface) code that is stored in flash memory on the x86-based system board. runs the Power-On-Self-Test (POST) to detect, test, and initialize the system hardware components. Installs appropriate drivers for the video hardware exhibits system messages on the screen. scans available storage devices to locate a boot device, starting with a 512-byte image that contains 446 bytes of the bootloader program, 64 bytes for the partition table last two bytes with the boot signature. referred to as the Master Boot Record (MBR) located on the first sector of the boot disk. As soon as it discovers a usable boot device, it loads the bootloader into memory and passes control over to it. BIOS\nsmall memory chip in the computer that stores system date and time, list and sequence of boot devices, I/O configuration, etc. configuration is customizable. hardware initialization phase detecting and diagnosing peripheral devices. runs the POST on the devices as it finds them installs drivers for the graphics card and the attached monitor begins exhibiting system messages on the video hardware. discovers a usable boot device loads the bootloader program into memory, and passes control over to it. UEFI\nnew 32/64-bit architecture-independent specification replacing BIOS. delivers enhanced boot and runtime services superior features such as speed over the legacy 16-bit BIOS. has its own device drivers able to mount and read extended file systems includes UEFI-compliant application tools supports one or more bootloader programs. comes with a boot manager that allows you to choose an alternative boot source. Bootloader Phase # Once the firmware phase is over and a boot device is detected, system loads a piece of software called bootloader that is located in the boot sector of the boot device. RHEL uses GRUB2 (GRand Unified Bootloader) version 2 as the bootloader program. GRUB2 supports both BIOS and UEFI firmware. The primary job of the bootloader program is to\nspot the Linux kernel code in the /boot file system decompress it load it into memory based on the configuration defined in the /boot/grub2/grub.cfg file transfer control over to it to further the boot process. UEFI-based systems,\nGRUB2 looks for the EFI system partition /boot/efi instead Runs the kernel based on the configuration defined in the /boot/efi/EFI/redhat/grub.efi file. Kernel Phase # kernel is the central program of the operating system, providing access to hardware and system services. After getting control from the bootloader, the kernel: extracts the initial RAM disk (initrd) file system image found in the /boot file system into memory,\ndecompresses it\nmounts it as read-only on /sysroot to serve as the temporary root file system\nloads necessary modules from the initrd image to allow access to the physical disks and the partitions and file systems therein.\nloads any required drivers to support the boot process.\nLater, it unmounts the initrd image and mounts the actual physical root file system on / in read/write mode.\nAt this point, the necessary foundation has been built for the boot process to carry on and to start loading the enabled services.\nkernel executes the systemd process with PID 1 and passes the control over to it.\nInitialization Phase # fourth and the last phase in the boot process.\nSystemd:\ntakes control from the kernel and continues the boot process.\nis the default system initialization scheme used in RHEL 9.\nstarts all enabled userspace system and network services\nBrings the system up to the preset boot target.\nA boot target is an operational level that is achieved after a series of services have been started to get to that state.\nsystem boot process is considered complete when all enabled services are operational for the boot target and users are able to log in to the system\nGRUB2 Bootloader # After the firmware phase has concluded: Bootloader presents a menu with a list of bootable kernels available on the system Waits for a predefined amount of time before it times out and boots the default kernel. You may want to interact with GRUB2 before the autoboot times out to boot with a non-default kernel boot to a different target, or customize the kernel boot string. Press a key before the timeout expires to interrupt the autoboot process and interact with GRUB2. autoboot countdown default value is 5 seconds. Interacting with GRUB2 # GRUB2 main menu shows a list of bootable kernels at the top. Edit a selected kernel menu entry by pressing an e or go to the grub\u0026gt; command prompt by pressing a c. edit mode,\nGRUB2 loads the configuration for the selected kernel entry from the /boot/grub2/grub.cfg file in an editor enables you to make a desired modification before booting the system. you can boot the system into a less capable operating target by adding \u0026ldquo;rescue\u0026rdquo;, \u0026ldquo;emergency\u0026rdquo;, or \u0026ldquo;3\u0026rdquo; to the end of the line that begins with the keyword \u0026ldquo;linux\u0026rdquo;, Press Ctrl+x when done to boot. one-time temporary change and it won\u0026rsquo;t touch the grub.cfg file. press ESC to discard the changes and return to the main menu. grub\u0026gt; command prompt appears when you press Ctrl+c while in the edit window or a c from the main menu. command mode: execute debugging, recovery, etc. view available commands by pressing the TAB key. GRUB2 Commands # Understanding GRUB2 Configuration Files # /boot/grub2/grub.cfg\nReferenced at boot time. Generated automatically when a new kernel is installed or upgraded not advisable to modify it directly, as your changes will be overwritten. : /etc/default/grub\nprimary source file that is used to regenerate grub.cfg. Defines the directives that govern how GRUB2 should behave at boot time. Any changes made to the grub file will only take effect after the grub2-mkconfig utility has been executed Defines the directives that control the behavior of GRUB2 at boot time. Any changes in this file must be followed by the execution of the grub2-mkconfig command in order to be reflected in grub.cfg. Default settings:\n[root@localhost default]# nl /etc/default/grub 1\tGRUB_TIMEOUT=5 2\tGRUB_DISTRIBUTOR=\u0026#34;$(sed \u0026#39;s, release .*$,,g\u0026#39; /etc/system-release)\u0026#34; 3\tGRUB_DEFAULT=saved 4\tGRUB_DISABLE_SUBMENU=true 5\tGRUB_TERMINAL_OUTPUT=\u0026#34;console\u0026#34; 6\tGRUB_CMDLINE_LINUX=\u0026#34;crashkernel=1G-4G:192M,4G-64G:256M,64G-:512M resume=/dev/mapper/rhel-swap rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet\u0026#34; 7\tGRUB_DISABLE_RECOVERY=\u0026#34;true\u0026#34; 8\tGRUB_ENABLE_BLSCFG=true Directive Description GRUB_TIMEOUT Wait time, in seconds, before booting off the default kernel. Default is 5. GRUB_DISTRIBUTOR Name of the Linux distribution GRUB_DEFAULT Boots the selected option from the previous system boot GRUB_DISABLE_SUBMENU Enables/disables the appearance of GRUB2 submenu GRUB_TERMINAL_OUTPUT Sets the default terminal GRUB_CMDLINE_LINUX Specifies the command line options to pass to the kernel at boot time GRUB_DISABLE_RECOVERY Lists/hides system recovery entries in the GRUB2 menu GRUB_ENABLE_BLSCFG Defines whether to use the new bootloader specification to manage bootloader configuration Default settings are good enough for normal system operation. /boot/grub2/grub.cfg - /boot/efi/EFI/redhat/grub.cfg # Main GRUB2 configuration file that supplies boot-time configuration information. located in the /boot/grub2/ on BIOS-based systems /boot/efi/EFI/redhat/ on UEFI-based systems. can be recreated manually with the grub2-mkconfig utility automatically regenerated when a new kernel is installed or upgraded. file will lose any previous manual changes made to it. grub2-mkconfig command\nUses the settings defined in helper scripts located in the /etc/grub.d directory. [root@localhost default]# ls -l /etc/grub.d total 104 -rwxr-xr-x. 1 root root 9346 Jan 9 09:51 00_header -rwxr-xr-x. 1 root root 1046 Aug 29 2023 00_tuned -rwxr-xr-x. 1 root root 236 Jan 9 09:51 01_users -rwxr-xr-x. 1 root root 835 Jan 9 09:51 08_fallback_counting -rwxr-xr-x. 1 root root 19665 Jan 9 09:51 10_linux -rwxr-xr-x. 1 root root 833 Jan 9 09:51 10_reset_boot_success -rwxr-xr-x. 1 root root 892 Jan 9 09:51 12_menu_auto_hide -rwxr-xr-x. 1 root root 410 Jan 9 09:51 14_menu_show_once -rwxr-xr-x. 1 root root 13613 Jan 9 09:51 20_linux_xen -rwxr-xr-x. 1 root root 2562 Jan 9 09:51 20_ppc_terminfo -rwxr-xr-x. 1 root root 10869 Jan 9 09:51 30_os- prober -rwxr-xr-x. 1 root root 1122 Jan 9 09:51 30_uefi- firmware -rwxr-xr-x. 1 root root 218 Jan 9 09:51 40_custom -rwxr-xr-x. 1 root root 219 Jan 9 09:51 41_custom -rw-r--r--. 1 root root 483 Jan 9 09:51 README 00_header\nsets the GRUB2 environment 10_linux searches for all installed kernels on the same disk partition 30_os-prober searches for the presence of other operating systems 40_custom and 41_custom are to introduce any customization. like add custom entries to the boot menu. grub.cfg file\nSources /boot/grub2/grubenv for kernel options and other settings. [root@localhost grub2]# cat grubenv # GRUB Environment Block # WARNING: Do not edit this file by tools other than grub-editenv!!! saved_entry=8215ac7e45d34823b4dce2e258c3cc47-5.14.0- 362.24.1.el9_3.x86_64 menu_auto_hide=1 boot_success=0 boot_indeterminate=0 ############################################################################ ##################################################### ####################### If a new kernel is installed:\nthe existing kernel entries remain intact. All bootable kernels are listed in the GRUB2 menu any of the kernel entries can be selected to boot. Lab: Change Default System Boot Timeout # change the default system boot timeout value to 8 seconds persistently, and validate. Edit the /etc/default/grub file and change the setting as follows: `GRUB_TIMEOUT=8\nExecute the grub2-mkconfig command to reproduce grub.cfg:\ngrub2-mkconfig -o /boot/grub2/grub.cfg 3.Restart the system with sudo reboot and confirm the new timeout value when GRUB2 menu appears.\nBooting into Specific Targets # RHEL\nboots into graphical target state by default if the Server with GUI software selection is made during installation.\ncan also be directed to boot into non-default but less capable operating targets from the GRUB2 menu.\noffers emergency and rescue boot targets.\nspecial target levels can be launched from the GRUB2 interface by selecting a kernel pressing e to enter the edit mode appending the desired target name to the line that begins with the keyword \u0026ldquo;linux\u0026rdquo;. Press ctrl+x to boot into the supplied target Enter root password reboot when you are done You must know how to boot a RHEL 9 system into a specific target from the GRUB2 menu to modify the fstab file or reset an unknown root user password.\nAppend \u0026ldquo;emergency\u0026rdquo; to the kernel line entry:\nOther options:\n\u0026ldquo;rescue\u0026rdquo; \u0026ldquo;1\u0026rdquo; \u0026ldquo;s\u0026rdquo; \u0026ldquo;single\u0026rdquo; Reset the root User Password # Terminate the boot process at an early stage to be placed in a special debug shell in order to reset the root password. Reboot or reset server1, and interact with GRUB2 by pressing a key before the autoboot times out. Highlight the default kernel entry in the GRUB2 menu and press e to enter the edit mode. Scroll down to the line entry that begins with the keyword \u0026ldquo;linux\u0026rdquo; and press the End key to go to the end of that line:\nModify this kernel string and append \u0026ldquo;rd.break\u0026rdquo; to the end of the line. Press Ctrl+x when done to boot to the special shell. The system mounts the root file system read-only on the /sysroot directory. Make /sysroot appear as mounted on / using the chroot command:\nchroot sysroot 3. Remount the root file system in read/write mode for the passwd command to be able to modify the shadow file with a new password:\nmount -o remount,rw / Enter a new password for root by invoking the passwd command: passwd Create a hidden file called .autorelabel to instruct the operating system to run SELinux relabeling on all files, including the shadow file that was updated with the new root password, on the next reboot: touch .autorelabel Issue the exit command to quit the chroot shell and then the reboot command to restart the system and boot it to the default target. exit reboot Second method # Look into using init=/bin/bash for password recovery as a second method.\nBoot Grub2 Kernel Labs # Lab: Enable Verbose System Boot # Remove \u0026ldquo;quiet\u0026rdquo; from the end of the value of the variable GRUB_CMDLINE_LINUX in the /etc/default/grub file Run grub2-mkconfig to apply the update. Reboot the system and observe that the system now displays verbose information during the boot process. Lab: Reset root User Password # Reset the root user password by booting the system into emergency mode with SELinux disabled. Try to log in with root and enter the new password after the reboot. Lab: Install New Kernel # Check the current version of the kernel using the uname or rpm command. Download a higher version from the Red Hat Customer Portal or rpmfind.net and install it. Reboot the system and ensure the new kernel is listed on the bootloader menu. 5.14.0-427.35.1.el9_4.x86_64 Lab: Download and Install a New Kernel # download the latest available kernel packages from the Red Hat Customer Portal install them using the dnf command. ensure that the existing kernel and its configuration remain intact. As an alternative (preferred) to downloading kernel packages individually and then installing them, you can follow the instructions provided in \u0026ldquo;Containers\u0026rdquo; chapter to register server1 with RHSM and run sudo dnf install kernel to install the latest kernel and all the dependencies collectively. Check the version of the running kernel: uname -r\nList the kernel packages currently installed: rpm -qa | grep kernel\nSign in to the Red Hat Customer Portaland click downloads.\nClick \u0026ldquo;Red Hat Enterprise Linux 8\u0026rdquo; under \u0026ldquo;By Category\u0026rdquo;:\nClick Packages and enter \u0026ldquo;kernel\u0026rdquo; in the Search bar to narrow the list of available packages:\nClick \u0026ldquo;Download Latest\u0026rdquo; against the packages kernel, kernel-core, kernel-headers, kernel-modules, kernel-tools, and kernel-tools-libs to download them.\nOnce downloaded, move the packages to the /tmp directory using the mv command.\nList the packages after moving them:\nInstall all the six packages at once using the dnf command: dnf install /tmp/kernel* -y\nConfirm the installation alongside the previous version: sudo dnf list installed kernel*\nThe /boot/grub2/grubenv/ file now has the directive \u0026ldquo;saved_entry\u0026rdquo; set to the new kernel, which implies that this new kernel will boot up on the next system restart: sudo cat /boot/grub2/grubenv\nReboot the system. You will see the new kernel entry in the GRUB2 boot list at the top. The system will autoboot this new default kernel.\nRun the uname command once the system has been booted up to confirm the loading of the new kernel: uname -r\nView the contents of the version and cmdline files under /proc to verify the active kernel: `cat /proc/version\nOr just dnf install kernel\n","externalUrl":null,"permalink":"/tech/linux/boot-process-grub2-and-kernel/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eLinux Kernel\n    \u003cdiv id=\"linux-kernel\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#linux-kernel\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003econtrols everything on the system.\n\u003cul\u003e\n\u003cli\u003ehardware\u003c/li\u003e\n\u003cli\u003eenforces security and access controls\u003c/li\u003e\n\u003cli\u003eruns, schedules, and manages processes and service daemons.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ecomprised of several modules.\u003c/li\u003e\n\u003cli\u003enew kernel must be installed or an existing kernel must be upgraded when the need arises from an application or functionality standpoint.\u003c/li\u003e\n\u003cli\u003ecore of the Linux system.\u003c/li\u003e\n\u003cli\u003emanages\u003c/li\u003e\n\u003cli\u003ehardware\u003c/li\u003e\n\u003cli\u003eenforces security\u003c/li\u003e\n\u003cli\u003eregulates access to the system\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ehandles\u003c/p\u003e","title":"Boot Process, Grub2, and Kernel","type":"tech"},{"content":"When I started studying for RHCE, the study guide had me manually set up virtual machines for the Ansible lab environment. I thought.. Why not start my automation journey right, and automate them using Vagrant.\nI use Libvirt to manage KVM/QEMU Virtual Machines and the Virt-Manager app to set them up. I figured I could use Vagrant to automatically build this lab from a file. And I got part of the way. I ended up with this Vagrant file:\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;almalinux/9\u0026#34; config.vm.provider :libvirt do |libvirt| libvirt.uri = \u0026#34;qemu:///system\u0026#34; libvirt.cpus = 2 libvirt.memory = 2048 end config.vm.define \u0026#34;control\u0026#34; do |control| control.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.124.200\u0026#34; control.vm.hostname = \u0026#34;control.example.com\u0026#34; end config.vm.define \u0026#34;ansible1\u0026#34; do |ansible1| ansible1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.124.201\u0026#34; ansible1.vm.hostname = \u0026#34;ansible1.example.com\u0026#34; end config.vm.define \u0026#34;ansible2\u0026#34; do |ansible2| ansible2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.124.202\u0026#34; ansible2.vm.hostname = \u0026#34;ansible2.example.com\u0026#34; end end I could run this Vagrant file and Build and destroy the lab in seconds. But there was a problem. The Libvirt plugin, or Vagrant itself, I\u0026rsquo;m not sure which, kept me from doing a couple important things.\nFirst, I could not specify the initial disk creation size. I could add additional disks of varying sizes but, if I wanted to change the size of the first disk, I would have to go back in after the fact and expand it manually\u0026hellip;\nSecond, the Libvirt plugin networking settings were a bit confusing. When you add the private network option as seen in the Vagrant file, it would add this as a secondary connection, and route everything through a different public connection.\nNow I couldn\u0026rsquo;t get the VMs to run using the public connection for whatever reason, and it seems the only workaround was to make DHCP reservations for the guests Mac addresses which gave me even more problems to solve. But I won\u0026rsquo;t go there..\nSo why not get my feet wet and learn how to deploy VMs with Ansible? This way, I would get the granularity and control that Ansible gives me, some extra practice with Ansible, and not having to use software that has just enough abstraction to get in the way.\nThe guide I followed to set this up can be found on Redhat\u0026rsquo;s blog here. And it was pretty easy to set up all things considered.\nI\u0026rsquo;ll rehash the steps here:\nDownload a cloud image Customize the image Install and start a VM Access the VM Creating the role # Move to roles directory cd roles\nInitialize the role ansible-galaxy role init kvm_provision\nSwitch into the role directory cd kvm_provision/\nRemove unused directories rm -r files handlers vars\nDefine variables # Add default variables to main.yml cd defaults/ \u0026amp;\u0026amp; vim main.yml\n--- # defaults file for kvm_provision base_image_name: AlmaLinux-9-GenericCloud-9.5-20241120.x86_64.qcow2 base_image_url: https://repo.almalinux.org/almalinux/9/cloud/x86_64/images/{{ base_image_name }} base_image_sha: abddf01589d46c841f718cec239392924a03b34c4fe84929af5d543c50e37e37 libvirt_pool_dir: \u0026#34;/var/lib/libvirt/images\u0026#34; vm_name: f34-dev vm_vcpus: 2 vm_ram_mb: 2048 vm_net: default vm_root_pass: test123 cleanup_tmp: no ssh_key: /root/.ssh/id_rsa.pub # Added option to configure ip address ip_addr: 192.168.124.250 gw_addr: 192.168.124.1 # Added option to configure disk size vm_disksize: 20 Defining a VM template # The community.libvirt.virt module is used to provision a KVM VM. This module uses a VM definition in XML format with libvirt syntax. You can dump a VM definition of a current VM and then convert it to a template from there. Or you can just use this:\ncd templates/ \u0026amp;\u0026amp; vim vm-template.xml.j2\n\u0026lt;domain type=\u0026#39;kvm\u0026#39;\u0026gt; \u0026lt;name\u0026gt;{{ vm_name }}\u0026lt;/name\u0026gt; \u0026lt;memory unit=\u0026#39;MiB\u0026#39;\u0026gt;{{ vm_ram_mb }}\u0026lt;/memory\u0026gt; \u0026lt;vcpu placement=\u0026#39;static\u0026#39;\u0026gt;{{ vm_vcpus }}\u0026lt;/vcpu\u0026gt; \u0026lt;os\u0026gt; \u0026lt;type arch=\u0026#39;x86_64\u0026#39; machine=\u0026#39;pc-q35-5.2\u0026#39;\u0026gt;hvm\u0026lt;/type\u0026gt; \u0026lt;boot dev=\u0026#39;hd\u0026#39;/\u0026gt; \u0026lt;/os\u0026gt; \u0026lt;cpu mode=\u0026#39;host-model\u0026#39; check=\u0026#39;none\u0026#39;/\u0026gt; \u0026lt;devices\u0026gt; \u0026lt;emulator\u0026gt;/usr/bin/qemu-system-x86_64\u0026lt;/emulator\u0026gt; \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;qcow2\u0026#39;/\u0026gt; \u0026lt;source file=\u0026#39;{{ libvirt_pool_dir }}/{{ vm_name }}.qcow2\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;vda\u0026#39; bus=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;address type=\u0026#39;pci\u0026#39; domain=\u0026#39;0x0000\u0026#39; bus=\u0026#39;0x05\u0026#39; slot=\u0026#39;0x00\u0026#39; function=\u0026#39;0x0\u0026#39;/\u0026gt; \u0026lt;!-- Added: Specify the disk size using a variable --\u0026gt; \u0026lt;size unit=\u0026#39;GiB\u0026#39;\u0026gt;{{ disk_size }}\u0026lt;/size\u0026gt; \u0026lt;/disk\u0026gt; \u0026lt;interface type=\u0026#39;network\u0026#39;\u0026gt; \u0026lt;source network=\u0026#39;{{ vm_net }}\u0026#39;/\u0026gt; \u0026lt;model type=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;address type=\u0026#39;pci\u0026#39; domain=\u0026#39;0x0000\u0026#39; bus=\u0026#39;0x01\u0026#39; slot=\u0026#39;0x00\u0026#39; function=\u0026#39;0x0\u0026#39;/\u0026gt; \u0026lt;/interface\u0026gt; \u0026lt;channel type=\u0026#39;unix\u0026#39;\u0026gt; \u0026lt;target type=\u0026#39;virtio\u0026#39; name=\u0026#39;org.qemu.guest_agent.0\u0026#39;/\u0026gt; \u0026lt;address type=\u0026#39;virtio-serial\u0026#39; controller=\u0026#39;0\u0026#39; bus=\u0026#39;0\u0026#39; port=\u0026#39;1\u0026#39;/\u0026gt; \u0026lt;/channel\u0026gt; \u0026lt;channel type=\u0026#39;spicevmc\u0026#39;\u0026gt; \u0026lt;target type=\u0026#39;virtio\u0026#39; name=\u0026#39;com.redhat.spice.0\u0026#39;/\u0026gt; \u0026lt;address type=\u0026#39;virtio-serial\u0026#39; controller=\u0026#39;0\u0026#39; bus=\u0026#39;0\u0026#39; port=\u0026#39;2\u0026#39;/\u0026gt; \u0026lt;/channel\u0026gt; \u0026lt;input type=\u0026#39;tablet\u0026#39; bus=\u0026#39;usb\u0026#39;\u0026gt; \u0026lt;address type=\u0026#39;usb\u0026#39; bus=\u0026#39;0\u0026#39; port=\u0026#39;1\u0026#39;/\u0026gt; \u0026lt;/input\u0026gt; \u0026lt;input type=\u0026#39;mouse\u0026#39; bus=\u0026#39;ps2\u0026#39;/\u0026gt; \u0026lt;input type=\u0026#39;keyboard\u0026#39; bus=\u0026#39;ps2\u0026#39;/\u0026gt; \u0026lt;graphics type=\u0026#39;spice\u0026#39; autoport=\u0026#39;yes\u0026#39;\u0026gt; \u0026lt;listen type=\u0026#39;address\u0026#39;/\u0026gt; \u0026lt;image compression=\u0026#39;off\u0026#39;/\u0026gt; \u0026lt;/graphics\u0026gt; \u0026lt;video\u0026gt; \u0026lt;model type=\u0026#39;qxl\u0026#39; ram=\u0026#39;65536\u0026#39; vram=\u0026#39;65536\u0026#39; vgamem=\u0026#39;16384\u0026#39; heads=\u0026#39;1\u0026#39; primary=\u0026#39;yes\u0026#39;/\u0026gt; \u0026lt;address type=\u0026#39;pci\u0026#39; domain=\u0026#39;0x0000\u0026#39; bus=\u0026#39;0x00\u0026#39; slot=\u0026#39;0x01\u0026#39; function=\u0026#39;0x0\u0026#39;/\u0026gt; \u0026lt;/video\u0026gt; \u0026lt;memballoon model=\u0026#39;virtio\u0026#39;\u0026gt; \u0026lt;address type=\u0026#39;pci\u0026#39; domain=\u0026#39;0x0000\u0026#39; bus=\u0026#39;0x06\u0026#39; slot=\u0026#39;0x00\u0026#39; function=\u0026#39;0x0\u0026#39;/\u0026gt; \u0026lt;/memballoon\u0026gt; \u0026lt;rng model=\u0026#39;virtio\u0026#39;\u0026gt; \u0026lt;backend model=\u0026#39;random\u0026#39;\u0026gt;/dev/urandom\u0026lt;/backend\u0026gt; \u0026lt;address type=\u0026#39;pci\u0026#39; domain=\u0026#39;0x0000\u0026#39; bus=\u0026#39;0x07\u0026#39; slot=\u0026#39;0x00\u0026#39; function=\u0026#39;0x0\u0026#39;/\u0026gt; \u0026lt;/rng\u0026gt; \u0026lt;/devices\u0026gt; \u0026lt;/domain\u0026gt; The template uses some of the variables from earlier. This allows flexibility to changes things by just changing the variables.\nDefine tasks for the role to perform # cd ../tasks/ \u0026amp;\u0026amp; vim main.yml\n--- # tasks file for kvm_provision # ensure the required package dependencies `guestfs-tools` and `python3-libvirt` are installed. This role requires these packages to connect to `libvirt` and to customize the virtual image in a later step. These package names work on Fedora Linux. If you\u0026#39;re using RHEL 8 or CentOS, use `libguestfs-tools` instead of `guestfs-tools`. For other distributions, adjust accordingly. - name: Ensure requirements in place package: name: - guestfs-tools - python3-libvirt state: present become: yes # obtain a list of existing VMs so that you don\u0026#39;t overwrite an existing VM on accident. uses the `virt` module from the collection `community.libvirt`, which interacts with a running instance of KVM with `libvirt`. It obtains the list of VMs by specifying the parameter `command: list_vms` and saves the results in a variable `existing_vms`. `changed_when: no` for this task to ensure that it\u0026#39;s not marked as changed in the playbook results. This task doesn\u0026#39;t make any change in the machine; it only checks the existing VMs. This is a good practice when developing Ansible automation to prevent false reports of changes. - name: Get VMs list community.libvirt.virt: command: list_vms register: existing_vms changed_when: no #execute only when the VM name the user provides doesn\u0026#39;t exist. And uses the module `get_url` to download the base cloud image into the `/tmp` directory - name: Create VM if not exists block: - name: Download base image get_url: url: \u0026#34;{{ base_image_url }}\u0026#34; dest: \u0026#34;/tmp/{{ base_image_name }}\u0026#34; checksum: \u0026#34;sha256:{{ base_image_sha }}\u0026#34; # copy the file to libvirt\u0026#39;s pool directory so we don\u0026#39;t edit the original, which can be used to provision other VMS later - name: Copy base image to libvirt directory copy: dest: \u0026#34;{{ libvirt_pool_dir }}/{{ vm_name }}.qcow2\u0026#34; src: \u0026#34;/tmp/{{ base_image_name }}\u0026#34; force: no remote_src: yes mode: 0660 register: copy_results - # Resize the VM disk - name: Resize VM disk command: qemu-img resize \u0026#34;{{ libvirt_pool_dir }}/{{ vm_name }}.qcow2\u0026#34; \u0026#34;{{ disk_size }}G\u0026#34; when: copy_results is changed # uses command module to run virt-customize to customize the image - name: Configure the image command: | virt-customize -a {{ libvirt_pool_dir }}/{{ vm_name }}.qcow2 \\ --hostname {{ vm_name }} \\ --root-password password:{{ vm_root_pass }} \\ --ssh-inject \u0026#39;root:file:{{ ssh_key }}\u0026#39; \\ --uninstall cloud-init --selinux-relabel # Added option to configure an IP address --firstboot-command \u0026#34;nmcli c m eth0 con-name eth0 ip4 {{ ip_addr }}/24 gw4 {{ gw_addr }} ipv4.method manual \u0026amp;\u0026amp; nmcli c d eth0 \u0026amp;\u0026amp; nmcli c u eth0\u0026#34; when: copy_results is changed - name: Define vm community.libvirt.virt: command: define xml: \u0026#34;{{ lookup(\u0026#39;template\u0026#39;, \u0026#39;vm-template.xml.j2\u0026#39;) }}\u0026#34; when: \u0026#34;vm_name not in existing_vms.list_vms\u0026#34; - name: Ensure VM is started community.libvirt.virt: name: \u0026#34;{{ vm_name }}\u0026#34; state: running register: vm_start_results until: \u0026#34;vm_start_results is success\u0026#34; retries: 15 delay: 2 - name: Ensure temporary file is deleted file: path: \u0026#34;/tmp/{{ base_image_name }}\u0026#34; state: absent when: cleanup_tmp | bool Changed my user to own the libvirt directory: chown -R david:david /var/lib/libvirt/images\nCreate playbook kvm_provision.yaml\n--- - name: Deploys VM based on cloud image hosts: localhost gather_facts: yes become: yes vars: pool_dir: \u0026#34;/var/lib/libvirt/images\u0026#34; vm: control vcpus: 2 ram_mb: 2048 cleanup: no net: default ssh_pub_key: \u0026#34;/home/davidt/.ssh/id_ed25519.pub\u0026#34; disksize: 20 tasks: - name: KVM Provision role include_role: name: kvm_provision vars: libvirt_pool_dir: \u0026#34;{{ pool_dir }}\u0026#34; vm_name: \u0026#34;{{ vm }}\u0026#34; vm_vcpus: \u0026#34;{{ vcpus }}\u0026#34; vm_ram_mb: \u0026#34;{{ ram_mb }}\u0026#34; vm_net: \u0026#34;{{ net }}\u0026#34; cleanup_tmp: \u0026#34;{{ cleanup }}\u0026#34; ssh_key: \u0026#34;{{ ssh_pub_key }}\u0026#34; Add the libvirt collection ansible-galaxy collection install community.libvirt\nCreate a VM with a new name ansible-playbook -K kvm_provision.yaml -e vm=ansible1\n\u0026ndash;run-command \u0026rsquo;nmcli c a type Ethernet ifname eth0 con-name eth0 ip4 192.168.124.200 gw4 192.168.124.1'\nparted /dev/vda resizepargit t 4 100% Warning: Partition /dev/vda4 is being used. Are you sure you want to continue? Yes/No? y Information: You may need to update /etc/fstab.\nlsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS vda 252:0 0 20G 0 disk ├─vda2 252:2 0 200M 0 part /boot/efi ├─vda3 252:3 0 1G 0 part /boot └─vda4 252:4 0 8.8G 0 part /\nvariables {{ ansible_user }} {{ ansible_password }} {{ gw_addr }} {{ ip_addr }}\n; useradd -m -p {{ ansible_user }} ; chage -d 0 {{ ansible_user }} ; cat {{ ansible_password }} \u0026gt; passwd {{ ansible_user }} \u0026ndash;stdin\u0026quot; \\\n- name: Configure the image command: | virt-customize -a {{ libvirt_pool_dir }}/{{ vm_name }}.qcow2 \\ --hostname {{ vm_name }} \\ --root-password password:{{ vm_root_pass }} \\ --uninstall cloud-init --selinux-relabel \\ --firstboot-command \u0026#34;nmcli c m eth0 con-name eth0 ip4 \\ {{ ip_addr }}/24 gw4 {{ gw_addr }} \\ ipv4.method manual \u0026amp;\u0026amp; nmcli c d eth0 \\ \u0026amp;\u0026amp; nmcli c u eth0 \u0026amp;\u0026amp; adduser \\ {{ ansible_user }} \u0026amp;\u0026amp; echo \\ \u0026#34;{{ ansible_password }}\u0026#34; | passwd \\ --stdin {{ ansible_user }}\u0026#34; when: copy_results is changed - name: Add ssh keys command: | virt-customize -a {{ libvirt_pool_dir }}/{{ vm_name }}.qcow2 \\ --ssh-inject \u0026#39;{{ ansible_user }}:file:{{ ssh_key }}\u0026#39; ","externalUrl":null,"permalink":"/tech/ansible/building-an-ansible-lab-with-ansible/","section":"Teches","summary":"\u003cp\u003eWhen I started studying for RHCE, the study guide had me manually set up virtual machines for the Ansible lab environment. I thought.. Why not start my automation journey right, and automate them using Vagrant.\u003c/p\u003e","title":"Building an Ansible lab with Ansible","type":"tech"},{"content":"I couldn’t find a guide on how to set up Calibre web step-by-step as a Docker container. Especially not one that used Nginx as a reverse proxy.\nThe good news is that it is really fast and simple. You’ll need a few tools to get this done:\nA server with a public IP address A DNS Provider (I use CloudFlare) Docker Nginx A Calibre Library Certbot Rsync First, sync your local Calibre library to a folder on your server:\nrsync -avuP your-library-dir root@example.org:/opt/calibre/ Install Docker # sudo apt update sudo apt install docker.io Create a Docker network\nsudo docker network create calibre_network Create a Docker volume to store Calibre Web data\nsudo docker volume create calibre_data Pull the Calibre Web Docker image\nsudo docker pull linuxserver/calibre-web Start the Calibre Web Docker container\nsudo docker run -d \\ --name=calibre-web \\ --restart=unless-stopped \\ -p 8083:8083 \\ -e PUID=$(id -u) \\ -e PGID=$(id -g) \\ -v calibre_data:/config \\ -v /opt/calibre/Calibre:/books \\ --network calibre_network \\ linuxserver/calibre-web Configure Nginx to act as a reverse proxy for Calibre Web # Create the site file\nsudo vim /etc/nginx/sites-available/calibre-web Add the following to the file\nserver { listen 80; server_name example.com; # Replace with your domain or server IP location / { proxy_pass http://localhost:8083; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Enable the site\nsudo ln -s /etc/nginx/sites-available/calibre-web /etc/nginx/sites-enabled/ Restart Nginx\nsudo service nginx restart DNS CNAME Record # Make sure to set up a cname record for your site with your DNS provider such as: calibre.example.com\nSSL Certificate # Install ssl cert using certbot\ncertbot --nginx Site Setup # Head to the site at https://calibre.example.com and log in with default credentials:\nusername: admin password: admin123\nSelect /books as the library directory. Go into admin settings and change your password.\nAdding new books # Whenever you add new books to your server via the rsync command from earlier, you will need to restart the Calibre Web Docker container. Then restart Nginx.\nsudo docker restart calibre-web systemctl restart nginx That’s all there is to it. Feel free to reach out if you have issues.\n","externalUrl":null,"permalink":"/tech/tools/calibre-web-with-docker-and-nginx/","section":"Teches","summary":"\u003cp\u003eI couldn’t find a guide on how to set up Calibre web step-by-step as a Docker container. Especially not one that used Nginx as a reverse proxy.\u003c/p\u003e\n\u003cp\u003eThe good news is that it is really fast and simple. You’ll need a few tools to get this done:\u003c/p\u003e","title":"Calibre Web with Docker and NGINX","type":"tech"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Can be used to match packets for applying Quality of Service (QoS) features.\nACL Location and Direction\ninbound to the router, before the router makes its forwarding (routing) decisionoutbound, after the router makes its forwarding decision and has determined the exit interface to use.enable an ACL on an interface that processes the packet, in the direction the packet flows through that interface.the router then processes every inbound or outbound IP packet using that ACL Taking Action When a Match Occurs\ndeny or permit\nTypes of IP ACLs Standard numbered ACLs (1Extended numbered ACLs (100–99) or (1300–199) or (2000-1999)-2699) Named ACLs\nEditing with sequence numbersconfiguration identifies the ACL either using a number or a name. ACLs will also be either standard or extended\nStandard Numbered IPv4 ACLs matches only the source IP address identify the ACL using numbers rather than names (numbered) Looks at IPv4 packets.\nList Logic with IP ACLs router takes the action listed in that line of the ACL and stops looking further in the ACLevery IP ACL has a deny all statement implied at the end of the ACL Matching Logic and Command Syntax\nACL is one or more accessany number from the ranges shown in the preceding line of syntax. -list commands with the same number, (One number is no better than the other.) IOS refers to each line in an ACL is an Access Control Entry (ACE engineers just call them ACL statements.each access-list command also lists the action (permit or deny), plus the matching logic. Matching the Exact IP Address\n2 Standard ACLs # Friday, September 17, 2021 12:28 PM\nMatching the Exact IP Address\npermit if source = 10.1.1.1 accessIf you use Host keyword IOS will remove the keyword in the config-list 1 permit 10.1.1.1 access-list 1 permit any Matching Any/All Addresses\nACL show commands list\ncounters for the number of packets matched by each command in the ACLno counter for that implicit denyany concept at the end of the ACL. , but there is Configure deny any command to see deny counts\nImplementing Standard IP ACLs access-list access-list-number {deny | permit} source [source-wildcard]\nPlan the location (router and interface) and direction (in or out) on that interface:\nplaced near to the destination of the packetsdiscard packets that should not be discarded.so that they do not unintentionally identify the source IP addresses of packets as they go in the direction that the ACL is examining. access-list access-list-number {deny | permit} source [source-wildcard] # Configure one or more access-list\nEnable the ACL\n(config-if)# ip access-group number {in | out}\nStandard Numbered ACL Example 1\nR2(config)# accessR2(config)# access\u0026ndash;list 1 permit 10.1.1.1list 1 deny 10.1.1.0 0.0.0.255\nR2(config)# access-list 1 permit 10.0.0.0 0.255.255.255\nR2(config-if)# ip access-group 1 in show ip access-lists\ndetails about IPv4 ACLs only show access-lists\nlists details about any configure ACL, not just IPv4 lists the number or name of any IP ACL enabled on the interface show ip interface s0/0/1\nStandard Numbered ACL Example 2\nstandard ACLs cannot check the destination IP address.\nstandard ACLs cannot check the destination IP address.extended ACL lets you check both the source and destination IP address. accessrouter checks packets that it routes against the ACL for outbound ACLs- to leave text documentation that stays with the ACL.-list remark parameter a router does not filter packets that the router itself creates with an outbound ACL\nTroubleshooting and Verification Tips\nIOS keeps statistics about the packets matched by each line of an ACL\nlogkeyword\n▪ add to end of accessIOS then issues log messages with occasional statistics about matches of that -list command\n▪ ACL line\nDouble check the ACL is enabled on the right interface, or for the right direction\nPractice Building access-list Commands\nTips to consider when choosing matching parameters to any access-list command:\nTo match a specific address, just list the address.To match any and all addresses, use the any keyword. several practice problems (wildcard)\nPackets from 172.16.5.4- 0.0.0.0 Packets from hosts with 192.168.6- 0.0.0.255 Packets from hosts with 192.168- 0.0.255.255 Packets from any hosts- 255.255.255.255 Packets from subnet 10.1.200.0/21- 0.0.7.255 Packets from subnet 172.20.112.0/23- 0.0.1.255 Packets from subnet 172.20.112.0/26- 0.0.0.63 Packets from subnet 192.168.9.64/28- 0.0.0.15 Packets from subnet 192.168.9.64/30- 0.0.0.3 Reverse Engineering from ACL to Address Range (practice problems)\n1.2. one address192.168.4.0 -192.168.4.127\n3.4. 192.168.6.0 172.30.96.0 \u0026ndash;192.168.6. 31172.30.96.255\n5.6. 172.30.96.0 10.1.192.0 \u0026ndash;10.1.192..3172.30.96. 63\n7.8. 10.1.192.0 10.1.192.0 \u0026ndash;10.1.193.25510.1.255.255\n128 64 32 16 8 4 2 1 128 192 224 240 248 252 254 255 This chapter covers the following exam topics:\n5.0 Security Fundamentals\n5.6 Configure and verify access control lists\nall the parameters must be matched correctly to match that one ACE..\nMatching the Protocol, Source IP, and Destination IP 9Extended) Uses the access-list global command. The syntax is identical up until permit or deny keywordRequires three matching parameters:\n○ IP protocol type\n○ source IP address\n○ destination IP address. identifies the header that follows the IP header (layer 4)TCP, UDP, EIGRP, IGMP, etc Use protocol as keywordKeyword IP means all IPv4 packets IP header’s Protocol Type field\nSyntax\nAccess(Destination-list 101 (list #) permit/ Deny tcp (protocol) 10.0.0.1 0.0.0.0 (Source) 10.1.0.1 0.0.0.255\nRequires the use of the host keyword for specific address Examples ▪ Any IP packet that has a TCP header access-list 101 deny tcp any any access▪ Any IP packet that that has a UDP header-list 101 deny udp any any access▪ Any IP packet that has a ICMP header-list 101 deny icmp any any ▪ All IP packets from host 1.1.1.1 going to host 2.2.2.2 access-list 101 deny ip host 1.1.1.1 host 2.2.2.2 access▪ All IP packets that have a UDP header following the IP header, from subnet 1.1.1.0/24 going to any destination-list 101 deny udp 1.1.1.0 0.0.0.255 any - # IP and TCP Header\n","externalUrl":null,"permalink":"/tech/networking/ciscoacls/","section":"Teches","summary":"\u003cp\u003eCan be used to match packets for applying Quality of Service (QoS) features.\u003cbr\u003e\nACL Location and Direction\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einbound to the router, before the router makes its forwarding (routing) decisionoutbound, after the router makes its forwarding decision and has determined the exit\u003c/li\u003e\n\u003cli\u003einterface to use.enable an ACL on an interface that processes the packet, in the direction the packet flows\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003ethrough that interface.the router then processes every inbound or outbound IP packet using that ACL\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTaking Action When a Match Occurs\u003c/p\u003e","title":"Cisco ACLs","type":"tech"},{"content":"Reload\nTells the system to reboot IOS\nKeyboard Shortcuts Recently used (Previous)\nup arrow or Ctrl+P Go back up from the above command (next) Down arrow or Ctrl+N Left arrow or - Move cursor (back) Ctrl+B Right Arrow or Ctrl+F- Move cursor (forward) Delete Back Space Debug\nTells user details about the operation of the switch\nNavigation Global Config \u0026gt; Enable Mode\nEnd or Ctrl+Z Line or VLAN modes \u0026gt; Global config Exit Memory\nRAM- Stores Running config\nChip or removable memory Stores Cisco IOS imagesDefault IOS location for booting Store backup config files Flash Memory Stores Bootstrap ROM NVRAM- stores initial or startup config 4. CLI # ","externalUrl":null,"permalink":"/tech/networking/ciscocli/","section":"Teches","summary":"\u003cp\u003eReload\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTells the system to reboot IOS\u003cbr\u003e\nKeyboard Shortcuts\u003c/li\u003e\n\u003cli\u003eRecently used (Previous)\u003cbr\u003e\nup arrow or \u003cstrong\u003eCtrl+P\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eGo back up from the above command (next)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eDown arrow or Ctrl+N\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eLeft arrow or - Move cursor (back) Ctrl+B\nRight Arrow or Ctrl+F- Move cursor (forward)\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eDelete\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eBack Space\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eDebug\u003c/p\u003e","title":"Cisco CLI","type":"tech"},{"content":"IPv4 Address Classes Based on First Octet Values\nClass First Octet Values Purpose\nA 1–126 Unicast (large networks)\nB 128–191 Unicast (medium-sized networks)\nC 192–223 Unicast (small networks)\nD 224–239 Multicast\nE 240–255 Reserved (formerly experimental)\nAddress ranges of all addresses that begin with 0 and all addresses that begin with 127 are reserved\nAn IPv4 address is comprised of four period-separated octets (4 x 8 = 32-bit address) Divided into a network portion (or network ID/bits) comprising of the Most Significant Bits (MSBs) and a node portion (or node/host ID/bits) containing the Least Significant Bits (LSBs). 0 and 255 are network and broadcast addresses, and they are always reserved.\nClass A # Up to 16,777,214 million nodes. Up to 126 networks. first octet as the network portion rest of the octets as the node portion. respectively.\nClass B # Up to 16,384 networks Up to 65,534 thousand nodes. first two octets as the network portion other two as the node portion. address range for class B networks is between 128 and 191. 0 and 255 are network and broadcast addresses, and they are always reserved.\nClass C # Up to 2,097,152 networks Up to 254 nodes. first three octets as the network portion last octet as the node portion.\nrange for class C networks is between 192 and 223.\nClass D # Range from 224 to 239.\nClass E # Range from 240 to 255.\n","externalUrl":null,"permalink":"/tech/networking/classfulipv4/","section":"Teches","summary":"\u003cp\u003eIPv4 Address Classes Based on First Octet Values\u003c/p\u003e\n\u003cp\u003eClass      First Octet Values      Purpose\u003c/p\u003e\n\u003cp\u003eA            1–126                         Unicast (large networks)\u003c/p\u003e\n\u003cp\u003eB            128–191                     Unicast (medium-sized networks)\u003c/p\u003e\n\u003cp\u003eC            192–223                     Unicast (small networks)\u003c/p\u003e\n\u003cp\u003eD            224–239                     Multicast\u003c/p\u003e","title":"Classful IPv4","type":"tech"},{"content":"1.0 Network Fundamentals 1.1 Explain the role and function of network components 1.1.g Servers 1.2 Describe the characteristics of network topology architectures 1.2.f On-premises and cloud 1.12 Explain virtualization fundamentals (virtual machines)\nIt it has a variety of network access options; and it can be measured and billed back to the user based on can be requested on-demand; it can dynamically scale (that is, it is elastic); it uses a pool of resources; the amount used. Cisco Server Hardware Cisco expanded its product line into the server market, with the Cisco Unified Computing System (UCS) product line Server Virtualization Basics\nThe the settings for the VMhypervisor manages and allocates the host hardware (CPU, RAM, etc.) to each VM based on few of the vendors and product family names associated with virtualized data centers: ○ VMware vCenter ○ Microsoft HyperV\n### 15 Cloud Architecture Thursday, October 28, 2021 6:57 AM ○ Citrix XenServer ○ Red Hat KVM Networking with Virtual Switches on a Virtualized Host in VMware’s virtualization systems, the VM’s virtual NIC goes by the name vNIC.) virtual switch, or vSwitch Ports connected to VMs or share the same VLAN with other VMs, or even use VLAN trunking to the VM itself.: The vSwitch can configure a port so that the VM will be in its own VLAN, Ports connected to physical NICs: that the switch is adjacent to the external physical LAN switch. The vSwitch can (and likely does) The vSwitch uses the physical NICs in the server hardware so use VLAN trunking. Automated configuration: virtualization software that controls the VMs. That programmability allows the virtualization The configuration can be easily done from within the same software to move VMs between hosts (servers) and reprogram the vSwitches so that the VM has the same networking capabilities no matter where the VM is running.\nThe Physical Data Center Network each host is cabled to two different switches in the top of the rackswitches—to provide redundant paths into the LAN. Each ToR switch acts as an access layer —called Top of Rack (ToR) switch from a design perspective. Each ToR switch is then cabled to an End of Row (EoR) switch, which acts as a distribution switch and also connects to the rest of the network.\nCisco Application Centric Infrastructure (ACI). ACI places the server and switch hardware into racks, but cables the switches with a different topology—a topology required for proper operation of the ACI fabric Workflow with a Virtualized Data Center Cloud Computing Services five criteria for a cloud computing service **On** without any direct interaction with the provider of the service. **- demand self-service:** The IT consumer chooses when to start and stop using the service, **Broad network access** many types of networks (including the Internet).: The service must be available from many types of devices and over **Resource pooling:** servers for use only by certain consumers) and dynamically allocates resources from that The provider creates a pool of resources (rather than dedicating specific pool for each new request from a consumer. **Rapid elasticity:** expands quickly, so it is called elastic), and the requests for new service are filled quickly.To the consumer, the resource pool appears to be unlimited (that is, it **Measured service:** consumer, both for transparency and for billing.The provider can measure the usage and report that usage to the Private Cloud (On-Premise) cloud services catalog. That catalog exists for the user as a web application that lists anything that can be requested via the company’s cloud infrastructure. Public Cloud\nCloud and the “As a Service” Model Infrastructure as a Service Software as a Service\n(Development) Platform as a Service like IaaS in some ways. Both supply the consumer with one or more VMs, with a configurable amount of CPU, RAM, and other resources. includes many more software tools beyond the basic OS.\noften include an integrated development environment (IDE) include continuous integration tools that allow the developer to update code and have that code automatically tested and integrated into a larger software project. Examples include Google’s App Engine PaaS offering (integrated development environment (see https://cloud.google.com/appenginewww.eclipse.org), and the Jenkins continuous ), the Eclipse integration and automation tool (see https://jenkins.io).\nWAN Traffic Paths to Reach Cloud Services Accessing Public Cloud Services Using the Internet Pros and Cons with Connecting to Public Cloud with Internet good reasons to use the Internet as the WAN connection to a public cloud service: Agility to order a private WAN connection to the cloud provider because cloud : An enterprise can get started using public cloud without having to wait providers support Internet connectivity. Migration another more easily because cloud providers all connect to the Internet.: An enterprise can switch its workload from one cloud provider to Distributed user Internet with their devices (as in the sales SaaS app example).s: The enterprise’s users are distributed and connect to the negatives for using the Internet for public cloud access Security “man in the middle” can attempt to read the contents of data that passes : The Internet is less secure than private WAN connections in that a to/from the public cloud. Capacity traffic, so the question of whether the enterprise’s Internet links can handle the : Moving an internal application to the public cloud increases network additional load needs to be considered. Quality of Service (QoS): WANs can. Using the Internet may result in a worse user experience than The Internet does not provide QoS, whereas private desired because of higher delay (latency), jitter, and packet loss.\n**No WAN SLA:** WAN performance and availability to all destinations of a network. WAN service ISPs typically will not provide a service-level agreement (SLA) for providers are much more likely to offer performance and availability SLAs Private WAN and Internet VPN Access to Public Cloud Cisco makes the that runs as a VM in a cloud service, controlled by the cloud consumer, to do various Cloud Services Router (CSR )to do exactly that: to be a router, but a router functions that routers do, including terminating VPNs. Pros and Cons of Connecting to Cloud with Private WANs\nmore secure\nPros\nCons Installing the new private WAN connections takes time, delaying when a company gets started in cloud computing cost more than using the Internet migrating to a new cloud provider can require another round of private WAN installation, again delaying work projects\nIntercloud Exchanges a cloud consumer can move his workload from one cloud provider to another using a private WAN for the cloud is that it adds another barrier to migrating to a new public cloud provider. One solution adds easier migration to the use of a private WAN through a cloud service called an intercloud exchange (or simply an intercloud) a company that creates a private network as a service you get the same benefits as when connecting with a private WAN connection to a public cloud, but with the additional pro of easier migration to a new cloud provider. The main con is that using an intercloud exchange introduces another company into the mix.\nMigrating Traffic Flows When Migrating to Email SaaS\nBranch Offices with Internet and Private WAN\n16 Controller-Based Networking # 1.0 Network Fundamentals\n1.1 Explain the role and function of network components\n1.1.f Endpoints\n1.1.g Servers\n1.2 Describe characteristics of network topology architectures\n1.2.c Spine-leaf\n6.0 Automation and Programmability\n6.1 Explain how automation impacts network management\n6.2 Compare traditional networks with controller-based networking\n6.3 Describe controller-based and software defined architectures (overlay, underlay, and fabric)\n6.3.a Separation of control plane and data plane\n6.3.b Northbound and southbound APIs\nSDN makes use of a controller that centralizes some network functions.\ncontrollers enable programs to automatically configure and operate networks through power application programming interfaces (APIs).\nThe Data Plane\nthe tasks that a networking device does to forward a message.\nanything to do with receiving data, processing it, and forwarding that same data\nframe, a packet, or, more generically, a message\noften called the forwarding plane.\nactions that a networking device does that fit into the data plane:\nDe-encapsulating and re-encapsulating a packet in a data-link frame (routers, Layer 3\nswitches)\n○ # ○ Adding or removing an 802.1Q trunking header (routers and switches) Matching an Ethernet frame’s destination Media Access Control (MAC) address to the MAC ","externalUrl":null,"permalink":"/tech/networking/cloudarchitecture/","section":"Teches","summary":"\u003cp\u003e1.0 Network Fundamentals\n1.1 Explain the role and function of network components\n1.1.g Servers\n1.2 Describe the characteristics of network topology architectures\n1.2.f On-premises and cloud\n1.12 Explain virtualization fundamentals (virtual machines)\u003c/p\u003e","title":"Cloud Architecture","type":"tech"},{"content":"sudo dnf -y install vim ### Make vim default sudoer editor echo \u0026#34;Defaults editor=/usr/bin/vim\u0026#34; | sudo tee /etc/sudoers.d/99_custom_editor ### remove password prompts when using sudo sudo sed -i \u0026#39;s/^#\\s*%wheel\\s\\+ALL=(ALL)\\s\\+NOPASSWD: ALL/%wheel ALL=(ALL) NOPASSWD: ALL/\u0026#39; /etc/sudoers sudo sed -i \u0026#39;s/^%wheel\\s\\+ALL=(ALL)\\s\\+ALL/# %wheel ALL=(ALL) ALL/\u0026#39; /etc/sudoers sudo dnf -y install https://mirrors.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm \\ https://mirrors.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm sudo dnf5 install \u0026#39;dnf5-command(groupinstall)\u0026#39; sudo dnf -y groupinstall \\ \u0026#34;Development Tools\u0026#34; /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; echo \u0026#39;eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34; sudo dnf -y install ansible ansible setup vim setup.yml\n--- - name: Setup Development Environment hosts: localhost become: yes tasks: # Install Flatpak applications - name: Install Flatpak applications flatpak: name: \u0026#34;{{ item }}\u0026#34; state: present loop: - com.bitwarden.desktop - com.brave.Browser - org.gimp.GIMP - org.gnome.Snapshot - org.libreoffice.LibreOffice - org.remmina.Remmina - com.termius.Termius - com.slack.Slack - org.keepassxc.KeePassXC - md.obsidian.Obsidian - com.calibre_ebook.calibre - org.mozilla.Thunderbird - us.zoom.Zoom - org.wireshark.Wireshark - com.google.Chrome - io.github.shiftey.Desktop - io.github.dvlv.boxbuddyrs - com.github.tchx84.Flatseal - io.github.flattool.Warehouse - io.missioncenter.MissionCenter - com.github.rafostar.Clapper - com.mattjakeman.ExtensionManager - com.jgraph.drawio.desktop - org.adishatz.Screenshot - com.github.finefindus.eyedropper - com.github.johnfactotum.Foliate - com.obsproject.Studio - com.vivaldi.Vivaldi - com.vscodium.codium - io.podman_desktop.PodmanDesktop - org.kde.kdenlive - org.virt_manager.virt-manager - io.github.input_leap.input-leap - com.nextcloud.desktopclient.nextcloud # Install Development Tools group using dnf - name: Install Development Tools group dnf: name: \u0026#34;@Development Tools\u0026#34; state: present - name: Install @virtualization group package dnf: name: \u0026#39;@virtualization\u0026#39; state: present # Update dnf configuration - name: Update dnf configuration for fastestmirror and parallel downloads block: - lineinfile: path: /etc/dnf/dnf.conf line: \u0026#34;fastestmirror=True\u0026#34; - lineinfile: path: /etc/dnf/dnf.conf line: \u0026#34;max_parallel_downloads=10\u0026#34; - lineinfile: path: /etc/dnf/dnf.conf line: \u0026#34;defaultyes=True\u0026#34; - lineinfile: path: /etc/dnf/dnf.conf line: \u0026#34;keepcache=True\u0026#34; # Perform DNF update and install required packages - name: Update DNF and install required packages dnf: name: - gnome-screenshot - wireguard-tools - gnome-tweaks - gnome-themes-extra - telnet - nmap state: present # Set GNOME theme (using gsettings directly) - name: Set GNOME theme to Adwaita-dark shell: gsettings set org.gnome.desktop.interface gtk-theme \u0026#34;Adwaita-dark\u0026#34; become_user: \u0026#34;davidt\u0026#34; - name: Enable experimental Mutter features shell: gsettings set org.gnome.mutter experimental-features \u0026#34;[\u0026#39;scale-monitor-framebuffer\u0026#39;]\u0026#34; become_user: \u0026#34;davidt\u0026#34; # Install Go programming language - name: Install Go dnf: name: go state: present - name: Add Go to the PATH in .bashrc lineinfile: path: \u0026#34;/home/davidt/.bashrc\u0026#34; line: \u0026#39;export PATH=$PATH:/usr/local/go/bin\u0026#39; state: present become_user: \u0026#34;davidt\u0026#34; - name: Source .bashrc shell: source /home/davidt/.bashrc become_user: \u0026#34;davidt\u0026#34; - name: Install pip using yum yum: name: python-pip state: present run the playbook: ansible-playbook setup.yml\nThen reboot\u0026hellip;\nThen sign into nextcloud and begin sync.\nInstall Homebrew packages: # brew install hugo\nInstall gnome extentions:\npip install --user gnome-extensions-cli gext install \u0026#34;appindicatorsupport@rgcjonas.gmail.com\u0026#34; gext enable \u0026#34;appindicatorsupport@rgcjonas.gmail.com\u0026#34; gext install \u0026#34;legacyschemeautoswitcher@joshimukul29.gmail.com\u0026#34; gext install \u0026#34;blur-my-shell@aunetx\u0026#34; gext install \u0026#34;dash-to-dock@micxgx.gmail.com\u0026#34; gext install \u0026#34;gsconnect@andyholmes.github.io\u0026#34; gext install \u0026#34;logomenu@aryan_k\u0026#34; gext install \u0026#34;search-light@icedman.github.com\u0026#34; Restore remmina connections cp ~/Nextcloud/remmina/* ~/.var/app/org.remmina.Remmina/data/remmina/\nRestore vimrc cat ~/Nextcloud/Documents/dotfiles/vimrc.bak \u0026gt; ~/.vimrc\nRestore ~/.bashrc: (if username is the same) cat ~/Nextcloud/Documents/dotfiles/bashrc.bak \u0026gt; ~/.bashrc\nGit config\ngit config --global user.email \u0026#34;tdavetech@gmail.com\u0026#34; git config --global user.name \u0026#34;linuxreader\u0026#34; # Store git credentials (from inside a git directory): git config credential.helper store OneDrive # Install: sudo dnf -y install onedrive\nStart: onedrive\nDisplay config:\nonedrive --display-config Sync and prefer local copy: onedrive --sync --local-first\nEnable the user level service: `onedrive \u0026ndash;user enable \u0026ndash;now onedrive\nForce local to the cloud onedrive \u0026ndash;synchronize \u0026ndash;force\nRestore files from cloud onedrive \u0026ndash;synchronize \u0026ndash;resync\nAdd foce option to top of the user service file to ignore big delete flag. systemctl --user edit onedrive\n[Service] ExecStart= ExecStart=/usr/bin/onedrive --monitor --verbose --force ","externalUrl":null,"permalink":"/tech/tools/new-setup/","section":"Teches","summary":"\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo dnf -y install vim\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e### Make vim default sudoer editor\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Defaults editor=/usr/bin/vim\u0026#34;\u003c/span\u003e \u003cspan class=\"p\"\u003e|\u003c/span\u003e sudo tee /etc/sudoers.d/99_custom_editor\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e### remove password prompts when using sudo\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo sed -i \u003cspan class=\"s1\"\u003e\u0026#39;s/^#\\s*%wheel\\s\\+ALL=(ALL)\\s\\+NOPASSWD: ALL/%wheel  ALL=(ALL) NOPASSWD: ALL/\u0026#39;\u003c/span\u003e /etc/sudoers\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo sed -i \u003cspan class=\"s1\"\u003e\u0026#39;s/^%wheel\\s\\+ALL=(ALL)\\s\\+ALL/# %wheel  ALL=(ALL) ALL/\u0026#39;\u003c/span\u003e /etc/sudoers\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo dnf -y install https://mirrors.rpmfusion.org/free/fedora/rpmfusion-free-release-\u003cspan class=\"k\"\u003e$(\u003c/span\u003erpm -E %fedora\u003cspan class=\"k\"\u003e)\u003c/span\u003e.noarch.rpm \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e    https://mirrors.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-\u003cspan class=\"k\"\u003e$(\u003c/span\u003erpm -E %fedora\u003cspan class=\"k\"\u003e)\u003c/span\u003e.noarch.rpm\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo dnf5 install \u003cspan class=\"s1\"\u003e\u0026#39;dnf5-command(groupinstall)\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo dnf -y groupinstall \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e      \u003cspan class=\"s2\"\u003e\u0026#34;Development Tools\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e/bin/bash -c \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"k\"\u003e$(\u003c/span\u003ecurl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh\u003cspan class=\"k\"\u003e)\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34;\u0026#39;\u003c/span\u003e \u0026gt;\u0026gt; ~/.bash_profile\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eeval\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"k\"\u003e$(\u003c/span\u003e/home/linuxbrew/.linuxbrew/bin/brew shellenv\u003cspan class=\"k\"\u003e)\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo dnf -y install ansible\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eansible setup\n\u003ccode\u003evim setup.yml\u003c/code\u003e\u003c/p\u003e","title":"Configure Fedora Desktop using Ansible","type":" "},{"content":"Plug console cable in\nfind out what your serial line name is:\n$ dmesg | grep -i FTDI Open putty \u0026gt; change to serial \u0026gt; change the tty line name\nMake sure your serial settings are correct\nhttps://www.juniper.net/documentation/us/en/hardware/mx5-mx10-mx40-mx80/topics/task/management-devices-mx5-mx10-mx40-mx80-connecting.html\nPress open \u0026gt; when terminal appears press enter\nJuniper Password recovery\nttps://www.juniper.net/documentation/en_US/junos/topics/task/configuration/authentication-root-password-recovering-mx80.html\nhttps://www.juniper.net/documentation/us/en/software/junos/junos-install-upgrade/topics/topic-map/rescue-and-recovery-config-file.html#load-commit-configuration\naccidentally deleted the wrong line in Juniper.conf file ? failing over to juniper.conf\nhttps://www.juniper.net/documentation/en_US/junos/topics/concept/junos-configuration-files.html\n","externalUrl":null,"permalink":"/tech/tools/consoling_in_to_mx80_from_linux/","section":"Teches","summary":"\u003cp\u003ePlug console cable in\u003c/p\u003e\n\u003cp\u003efind out what your serial line name is:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ dmesg \u003cspan class=\"p\"\u003e|\u003c/span\u003e grep -i FTDI\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOpen putty \u0026gt; change to serial \u0026gt; change the tty line name\u003c/p\u003e\n\u003cp\u003eMake sure your serial settings are correct\u003c/p\u003e","title":"Consoling in to MX80 from linux","type":"tech"},{"content":" Introduction to Containers # Take advantage of the native virtualization features available in the Linux kernel. Each container typically encapsulates one self-contained application that includes all dependencies such as library files, configuration files, software binaries, and services. Traditional server/ application deployment:\nApplications may have conflicting requirements in terms of shared library files, package dependencies, and software versioning. Patching or updating the operating system may result in breaking an application functionality. Developers perform an analysis on their current deployments before they decide whether to collocate a new application with an existing one or to go with a new server without taking the risk of breaking the current operation. Container Model:\nDevelopers can now package their application alongside dependencies, shared library files, environment variables, and other specifics in a single image file and use that file to run the application in a unique, isolated \u0026ldquo;environment\u0026rdquo; called container.\nA container is essentially a set of processes that runs in complete seclusion on a Linux system.\nA single Linux system running on bare metal hardware or in a virtual machine may have tens or hundreds of containers running at a time.\nThe underlying hardware may be located either on the ground or in the cloud.\nEach container is treated as a complete whole, which can be tagged, started, stopped, restarted, or even transported to another server without impacting other running containers.\nAny conflicts that may exist among applications, within application components, or with the operating system can be evaded.\nApplications encapsulated to run inside containers are called containerized applications.\nContainerization is a growing trend for architecting and deploying applications, application components, and databases in real world environments.\nContainers and the Linux Features # Container technology employs some of the core features available in the Linux kernel. These features include: control groups namespaces seccomp (secure computing mode) SELinux Control Groups (cgroups)\nSplit processes into groups to set limits on their consumption of compute resources\u0026mdash;CPU, memory, disk, and network I/O. These restrictions result in controlling individual processes from over utilizing available resources. Namespaces\nRestrict the ability of process groups from seeing or accessing system resources\u0026mdash;PIDs, network interfaces, mount points, hostname, etc. Creates a layer of isolation between process groups and the rest of the system. Guarantees a secure, performant, and stable environment for containerized applications as well as the host operating system. Secure Computing Mode (seccomp) and SELinux\nImpose security constraints thereby protecting processes from one another and the host operating system from running processes. Container technology employs these characteristics to run processes isolated in a highly secure environment with full control over what they can or cannot do. Benefits of Using Containers # Isolation # Containers are not affected due to changes in the host operating system or in other hosted or containerized applications, as they run fully isolated from the rest of the environment. Loose Coupling # Containerized applications are loosely coupled with the underlying operating system due to their self-containment and minimal level of dependency. Maintenance Independence # Maintenance is performed independently on individual containers. Less Overhead # Containers require fewer system resources than do bare metal and virtual servers. Transition Time # Containers require a few seconds to start and stop. Transition Independence # Transitioning from one state to another (start or stop) is independent of other containers, and it does not affect or require a restart of any underlying host operating system service. Portability # Containers can be migrated to other servers without modifications to the contained applications. Target servers may be bare metal or virtual and located on-premises or in the cloud. Reusability # The same container image can be used to run identical containers in development, test, preproduction, and production environments. There is no need to rebuild the image. Rapidity # The container technology allows for accelerated application development, testing, deployment, patching, and scaling. There is no need for an exhaustive testing. Version Control # Container images can be version-controlled, which gives users the flexibility in choosing the right version to run a container. Container Home: Bare Metal or Virtual Machine # Containers\nrun directly on the underlying operating system whether it be running on a bare metal server or in a virtual machine. Share hardware and operating system resources securely among themselves. Containerized applications stay lightweight and isolated, and run in parallel. Share the same Linux kernel and require far fewer hardware resources than do virtual machines, which contributes to their speedy start and stop. Given the presence of an extra layer of hypervisor services, it may be more beneficial and economical to run containers directly on non-virtualized physical servers. Container Images and Container Registries # Launching a container requires a pre-packaged image to be available. container image\nEssentially a static file that is built with all necessary components (application binaries, library files, configuration settings, environment variables, static data files, etc.)\nRequired by an application to run smoothly, securely, and independently.\nRHEL follows the open container initiative (OCI) to allow users to build images based on industry standard specifications that define the image format, host operating system metadata, and supported hardware architectures.\nAn OCI-compliant image can be executed and managed with OCI-compliant tools such as podman (pod manager) and Docker.\nImages can be version-controlled giving users the suppleness to use the latest or any of the previous versions to launch their containers.\nA single image can be used to run several containers at once.\nContainer images adhere to a standard naming convention for identification.\nThis is referred to as fully qualified image name (FQIN).\nComprised of four components: (1) the storage location (registry_name) (2) the owner or organization name (user_name) (3) a unique repository name (repo_name) (4) an optional version (tag). The syntax of an FQIN is: registry_hostname/user_name/repo_name:tag. Images are stored and maintained in public or private registries;\nThey need to be downloaded and made locally available for consumption.\nThere are several registries available on the Internet.\nregistry.redhat.io (/images/images based on official Red Hat products; requires authentication), registry.access.redhat.com (requires no authentication) registry.connect.redhat.com (/images/images based on third-party products) hub.docker.com (Docker Hub). The three Red Hat registries may be searched using the Red Hat Container Catalog at catalog.redhat.com/software/containers/search.\nAdditional registries may be added as required.\nPrivate registries may also require authentication for access.\nRootful vs. Rootless Containers # Containers can be launched with the root user privileges (sudo or directly as the root user).\nThis gives containers full access to perform administrative functions including the ability to map privileged network ports (1024 and below).\nLaunching containers with superuser rights opens a gate to potential unauthorized access to the container host if a container is compromised due to a vulnerability or misconfiguration.\nTo secure containers and the underlying operating system, containers should be launched and interacted with as normal Linux users.\nSuch containers are referred to as rootless containers.\nRootless containers allow regular, unprivileged users to run containers without the ability to perform tasks that require privileged access.\nWorking with Images and Containers # Lab: Install Necessary Container Support # Install the necessary software to set the foundation for completing the exercises in the remainder of the chapter. The standard RHEL 9.1 image includes a package called container-tools that consists of all the required components and commands. Use the standard dnf command to install the package. 1. Install the container-tools package:\nroot@server10 ~]# dnf install -y container-tools Upgraded: aardvark-dns-2:1.10.0-3.el9_4.x86_64 buildah-2:1.33.7-3.el9_4.x86_64 netavark-2:1.10.3-1.el9.x86_64 podman-4:4.9.4-6.el9_4.x86_64 Installed: container-tools-1-14.el9.noarch podman-docker-4:4.9.4-6.el9_4.noarch podman-remote-4:4.9.4-6.el9_4.x86_64 python3-podman-3:4.9.0-1.el9.noarch python3-pyxdg-0.27-3.el9.noarch python3-tomli-2.0.1-5.el9.noarch skopeo-2:1.14.3-3.el9_4.x86_64 toolbox-0.0.99.5-2.el9.x86_64 udica-0.2.8-1.el9.noarch 2. Verify the package installation:\n[root@server10 ~]# dnf list container-tools Updating Subscription Management repositories. Last metadata expiration check: 14:53:32 ago on Wed 31 Jul 2024 05:45:56 PM MST. Installed Packages container-tools.noarch 1-14.el9 @rhel-9-for-x86_64-appstream-rpms podman Command # Finding, inspect, retrieve, and delete images Run, stop, list, and delete containers. Used for most of these operations. Subcommands # Image Management # build\nBuilds an image using instructions delineated in a Container file images\nLists downloaded images from local storage inspect\nExamines an image and displays its details login/logout\nLogs in/out to/from a container registry. A login may be required to access private and protected registries. pull\nDownloads an image to local storage from a registry rmi\nRemoves an image from local storage search\nSearches for an image. The following options can be included with this subcommand: A partial image name in the search will produce a list of all images containing the partial name. The --no-trunc option makes the command exhibit output without truncating it. The --limit \u0026lt;number\u0026gt; option limits the displayed results to the specified number. tag\nAdds a name to an image. The default is \u0026rsquo;latest\u0026rsquo; to classify the image as the latest version. Older images may have specific version identifiers. Container Management # attach\nAttaches to a running container exec\nRuns a process in a running container generate\nGenerates a systemd unit configuration file that can be used to control the operational state of a container. The --new option is important and is employed in later exercises. info\nReveals system information, including the defined registries inspect\nExhibits the configuration of a container ps\nLists running containers (includes stopped containers with the -a option) rm\nRemoves a container run\nLaunches a new container from an image. Some options such as -d (detached), -i (interactive), and -t (terminal) are important and are employed in exercises where needed. start/stop/restart\nStarts, stops, or restarts a container skopeo Command # Utilized for interacting with local and remote images and registries. Has numerous subcommands available; however, you will be using only the inspect subcommand to examine the details of an image stored in a remote registry. /etc/containers/registries.conf\nSystem-wide configuration file for image registries. Normal Linux users may store a customized copy of this file, if required, under the ~/.config/containers directory. Settings stored in the per-user file will take precedence over those stored in the system-wide file. Useful for running rootless containers. Defines searchable and blocked registries. [root@server10 ~]# grep -Ev \u0026#39;^#|^$\u0026#39; /etc/containers/registries.conf unqualified-search-registries = [\u0026#34;registry.access.redhat.com\u0026#34;, \u0026#34;registry.redhat.io\u0026#34;, \u0026#34;docker.io\u0026#34;] short-name-mode = \u0026#34;enforcing\u0026#34; The output shows three registries. The podman command searches these registries for container images in the given order. Can add additional registries to the list. Add a private registry called registry.private.myorg.io to be added with the highest priority:\n[root@server10 ~]# vim /etc/containers/registries.conf unqualified-search-registries = \\[\u0026#34;registry.private.myorg.io\u0026#34;, \u0026#34;registry.access.redhat.com\u0026#34;, \u0026#34;registry.redhat.io\u0026#34;, \u0026#34;docker.io\u0026#34;\\] If this private registry is the only one to be used, you can take the rest of the registry entries out of the list:\nunqualified-search-registries = \\[\u0026#34;registry.private.myorg.io\u0026#34;\\] EXAM TIP: As there is no Internet access provided during Red Hat exams, you may have to access a network-based registry to download images.\nViewing Podman Configuration and Version # The podman command references various system runtime and configuration files and runs certain Linux commands in the background to gather and display information. For instance, it looks for registries and storage data in the system-wide and per-user configuration files, pulls memory information from the /proc/meminfo file, executes uname -rto obtain the kernel version, and so on. podman\u0026rsquo;s info subcommand shows all this information. Here is a sample when this command is executed as a normal user (user1):\n[[user1@server10 root]$ podman info ERRO[0000] XDG_RUNTIME_DIR directory \u0026#34;/run/user/0\u0026#34; is not owned by the current user](\u0026lt;[user1@server10 ~]$ podman info host: arch: amd64 buildahVersion: 1.33.8 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: ... Re-run the command as root (preceded by sudo if running as user1) and compare the values for the settings \u0026ldquo;rootless\u0026rdquo; under host and \u0026ldquo;ConfigFile\u0026rdquo; and \u0026ldquo;ImageStore\u0026rdquo; under store.\nThe differences lie between where the root and rootless (normal) users store and obtain configuration data, the number of container images they have locally available, and so on.\n[root@server10 ~]# podman info host: arch: amd64 buildahVersion: 1.33.8 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc ... Similarly, you can run the podman command as follows to check its version:\n[root@server10 ~]# podman version Client: Podman Engine Version: 4.9.4-rhel API Version: 4.9.4-rhel Go Version: go1.21.11 (Red Hat 1.21.11-1.el9_4) Built: Mon Jul 1 03:27:14 2024 OS/Arch: linux/amd64 Image Management # Container images\nAre available from numerous private and public registries. They are pre-built for a variety of use cases. You can search through registries to find the one that suits your needs. You can examine their metadata before downloading them for consumption. Downloaded images can be removed when no longer needed to conserve local storage. The same pair of commands\u0026mdash;podman and skopeo\u0026mdash;is employed for these operations. Lab: Search, Examine, Download, and Remove an Image # Log in to the registry.access.redhat.com registry Look for an image called mysql-80 in the registry, examine its details, pull it to your system, confirm the retrieval, and finally erase it from the local storage. 1. Log in to the specified Red Hat registry:\n[user1@server10 ~]$ podman login registry.redhat.io 2. Confirm a successful login:\n[user1@server10 ~]$ podman login registry.redhat.io --get-login 3. Find the mysql-80 image in the specified registry. Add the --no-trunc option to view full output.\n[user1@server10 ~]$ podman search registry.redhat.io/mysql-80 --no-trunc NAME DESCRIPTION registry.redhat.io/rhel8/mysql-80 This container image provides a containerized packaging of the MySQL mysqld daemon and client application. The mysqld server daemon accepts connections from clients and provides access to content from MySQL databases on behalf of the clients. ... 4. Select the second image rhel9/mysql-80 for this exercise. Inspect the image without downloading it using skopeo inspect. A long output will be generated. The command uses the docker:// mechanism to access the image.\n[user1@server10 ~]$ skopeo inspect docker://registry.redhat.io/rhel9/mysql-80 { \u0026#34;Name\u0026#34;: \u0026#34;registry.redhat.io/rhel9/mysql-80\u0026#34;, \u0026#34;Digest\u0026#34;: \u0026#34;sha256:247903d2103a3c1db9401f6340ecdcd97c6244480b7a3419e6303dda650491dc\u0026#34;, \u0026#34;RepoTags\u0026#34;: [ \u0026#34;1\u0026#34;, \u0026#34;1-190\u0026#34;, \u0026#34;1-190.1655192188\u0026#34;, \u0026#34;1-190.1655192188-source\u0026#34;, \u0026#34;1-190-source\u0026#34;, \u0026#34;1-197\u0026#34;, \u0026#34;1-197-source\u0026#34;, \u0026#34;1-206\u0026#34;, ... Output:\nShows older versions under RepoTags\nCreation time for the latest version\nBuild date of the image\ndescription\nother information.\nIt is a good practice to analyze the metadata of an image prior to downloading and consuming it.\n5. Download the image by specifying the fully qualified image name using podman pull:\n[user1@server10 ~]$ podman pull docker://registry.redhat.io/rhel9/mysql-80 Trying to pull registry.redhat.io/rhel9/mysql-80:latest... Getting image source signatures Checking if image destination supports signatures Copying blob 846c0bdf4e30 done | Copying blob cc296d75b612 done | Copying blob db22e630b1c7 done | Copying config b5782120a3 done | Writing manifest to image destination Storing signatures b5782120a320e5915d86555e661c357cfa56dd8320ba4c54a58caa1e1c91925f 6. List the image to confirm the retrieval using podman images:\n[user1@server10 ~]$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE registry.redhat.io/rhel9/mysql-80 latest b5782120a320 2 weeks ago 555 MB 7. Display the image\u0026rsquo;s details using podman inspect:\n[user1@server10 ~]$ podman inspect mysql-80 [ { \u0026#34;Id\u0026#34;: \u0026#34;b5782120a320e5915d86555e661c357cfa56dd8320ba4c54a58caa1e1c91925f\u0026#34;, \u0026#34;Digest\u0026#34;: \u0026#34;sha256:247903d2103a3c1db9401f6340ecdcd97c6244480b7a3419e6303dda650491dc\u0026#34;, \u0026#34;RepoTags\u0026#34;: [ \u0026#34;registry.redhat.io/rhel9/mysql-80:latest\u0026#34; ], 8. Remove the mysql-80 image from local storage:\n[user1@server10 ~]$ podman rmi mysql-80 Untagged: registry.redhat.io/rhel9/mysql-80:latest Deleted: b5782120a320e5915d86555e661c357cfa56dd8320ba4c54a58caa1e1c91925f Shows the ID of the image after deletion. 9. Confirm the removal:\n[user1@server10 ~]$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE Containerfile # Can build a custom image by outlining the steps you need to be run in a file called Containerfile. The podman command can then be used to read those instructions and executes them to produce a new image. File name containerfile is widespread; but you can use any name of your liking. Instructions that may be utilized inside a Containerfile to perform specific functions during the build process:\nCMD\nRuns a command COPY\nCopies files to the specified location ENV\nDefines environment variables to be used during the build process EXPOSE\nA port number that will be opened when a container is launched using this image FROM\nIdentifies the base container image to use RUN\nExecutes the specified commands USER\nDefines a non-root user to run the commands as WORKDIR\nSets the working directory. This directory is automatically created if it does not already exist. A sample container file is presented below:\n[user1@server10 ~]$ vim containerfile # Use RHEL9 base image FROM registry.redhat.io/ubi9/ubi # Install Apache web server software RUN dnf -y install httpd # Copy the website COPY ./index.html /var/www/html/ # Expose Port 80/tcp EXPOSE 80 # Start Apache web server CMD [\u0026#34;httpd\u0026#34;] The index.html file may contain a basic statement such as \u0026ldquo;This is a custom-built Apache web server container image based on RHEL 9\u0026rdquo;. Lab: Use Containerfile to Build Image # Use a containerfile to build a custom image based on the latest version of the RHEL 9 universal base image (ubi) available from a Red Hat container registry. Confirm the image creation. Use the podman command for these activities. 1. Log in to the specified Red Hat registry:\n[user1@server10 ~]$ podman login registry.redhat.io Authenticating with existing credentials for registry.redhat.io Existing credentials are valid. Already logged in to registry.redhat.io 2. Confirm a successful login:\n[user1@server10 ~]$ podman login registry.redhat.io --get-login 3. Create a file called containerfile with the following code:\n[user1@server10 ~]$ vim containerfile2 # Use RHEL9 base image FROM registry.redhat.io/ubi9/ubi # Count the number of characters CMD echo \u0026#34;RHCSA exam is hands-on.\u0026#34; | wc # Copy a local file to /tmp COPY ./testfile /tmp 4. Create a file called testfile with some random text in it and place it in the same directory as the containerfile.\n[user1@server10 ~]$ echo \u0026#34;boo bee doo bee doo\u0026#34; \u0026gt;\u0026gt; testfile [user1@server10 ~]$ cat testfile boo bee doo bee doo 5. Build an image by specifying the containerfile name and an image tag such as ubi9-simple-image. The period character at the end represents the current directory and this is where both containerfile and testfile are located.\n[user1@server10 ~]$ podman image build -f containerfile2 -t ubi9-simple-image . STEP 1/3: FROM registry.redhat.io/ubi9/ubi Trying to pull registry.redhat.io/ubi9/ubi:latest... Getting image source signatures Checking if image destination supports signatures Copying blob cc296d75b612 done | Copying config 159a1e6731 done | Writing manifest to image destination Storing signatures STEP 2/3: CMD echo \u0026#34;RHCSA exam is hands-on.\u0026#34; | wc --\u0026gt; 4c005bfd0b34 STEP 3/3: COPY ./testfile /tmp COMMIT ubi9-simple-image --\u0026gt; a2797b06a129 Successfully tagged localhost/ubi9-simple-image:latest a2797b06a1294ed06edab2ba1c21d2bddde3eb3af1d8ed286781837f62992622 6. Confirm image creation:\n[user1@server10 ~]$ podman image ls REPOSITORY TAG IMAGE ID CREATED SIZE localhost/ubi9-simple-image latest a2797b06a129 2 minutes ago 220 MB registry.redhat.io/ubi9/ubi latest 159a1e67312e 2 weeks ago 220 MB Output:\ndownloaded image\nnew custom image along with their image IDs, creation time, and size.\nDo not remove the custom image yet as you will be using it to launch a container in the next section.\nBasic Container Management # Starting, stopping, listing, viewing information about, and deleting them. Depending on the use case, containers can be launched in different ways. They can: Have a name assigned or be nameless Have a terminal session opened for interaction Execute an entry point command (the command specified at the launch time) and be auto-terminated right after. etc. Running containers can be stopped and restarted, or discarded if no longer needed. The podman command is utilized to start containers and manage their lifecycle. This command is also employed to list stopped and running containers, and view their details. Lab: Run, Interact with, and Remove a Named Container # Run a container based on the latest version of the RHEL 8 ubi available in the Red Hat container registry. Assign this container a name and run a few native Linux commands in a terminal window interactively. Exit out of the container to mark the completion of the exercise. 1. Launch a container using ubi8 (RHEL 8). Name this container rhel8-base-os and open a terminal session for interaction:\n[user1@server10 ~]$ podman run -ti --name rhel8-base-os ubi8 Resolved \u0026#34;ubi8\u0026#34; as an alias (/etc/containers/registries.conf.d/001-rhel-shortnames.conf) Trying to pull registry.access.redhat.com/ubi8:latest... Getting image source signatures Checking if image destination supports signatures Copying blob 8694db102e5b done | Copying config 269749ad51 done | Writing manifest to image destination Storing signatures [root@30c7cccd8490 /]# Downloaded the latest version of the specified image automatically even though no FQIN was provided.\nThis is because it searched through the registries listed in the /etc/containers/registries.conf file and retrieved the image from wherever it found it first (registry.access.redhat.com). Opened a terminal session inside the container as the root user to interact with the containerized RHEL 8 OS.\nThe container ID is reflected as the hostname in the container\u0026rsquo;s command prompt (last line in the output). This is an auto-generated ID.\nIf you encounter any permission issues, delete the /etc/docker directory (if it exists) and try again.\n2. Run a few basic commands such as pwd, ls, cat, and date inside the container for verification:\n[root@30c7cccd8490 /]# pwd / [root@30c7cccd8490 /]# ls bin dev home lib64\tmedia opt root\tsbin sys usr boot etc lib\tlost+found mnt proc run\tsrv tmp var [root@30c7cccd8490 /]# cat /etc/redhat-release Red Hat Enterprise Linux release 8.10 (Ootpa) [root@30c7cccd8490 /]# date Thu Aug 1 21:09:13 UTC 2024 3. Close the terminal session when done:\n[root@30c7cccd8490 /]# exit exit [user1@server10 ~]$ 4. Delete the container using podman rm:\n[user1@server10 ~]$ podman rm rhel8-base-os rhel8-base-os Confirm the removal with podman ps.\n[user1@server10 ~]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Lab: Run a Nameless Container and Auto-Remove it After Entry Point Command Execution # Launch a container based on the latest version of RHEL 7 ubi available in a Red Hat container registry. This image provides the base operating system layer to deploy containerized applications. Enter a Linux command at the command line for execution inside the container as an entry point command and the container should be automatically deleted right after that. 1. Start a container using ubi7 (RHEL 7) and run ls as an entry point command. Remove the container as soon as the entry point command has finished running.\n[user1@server10 ~]$ podman run --rm ubi7 ls Resolved \u0026#34;ubi7\u0026#34; as an alias (/etc/containers/registries.conf.d/001-rhel-shortnames.conf) Trying to pull registry.access.redhat.com/ubi7:latest... Getting image source signatures Checking if image destination supports signatures Copying blob 7f2c2c4492b6 done | Copying config a084eb42a5 done | Writing manifest to image destination Storing signatures bin boot dev etc home ... 2. Confirm the container removal with podman ps:\npodman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Advanced Container Management # Preset environment variables may be passed when launching containers or new variables may be set for containerized applications to consume for proper operation. Information stored during an application execution is lost when a container is restarted or erased. This behavior can be overridden by making a directory on the host available inside the container for saving data persistently. Containers may be configured to start and stop with the transitioning of the host system via the systemd service. These advanced tasks are also performed with the podman command. Containers and Port Mapping # Applications running in different containers often need to exchange data for proper operation. For instance, a containerized Apache web server may need to talk to a MySQL database instance running in a different container. It may also need to talk to the outside world over a port such as 80 or 8080. To support this traffic flow, appropriate port mappings are established between the host system and each container. EXAM TIP: As a normal user, you cannot map a host port below 1024 to a container port.\nLab: Configure Port Mapping # Launch a container called rhel7-port-map in detached mode (as a daemon) with host port 10000 mapped to port 8000 inside the container. Use a version of the RHEL 7 image with Apache web server software pre-installed. This image is available from a Red Hat container registry. List the running container and confirm the port mapping. 1. Search for an Apache web server image for RHEL 7 using podman search:\n[user1@server30 ~]$ podman search registry.redhat.io/rhel7/httpd NAME DESCRIPTION registry.redhat.io/rhscl/httpd-24-rhel7 Apache HTTP 2.4 Server 2. Log in to registry.redhat.io using the Red Hat credentials to access the image:\n[user1@server30 ~]$ podman login registry.redhat.io Username: tdavetech@gmail.com Password: Login Succeeded! 3. Download the latest version of the Apache image using podman pull:\n[user1@server30 ~]$ podman pull registry.redhat.io/rhscl/httpd-24- rhel7 Trying to pull registry.redhat.io/rhscl/httpd-24-rhel7:latest... Getting image source signatures Checking if image destination supports signatures Copying blob fd77da0b900b done | Copying blob 7f2c2c4492b6 done | Copying blob ea092d7970b2 done | Copying config 847db19d6c done | Writing manifest to image destination Storing signatures 847db19d6cbc726106c901a7713d30dccc9033031ec812037c4c458319a1b328 4. Verify the download using podman images:\n[user1@server30 ~]$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE registry.redhat.io/rhscl/httpd-24-rhel7 latest 847db19d6cbc 2 months ago 332 MB 5. Launch a container named rhel7-port-map in detached mode to run the containerized Apache web server with host port 10000 mapped to container port 8000.\n[user1@server30 ~]$ podman run -dp 10000:8000 --name rhel7-port-map httpd-24-rhel7 cd063dff352dfbcd57dd417587513b12ca4033ed657f3baaa28d54df19d4df1c 6. Verify that the container was launched successfully using podman ps:\n[user1@server30 ~]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd063dff352d registry.redhat.io/rhscl/httpd-24-rhel7:latest /usr/bin/run-http... 36 seconds ago Up 36 seconds 0.0.0.0:10000- \u0026gt;8000/tcp rhel7-port-map 7. You can also use podman port to view the mapping:\n[user1@server30 ~]$ podman port rhel7-port-map 8000/tcp -\u0026gt; 0.0.0.0:10000 Now any inbound web traffic on host port 10000 will be redirected to the container. Exercise 22-7: Stop, Restart, and Remove a Container # Stop the container, restart it, stop it again, and then erase it. Use appropriate podman subcommands and verify each transition. 1. Verify the current operational state of the container rhel7-port-map:\n[user1@server30 ~]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd063dff352d registry.redhat.io/rhscl/httpd-24-rhel7:latest /usr/bin/run-http... 3 minutes ago Up 3 minutes 0.0.0.0:10000- \u0026gt;8000/tcp rhel7-port-map 2. Stop the container and confirm. (the -a option with ps also includes the stopped containers in the output):\n[user1@server30 ~]$ podman stop rhel7-port-map rhel7-port-map [user1@server30 ~]$ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd063dff352d registry.redhat.io/rhscl/httpd-24-rhel7:latest /usr/bin/run-http... 6 minutes ago Exited (0) 5 seconds ago 0.0.0.0:10000-\u0026gt;8000/tcp rhel7-port-map 3. Start the container and confirm:\n[user1@server30 ~]$ podman start rhel7-port-map rhel7-port-map [user1@server30 ~]$ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cd063dff352d registry.redhat.io/rhscl/httpd-24-rhel7:latest /usr/bin/run-http... 8 minutes ago Up 11 seconds 0.0.0.0:10000- \u0026gt;8000/tcp rhel7-port-map 4. Stop the container and remove it:\n[user1@server30 ~]$ podman rm rhel7-port-map rhel7-port-map 5. Confirm the removal:\n[user1@server30 ~]$ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Containers and Environment Variables # Many times it is necessary to pass a host\u0026rsquo;s pre-defined environment variable, such as PATH, to a containerized application for consumption. Moreover, it may also be necessary at times to set new variables to inject debugging flags or sensitive information such as passwords, access keys, or other secrets for use inside containers. Passing host environment variables or setting new environment variables is done at the time of launching a container. The podman command allows multiple variables to be passed or set with the -e option. EXAM TIP: Use the -e option with each variable that you want to pass or set.\nLab: Pass and Set Environment Variables # Launch a container using the latest version of a ubi for RHEL 9 available in a Red Hat container registry. Inject the HISTSIZE environment variable, and a variable called SECRET with a value \u0026ldquo;secret123\u0026rdquo;. Name this container rhel9-env-vars and have a shell terminal opened to check the variable settings. Remove this container. 1. Launch a container with an interactive terminal session and inject variables HISTSIZE and SECRET as directed. Use the specified container image.\n[user1@server30 ~]$ podman run -it -e HISTSIZE -e SECRET=\u0026#34;secret123\u0026#34; --name rhel9-env-vars ubi9 Resolved \u0026#34;ubi9\u0026#34; as an alias (/etc/containers/registries.conf.d/001- rhel-shortnames.conf) Trying to pull registry.access.redhat.com/ubi9:latest... Getting image source signatures Checking if image destination supports signatures Copying blob cc296d75b612 done | Copying config 159a1e6731 done | Writing manifest to image destination Storing signatures [root@b587355b8fc1 /]# 2. Verify both variables using the echo command:\n[root@b587355b8fc1 /]# echo $HISTSIZE $SECRET 1000 secret123 [root@b587355b8fc1 /]# 3. Disconnect from the container, and stop and remove it:\n[user1@server30 ~]$ podman stop rhel9-env-vars rhel9-env-vars [user1@server30 ~]$ podman rm rhel9-env-vars rhel9-env-vars Confirm the deletion:\n[user1@server30 ~]$ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Containers and Persistent Storage # Containers are normally launched for a period of time to run an application and then stopped or deleted when their job is finished. Any data that is produced during runtime is lost on their restart, failure, or termination. This data may be saved for persistence on a host directory by attaching the host directory to a container. The containerized application will see the attached directory just like any other local directory and will use it to store data if it is configured to do so. Any data that is saved on the directory will be available even after the container is rebooted or removed. Later, this directory can be re-attached to other containers to give them access to the stored data or to save their own data. The source directory on the host may itself exist on any local or remote file system. EXAM TIP: Proper ownership, permissions, and SELinux file type must be set to ensure persistent storage is accessed and allows data writes without issues.\nThere are a few simple steps that should be performed to configure a host directory before it can be attached to a container. These steps include the correct ownership, permissions, and SELinux type (container_file_t). The special SELinux file type is applied to prevent containerized applications (especially those running in root containers) from gaining undesired privileged access to host files and processes, or other running containers on the host if compromised. Lab: Attach Persistent Storage and Access Data Across Containers # Set up a directory on server20 and attach it to a new container. Write some data to the directory while in the container. Delete the container and launch another container with the same directory attached. Observe the persistence of saved data in the new container and that it is accessible. Remove the container to mark the completion of this exercise. 1. Create a directory called /host_data, set full permissions on it, and confirm:\n[user1@server30 ~]$ sudo mkdir /host_data [sudo] password for user1: [user1@server30 ~]$ sudo chmod 777 /host_data/ [user1@server30 ~]$ ll -d /host_data/ drwxrwxrwx. 2 root root 6 Aug 1 22:59 /host_data/ 2. Launch a root container called rhel9-persistent-data in interactive mode using the latest ubi9 image. Specify the attachment point (/container_data) to be used inside the container for the host directory (/host_data) Ensure the SELinux type container_file_t is automatically set on the directory and files within.\n[user1@server30 ~]$ sudo podman run --name rhel9-persistent-data -v /host_data:/container_data:Z -it ubi9 Resolved \u0026#34;ubi9\u0026#34; as an alias (/etc/containers/registries.conf.d/001- rhel-shortnames.conf) Trying to pull registry.access.redhat.com/ubi9:latest... Getting image source signatures Checking if image destination supports signatures Copying blob cc296d75b612 done | Copying config 159a1e6731 done | Writing manifest to image destination Storing signatures 3. Confirm the presence of the directory inside the container with ls on /container_data:\n[root@e8711892370f /]# ls -ldZ /container_data drwxrwxrwx. 2 root root system_u:object_r:container_file_t:s0:c376,c965 6 Aug 2 05:59 /container_data 4. Create a file called testfile with the echo command under /container_data:\n[root@e8711892370f /]# echo \u0026#34;This is persistent storage.\u0026#34; \u0026gt; /container_data/testfile 5. Verify the file creation and the SELinux type on it:\n[root@e8711892370f /]# ls -lZ /container_data/ total 4 -rw-r--r--. 1 root root system_u:object_r:container_file_t:s0:c376,c965 28 Aug 2 06:03 testfile 6. Exit out of the container and check the presence of the file in the host directory:\n[root@e8711892370f /]# exit exit [user1@server30 ~]$ ls -lZ /host_data/ total 4 -rw-r--r--. 1 root root system_u:object_r:container_file_t:s0:c376,c965 28 Aug 1 23:03 testfile 7. Stop and remove the container:\n[user1@server30 ~]$ sudo podman stop rhel9-persistent-data rhel9-persistent-data [user1@server30 ~]$ sudo podman rm rhel9-persistent-data rhel9-persistent-data 8. Launch a new root container called rhel8-persistent-data in interactive mode using the latest ubi8 image from any of the defined registries. Specify the attachment point (/container_data2) to be used inside the container for the host directory (/host_data). Ensure the SELinux type container_file_t is automatically set on the directory and files within.\n[user1@server30 ~]$ sudo podman run -it --name rhel8-persistent-data -v /host_data:/container_data2:Z ubi8 Resolved \u0026#34;ubi8\u0026#34; as an alias (/etc/containers/registries.conf.d/001- rhel-shortnames.conf) Trying to pull registry.access.redhat.com/ubi8:latest... Getting image source signatures Checking if image destination supports signatures Copying blob 8694db102e5b done | Copying config 269749ad51 done | Writing manifest to image destination Storing signatures 9. Confirm the presence of the directory inside the container with ls on /container_data2:\n[root@af6773299c7e /]# ls -ldZ /container_data2/ drwxrwxrwx. 2 root root system_u:object_r:container_file_t:s0:c198,c914 22 Aug 2 06:03 /container_data2/ [root@af6773299c7e /]# ls -lZ /container_data2/ total 4 -rw-r--r--. 1 root root system_u:object_r:container_file_t:s0:c198,c914 28 Aug 2 06:03 testfile [root@af6773299c7e /]# cat /container_data2/testfile This is persistent storage. 10. Create a file called testfile2 with the echo command under /container_data2:\n[root@af6773299c7e /]# echo \u0026#34;This is persistent storage2.\u0026#34; \u0026gt; /container_data2/testfile2 [root@af6773299c7e /]# ls -lZ /container_data2/ total 8 -rw-r--r--. 1 root root system_u:object_r:container_file_t:s0:c198,c914 28 Aug 2 06:03 testfile -rw-r--r--. 1 root root system_u:object_r:container_file_t:s0:c198,c914 29 Aug 2 06:10 testfile2 11. Exit out of the container and confirm the existence of both files in the host directory:\n[root@af6773299c7e /]# exit exit [user1@server30 ~]$ ls -lZ /host_data/ total 8 -rw-r--r--. 1 root root system_u:object_r:container_file_t:s0:c198,c914 28 Aug 1 23:03 testfile -rw-r--r--. 1 root root system_u:object_r:container_file_t:s0:c198,c914 29 Aug 1 23:10 testfile2 12. Stop and remove the container using the stop and rm subcommands:\n[user1@server30 ~]$ sudo podman stop rhel8-persistent-data rhel8-persistent-data [user1@server30 ~]$ sudo podman rm rhel8-persistent-data rhel8-persistent-data 13. Re-check the presence of the files in the host directory:\n[user1@server30 ~]$ ll /host_data total 8 -rw-r--r--. 1 root root 28 Aug 1 23:03 testfile -rw-r--r--. 1 root root 29 Aug 1 23:10 testfile2 Container State Management with systemd # Multiple containers run on a single host and it becomes a challenging task to change their operational state or delete them manually.\nIn RHEL 9, these administrative functions can be automated via the systemd service\nThere are several steps that need to be completed to configure container state management via systemd.\nThese steps vary for rootful and rootless container setups and include the creation of service unit files and their storage in appropriate directory locations (~/.config/systemd/user for rootless containers and /etc/systemd/system for rootful containers).\nOnce setup and enabled, the containers will start and stop automatically as a systemd service with the host state transition or manually with the systemctl command.\nThe podman command to start and stop containers is no longer needed if the systemd setup is in place.\nYou may experience issues if you continue to use podman for container state transitioning alongside.\nThe start and stop behavior for rootless containers differs slightly from that of rootful containers.\nFor the rootless setup, the containers are started when the relevant user logs in to the host and stopped when that user logs off from all their open terminal sessions;\nHowever, this default behavior can be altered by enabling lingering for that user with the loginctl command.\nUser lingering is a feature that, if enabled for a particular user, spawns a user manager for that user at system startup and keeps it running in the background to support long-running services configured for that user.\nThe user need not log in.\nEXAM TIP: Make sure that you use a normal user to launch rootless containers and the root user (or sudo) for rootful containers.\nRootless setup does not require elevated privileges of the root user. Lab: Configure a Rootful Container as a systemd Service # Create a systemd unit configuration file for managing the state of your rootful containers. Launch a new container and use it as a template to generate a service unit file. Stop and remove the launched container to avoid conflicts with new containers that will start. Use the systemctl command to verify the automatic container start, stop, and deletion. 1. Launch a new container called rootful-container in detached mode using the latest ubi9:\n[user1@server30 ~]$ sudo podman run -dt --name rootful-container ubi9 [sudo] password for user1: 0ed04dcedec418068acd14c864e95e78f56a38dd57d2349cf2c46b0de1a1bf1b 2. Confirm the new container using podman ps. Note the container ID.\n[user1@server30 ~]$ sudo podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0ed04dcedec4 registry.access.redhat.com/ubi9:latest /bin/bash 20 seconds ago Up 20 seconds rootful-container 3. Create (generate) a service unit file called rootful-container.service under /etc/systemd/system while ensuring that the next new container that will be launched based on this configuration file will not require the source container to work. The tee command will show the generated file content on the screen as well as store it in the specified file.\n[user1@server30 ~]$ sudo podman generate systemd --new --name rootful-container | sudo tee /etc/systemd/system/rootful-qcontainer.service [Unit] Description=Podman container-rootful-container.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm \\ --sdnotify=conmon \\ --replace \\ -dt \\ --name rootful-container ubi9 ExecStop=/usr/bin/podman stop \\ --ignore -t 10 \\ --cidfile=%t/%n.ctr-id ExecStopPost=/usr/bin/podman rm \\ -f \\ --ignore -t 10 \\ --cidfile=%t/%n.ctr-id Type=notify NotifyAccess=all [Install] WantedBy=default.target The unit file has the same syntax as any other systemd service configuration file. There are three sections\u0026mdash;Unit, Service, and Install. (1) The unit section provides a short description of the service, the manual page location, and the dependencies (wants and after). (2) The service section highlights the full commands for starting (ExecStart) and stopping (ExecStop) containers. It also highlights the commands that will be executed before the container start (ExecStartPre) and after the container stop (ExecStopPost). There are a number of options and arguments with the commands to ensure a proper transition. The restart on-failure stipulates that systemd will try to restart the container in the event of a failure. (3) The install section identifies the operational target the host needs to be running in before this container service can start. 4. Stop and delete the source container (rootful-container):\n[user1@server30 ~]$ sudo podman stop rootful-container [sudo] password for user1: WARN[0010] StopSignal SIGTERM failed to stop container rootful- container in 10 seconds, resorting to SIGKILL rootful-container [user1@server30 ~]$ sudo podman rm rootful-container rootful-container Verify the removal by running sudo podman ps -a:\n[user1@server30 ~]$ sudo podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5. Update systemd to bring the new service under its control (reboot the system if required):\n[user1@server30 ~]$ sudo systemctl daemon-reload 6. Enable and start the container service:\n[user1@server30 ~]$ sudo systemctl enable --now rootful-container Created symlink /etc/systemd/system/default.target.wants/rootful- container.service → /etc/systemd/system/rootful-container.service. 7. Check the running status of the new service:\n[user1@server30 ~]$ sudo systemctl status rootful-container rootful-container.service - Podman container-rootful-container.s\u0026gt; Loaded: loaded (/etc/systemd/system/rootful-container.service\u0026gt; Active: active (running) 8. Verify the launch of a new container (compare the container ID with that of the source root container):\n[user1@server30 ~]$ sudo podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 440a57c26186 registry.access.redhat.com/ubi9:latest /bin/bash About a minute ago Up About a minute rootful-container 9. Restart the container service using the systemctl command:\n[user1@server30 ~]$ sudo systemctl restart rootful-container sudo systemctl status rootful- [user1@server30 ~]$ sudo systemctl status rootful-container rootful-container.service - Podman container-rootful-container.s\u0026gt; Loaded: loaded (/etc/systemd/system/rootful-container.service\u0026gt; Active: active (running) 10. Check the status of the container again. Observe the removal of the previous container and the launch of a new container (compare container IDs).\n[user1@server30 ~]$ sudo podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0a980537b83a registry.access.redhat.com/ubi9:latest /bin/bash 59 seconds ago Up About a minute rootful-container Each time the rootful-container service is restarted or server20 is rebooted, a new container will be launched. Lab: Configure Rootless Container as a systemd Service # Create a systemd unit configuration file for managing the state of your rootless containers. Launch a new container as conuser1 (create this user) and use it as a template to generate a service unit file. Stop and remove the launched container to avoid conflicts with new containers that will start. Use the systemctl command as conuser1 to verify the automatic container start, stop, and deletion. 1. Create a user account called conuser1 and assign a simple password:\n[user1@server30 ~]$ sudo useradd conuser1 [user1@server30 ~]$ echo conuser1 | sudo passwd -- stdin conuser1 Changing password for user conuser1. passwd: all authentication tokens updated successfully. 2. Open a new terminal window on server20 and log in as conuser1. Create directory ~/.config/systemd/user to store a service unit file:\n[conuser1@server30 ~]$ mkdir ~/.config/systemd/user -p 3. Launch a new container called rootless-container in detached mode using the latest ubi8:\n[conuser1@server30 ~]$ podman run -dt --name rootless-container ubi8 Resolved \u0026#34;ubi8\u0026#34; as an alias (/etc/containers/registries.conf.d/001-rhel- shortnames.conf) Trying to pull registry.access.redhat.com/ubi8:latest... Getting image source signatures Checking if image destination supports signatures Copying blob 8694db102e5b done | Copying config 269749ad51 done | Writing manifest to image destination Storing signatures 381d46ae9a3e11723c3bde35090782129e6937c461f8c2621bc9725f6b9efc27 4. Confirm the new container using podman ps. Note the container ID.\n[conuser1@server30 ~]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 381d46ae9a3e registry.access.redhat.com/ubi8:latest /bin/bash 27 seconds ago Up 27 seconds rootless-container 5. Create (generate) a service unit file called rootless-container.service under ~/.config/systemd/user while ensuring that the next new container that will be launched based on this configuration will not require the source container to work:\n[conuser1@server30 ~]$ podman generate systemd --new --name rootless-container \u0026gt; ~/.config/systemd/user/rootless-container.service DEPRECATED command: It is recommended to use Quadlets for running containers and pods under systemd. Please refer to podman-systemd.unit(5) for details. 6. Display the content of the unit file:\n[conuser1@server30 ~]$ cat ~/.config/systemd/user/rootless-container.service # container-rootless-container.service # autogenerated by Podman 4.9.4-rhel # Thu Aug 1 23:42:11 MST 2024 [Unit] Description=Podman container-rootless- container.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm \\ --sdnotify=conmon \\ --replace \\ -dt \\ --name rootless-container ubi8 ExecStop=/usr/bin/podman stop \\ --ignore -t 10 \\ --cidfile=%t/%n.ctr-id ExecStopPost=/usr/bin/podman rm \\ -f \\ --ignore -t 10 \\ --cidfile=%t/%n.ctr-id Type=notify NotifyAccess=all [Install] WantedBy=default.target 7. Stop and delete the source container rootless-container using the stop and rm subcommands:\n[conuser1@server30 ~]$ podman stop rootless-container rootless-container [conuser1@server30 ~]$ podman rm rootless-container rootless-container Verify the removal by running podman ps -a:\n[conuser1@server30 ~]$ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8. Update systemd to bring the new service to its control\n[conuser1@server30 ~]$ systemctl --user daemon-reload 9. Enable and start the container service:\n[conuser1@server30 ~]$ systemctl --user enable --now rootless-container.service Created symlink /home/conuser1/.config/systemd/user/default.target.wa nts/rootless-container.service → /home/conuser1/.config/systemd/user/rootless- container.service. 10. Check the running status of the new service:\nconuser1@server30 ~]$ systemctl --user status rootless-container rootless-container.service - Podman container- rootless-container\u0026gt; Loaded: loaded (/home/conuser1/.config/systemd/user/rootless-\u0026gt; Active: active (running) 11. Verify the launch of a new container (compare the container ID with that of the source rootless container):\n[conuser1@server30 ~]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 57f946085605 registry.access.redhat.com/ubi8:latest /bin/bash About a minute ago Up About a minute rootless-container 12. Enable the container service to start and stop with host transition using the loginctl command (systemd login manager) and confirm:\n[conuser1@server30 ~]$ loginctl enable-linger [conuser1@server30 ~]$ loginctl show-user conuser1 | grep -i linger Linger=yes 13. Restart the container service using the systemctl command:\n[conuser1@server30 ~]$ systemctl --user restart rootless-container [conuser1@server30 ~]$ systemctl --user status rootless-container rootless-container.service - Podman container- rootless-container\u0026gt; Loaded: loaded (/home/conuser1/.config/systemd/user/rootless-\u0026gt; Active: active (running) 14. Check the status of the container again. Observe the removal of the previous container and the launch of a new container (compare container IDs).\n[conuser1@server30 ~]$ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4dec33db41b5 registry.access.redhat.com/ubi8:latest /bin/bash 41 seconds ago Up 41 seconds rootless-container Each time the rootless-container service is restarted or server20 is rebooted, a new container will be launched. You can verify this by comparing their container IDs. Containers DIY Labs # Lab: Launch Named Root Container with Port Mapping # Create a new user account called conadm on server30 and give them full sudo rights. [root@se -bash: 3: command not found rver30 ~]# adduser conadm [root@server30 ~]# visudo conadm ALL=(ALL) ALL As conadm with sudo (where required) on server30, inspect the latest version of ubi9 and then download it to your computer. [root@server30 ~]# dnf install container-tools [root@server30 ~]# podman login registry.redhat.io [conuser1@server30 ~]$ podman pull ubi9 Resolved \u0026#34;ubi9\u0026#34; as an alias (/etc/containers/registries.conf.d/001-rhel- shortnames.conf) Trying to pull registry.access.redhat.com/ubi9:latest... Getting image source signatures Checking if image destination supports signatures Copying blob cc296d75b612 done | Copying config 159a1e6731 done | Writing manifest to image destination Storing signatures 159a1e67312ef50059357047ebe2a365afea904504fca9561abb3 85ecd942d62 [conuser1@server30 ~]$ podman inspect ubi9 Launch a container called rootful-cont-port in attached terminal mode with host port 80 mapped to container port 8080. sudo podman run -it --name rootful-cont-port -p 80:8080 ubi9 Run a few basic Linux commands such as ls, pwd, df, cat /etc/redhat-release, and os-release while in the container. [root@349163a6e431 /]# ls afs boot etc\tlib\tlost+found mnt proc run srv tmp var bin dev home lib64\tmedia\topt root sbin sys usr [root@349163a6e431 /]# pwd / [root@349163a6e431 /]# df -hT Filesystem Type Size Used Avail Use% Mounted on overlay overlay 17G 4.3G 13G 26% / tmpfs tmpfs 64M 0 64M 0% /dev shm tmpfs 63M 0 63M 0% /dev/shm tmpfs tmpfs 356M 6.0M 350M 2% /etc/hosts devtmpfs devtmpfs 4.0M 0 4.0M 0% /proc/keys [root@349163a6e431 /]# cat /etc/redhat-release Red Hat Enterprise Linux release 9.4 (Plow) Check to confirm the port mapping from server30. [conadm@server30 ~]$ sudo podman port rootful-cont- port 8080/tcp -\u0026gt; 0.0.0.0:80 Do not remove the container yet. Lab: Launch Nameless Rootless Container with Two Variables # As conadm on server30, launch a container using the latest version of ubi8 in interactive mode (-it) with two environment variables VAR1=lab1 and VAR2=lab2 defined. [conadm@server30 ~]$ podman run -d -e VAR1=\u0026#34;lab1\u0026#34; -e VAR2=\u0026#34;lab2\u0026#34; --name variables8 ubi8 Check the variables from within the container. [root@803642faea28 /]# echo $VAR1 lab1 [root@803642faea28 /]# echo $VAR2 lab2 Delete the container and the image when done. Lab: Launch Named Rootless Container with Persistent Storage # As conadm with sudo (where required) on server30, create a directory called /host_perm1 with full permissions, and a file called str1 in it. [conadm@server30 ~]$ sudo mkdir /host_perm1 [sudo] password for conadm: [conadm@server30 ~]$ sudo chmod 777 /host_perm1 [conadm@server30 ~]$ sudo touch /host_perm1/str1 Launch a container called rootless-cont-str in attached terminal mode with the created directory mapped to /cont_perm1 inside the container. [conadm@server30 ~]$ sudo podman run --name rootless-cont-str -v /host_perm1:/cont_perm1:Z -it ubi8 [root@a1326200eae1 /]# While in the container, check access to the directory and the presence of the file. [root@a1326200eae1 /]# ls /cont_perm1 str1 Create a sub-directory and a file under /cont_perm1 and exit out of the container shell. [root@a1326200eae1 cont_perm1]# mkdir permdir2 [root@a1326200eae1 cont_perm1]# ls permdir2 str1 [root@a1326200eae1 cont_perm1]# exit exit [conadm@server30 ~]$ List /host_perm1 on server30 to verify the sub-directory and the file. [conadm@server30 ~]$ sudo ls /host_perm1 permdir2 str1 Stop and delete the container. [conadm@server30 ~]$ podman stop rootless-cont-str rootless-cont-str [conadm@server30 ~]$ podman rm rootless-cont-str rootless-cont-str Remove /host_perm1. [conadm@server30 ~]$ sudo rm -r /host_perm1 Lab: Launch Named Rootless Container with Port Mapping, Environment Variables, and Persistent Storage # As conadm with sudo (where required) on server30, launch a named rootless container called rootless-cont-adv in attached mode with two variables (HISTSIZE=100 and MYNAME=RedHat), host port 9000 mapped to container port 8080, and /host_perm2 mounted at /cont_perm2 [conadm@server30 ~]$ podman run --name rootless-cont-adv -v ~/host_perm2:/cont_perm2:Z -e HISTSIZE=\u0026#34;100\u0026#34; -e MYNAME=\u0026#34;RedHat\u0026#34; -p 9000:8080 -it --replace ubi8 [root@79e965cd1436 /]# Check and confirm the settings while inside the container. [root@79e965cd1436 /]# echo $HISTSIZE 100 [root@79e965cd1436 /]# echo $MYNAME RedHat [root@79e965cd1436 /]# ls -ld /cont_perm2 drwxrwxrwx. 2 root root 6 Aug 4 02:16 /cont_perm2 [conadm@server30 ~]$ podman port rootless-cont-adv 8080/tcp -\u0026gt; 0.0.0.0:9000 Exit out of the container. [root@5d510a1b2293 /]# exit exit [conadm@server30 ~]$ Do not remove the container yet. Lab 22-5: Control Rootless Container States via systemd # As conadm on server30, use the rootless-cont-adv container launched in the last lab as a template and generate a systemd service configuration file and store the file in the appropriate directory. [conadm@server30 ~]$ podman run --name rootless-cont-adv -v ~/host_perm2:/cont_perm2:Z -e HISTSIZE=\u0026#34;100\u0026#34; -e MYNAME=\u0026#34;RedHat\u0026#34; -p 9000:8080 -dt --replace ubi8 da8faf434813242985b8e332dc06b0e6da78e7125bc36579ffc8d82b0bcafb8e [conadm@server30 ~]$ podman generate systemd --new --name rootless-cont-adv \u0026gt; ~/.config/systemd/user/rootless-container.service DEPRECATED command: It is recommended to use Quadlets for running containers and pods under systemd. Please refer to podman-systemd.unit(5) for details. Stop and remove the source container rootless-cont-adv. [conadm@server30 ~]$ podman stop rootless-cont-adv rootless-cont-adv [conadm@server30 ~]$ podman rm rootless-cont-adv rootless-cont-adv Add the support for the new service to systemd and enable the new service to auto-start at system reboots. [conadm@server30 ~]$ systemctl --user daemon-reload [conadm@server30 user]$ systemctl --user enable -- now rootless-container.service Created symlink /home/conadm/.config/systemd/user/default.target.want s/rootless-container.service → /home/conadm/.config/systemd/user/rootless- container.service. Perform the required setup to ensure the container is launched without the need for the conadm user to log in. [conadm@server30 user]$ loginctl enable-linger [conadm@server30 user]$ loginctl show-user conadm | grep -i linger Linger=yes Reboot server30 and confirm a successful start of the container service and the container. [root@rhcsa3 ~]# systemctl --user --machine=conadm@ list-units --type=service UNIT LOAD ACTIVE SUB DESCRIPTION \u0026gt; dbus-broker.service loaded active running D-Bus User Message Bus rootless-cont-adv.service loaded active running Podman container-rootl\u0026gt; systemd-tmpfiles-setup.service loaded active exited Create User\u0026#39;s Volatile\u0026gt; LOAD = Reflects whether the unit definition was properly loaded. ACTIVE = The high-level unit activation state, i.e. generalization of SUB. SUB = The low-level unit activation state, values depend on unit type. 3 loaded units listed. Pass --all to see loaded but inactive units, too. To show all installed unit files use \u0026#39;systemctl list-unit-files\u0026#39;. [root@rhcsa3 ~]# sudo -i -u conadm podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a48fd2c25be4 registry.access.redhat.com/ubi9:latest /bin/bash 10 minutes ago Up 10 minutes 0.0.0.0:9000-\u0026gt;8080/tcp rootless-cont-adv Lab 22-6: Control Rootful Container States via systemd # As conadm with sudo where required on server10, use the rootful-cont-port container launched in Lab 22-1 as a template and generate a systemd service configuration file and store the file in the appropriate directory. [root@server30 ~]# podman generate systemd --new -- name rootful-cont-port | tee /etc/systemd/system/rootful-cont-port.service DEPRECATED command: It is recommended to use Quadlets for running containers and pods under systemd. Please refer to podman-systemd.unit(5) for details. # container-rootful-cont-port.service # autogenerated by Podman 4.9.4-rhel # Sat Aug 3 20:49:32 MST 2024 [Unit] Description=Podman container-rootful-cont- port.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm \\ --sdnotify=conmon \\ -d \\ --replace \\ -it \\ --name rootful-cont-port \\ -p 80:8080 ubi9 ExecStop=/usr/bin/podman stop \\ --ignore -t 10 \\ --cidfile=%t/%n.ctr-id ExecStopPost=/usr/bin/podman rm \\ -f \\ --ignore -t 10 \\ --cidfile=%t/%n.ctr-id Type=notify NotifyAccess=all [Install] WantedBy=default.target Stop and remove the source container rootful-cont-port. [root@server30 ~]# podman stop rootful-cont-port WARN[0010] StopSignal SIGTERM failed to stop container rootful-cont-port in 10 seconds, resorting to SIGKILL rootful-cont-port [root@server30 ~]# podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fe0d07718dda registry.access.redhat.com/ubi9:latest /bin/bash 16 minutes ago Up 16 minutes rootful-container [root@server30 ~]# podman rm rootfil-cont-port Error: no container with ID or name \u0026#34;rootfil-cont- port\u0026#34; found: no such container [root@server30 ~]# podman rm rootful-cont-port rootful-cont-port Add the support for the new service to systemd and enable the service to auto-start at system reboots. [root@server30 ~]# systemctl daemon-reload [root@server30 ~]# systemctl enable --now rootful- cont-port Created symlink /etc/systemd/system/default.target.wants/rootful- cont-port.service → /etc/systemd/system/rootful-cont- port.service. Reboot server10 and confirm a successful start of the container service and the container. [root@server30 ~]# reboot [root@server30 ~]# podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5c030407a7d6 registry.access.redhat.com/ubi9:latest /bin/bash About a minute ago Up About a minute 0.0.0.0:80- \u0026gt;8080/tcp rootful-cont-port 9d1e8a429ac6 registry.access.redhat.com/ubi9:latest /bin/bash About a minute ago Up About a minute rootful-container [root@server30 ~]# Lab 22-7: Build Custom Image Using Containerfile # As conadm on server10, write a containerfile to use the latest version of ubi8 and create a user account called user-in-container in the resultant custom image. [conadm@server30 ~]$ vim containerfile FROM registry.access.redhat.com/ubi8/ubi:latest RUN useradd -ms /bin/bash -u 1001 user-in-container USER 1001 [conadm@server30 ~]$ podman image build -f containerfile --no-cache -t ubi8-user . STEP 1/3: FROM registry.access.redhat.com/ubi8/ubi:latest STEP 2/3: RUN useradd -ms /bin/bash -u 1001 user-in- container --\u0026gt; b330095e91eb STEP 3/3: USER 1001 COMMIT ubi8-user --\u0026gt; e8cde30fc020 Successfully tagged localhost/ubi8-user:latest e8cde30fc020051caa2a4e2f58aaaf90f088709462a1314b936fd608facfdb5e Test the image by launching a container in interactive mode and verifying the user. [conadm@server30 ~]$ podman run -ti --name test12 ubi8-user [user-in-container@30558ffcb227 /]$ ","externalUrl":null,"permalink":"/tech/podman/containers/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003eIntroduction to Containers\n    \u003cdiv id=\"introduction-to-containers\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#introduction-to-containers\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eTake advantage of the native virtualization features available in the Linux kernel.\u003c/li\u003e\n\u003cli\u003eEach container typically encapsulates one self-contained application that includes all dependencies such as library files, configuration files, software binaries, and services.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTraditional server/ application deployment:\u003c/p\u003e","title":"Containers","type":"tech"},{"content":"Friday, October 29, 2021 2:40 PM\n○ Matching an Ethernet frame’s destination Media Access Control (MAC) address to the MAC address table (Layer 2 switches) Matching an IP packet’s destination IP address to the IP routing table (routers, Layer 3 switches) ○ # Encrypting the data and adding a new IP header (for virtual private network [VPN] processing) ○ # ○ Changing the source or destination IP address (for Network Address Translation [NAT] processing)\n○ Discarding a message due to a filter (access control lists [ACLs], port security)\nThe Control Plane\nany action that controls the data plane.\ncreating the tables used by the data plane, tables like the IP routing table, an IP Address Resolution Protocol (ARP) table, a switch MAC address table, and so on.\nadding to, removing, and changing entries to the tables used by the data plane\ncommon control plane protocols:\nRouting protocols (OSPF, Enhanced Interior Gateway Routing Protocol [EIGRP], Routing\nInformation Protocol [RIP], Border Gateway Protocol [BGP])\nIPv4 ARP\nIPv6 Neighbor Discovery Protocol (NDP)\nSwitch MAC learning\nSTP\nThe Management Plane\nincludes protocols that allow network engineers to manage the devices.\nTelnet and Secure Shell (SSH)are two of the most obvious management plane protocols.\nCisco Switch Data Plane Internals\nswitch uses a ternary contentspecialized type of memory to store the equivalent of the MAC address table: -addressable memory (TCAM).\nthe ASIC can feed the fields to be matched, like a MAC address value, into the TCAM, and the TCAM returns the matching table entry, without a need to run a search algorithm.\nMost of the control and management plane functions run in IOS. The data plane function (and the\ncontrol plane function of MAC learning) happens in the ASIC.\nControllers and Software-Defined Architecture\nControllers and Centralized Control\nTo do their work, those distributed control plane processes use messages to communicate\nwith each othernetworks are said to use a , like OSPF protocol messages between routers. As a result, traditional distributed control plane.\ncentralized control plane,with its foundations in a service called a controller.\nA controller, or SDN controller, centralizes the control of the networking devices\nthe controller can perform all control plane functions, replacing the devices’ distributed control plane. Alternately, the controller can simply be aware of the ongoing work of the\ndistributed data, control, and management planes on the devices, without changing how those operate. And the list goes on, with many variations.\nThe networking devices do not populate their forwarding tables with traditional distributed\nThe networking devices do not populate their forwarding tables with traditional distributed control plane processes The Southbound Interface\nan interface between the controller and those devices, and given its location at the bottom part of\ndrawings,\nin SDN, the word interface (including in the names of SBI, NBI, and API) refers to software\ninterfaces\noften includes a protocol, so that the controller and devices can communicate, but it often includes an application programming interface (API). An APIis a method for one application\n(program) to exchange data with another application.\nAPI is an interface to an application program. Programs process data, so an API lets two programs\nexchange data\nfunctions, variables, and data structurescopy structured data between the programs across a network.—that can be used by one program to communicate and\nan interface between a program (the controller) and a program (on the networking device) that lets the two programs communicate,\none goal being to allow the controller to program the data plane forwarding tables of the\nnetworking device.\nsome controllers might support one or a few SBIs, for a specific purpose, while others might support many more SBIs, allowing a choice of SBIs to use\nthree sample architectures that happen to show three separate SBIs, specifically:\nOpenFlow (from the ONF; http://www.opennetworking.org))\n(SBI) OpFlex (from Cisco; used with ACI)\nCLI (Telnet/SSH) and SNMP (used with Cisco APIC-EM)\nCLI (Telnet/SSH) and SNMP, and NETCONF (used with Cisco Software-Defined Access)\nThe Northbound Interface\nThe Northbound Interface\nhow does the controller know what to add? How does it choose? What kind of information would your program need to gather before it could attempt to add something like MAC table entries or\nIP routes to a network?\n○ A list of all the devices in the network\n○ The capabilities of each devices\n○ The interfaces/ports on each device\n○ The current state of each port\n○ The topology—which devices connect to which, over which interfaces\n○ Device configuration—IP addresses, VLANs, and so on as configured on the devices\nA controller gathers all sorts of useful information about the network, like the items in the previous list. The controller itself can create a centralized repository of all this useful information\nabout the network.\nNBI) opens the controller so its data and functions can be used by other programs, enabling\nnetwork programmability\nPrograms can pull information from the controller, using the controller’s APIs. The NBIs also enable programs to use the controller’s capabilities to program flows into the devices using the\ncontroller’s SBIs.\nAn application can run on the same server as the controller and use an NBI, which is an API, so\nthat two programs can communicate.\nREST (Representational State Transfer)different hosts, using HTTP messages to transfer data over the APIdescribes a type of API that allows applications to sit on application running on the same system as the controller, the API does not need to send messages over a network because both programs run on the same system. over a network because both programs run on the same system. when the application runs on a different system somewhere else in the network other than running on the controller, the API needs a way to send the data back and forth over an IP network, and RESTful APIs meet that need REST API HTTP GET request to a particular URI. The HTTP GET is like any other HTTP GET, even like those used to retrieve web pages. the URI is not for a web page, but rather identifies an object on the controller, typically a data structure that the application needs to learn and then process the URI might identify an object that is the list of physical interfaces on a specific device along with the status of each. the response holds variable names and their values, in a format that can be easily used by a program. The common formats for data used for network programmabilityare JavaScript Object\nNotation (JSON) and eXtensible Markup Language (XML), shown as step 3.\nExamples of Network Programmability and SDN\nOpenDaylight(Controller) and OpenFlow (SBI) # defines the concept of a controller along with an IP-based SBI between the controller and the network devices. defines a standard idea of what a switch’s capabilities are, based on the ASICs and TCAMs commonly used in switches today. (switch abstraction) An with great flexibility beyond the traditional model of a Layer 2/3 switch.OpenFlowswitch can act as a Layer 2 switch, a Layer 3 switch, or in different ways and centralizes most control plane functions, with control of the network done by the controller plus any applications that use the controller’s NBIs. applications may use any APIs (NBIs) supported on the controller platform to dictate what applications may use any APIs (NBIs) supported on the controller platform to dictate what kinds of forwarding table entries are placed into the devices; however, it calls for OpenFlow\nas the SBI protocolOpenFlow .Additionally,the networking devices need to be switches that support\na controller with an OpenFlow SBI, the controller plays a big role in the network. The next few pages provide a brief background about two such controllers.\nThe OpenDaylightController\nopen-source\nageneralized version of the ODL architecture.\nThe Cisco Open SDN Controller (OSC)\na commercial version of the OpenDaylight controller\nfollowed the intended model for the ODL project:\nno longer produces and sells the Cisco OSC\ntwo Cisco offerings that use an IBN approach to SDN\nCisco Application Centric Infrastructure (ACI)\nCisco’s current SDN offerings of Access (SDA) in the enterprise campusACI in the data center, and Software-Defined WAN (SD, Software-Defined -\nWAN) in the enterprise WAN.\nCisco made the network infrastructure become application centric, hence the name of the Cisco data center SDN solution: Application Centric\nInfrastructure, or ACI.\nhow ACI creates a powerful and flexible network to support a modern\ndata center in which VMs and containers are created, run, move, and are stopped dynamically as a matter of routine.\nACI Physical Design: Spine and Leaf uses a specific physical switch topology called spine and leaf. also called a Clos network  Each leaf switch must connect to every spine switch.  Each spine switch must connect to every leaf switch.  Leaf switches cannot connect to each other.  Spine switches cannot connect to each other.  Endpoints connect only to the leaf switches. The center, like the router on the leftendpoints can be connections to devices outside the data most of the endpoints will be either physical servers running a native OS or servers running virtualization software with numbers of VMs and containers as shown in the center of the figure. ACI Operating Model with Intent-Based Networking\nendpoints are the VMs, containers, or even traditional servers with\nthe OS running directly on the hardware. ACI then uses several constructs as implemented via the Application Policy Infrastructure\nController (APIC)controller for ACI., the software that serves as the centralized\none web app often exists as three separate servers:\nWeb server web server, which sends web page content to the user.: Users from outside the data center connect to a\nApp (Application) server dynamic content, the app server does the processing to build : Because most web pages contain\nthe next web page for that particular user based on the user’s profile and latest actions and input.\nDB (Database) server: Many of the app server’s actions\nrequire data; the DB server retrieves and stores the data as requested by the app server.\nthe engineer, or some automation program, defines the policies and intent for which endpoints should be allowed to communicate and which should not. Then the controller determines what that means for this network at this moment in time, depending on where the endpoints are right now. intent-based networking (IBN) model. when starting the VMs for this app, the virtualization software would create (via the APIC) several endpoint groups (EPGs) as shown in Figure 16-11. The controller must also be told the access policies, which define which EPGs should be able to communicate (and which should not), as implied in the figure with arrowed lines. For example, the routers that connect to the network external to For example, the routers that connect to the network external to the data center should be able to send packets to all web servers,\nbut not to the app servers or DB servers.\nACI uses a centralized controller called the Application Policy Infrastructure Controller (APIC),\nit is the controller that creates application policies for the data center infrastructure. The APIC takes the intent (EPGs, policies, and so on), which completely changes the operational model away from configuring VLANs, trunks, EtherChannels, ACLs, and so on. (Some control plane in switches) Cisco APIC Enterprise Module (APIC-EM)\na Cisco SDN solution:\nAPIC Enterprise Module (APICbegan to reimagine networking in the enterprise, they saw a -EM), solves a different problem. When Cisco huge barrier: the\ninstalled base of their own products in most of their customer’s networks.\nAPIC-EM Basics\noffer enterprise SDN using the same switches and routers already\ninstalled in networks.\nIt includes these applications,\nTopology map: of the network.The application discovers and displays the topology\nPath Trace: the application shows the path through the network, along with The user supplies a source and destination device, and\ndetails about the forwarding decision at each step.\nPlug and Play: that you can unbox a new device and make it IP reachable through This application provides Day 0 installation support so\nautomation in the controller.\nEasy QoS: With a few simple decisions at the controller, you can\nconfigure complex QoS features at each device.\ndoes not directly program the data or control planes, but it does interact with the management plane via Telnet, SSH, and/or SNMP; APIC-EM Replacement\nmany of the functions of APICCisco DNA Center (DNAC) product-EM have become core features of the\nlist of applications just above this chapter’s Figure 16-14 also exist\nas part of DNAC, for instance.\nComparing Traditional Versus Controller-Based Networks\nConfiguration management refers to any feature that changes device configuration, with automated configuration management doing so with software (program) control.\nOperational network management includes monitoring, gathering operational data, reporting, and\nalerting humans to possible issues\nHow Automation Impacts Network Management\nCentralized controllers formalize and define data models for the configuration and operational data about networks.\nNorthbound APIs and their underlying data models make it much easier to automate functions versus traditional networks.\nThe robust data created by controllers makes it possible to automate functions that were not\neasily automated without controllers.\nThe new reimagined software defined networks that use new operational models simplify operations, with automation resulting in more consistent configuration and less errors.\nCentralized collection of operational data at controllers allows the application of modern data analytics to networking operational data, providing actionable insights that were likely not\nnoticeable with the former model.\nTime required to complete projects is reduced.\nNew operational models use external inputs, like considering time-of-day, day-of-week, and\nnetwork load.\nComparing Traditional Networks with Controller-Based Networks\nthree most likely to be seen from Cisco in an enterprise: Software-Defined Access (SDA), Software-\nDefined WAN (SDSDA.) -WAN), and Application Centric Infrastructure (ACI). (Chapter 17 introduces\nThe network engineer does not need to think about every command on every device.\nThe controller configures the devices with consistent and streamlined settings.\nThe result: faster and more consistent changes with fewer issues.\nUses new and improved operational models that allow the configuration of the network rather than per-device configuration\nEnables automation through northbound APIs that provide robust methods and modeldata -driven\nConfigures the network devices through southbound APIs, resulting in more consistent device\nconfiguration, fewer errors, and less time spent troubleshooting the network\nEnables a DevOps approach to networks\n","externalUrl":null,"permalink":"/tech/networking/controllerbasednetworking/","section":"Teches","summary":"\u003cp\u003eFriday, October 29, 2021 2:40 PM\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e○ Matching an Ethernet frame’s destination Media Access Control (MAC) address to the MAC address table (Layer 2 switches)\nMatching an IP packet’s destination IP address to the IP routing table (routers, Layer 3\nswitches)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch6 class=\"relative group\"\u003e○\n    \u003cdiv id=\"heading\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#heading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h6\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eEncrypting the data and adding a new IP header (for virtual private network [VPN]\nprocessing)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch6 class=\"relative group\"\u003e○\n    \u003cdiv id=\"heading-1\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#heading-1\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h6\u003e\n\u003cp\u003e○ Changing the source or destination IP address (for Network Address Translation [NAT] processing)\u003cbr\u003e\n○ Discarding a message due to a filter (access control lists [ACLs], port security)\u003cbr\u003e\nThe Control Plane\u003cbr\u003e\nany action that controls the data plane.\u003cbr\u003e\ncreating the tables used by the data plane, tables like the IP routing table, an IP Address Resolution Protocol (ARP) table, a switch MAC address table, and so on.\u003cbr\u003e\nadding to, removing, and changing entries to the tables used by the data plane\u003c/p\u003e","title":"Controller-Based Networking","type":"tech"},{"content":"In order to support compose, podman needs to expose it\u0026rsquo;s REST API service through a local UNIX socket. This supports Docker-compatible APIs and native Libpod APIs.\nInstall required packages:\n- podlet - podman-compose Enable podman-socket:\nplausible-ce on  v3.1.0 ❯ systemctl --user enable --now podman.socket Created symlink \u0026#39;/var/home/davidt/.config/systemd/user/sockets.target.wants/podman.socket\u0026#39; → \u0026#39;/usr/lib/systemd/user/podman.socket\u0026#39;. plausible-ce on  v3.1.0 ❯ systemctl --user status podman.socket ● podman.socket - Podman API Socket Loaded: loaded (/usr/lib/systemd/user/podman.socket; enabled; preset: disabled) Active: active (listening) since Mon 2025-11-24 04:23:24 MST; 2s ago Invocation: 3b507e6408b8497481567234857b057e Triggers: ● podman.service Docs: man:podman-system-service(1) Listen: /run/user/1000/podman/podman.sock (Stream) Nov 24 04:23:24 bluefin systemd[4543]: Listening on podman.socket - Podman API Socket. Clone plausible to your project directory: git clone -b v3.1.0 --single-branch https://github.com/plausible/community-edition plausible-ce\nCreate environment variable file: touch .env\nAdd the URL for the Plausible site: echo \u0026quot;BASE_URL=https://analytics.linuxreader.com\u0026quot; \u0026gt;\u0026gt; .env\nExpose the container to the internet:\necho \u0026#34;HTTP_PORT=80\u0026#34; \u0026gt;\u0026gt; .env echo \u0026#34;HTTPS_PORT=443\u0026#34; \u0026gt;\u0026gt; .env Convert the data volumes to easier to manage files and add :z to each volume for SELinux. Example:\nvolumes: - event-data:/var/lib/clickhouse Would be:\nvolumes: ${HOME}/Documents/data/event-data:/var/lib/clickhouse:z For the volumes with :ro at the end, replace :ro with :z\n./clickhouse/low-resources.xml:/etc/clickhouse-server/config.d/low-resources.xml:z\nand - ./clickhouse/ipv4-only.xml:/etc/clickhouse-server/config.d/ipv4-only.xml:z\nThis way I can stop the containers and take backups of the data easier.\nRun: podman compose up -d\nAdd a local firewall forward:\n- name: Forward 8443 traffic to port 443 ansible.posix.firewalld: permanent: true immediate: true state: enabled port_forward: - port: 8443 toport: 443 proto: tcp ","externalUrl":null,"permalink":"/tech/podman/convert-docker-compose-to-quadlets/","section":"Teches","summary":"\u003cp\u003eIn order to support compose, podman needs to expose it\u0026rsquo;s REST API service through a local UNIX socket. This supports Docker-compatible APIs and native Libpod APIs.\u003c/p\u003e\n\u003cp\u003eInstall required packages:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003epodlet\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003epodman-compose\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEnable podman-socket:\u003c/p\u003e","title":"Convert Docker Compose to Quadlets","type":"tech"},{"content":"This chapter covers the following subjects:\n• Using Modules to Manipulate Files • Managing SELinux Properties • Using Jinja2 Templates\nRHCE exam topics # • Use Ansible modules for system administration tasks that work with: • File contents • Use advanced Ansible features • Create and use templates to create customized configuration files\nUsing Modules to Manipulate Files # File Module Manipulation Overview # Common modules to manipulate files copy\nCopies files to remote locations fetch Fetches files from remote locations file Manage file and file properties Create new files or directories Create links Remove files Set permissions and ownership acl\nWork with file system ACLs find Find files based on properties lineinfile Manages lines in text files blockinfile Manage blocks in text files replace Replaces strings in text files based on regex synchronize Performs rsync-based synchronization tasks stat Retrieves file or file system status enables you to retrieve file status information. gets status information and is not used to change anything use it to check specific file and perform an action if the properties are not set as expected. Shows: which permission mode is set, whether it is a link, which checksum is set on the file etc. See ansible-doc stat for list of full output Lab: View information about /etc/hosts file # - name: stat module tests hosts: ansible1 tasks: - stat: path: /etc/hosts register: st - name: show current values debug: msg: current value of the st variable is {{ st }} Lab: write a message if the expected permission mode is not set. # --- - name: stat module test hosts: ansible1 tasks: - command: touch /tmp/statfile - stat: path: /tmp/statfile register: st - name: show current values debug: msg: current value of the st variable is {{ st }} - fail: msg: \u0026#34;unexpected file mode, should be set to 0640\u0026#34; when: st.stat.mode != \u0026#39;0640\u0026#39; Lab: Use the file Module to Correct File Properties Discovered with stat # --- - name: stat module tests hosts: ansible1 tasks: - command: touch /tmp/statfile - stat: path: /tmp/statfile register: st - name: show current values debug: msg: current value of the st variable is {{ st }} - name: changing file permissions if that\u0026#39;s needed file: path: /tmp/statfile mode: 0640 when: st.stat.mode != \u0026#39;0640\u0026#39; Managing File Contents # Use lineinfile or blockinfile instead of copy to manage text in a file\nLab: Change a string, based on a regular expression. # --- - name: configuring SSH hosts: all tasks: - name: disable root SSH login lineinfile: dest: /etc/ssh/sshd_config regexp: \u0026#34;^PermitRootLogin\u0026#34; line: \u0026#34;PermitRootLogin no\u0026#34; notify: restart sshd handlers: - name: restart sshd service: name: sshd state: restarted Lab: Manipulate multiple lines # --- - name: modifying file hosts: all tasks: - name: ensure /tmp/hosts exists file: path: /tmp/hosts state: touch - name: add some lines to /tmp/hosts blockinfile: path: /tmp/hosts block: | 192.168.4.110 host1.example.com 192.168.4.120 host2.example.com state: present When blockinfile is used, the text specified in the block is copied with a start and end indicator.\n[ansible@ansible1 ~]$ cat /tmp/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.122.201 ansible1 192.168.122.202 ansible2 192.168.122.203 ansible3 # BEGIN ANSIBLE MANAGED BLOCK 192.168.4.110 host1.example.com 192.168.4.120 host2.example.com # END ANSIBLE MANAGED BLOCK Lab: Creating and Removing Files # Use the file module to create a new directory and in that directory create an empty file, then remove the directory recursively.\n--- - name: using the file module hosts: ansible1 tasks: - name: create directory file: path: /newdir owner: ansible group: ansible mode: 770 state: directory - name: create file in that directory file: path: /newdir/newfile state: touch - name: show the new file stat: path: /newdir/newfile register: result - debug: msg: | This shows that newfile was created \u0026#34;{{ result }}\u0026#34; - name: removing everything again file: path: /newdir state: absent state: absent recursively removes the directory. Moving Files Around # copy module copies a file from the Ansible control host to a managed machine.\nfetch module enables you to do the opposite\nsynchronize module performs Linux rsync-like tasks, ensuring that a file from the control host is synchronized to a file with that name on the managed host.\ncopy module always creates a new file, whereas the synchronize module updates a current existing file.\nLab: Moving a File Around with Ansible # --- - name: file copy modules hosts: all tasks: - name: copy file demo copy: src: /etc/hosts dest: /tmp/ - name: add some lines to /tmp/hosts blockinfile: path: /tmp/hosts block: | 192.168.4.110 host1.example.com 192.168.4.120 host2.example.com state: present - name: verify file checksum stat: path: /tmp/hosts checksum_algorithm: md5 register: result - debug: msg: \u0026#34;The checksum of /tmp/hosts is {{ result.stat.checksum }}\u0026#34; - name: fetch a file fetch: src: /tmp/hosts dest: /tmp/ Ansible creates a subdirectory on the control node for each managed host in the dest directory and puts the file that fetch has copied from the remote host in that subdirectory: /tmp/ansible1/tmp/hosts /tmp/ansible2/tmp/hosts Lab: Managing Files with Ansible # 1. Create a file with the name exercise81.yaml and give it the following play header: 2. Add a task that creates a new empty file: 3. Use the stat module to check on the status of the new file: 4. To see what the status module is doing, add a line that uses the debug module: 5. Now that you understand which values are stored in newfile, you can add a conditional play that changes the current owner if not set correctly: 6. Add a second play to the playbook that fetches a remote file: 7. Now that you have fetched the file so that it is on the Ansible control machine, use blockinfile to edit it: 8. In the final step, copy the modified file to ansible2 by including the following play: 9. At this point you\u0026rsquo;re ready to run the playbook. Type ansible-playbook exercise81.yaml to run it and observe the results. 10. Type ansible ansible2 -a \u0026quot;cat /tmp/motd\u0026quot; to verify that the modified motd file was successfully copied to ansible2.\n--- - name: testing file manipulation skills hosts: ansible1 tasks: - name: create new file file: name: /tmp/newfile state: touch - name: check the status of the new file stat: path: /tmp/newfile register: newfile - name: for debugging only debug: msg: the current values for newfile are {{ newfile }} - name: change file owner if needed file: path: /tmp/newfile owner: ansible when: newfile.stat.pw_name != \u0026#39;ansible\u0026#39; - name: fetching a remote file hosts: ansible1 tasks: - name: fetch file from remote machine fetch: src: /etc/motd dest: /tmp - name: adding text to the text file that is now on localhost hosts: localhost tasks: - name: add a message blockinfile: path: /tmp/ansible1/etc/motd block: | welcome to this server for authorized users only state: present - name: copy the modified file to ansible2 hosts: ansible2 tasks: - name: copy motd file copy: src: /tmp/ansible1/etc/motd dest: /tmp ","externalUrl":null,"permalink":"/tech/ansible/deploying-files/","section":"Teches","summary":"\u003cp\u003eThis chapter covers the following subjects:\u003c/p\u003e\n\u003cp\u003e• Using Modules to Manipulate Files\n• Managing SELinux Properties\n• Using Jinja2 Templates\u003c/p\u003e\n\n\u003ch3 class=\"relative group\"\u003eRHCE exam topics\n    \u003cdiv id=\"rhce-exam-topics\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#rhce-exam-topics\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003e• Use Ansible modules for system administration tasks that work with:\n• File contents\n• Use advanced Ansible features\n• Create and use templates to create customized configuration files\u003c/p\u003e","title":"Deploying files","type":"tech"},{"content":"To configure a router or switch to logging host {address | hostname } send log messages to a syslog server global command.\n*Dec 18 17:10:15.079: %LINEPROTOchanged state to down - 5 - UPDOWN: Line protocol on Interface FastEthernet0/0, A timestamp: *Dec 18 17:10:15. The facility on the router that generated the message : %LINEPROTO The severity level : 5 A mnemonic for the message : UPDOWN The description of the message: Line protocol on Interface FastEthernet0/0, changed state to down you can at least toggle on and off the use of thelog message sequence number (which is not enabled by default)timestamp (which is included by default). Example 9-1 reverses those and a defaults by turning off timestamps and turning on sequence numbers. Log Message Format\nLog Message Severity Levels\nthe lower the number, the more severe the event that caused the message.\nlast level in the figure is used for messages requested by the debug command Table 9severity level for each type.-2 summarizes the configuration commands used to enable logging and to set the For example, the command logging console 4 causes IOS to send severity level 0the console –4 messages to Configuring and Verifying System Logging\nthe example configures the same message level at the console and for terminal monitoring\n(level 7, or debug), and the same level for both buffered and logging to the syslog server (level 4, or warning). The levels may be set using the numeric severity level or the name as\nshown earlier in Figure 9-3.\nshow logging messages per the logging buffered configuration.commandconfirms those same configuration settings and also lists the log\nIf any log messages had been buffered, the actual log messages would be listed at\nthe end of the command\nclear out the old messages from the log with the clear logging EXEC command.\nThe debug Command and Log Messages\ndebug internal events, with that monitoring process continuing over time, so that IOS can issue log EXEC command gives the network engineer a way to ask IOS to monitor for certain\nmessages when those events occur\ndebug remains active until some user issues the no debug command with the same parameters,\ndisabling the debug.\nanyone logged in with SSH at the time Example 9the output, even with the logging monitor debug - 4’s output was gathered would not have seen command configured on router R1, without first issuing a terminal monitor command. all enabled debug options use router CPU, which can cause problems for the router. You can monitor CPU use withtheshow process cpucommand use caution when using debug commands carefully on production devices. the more CLI users that receive debug messages, the more CPU that is consumed. Network Time Protocol (NTP) # Devices send timestamps to each other with NTP messages, continually exchanging messages, with one device changing its clock to match the other, eventually synchronizing the clocks. How NTP defines the sources of time data (stratum). (reference clocks) andhow good each time source is Setting the Time and Timezone\nNTP works best if you set the device clock to a reasonably close time before enabling the NTP\nclient functionwith the ntp server command\nI shouldset the time to 8:52 p.m., set the correct date and timezone, and even tell the device to\nadjust for daylight savings time—and then enable NTP\nExample 9-7 shows how to set the date, time, timezone, and daylight savings time.\nYou should set the first two commands before setting the time of day with the EXEC command because the two configuration commands impact the time that is set. clock set chose EDT because it is the acronym for daylight savings time in that same EST time zone. Finally, thehour automatically over the years.recurring keyword tells the router to spring forward an hour and fall back an clock set EXEC command uses a time syntax with a 24-hour format, not with a 12-hour format plus a.m./p.m.). uses a time syntax with a 24-hour format, not with a 12-hour format plus a.m./p.m.). The EDT, rather than UTC time. show clock command (issued seconds later)lists that time, but also notes the time as Basic NTP Configuration\ntwo ntp configuration commands\nntp master {stratum not as an NTP client. The device gets its time information from the internal clock on the - level}: NTP server mode—the device acts only as an NTP server, and\ndevice.\nntp server {address | hostname} : NTP client/server mode—the device acts as both client\nand serversynchronized, the device can then act as an NTP server, to supply time to other NTP. First, it acts as an NTP client, to synchronize time with a server. Once\nclients.\nshow ntp status command show ntp status command\nlists a status of synchronizedchanging its time to match the server’s time. , which confirms the NTP client has completed the process of Any router acting as an NTP client will list\n“unsynchronized” in that first line until the NTP synchronization process completes with at least one server. It also confirms the IP address of the server—this device’s reference\nclock—with the IP address configured in Example 9-8 (172.16.2.2).\nshow ntp associations lists all the NTP servers that the local device can attempt to use, with status information about the association between the local device (client) and the various NTP servers. NTP Reference Clock and Stratum # Devices that act solely as an NTP server get their time from either internal device hardware or from some external clock using mechanisms other than NTP. NTP servers and clients use a number to show the perceived accuracy of their reference clock data based on stratum level. The lower the stratum level, the more accurate the reference clock is considered to be.sets its own stratum level. Then,An NTP server that uses its internal hardware or external reference clock an NTP client adds 1 to the stratum level it learns from its NTP server, so that the stratum level increases the more hops away from the original clock source. For instance, back in Figure 9which references R3, adds 1 so it has a stratum of 3. R1 uses R2 as its NTP server, so R1 adds 1 to -5, you can see the NTP primary server (R3) with a stratum of 2. R2, have a stratum of 4. These increasing stratum levels allow devices to refer to several NTP servers and then use time information from the best NTP server, best being the server with the lowest stratum level. Routers and switches use the default stratum level of 8 for their internal reference clock based on the default setting of 8 for the stratum level in the command allows you to set a value from 1 through 15 ntp master [stratum ; in Example 9-8, the - ntp master 2level] command. The command set router R3’s stratum level to 2. Note NTP considers 15 to be the highest useful stratum level, so any devices that calculate their stratum as 16 consider the time data unusable and do not trust the time. So, avoid setting higher stratum values on the ntp master command. stratum values on the ntp master command. Redundant NTP Configuration # an enterprise could use NTP to reference NTP servers that use an atomic clock as their reference source, like the NTP primary servers in Figure 9-6, which happen to be run by the US National Institute of Standards and Technology (NIST) (see tf.nist.gov). AnNTP primary server acts only as a server, with a reference clock external to the device, and has a stratum level of 1secondary servers are servers that use client/server mode as described throughout this , like the two NTP primary servers shown in Figure 9-6.NTP section, relying on synchronization with some other NTP server. NTP primary server and NTP secondary server. After losing their reference clock, R1 and R2 could no longer be useful NTP servers to the rest of the enterprise. To overcome this potential issue, the routers can also be configured with the ntp master command, resulting in this logic: Establish an association with the NTP servers per thentp server command. Establish an association with your internal clock using the ntp master stratum command. Set the stratum level of the internal clock (per thea higher (worse) stratum level than the Internet-based NTP servers.ntp master {stratum-level} command) to Synchronize with the best (lowest) known time source, which will be one of the Internet NTP servers in this scenario ntp master 7 command, with a much higher stratum, NTP Using a Loopback Interface for Better Availability # what happens when one interface on R4 fails for any NTP clients that had referred to that specific IP address There would likely still be a route to reach R4 itself. The NTP client would not be able to send packets to the configured address because that interface is down. loopback interface to meet that exact need once configured, it remains in an up/up state as long as Key Topic.The router remains up. You do not issue a shutdown command on that loopback interface. Analyzing Topology Using CDP and LLDP # Examining Information Learned by CDP # Cisco-proprietary Layer 2 protocol does not rely on a working Layer 3 protocol does not rely on a working Layer 3 protocol\nDevices that support CDP learn information about others by listening for the advertisements sent by other devices.\nCDP discovers several useful details from the neighboring Cisco devices:\nDevice identifier : Typically the host name\nAddress list: Network and data-link addresses\nPort identifier: link that sent the CDP advertisementThe interface on the remote router or switch on the other end of the\nCapabilities list : Information on what type of device it is (for example, a router or a\nswitch)\nPlatform : The model and OS level running on the device\nCisco IP Phones use CDP to learn the data and voice VLAN IDs as configured on the access\nswitch.\nrouters and switches support the same CDP commands, with the same parameters and\nsame types of output.\nTo ensure all devices receive a CDP message, tdestination MAC address (0100.0CCC.CCCC). he Ethernet header uses a multicast\nthe device processes the message and then discards it, rather than forwarding it\nshow cdp neighbors detail\nlists the full name of the switch model (WSconfigured on the neighboring device. -2960XR-24TS-I) and the IP address\nThe show cdp entry name command lists the exact same details shown in the output of the command. show cdp neighbors detail command, but for only the one neighbor listed in the Cisco recommends that CDP be disabled on any interface that might not have a need for CDP. For switches, any switch port connected to another switch, a router, or to an IP phone should use CDP. Configuring and Verifying CDP # IOS typically enables CDP globally and on each interface by defaultinterface with the no cdp enable interface subcommand. You can then disable CDP per re-enable it with the cdp enable interface subcommand disable and re-enable CDP globally on the device no cdp run and cdp run global commands send time and the hold time. CDP sends messages every 60 seconds by default, with a hold time of 180 seconds.device before removing those details from the CDP tables. You can override the defaults with The hold time tells the device how long to wait after no longer hearing from a the cdp timer seconds and cdp holdtime seconds global commands, respectively. Examining Information Learned by LLDP # the LLDP output in the example does differ from CDP in a few important ways:\nLLDP uses B as the capability code for switching, referring to bridgetype that existed before switches that performed the same basic functions., a term for the device LLDP does not identify IGMP as a capability, while CDP does (I). CDP lists the neighbor’s platform, a code that defines the device type, while LLDP does not. LLDP lists capabilities with different conventions (see upcoming Example 9-19). System Capabilities : What the device can do Enabled Capabilities : What the device does now with its current configuration LLDP uses the same messaging concepts as CDP, encapsulating messages directly in dataheaders. Devices do not forward LLDP messages so that LLDP learns only of directly connected -link neighbors. LLDP does use a differentmulticast MAC address(0180.C200.000E) Cisco devices default to disable LLDP LLDP separates the sending and receiving of LLDP messages as separate functions. LLDP support processing receives LLDP messages on an interface so that the switch or router learns about the neighboring device while not transmitting LLDP messages to the neighboring device. the commands include options to toggle on|off the transmission of LLDP messages separately from the processing of received messages. [no] lldp run: A global configuration command thatsets the default mode of LLDP operationfor any interface that does not have more specific LLDP subcommands (lldp transmit, lldp receive). The lldp run global command enables LLDP in both directions on those interfaces, while no lldp Configuring and Verifying LLDP # The lldp run global command enables LLDP in both directions on those interfaces, while no lldp run disables LLDP.\n[no] lldp transmit : An interface sub-command that defines the operation of LLDP on the\ninterface subcommand causes the device to transmit LLDP messages, while no lldp transmit causes it to regardless of the global [no] lldp run command. The lldp transmit interface\nnot transmit LLDP messages.\n[no] lldp receive :An interface subcommand that defines the operation of LLDP on the interface\nregardless of the global [no] lldp run command.the device to process received LLDP messages, while no lldp receive causes it to not process The lldp receive interface subcommand causes\nreceived LLDP messages.\nshow lldp interface lists the interfaces on which LLDP is enabled.\nlike CDP,shows the default settings of 30 seconds for the send timer and 120 seconds for the hold timer. LLDP uses a send timer and hold timer for the same purposes as CDP.The example\nYou can override the defaults with the commands lldp timer seconds and lldp holdtime seconds global\n4.0 IP Services\n4.1 Configure and verify inside source NAT using static and pools\nCIDR\nmost public address assignments for the last 20 years have been a CIDR block rather than an entire class A, B, or C network. Private Addressing\nno organization is allowed to advertise these networks using a routing protocol on the Internet. NAT\nQoS: Managing Bandwidth, Delay, Jitter, and Loss # four characteristics of network traffic: Bandwidth refers to the speed of a link, in bits per second (bps) helps to also think of bandwidth as the capacity of the link, in terms of how many bits can be sent over the link per second. Delay the time between sending one packet and that same packet arriving at the destination host Round-trip delay Jitter the time it takes to send one packet between two hosts and receive one back variation in one-way delay between consecutive packets sent by the same application Loss the number of lost messages, usually as a percentage of packets sent. people think of loss as something caused by faulty cabling or poor WAN services. That is one cause. However, more loss happens because of the normal operation of the networking devices, in which the devices’ queues get too full, so the device has nowhere to put new packets, and it discards the packet. Types of Traffic\nData Applications\n","externalUrl":null,"permalink":"/tech/networking/device-management-protocols/","section":"Teches","summary":"\u003cp\u003eTo configure a router or switch to logging host {address | hostname } send log messages to a syslog server global command.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e*Dec 18 17:10:15.079: %LINEPROTOchanged state to down - 5 - UPDOWN: Line protocol on Interface FastEthernet0/0,\nA timestamp: *Dec 18 17:10:15.\nThe facility on the router that generated the message : %LINEPROTO\nThe severity level : 5\nA mnemonic for the message : UPDOWN\nThe description of the message: Line protocol on Interface FastEthernet0/0, changed\nstate to down\nyou can at least toggle on and off the use of thelog message sequence number (which is not enabled by default)timestamp (which is included by default). Example 9-1 reverses those and a\ndefaults by turning off timestamps and turning on sequence numbers.\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLog Message Format\u003c/p\u003e","title":"Device Management Protocols","type":"tech"},{"content":"Thursday, September 30, 2021 12:17 PM\nWatch for incoming DHCP messages, with destination IP address 255.255.255.255Change that packet’s source IP address to the router’s incoming interface IP address.. Change that packet’s destination IP address to the address of the DHCP server (as configured in the addres scommand). ip helper-\nRoute the packet to the DHCP server.\nthe server simply reverses the source and destination IP address of the packet received from the router\n(relay agent)\nthe Discover message lists source IP address 172.16.1.1, so the server sends the Offer message back to destination IP address 172.16.1.1.\nthe DHCP relay agent (router R1) needs to change the destination IP address, so that the real DHCP client (host A), which does not have an IP address yet, can receive and process the packet.\nSends as broadcast back to the LAN\nfor the return packet from the DHCP server\ntypes of settings the DHCP server needs to know to support DHCP clients: Subnet ID and mask: Reserved (excluded) addresses: - which addresses in the subnet to not lease. Default router(s)DNS IP address(es)\nother parameters\nMaximum lease time- time limit for leasing an IP address\nDHCP uses three allocation modes\nDynamic allocation - the DHCP mechanisms and configuration described throughout this chapter.\nautomatic allocation-sets the DHCP lease time to infinite.\nstatic allocation- preconfigures the specific IP address for a client based on the client’s MAC address.\nTrivial File Transfer Protocol (TFTP) server address Information Stored at the DHCP Server\nConfiguring DHCP Relay\nWhat for?\u0026ndash; DHCP clients exist in the subnetDHCP servers do not exist in the subnet\n(config-if)# ip helper-address address Show ip interface g0/0 - Helper Address is address or Helper address is not set Configuring a Switch as DHCP Client\n(config-if) ip address dhcp the switch does not attempt DHCP until the interface reaches an up/up state\nshow interfaces - verify ip address interface\nshow dhcp lease - - see the (temporarily) leased IP address and other parametersthe switch does not store the DHCP-learned IP configuration in the running-config show ip default - view address of the default gateway - gateway (config-if) # ip address dhcp IOS displays this route as a static route (destination 0.0.0.0/0)\nTo recognize this route as a DHCP\u0026ndash; administrative distance of 1 for static routes configured with the default of 254 for default routes added because of DHCP.-learned default route, look to the administrative distance value of 254. ip route configuration command Configuring a Router as DHCP Client\nTo work correctly, an IPv4 host needs to know these values: DNS server IP addressesDefault gateway (router) IP address Device’s own IP addressDevice’s own subnet mask Host Settings for IPv\nHost IP Settings on Windows\nlists the host’s IP routing tablethe top of the table lists a route based on the default gateway (0.0.0.0 and 0.0.0.0) top of the output also lists several other routes related to having a working interface, like a route to the subnet connected to the interface. netstat -rn the PC thinks the destination is on the local subnet (link) gateway of “on-link” Host IP Settings on macOS\ndoes not have an /all optiondoes not list the default gateway or DNS servers Shows mac address and ip address ifconfig networksetup - Shows dhcp config, ip, subnet mask, and default gateway - getinfo Ethernet networksetup - Shows dns server - getdnsservers Ethernet adds a default route to its host routing table based on the default gateway output represents the default route using the word default rather than 0.0.0.0 and 0.0.0. adds route to the local subnet calculated based on the IP address and mask learned with DHCP- uses the netstat -rn command to list those routes some Linux distributions do not include net-tools. You can add netincludes ifconfig-tools to most Linux distributions.and netstat -rn. preinstalled on Linuxincludes a set of replacement commands and functions, many performed with the ip command and some parameters.similar to the macOS version but also shows some interface counters. - shows basic addressing info - Shows subnet in prefix notation rather than in dotted decimal ipaddress iproute library ifconfig wlan0 - - shows l2 and l3 addressesfrom net-tools from net-tools lists a default route, but with a style that shows the destination as 0.0.0.0. - points to the default gateway as learned with DHCP: The bottom of the example shows the command meant to replace netstat shows a default route that references the default router, along with a route for the local subnet.-rn: ip route. Note that it also lists a route to the local subnet netstat -rn Host IP Settings on Linux\n5.0 Security Fundamentals 5.7 Configure Layer 2 security features (DHCP snooping, dynamic ARP inspection, and port security) notices DHCP messages that fall outside the normal use of DHCP watches the DHCP messages that flow through a LAN switchbuilds a table that lists the details of legitimate DHCP flows other switch features can know what legitimate DHCP leases exist for devices connected to the switch. DHCP Snooping helps prevent packets being redirected to an attacking host. Some ARP attacks try to convince hosts to send packets to the attacker’s device instead of the true destination.\n○ checks incoming ARP messages,\n○ checks those against normal ARP operation as well as\n○ checks the details against other data sources, including the DHCP Snooping binding table. When the ARP message does not match the known information about the legitimate\naddresses in the network, the switch filters the ARP message. ○ The switch watches ARP messages as they flow through the switch. Dynamic ARP Inspection (DAI) DHCP Snooping Concepts analyzes incoming messages on the specified subset of ports in a VLANnever filters non-DHCP messages allow the incoming DHCP message or discard the message. operates on LAN switches and is commonly used on Layer 2 LAN switches and enabled on Layer 2 ports. Client facing devices are untrustedDHCP facing devices (server, router, switch) are trusted works first on all ports in a VLAN, but with each port being trusted or untrusted by DHCP Snooping\nthe rules differentiate between messages normally sent by servers (like DHCPOFFER and\nDHCPACK) versus those normally sent by DHCP clients: DHCP messages received on an untrusted port, for messages normally sent by a server, will always be discarded. DHCP messages received on an untrusted port, as normally sent by a DHCP client, may be filtered if they appear to be part of an attack.\nDHCP messages received on a trusted port will be forwarded; trusted ports do not filter\n(discard) any DHCP messages. A Sample Attack: A Spurious DHCP Server Spurious DHCP server (attacker) responds to dhcp message by a client pcattacker replies with false dhcp information by naming itself as the default gateway Four messages between the client and server (DORA): Discover: - Sent by the DHCP client to find a willing DHCP server Offer: - Sent by a DHCP server to offer to lease\nRequest: - Sent by the DHCP client to ask the server to lease the IPv4 address listed in the Offer message\nAcknowledgment: - Sent by the DHCP server to assign the address and to list the mask, default router, and DNS server IP addresses IPv4 addresses that allow a host that has no IP address to still be able to send and receive messages on the local subnet: 0.0.0.0: - An address reserved for use as a source IPv4 address for hosts that do not yet have an IP address. 255.255.255.255: - The local broadcast IP address. Packets sent to this destination address are broadcast on the local data link, but routers do not forward them. a client, sends a Discover message, with source IP address of 0.0.0.0Host A sends the packet to destination 255.255.255.255 router R1 will not forward this packet. - - assuming the DHCP client chooses to use a DHCP option called the broadcast flag; all examples in this book assume the broadcast flag is used. - all hosts in the subnet receive the Offer message - the original Discover message lists a number called the client ID□□ includes the host’s MAC address, that identifies the original host (host A in this case).As a result, host A knows that the Offer message is meant for host A, which. Supporting DHCP for Remote Subnets with DHCP Relay\nip helper - Watch for incoming DHCP messages, with destination IP address 255.255.255.255 - address server-ip. ","externalUrl":null,"permalink":"/tech/networking/dhcp-config/","section":"Teches","summary":"\u003cp\u003eThursday, September 30, 2021 12:17 PM\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eWatch for incoming DHCP messages, with destination IP address 255.255.255.255Change that packet’s source IP address to the router’s incoming interface IP address..\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eChange that packet’s destination IP address to the address of the DHCP server (as configured in the \u003cstrong\u003eaddres\u003c/strong\u003e scommand). \u003cstrong\u003eip helper-\u003c/strong\u003e\u003c/p\u003e","title":"DHCP Configuration","type":"tech"},{"content":"Attacker replies with false dhcp information by naming itself as the default gateway Pc1 send all messages to another network to attacker, becoming a man-in-the-middle\nattack (pc2 could forward the messages to the actual default gateway\nthe legitimate DHCP also returns a DHCPOFFER message to host PC1, but most hosts use the\nfirst received DHCPOFFER, and the attacker will likely be first in this scenario. DHCP Snooping Logic\nstops attacks by making ports untrusted ▪ client can tell the DHCP server it no longer needs the address, releasing it back to the DHCP server DHCP RELEASE ▪ turn down the use of an IP address during the normal DORA flow on messages. DHCP DECLINE DHCP clients also use the DHCP RELEASE and DHCP DECLINE messages. DHCP Snooping rules: Examine all incoming DHCP messages.If normally sent by servers, discard the message. If normally sent by clients, filter as follows: check for MAC address consistency between the Ethernet frame and the DHCP message. DISCOVER and REQUEST check the incoming interface plus IP address versus the DHCP Snooping binding table. RELEASE or DECLINE - FSnooping binding table.or messages not filtered that result in a DHCP lease, build a new entry to the DHCP DHCP snooping checking MAC Address\nSource MAC address included in DISCOVER message\nframe encapsulating the dhcp message also includes the source mac address\nif not the packet is dropped DHCP Snooping checks to make sure those values match.\nUsed on untrusted ports\nchaddr (client hardware address)\nstops attacker from leasing all available addresses so no other client can get a lease\nFiltering Messages that Release IP Addresses\ncreates entry for all legitimate DHCP entriesDHCP Snooping, and other features like Dynamic ARP Inspection, can use the table to make\ndecisions.\nMACIP VLAN\nDHCP Snooping Binding Table:\nDHCP Snooping binding table\nVLANINTERFACE attacker pretends his source IP is the legitimate IP trying to RELEASE\nthe source interface in the DHCP Snooping Binding Table does not match so dhcp snooping bloacks the request\nattacker could RELEASE an address that is not his and attempt steal it for himself\nchecks client-sent messages like RELEASE and DECLINE that would cause the DHCP server to be allowed to release an address DHCP Snooping Configuration\nip dhcp snooping enable DHCP Snooping ip dhcp snooping vlan vlan-id list the VLANs on which to use DHCP Snooping Required: default is untrusted. (config-if) # ip dhcp snooping trust Often used:- configure trusted ports DHCP Snooping verification\nshow ip dhcp snooping\nenabled or disabled operational in which vlanstrusted interfaces - (enabled by default)\nno ip dhcp information option - (because this switch is not a dhcp relay agent)command is configured\n- DHCP relay agents add option 82 DHCP header fields (RFC 3046) - - If enabled on a non relay agent then dhcp stops working for end usersThe switch sets fields in the DHCP messages as if it were a DHCP relay agent - changes to those messages cause most DHCP servers (and most DHCP relay agents) to ignore the received DHCP messages. Insertion of option 82 is disabled Rate limit on each interface Shows actual dhcp snooping binding table Show ip dhcp snooping binding DHCP Rate Limiting\nattacker generates large volumes of DHCP messages to overload the DHCP Snooping feature and the switch CPU itself\noptional feature that tracks the number of incoming DHCP messages. port changes to an err-disabled state. If the number of incoming DHCP messages exceeds that limit over a one-second period: can be on both on trusted and untrusted interfaces. Default of no rate limit Default of no rate limit\n(config-int) # ip dhcp snooping rate limit number errdisable recovery cause dhcp - recover from errdisable if caused by dhcp snooping - rate-limit errdisable recovery interval - seconds to wait before recovering number (config-int) # no ip dhcp snooping rate limit how to enable DHCP Snooping rate limits and err-disabled recovery. Dynamic ARP Inspection (DAI)\nexamines incoming ARP messages on untrusted ports to filter those it believes to be part of an\nattack.\nDHCP Snooping binding tableconfigured ARP ACLs. compares incoming ARP messages with two sources of data: If the incoming ARP message does not match the tables in the switch, the switch discards the ARP message. DAI Concepts\ngratuitous ARP (arp reply)triggers hosts to add incorrect ARP entries to their ARP tables.—\nBoth hosts learn the other host’s MAC address with this twolearn R2’s MAC address based on the ARP reply (message 2), but router R2 learns PC1’s IP and -message flow. Not only does PC MAC address because of the ARP request (message 1). source mac destination mac (broadcast) Ethernet Frame Origin IPTarget IP Origin MACTarget MAC ?? ARP ARP Request Source MACDest. MAC Ethernet Frame Origin IPTarget IP Origin MACTarget MAC ARP ARP Reply Review of Normal IP ARP\nGratuitous ARP as an Attack Vector\nwhen a host changes its MAC address it can send a gratuitous ARP message with these features: ARP reply. sent without having first received an ARP request.sent to an Ethernet destination broadcast address so that all hosts in the subnet receive the message. attacker uses gratuitous arp to make replies go back to it instead of original host could be part of a man-in-the-middle attack Dynamic ARP Inspection Logic\nHost does not need arp if it doesn\u0026rsquo;t have an IP yet\nOnce DHCP assigns and IP then ARP is usedfor untrusted interfaces DAI confirms an ARP’s correctness based on the DHCP Snooping Binding\ntable\ncompares origin MAC and IP\nif not found on the table, discard the arpsame trusted and untrusted rules as dhcp snooping - statically configured - list correct pairs of IP and mac addressesUseful for ip addresses configured statically because they will not be in the dhcp snooping\nbinding table ARP ACL DAI can check Ethernet header and arp source and destination messages mac addresses to make sure they match DAI can also check Messages with unexpected IP addresses in the two ARP IP address fields\nDAI itself can be more susceptible to DoS attacks. attacker could generate large numbers of ARP messages, driving up CPU usage in the switch.\nDAI can avoid these problems through rate limiting the number of ARP messages on a port\nover time. does its work in the switch CPU rather than in the switch ASIC\nConfiguring ARP Inspection on a Layer 2 Switch\ndecisions include the following:\nChoose whether to rely on DHCP Snooping, ARP ACLs, or both. If using DHCP Snooping, configure it and make the correct ports trusted for DHCP Snooping.Choose the VLAN(s) on which to enable DAI. Make DAI trusted (rather than theVLANs, typically for the same ports you trusted for DHCP Snooping.default setting of untrusted) on select ports in those\nip arp inspection vlan - enable arp DAI on that vlan vlan-id\n(config - dhcp snooping should be enabled or DAI will filter all packets - if)# ip arp inspection trust\nip dhcp snoopingip dhcp snooping vlan vlan-id\nip dhcp snooping trust ip dhcp snooping trust (configure arp ACLs) gives both -- forwarded, dropped, dhcp drops, acl drop countsIs logging enabled?configuration settings along with status variables and counters show ip arp inspection - counts per vlan- forwarded, dropped, dhcp drops, acl drops permits show ip arp inspection statistics ARP Rate Limiting\n(trusted and untrusted) Default for all interfaces x ARP messages over y seconds”(DHCP Snooping does not define a burst setting) optional configuration 8 messages in a burst of 4 seconds ip arp inspection limit rate 8 burst interval 4 default of 15 messages over a one second burst\nshow ip arp inspection interfaces - rate, burst interval, trust state, per interface burst interval (a number of seconds) ip arp inspection limit rate number disable rate limiting ip arp inspection limit rate none errdisable recovery cause arperrdisable recovery interval 30 - inspection\nConfiguring Optional DAI Message Checks\none, two, or all three of the options command with overrides previous versions of the same command show ip arp inspection to validate ip arp inspection validate {dst-mac, ip, src-mac} drop ARP packets the body of the ARP packets do not match the addresses specified in the Ethernet header.when the IP addresses in the packets are invalid or when the MAC addresses in Forin the ARP body. This check is src-mac, check the source MAC address in the Ethernet header against the sender MAC address performed on both ARP requests and responses. When enabled, packets with different MAC addresses are classified as invalid and are dropped. Foraddress in ARP body. This check isdst-mac, check the destination MAC address in the Ethernet header against the target MAC performed for ARP responses.When enabled, packets with\ndifferent MAC addresses are classified as invalid and are dropped. For255.255.255.255, and all IP multicast addresses. ip, check the ARP body for invalid and unexpected IP addresses. Addresses include 0.0.0.0, Sender IP addresses are checked in all ARP\nrequests and responses, and target IP addresses are checked only in ARP responses 2.0 Network Access 2.3 Configure and verify Layer 2 discovery protocols (Cisco Discovery Protocol and LLDP) 4.0 IP Services 4.2 Configure and verify NTP operating in a client and server mode 4.5 Describe the use of syslog features including facilities and levels System Message Logging (Syslog) # Sending Messages in Real Time to Current Users Telnet and SSH users, the device requires a two-step process before the user sees the messages.\nglobal configuration setting—logging monitor—tells IOS to enable the sending of log\nmessages to all logged users. must also issue theterminal monitorEXEC command during the login session, which tells\nIOS that this terminal session would like to receive log messages.\nStoring Log Messages for Later Review two primary means to keep a copy can configuration command.store copies of the log messages in RAMby virtue of the logging buffered global\nany user can come back later and see the old log messagesby using the show\nlogging EXEC command.\nstore log messages centrally to a syslog server.\nsyslog protocol - a UDP protocol to send messages to a syslog server for storage\nTo configure a router or switch to send log messages to a syslog server, add the ","externalUrl":null,"permalink":"/tech/networking/dhcp-snooping-and-arp-inspection/","section":"Teches","summary":"\u003cp\u003eAttacker replies with false dhcp information by naming itself as the default gateway Pc1 send all messages to another network to attacker, becoming a man-in-the-middle\u003cbr\u003e\nattack (pc2 could forward the messages to the actual default gateway\u003c/p\u003e","title":"DHCP Snooping and ARP Inspection","type":"tech"},{"content":" DNS and Name Resolution # DNS is also referred to as BIND (Berkeley Internet Name Domain) An implementation of DNS, Most popular DNS application in use. Name resolution is the technique that uses DNS/BIND for hostname lookups. DNS Name Space and Domains # DNS name space is a Hierarchical organization of all the domains on the Internet. Root of the name space is represented by a period (.) Hierarchy below the root (.) denotes the top-level domains (TLDs) with names such as .com, .net, .edu, .org, .gov, .ca, and .de. A DNS domain is a collection of one or more systems. Subdomains fall under their parent domains and are separated by a period (.). root of the name space is represented by a period (\t- redhat.com is a second-level subdomain that falls under .com, and bugzilla.redhat.com is a third-level subdomain that falls under redhat.com. Deepest level of the hierarchy are the leaves (systems, nodes, or any device with an IP address) of the name space. a network switch net01 in .travel.gc.ca subdomain will be known as net01.travel.gc.ca. If a period (.) is added to the end of this name to look like net01.travel.gc.ca., it will be referred to as the Fully Qualified Domain Name (FQDN) for net01. DNS Roles # A DNS system or nameserver can be a\nprimary server secondary server or client Primary server\nResponsible for its domain (or subdomain). Maintains a master database of all the hostnames and their associated IP addresses that are included in that domain. All changes in the database are done on this server. Each domain must have one primary server with one or more optional secondary servers for load balancing and redundancy. Secondary server\nStores an updated copy of the master database. Provide name resolution service in the event the primary server goes down. Client\nQueries nameservers for name lookups. DNS client on Linux involves two text files. /etc/resolv.conf /etc/resolv.conf # DNS resolver configuration file where information to support hostname lookups is defined. May be edited manually with a text editor. Referenced by resolver utilities to construct and transmit queries. Key directives\ndomain\nnameserver\nsearch\nDirective Description\ndomain\nIdentifies the default domain name to be searched for queries nameserver\nDeclares up to three DNS server IP addresses to be queried one at a time in the order in which they are listed. Nameserver entries may be defined as separate line items with the directive or on a single line. search\nSpecifies up to six domain names, of which the first must be the local domain. No need to define the domain directive if the search directive is used. Sample entry\ndomain example.com search example.net example.org example.edu example.gov nameserver 192.168.0.1 8.8.8.8 8.8.4.4 Variation\ndomain example.com search example.net example.org example.edu example.gov nameserver 192.168.0.1 nameserver 8.8.8.8 nameserver 8.8.4.4 Entries are automatically placed by the NetworkManager service. [root@server30 tmp]# cat /etc/resolv.conf # Generated by NetworkManager nameserver 2001:578:3f::30 nameserver 2001:578:3f:1::30 If this file is absent, the resolver utilities only query the nameserver configured on the localhost, determine the domain name from the hostname of the system, and construct the search path based on the domain name. Viewing and Adjusting Name Resolution Sources and Order # /etc/nsswitch.conf\nDirects the lookup utilities to the correct source to get hostname information.\nAlso identifies the order in which to consult source and an action to be taken next.\nFour keywords oversee this behavior\nsuccess notfoundq unavail tryagain Keyword Meaning Default Action\nsuccess\nInformation found in return (do not try the source and provided to next source) the requester. notfound\nInformation not found continue (try the next in source source). unavail\nSource down or not continue (try the next responding; service source) disabled or not configured. tryagain\nSource busy, retry continue (try the next later source). Example shows two sources for name resolution: files (/etc/hosts) and DNS (/etc/resolv.conf).\nhosts:files dns Default behavior Search will terminate if the requested information is found in the hosts table. Instruct the lookup programs to return if the requested information is not found there:\nhosts:files [notfound=return] dns Query tools available in RHEL 9: dig host nslookup getent dig command (domain information groper) # DNS lookup utility. Queries the nameserver specified at the command line or consults the resolv.conf file to determine the nameservers to be queried. May be used to troubleshoot DNS issues due to its flexibility and verbosity. To get the IP for redhat.com using the nameserver listed in the resolv.conf file:\n[root@server10 ~]# dig redhat.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.23-RH \u0026lt;\u0026lt;\u0026gt;\u0026gt; redhat.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 9017 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4000 ;; QUESTION SECTION: ;redhat.com.\tIN\tA ;; ANSWER SECTION: redhat.com.\t3599\tIN\tA\t52.200.142.250 redhat.com.\t3599\tIN\tA\t34.235.198.240 ;; Query time: 94 msec ;; SERVER: 172.16.10.150#53(172.16.10.150) ;; WHEN: Fri Jul 19 13:12:13 MST 2024 ;; MSG SIZE rcvd: 71 To perform a reverse lookup on the redhat.com IP (52.200.142.250), use the -x option with the command:\n[root@server10 ~]# dig -x 52.200.142.250 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.23-RH \u0026lt;\u0026lt;\u0026gt;\u0026gt; -x 52.200.142.250 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 23057 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4000 ;; QUESTION SECTION: ;250.142.200.52.in-addr.arpa.\tIN\tPTR ;; ANSWER SECTION: 250.142.200.52.in-addr.arpa. 299 IN\tPTR\tec2-52-200-142-250.compute-1.amazonaws.com. ;; Query time: 421 msec ;; SERVER: 172.16.10.150#53(172.16.10.150) ;; WHEN: Fri Jul 19 14:22:52 MST 2024 ;; MSG SIZE rcvd: 112 host Command # Works on the same principles as the dig command in terms of nameserver determination. Produces less data in the output by default. -v option if you want more info. Perform a lookup on redhat.com:\n[root@server10 ~]# host redhat.com redhat.com has address 34.235.198.240 redhat.com has address 52.200.142.250 redhat.com mail is handled by 10 us-smtp-inbound-2.mimecast.com. redhat.com mail is handled by 10 us-smtp-inbound-1.mimecast.com. Rerun with -v added:\n[root@server10 ~]# host -v redhat.com Trying \u0026#34;redhat.com\u0026#34; ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 28687 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;redhat.com.\tIN\tA ;; ANSWER SECTION: redhat.com.\t3127\tIN\tA\t52.200.142.250 redhat.com.\t3127\tIN\tA\t34.235.198.240 Received 60 bytes from 172.16.1.19#53 in 8 ms Trying \u0026#34;redhat.com\u0026#34; ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 47268 ;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0 ;; QUESTION SECTION: ;redhat.com.\tIN\tAAAA ;; AUTHORITY SECTION: redhat.com.\t869\tIN\tSOA\tdns1.p01.nsone.net. hostmaster.nsone.net. 1684376201 200 7200 1209600 3600 Received 93 bytes from 172.16.1.19#53 in 5 ms Trying \u0026#34;redhat.com\u0026#34; ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 61563 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 12 ;; QUESTION SECTION: ;redhat.com.\tIN\tMX ;; ANSWER SECTION: redhat.com.\t3570\tIN\tMX\t10 us-smtp-inbound-1.mimecast.com. redhat.com.\t3570\tIN\tMX\t10 us-smtp-inbound-2.mimecast.com. ;; ADDITIONAL SECTION: us-smtp-inbound-1.mimecast.com.\t270 IN\tA\t205.139.110.242 us-smtp-inbound-1.mimecast.com.\t270 IN\tA\t170.10.128.242 us-smtp-inbound-1.mimecast.com.\t270 IN\tA\t170.10.128.221 us-smtp-inbound-1.mimecast.com.\t270 IN\tA\t170.10.128.141 us-smtp-inbound-1.mimecast.com.\t270 IN\tA\t205.139.110.221 us-smtp-inbound-1.mimecast.com.\t270 IN\tA\t205.139.110.141 us-smtp-inbound-2.mimecast.com.\t270 IN\tA\t170.10.128.221 us-smtp-inbound-2.mimecast.com.\t270 IN\tA\t205.139.110.141 us-smtp-inbound-2.mimecast.com.\t270 IN\tA\t205.139.110.221 us-smtp-inbound-2.mimecast.com.\t270 IN\tA\t205.139.110.242 us-smtp-inbound-2.mimecast.com.\t270 IN\tA\t170.10.128.141 us-smtp-inbound-2.mimecast.com.\t270 IN\tA\t170.10.128.242 Received 297 bytes from 172.16.10.150#53 in 12 ms Perform a reverse lookup on the IP of redhat.com with verbosity:\n[root@server10 ~]# host -v 52.200.142.250 Trying \u0026#34;250.142.200.52.in-addr.arpa\u0026#34; ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 62219 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;250.142.200.52.in-addr.arpa.\tIN\tPTR ;; ANSWER SECTION: 250.142.200.52.in-addr.arpa. 300 IN\tPTR\tec2-52-200-142-250.compute-1.amazonaws.com. Received 101 bytes from 172.16.10.150#53 in 430 ms nslookup Command # Queries the nameservers listed in the resolv.conf file or specified at the command line. See man pages for interactive mode Get the IP for redhat.com using nameserver 8.8.8.8 instead of the nameserver defined in resolv.conf:\n[root@server10 ~]# nslookup redhat.com 8.8.8.8 Server:\t8.8.8.8 Address:\t8.8.8.8#53 Non-authoritative answer: Name:\tredhat.com Address: 34.235.198.240 Name:\tredhat.com Address: 52.200.142.250 Perform a reverse lookup on the IP of redhat.com using the nameserver from the resolver configuration file:\n[root@server10 ~]# nslookup 52.200.142.250 250.142.200.52.in-addr.arpa\tname = ec2-52-200-142-250.compute-1.amazonaws.com. Authoritative answers can be found from: getent Command # Fetch matching entries from the databases defined in the nsswitch.conf file. Reads the corresponding database and displays the information if found. For name resolution, use the hosts database and getent will attempt to resolve the specified hostname or IP address. Run the following for forward and reverse lookups:\n[root@server10 ~]# getent hosts redhat.com 34.235.198.240 redhat.com 52.200.142.250 redhat.com [root@server10 ~]# getent hosts 34.235.198.240 34.235.198.240 ec2-34-235-198-240.compute-1.amazonaws.com Hostname # \u0026ldquo;-\u0026rdquo;, \u0026ldquo;_ \u0026ldquo;, and \u0026ldquo;. \u0026quot; characters are allowed. Up to 253 characters. Stored in /etc/hostname. Can be viewed with several different commands, such as hostname, hostnamectl, uname, and nmcli, as well as by displaying the content of the /etc/hostname file. View the hostname:\nhostnamectl --static hostname uname -n cat /etc/hostname Lab: Change the Hostname # Server1\nOpen /etc/hostname and change the entry to server10.example.com restart the systemd-hostnamed service daemon sudo systemctl restart systemd-hostnamed confirm hostname server2\nChange the hostname with hostnamectl: sudo hostnamectl set-hostname server21.example.com Log out and back in for the prompt to update\nChange the hostname using nmcli\nnmcli general hostname server20.example.com ","externalUrl":null,"permalink":"/tech/tools/dns-and-time-synchronization/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eDNS and Name Resolution\n    \u003cdiv id=\"dns-and-name-resolution\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#dns-and-name-resolution\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDNS is also referred to as BIND (Berkeley Internet Name Domain)\n\u003cul\u003e\n\u003cli\u003eAn implementation of DNS,\u003c/li\u003e\n\u003cli\u003eMost popular DNS application in use.\u003c/li\u003e\n\u003cli\u003eName resolution is the technique that uses DNS/BIND for hostname lookups.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eDNS Name Space and Domains\n    \u003cdiv id=\"dns-name-space-and-domains\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#dns-name-space-and-domains\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDNS name space is a\n\u003cul\u003e\n\u003cli\u003eHierarchical organization of all the domains on the Internet.\u003c/li\u003e\n\u003cli\u003eRoot of the name space is represented by a period (.)\u003c/li\u003e\n\u003cli\u003eHierarchy below the root (.) denotes the top-level domains (TLDs) with names such as .com, .net, .edu, .org, .gov, .ca, and .de.\u003c/li\u003e\n\u003cli\u003eA DNS domain is a collection of one or more systems. Subdomains fall under their parent domains and are separated by a period (.).\nroot of the name space is represented by a period (\t- redhat.com is a second-level subdomain that falls under .com, and bugzilla.redhat.com is a third-level subdomain that falls under redhat.com.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"\"\n    src=\"../../img/image-TJ7OW7U5%201.jpg\"\n    \u003e\u003c/figure\u003e\n\u003cul\u003e\n\u003cli\u003eDeepest level of the hierarchy are the leaves (systems, nodes, or any device with an IP address) of the name space.\n\u003cul\u003e\n\u003cli\u003ea network switch net01 in .travel.gc.ca subdomain will be known as net01.travel.gc.ca.\u003c/li\u003e\n\u003cli\u003eIf a period (.) is added to the end of this name to look like net01.travel.gc.ca., it will be referred to as the Fully Qualified Domain Name (FQDN) for net01.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eDNS Roles\n    \u003cdiv id=\"dns-roles\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#dns-roles\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eA DNS system or nameserver can be a\u003c/p\u003e","title":"DNS","type":"tech"},{"content":"802.3 (Ethernet Standards\n10BASE-T\n10 MbpsEthernet 802.3Copper/ 100 m\n100BASE-T 100 MbpsFast Ethernet 802.3uCopper/ 100m\n1000BASE-LX 1000 MbpsGigabit Ethernet Gigabit Ethernet802.3z Fiber, 5000 m\n1000BASE-T\n1000 Mbps\nGigabit Ethernet802.3ab Copper, 100m\n10GBASE-T\n10 Gbps\n10 Gig Ethernet802.3an Copper100m\nThree most common\n10BASE-T, 100BASE-T, and 1000BASE-T\nTwisting of the wires helps reduce EMI.\nEMI between wire pairs\nCrosstalk\nGigabit Ethernet interface Converter (GBIC)\nThe original form factor for a removeable transceiver for Gigabit interfaces; larger than SFPs\nSmall Form Pluggable (SFP) The replacement for GBICs, used on gigabit interfaces, with a smaller size, taking less space on the side of the networking card or switch. Small Form Pluggable Plus (SFP+)\nSame size as the SFP, but used on 10-Gbps interfaces\nUTP Cabling Pinouts for 10BASE-T and 100BASE-T 2 wire pairs, one pair for each direction 10BASE-T and 100BASE-T Crossover ▪▪ transmits on pins 1/2RECEIVES ON PINS 3/ PC/ Router/ AP ▪ Transmits on pins 3/ Switch/ Hub ▪▪ Transmits on pins 3/6receives on pins 1/ Ethernet NIC transmitters send on pins 1 and 2 NIC receivers receive on pins 3 and 6 Straight through cable Pins 1/2 \u0026gt; 1/ Pins 3/6 \u0026gt; 3/\nauto-mdix (cisco) Notices when a wrong cable is used automatically changes it\u0026rsquo;s logic to make the link work. 4 wire pairsBoth ends transmit and receive simultaneously on each wire pair. pairs 1/2, 3/6, 4/5, 7/ Crossover cable crosses the pairs at pins 1/2 and 3/6, it also crosses pairs 4/5 with 7/ 1000BASE-T\nFiber Cabling Transmission Concepts\nPhysical cable\nCore \u0026gt; Cladding \u0026gt; Buffer \u0026gt; Strengthener \u0026gt;Outer Jacket Shines light into the core Optical Transmitter multiple angles (modes) of light waves Less expensive 10 gigabit over ethernet allows for distance up to 400m Multimode fiber Smaller diameter (around 1/5 of multimode) core lasersingle angle-based transmitter\nMore expensive SFP/ SFP+ hardware\nDistances up to tens of kilometers Single mode fiber Transmit port on one end of the fiber connects to the receive port on the other end (Tx and Rx) 10Gbps Fiber Standards 10Gbps Fiber Standards 10GBASE10GBASE\u0026ndash;S/ MM/ 400mLX4/ MM/ 300m 10GBASE10GBASE\u0026ndash;LR/ SM/ 10kmE/ SM/ 30km UTP, MM, and SM comparisons\nlow cable cost low switch port cost100m Max Distance Some susceptibility to interference Some risk of copying from cable emissions UTP Medium cable cost Medium switch port cost500m Max Distance No susceptibility to interference No risk of copying from cable emissions Multimode Medium cable cost High switch port cost40Km Max Distance No susceptibility to interference No risk of copying from cable emissions Single-Mode Ethernet Header (preamble/sfd/Destination/Source/Type/Dataandpad/FCS) Header □□ 7 bitesSynchronization ▪ Preamble □ 1 Byte □ Signifies next byte begins the Destination MAC Address Field ▪ Start Frame Delimiter (SFD) ▪ Destination MAC Address□ 6 Bytes ▪ Source MAC Address□ 6 Bytes □□ 2 bytesType of protocol listed in the frame (IPv4 or IPv6) ▪ Type Sending Data In Ethernet Networks\n▪▪ (^46) padding can be added to meet the minimum length requirement-1500Bytes\nData and Pad Trailer Frame Check Sequence (FCS)Used to determine if the frame experienced transmission errors Maximum layer 3 packet that can be sent\nMaximum Transmission Unit (MTU) Ethernet Addressing\n6 bytes long (48 bits) 12 digit hexadecimalCisco switch may list a mac address with periods: 0000.0C12.\nUnicast address- an address for a single NIC or port OUI(Organizationally unique indentifier) Universally unique manufacturer code24 Bits 6 Hex Digits\nVendor Assigned 24 Bits 6 Hex Digits Delivered to all devices on the Ethernet LAN. FFFF.FFFF.FFFF Broadcast Address Copied and forwarded to a subset of devices on the LAN Multicast Address Group addresses\nIdentifying Network Layer Protocols with the Ethernet Type Field\nThe type field identifies which type of layer 3 packet exists within the ethernet frame (IPv6 or\nIPv4)Ethertype is the term used for the type field in an ethernet frame\nError Detection with FCS\nError detection does not mean error recoveryEthernet decide whether the frame should be discarded and does not attempt to recover the lost\nframe. frame.\nSending Ethernet frames with switches and hubs\nSwitches allow the use of Full Duplex LogicHubs use half-duplex logic Sending in Modern Ethernet LANs Using Full Duplex\nmust wait to send if it is currently receiving a framecannot send and receive at the same time Half duplex Does not have to wait before sending, send and receive at the same time. Full Duplex Hubs\nUses physical link standards instead of data link standards and are considered layer 1 devices\nCarrier Sense multiple access with Collision Detection (CSMA/CD)\n1.2. Listen until line is not busySend frame ii.i. Send jamming signal telling all nodes a collision has occurredeach node waits a random time then tries to send again iii. Back to step one Listen for a collision while sending if a collision occurs: All links between PCs and switches use full duplexA link connected to a hub should be half duplex ○ refers to hubs that use CSMA/CD and share bandwidth Ethernet Shared media ○ network built with switches where links work independently of others ○ A frame can be sent on every Point-to-point link in an ethernet at the same time. Ethernet point-to-point ","externalUrl":null,"permalink":"/tech/networking/ethernetlans/","section":"Teches","summary":"\u003cp\u003e802.3 (Ethernet Standards\u003cbr\u003e\n10BASE-T\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e10 MbpsEthernet\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e802.3Copper/ 100 m\u003cbr\u003e\n100BASE-T\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e100 MbpsFast Ethernet\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e802.3uCopper/ 100m\u003cbr\u003e\n1000BASE-LX\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e1000 MbpsGigabit Ethernet\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eGigabit Ethernet802.3z\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFiber, 5000 m\u003cbr\u003e\n1000BASE-T\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e1000 Mbps\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eGigabit Ethernet802.3ab\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopper, 100m\u003cbr\u003e\n10GBASE-T\u003c/p\u003e","title":"Ethernet LANs","type":"tech"},{"content":"Why use EEs?\nPortable Ansible environments includes Ansible core version All desired collections Python dependencies Bindep dependencies Anything you need to run a playbook A container that has a specific version of Ansible. Can test execution in a specific Ansible environment to make sure it will work with that version.\nEEs are built leveraging ansible-bulder They can be pushed to a private automation hub or any container registry Run EEs from the cli using ansible-navigator Or run in your production environment using automation controller as part of the Ansible Automation Platform If you want them to automatically occur, schedule them as a job inside AAP\n","externalUrl":null,"permalink":"/tech/ansible/execution-environments/","section":"Teches","summary":"\u003cp\u003eWhy use EEs?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePortable Ansible environments\n\u003cul\u003e\n\u003cli\u003eincludes Ansible core version\u003c/li\u003e\n\u003cli\u003eAll desired collections\u003c/li\u003e\n\u003cli\u003ePython dependencies\u003c/li\u003e\n\u003cli\u003eBindep dependencies\u003c/li\u003e\n\u003cli\u003eAnything you need to run a playbook\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA container that has a specific version of Ansible. Can test execution in a specific Ansible environment to make sure it will work with that version.\u003c/p\u003e","title":"Execution Environments","type":"tech"},{"content":"","externalUrl":null,"permalink":"/categories/explanation/","section":"Categories","summary":"","title":"Explanation","type":"categories"},{"content":"IP and TCP Header\nIP Header\nMisc Header Fields▪ 9 bytes\n▪ 1 byte ▪▪ ie 6 = tcpidentify TCP header Protocol Header Checksum▪ 2 bytes ▪ 4 bytes Source IP Dest. IP▪ 4 bytes Options▪ variable TCP Header Source Port- 2 bytes 2 bytes Dest. port Rest of TCP- 16 bytes tcp or udp keyword\ncan optionally reference the source and/or destination portequal, not equal, less than, greater than, and for a range of port numbers can use port numbers or keywords for some well-known application ports\npositions of the source and destination port fields in the access-list command and these port\nnumber keywords. access-list 101 permit (protocol) Source_IP (source port) dest_IP (dest port) # Protocol\u0026ndash; tcpudp eq ne lt_ Source Port lt_gt_ range_ eq ne lt_gt_ range_ Dest. Port eq: =lt: \u0026lt; ne: not equal gt: \u0026gt;range: x to y ie: # access-list 101 permit tcp 172.16.1.0 0.0.0.255 172.16.3.0 0.0.0.255 eq 21 eq 21 is in the destination port position\nApps and Port number shortcuts for ACL Commands\n20 21 ftpftp-data\n22 -\n23 25 telnetsmtp\n53 67 domainbootps (dhcp server)\n68 69 bootpc (dhcp clienttftp )\n80 www\n110 161 pop3snmp\n443 514 - -\n16,384 -32,767 (RTP/ Voice/ Video) -\nExtended IP ACL Configuration\nenable the ACL using the sameip access-groupcommand used with standard ACLs.\nsaves some bandwidth. Place extended ACLs as close as possible to the source of the packets that will be filtered.\nACL numbers - 100 – 199 and 2000– 2699\n(If you were to type eq 80, the config would show eq http://www.) Extended IP Access Lists: Example 1\n#(int) ip access-group 101 in\nNamed IP Access Lists Easier to remember Uses ACL subcommands instead of global config commandsediting features allow deleting individual lines and inserting new ones\nConfig\n#(ACLmode) permit 1.1.1.1\n#(ACLmode) permit 2.2.2.2#(ACLmode) permit 3.3.3.3\n#(ACLmode) deny ip 10.1.2.0 0.0.0.255 10.2.3.0 0.0.0.255 # ip access-list (standard/ extended) (name) #(int) ip access-group barney out #(ACLmode) no deny ip 10.1.2.0 0.0.0.255 deleting a single entry from the ACL. delete and add new lines to the ACL from within ACL configuration mode Named ACLs and ACL Editing\nACL sequence number is added to each ACL permit or deny statement,\nnumbers represent the sequence of statements in the ACLNumbered ACLs can use a configuration style like named ACLs, as well as the traditional\nstyle, for the same ACL; the new style is required to perform advanced ACL editing.\nDeleting single lines:- delete an ACE with a no sequence-number subcommand.\nNew ACEs can be configured with a sequence number before the deny or permit\ncommand, dictating the location of the statement within the ACL.\nInserting new lines: -\nAutomatic sequence numbering: - sequence numbers are added to ACEs automatically\nShow ip access- Shows access list 24 and sequence numbers with each entry-lists 24 # - delete entry 20 #(ACLmode) no 20\nenters this new ace as sequence #5Places the sequence number in the list in order #(ACLmode) 5 deny 10.1.1.1 although Example 3-6 uses a numbered ACL, named ACLs use the same process to edit (add and remove) entries. Editing ACLs Using Sequence Numbers (named and numbered (not the global numbered way)\nnumbered ACLs are stored with the original style of configuration, as global access-list commands, no matter which method is used to configure the ACL. - # the parts of ACL 24 configured with both new-style commands and old-style commands are all listed in the same old-style ACL (show running-config). - # Numbered ACL Configuration Versus Named ACL Configuration\nPlace more specific statements early in the ACL. By doing so, you avoid issues with the ACL during an interim state Disable an ACL from its interfacemaking changes to the ACL. (using the no ip access-group interface subcommand)before ACL Implementation Considerations\nMitigating Security Issues with ACLs\nSecurity threats that can be mitigated with ACLs\nIP address spoofing, inbound -IP address spoofing, outboundDoS TCP SYN attacks, blocking external attacks Dos TCP SYN attacks, using TCP Intercept -DoS smurf attacksDenying/filtering ICMP messages, inbound -Denying/filtering ICCMP messages, outboundDenying/filtering Traceroute * Don\u0026#39;t allow external packets that have an internal destination address. Configuring ACLs from the internet -Deny any source addresses from your internal networks Deny any local host addresses (127.0.0.0/8) -Deny any reserved private addresses (RFC 1918)Deny any addresses in the IP multicast address range (224.0.0.0/4) Controlling VTY (Telnet/ SSH) Access\nCreate a standard IP access list that permits only the host or hosts you want to be able to telnet into the routers. Apply the access list to the VTY line with the access-class in command.\n(g) # access-list 50 permit host 172.16.10.3\n(g) # line vty 0 4(int) # access-class 50 in Monitoring Access Lists\nshow access-list\nshows access lists, parameters, statistics, etc.\nshow access-list 110\nShows info for access list 110\nshow ip access-list\nshows IP access lists on the router\nshow ip interface\nShows which interfaces have access lists set on them.\nshow running-config\nShows ACLs and what interfaces that have them.\n","externalUrl":null,"permalink":"/tech/networking/extended-acls/","section":"Teches","summary":"\u003cp\u003eIP and TCP Header\u003cbr\u003e\nIP Header\u003cbr\u003e\nMisc Header Fields▪ 9 bytes\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e▪ 1 byte\n▪▪ ie 6 = tcpidentify TCP header\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eProtocol\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eHeader Checksum▪ 2 bytes\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e▪ 4 bytes\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSource IP\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eDest. IP▪ 4 bytes\nOptions▪ variable\nTCP Header\nSource Port- 2 bytes\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e2 bytes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eDest. port\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eRest of TCP- 16 bytes\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003etcp or udp keyword\u003c/p\u003e","title":"Extended ACLs","type":"tech"},{"content":"Ansible roles are fully-featured solutions to accomplish a task. Roles can be downloaded from Ansible Galaxy, built from scratch, or vendor created like RHEL System Roles.\nRoles provide a reusable way to build a webserver, a database server, and more. That way you can have a role handle tasks the same way on all of your server.\nTo use a role, include it in your playbook with include_role: role_name or import_role:role_name\nRole Structure # A role includes subdirectories that have everything needed to run the role. These include variables, handlers, templates, files, tasks, and information on how to use the role. Ansible automatically knows where to look in these directories to run a role.\nHere is an example:\n[ansible@control roles]$ tree testrole/ testrole/ |-- defaults | `-- main.yml |-- files |-- handlers | `-- main.yml |-- meta | `-- main.yml |-- README.md |-- tasks | `-- main.yml |-- templates |-- tests | |-- inventory | `-- test.yml `-- vars `-- main.yml Directory Function defaults Default variables that can be replaced files Files needed by role tasks handlers Handlers used by tasks in the role meta Dependencies, license, and maintainer information tasks Tasks for the role templates Jinja2 template files tests Inventory and test file to test the role vars Variable not meant to be overwritten Some things may not always be necessary and unused folders can be deleted to keep things clean. Most of the role directories have a main.yml file. This is the the entrypoint to each of those directories. So when tasks are run for the playbook, it starts in tasks/main.yml then other task files can be referenced from there.\nWhere roles are stored # LEFT OFF HERE # Roles can be stored in different locations:\n./roles\nstore roles in the current project directory. highest precedence. ~/.ansible/roles\nexists in the current user home directory and makes the role available to the current user only. second-highest precedence. /etc/ansible/roles\nWhere roles are stored to make them accessible to any user. /usr/share/ansible/roles\nWhere roles are stored after they are installed from RPM files. lowest precedence should not be used for storing custom-made roles. ansible-galaxy init { newrolename }\ncreate a custom role creates the default role directory structure with a main.yml file includes sample files Using Roles from Playbooks # Call roles in a playbook the same way you call a task Roles are included as a list. --- - name: include some roles roles: - role1 - role2 Roles are executed before the tasks. In specific cases you might have to execute tasks before the roles. To do so, you can specify these tasks in a pre_tasks section. Also, it\u0026rsquo;s possible to use the post_tasks section to include tasks that will be executed after the roles, but also after tasks specified in the playbook as well as the handlers they call. Creating Custom Roles # Use mkdir roles to create a roles subdirectory in the current directory, and use cd roles to get into that subdirectory. Use ansible-galaxy init motd to create the motd role structure. Add contents to motd/tasks/main.yml Add contents to motd/templates/motd.j2 Add contents to motd/defaults/main.yml Add contents to motd/meta/main.yml Create the playbook exercise91.yaml to run the role Run the playbook by using ansible-playbook exercise91.yaml Verify that modifications have been applied correctly by using the ad hoc command ansible ansible2 -a \u0026quot;cat /etc/motd\u0026quot; Sample role all under roles/motd/:\ndefaults/main.yml\n--- # defaults file for motd system_manager: anna@example.com meta/main.yml\ngalaxy_info: author: Sander van V description: your description company: your company (optional) license: license (GPLv2, CC-BY, etc) min_ansible_version: 2.5 tasks/main.yml\n--- tasks file for motd - name: copy motd file template: src: templates/motd.j2 dest: /etc/motd owner: root group: root mode: 0444 templates/motd.j2\nWelcome to {{ ansible_hostname }} This file was created on {{ ansible_date_time.date }} Disconnect if you have no business being here Contact {{ system_manager }} if anything is wrong Playbook motd.yml:\n--- - name: use the motd role playbook hosts: ansible2 roles: - role: motd system_manager: bob@example.com handlers/main.yml example:\n--- # handlers file for base-config - name: source profile command: source /etc/profile - name: source bash command: source /etc/bash.bashrc Managing Role Dependencies # Roles may use other roles as a dependency. You can put role dependencies in meta/main.yml Dependent roles are always executed before the roles that depend on them. Dependent roles are executed once. When two roles that are used in a playbook call the same dependency, the dependent role is executed once only. When calling dependent roles, it is possible to pass variables to the dependent role. You can define a when statement to ensure that the dependent role is executed only in specific situations. Defining dependencies in meta/main.yml\ndependencies: - role: apache port: 8080 - role: mariabd when: environment == ’production’ Understanding File Organization Best Practices # Working with roles splits the contents of the role off the tasks that are run through the playbook.\nSplitting files to store them in a location that makes sense is common in Ansible\nWhen you\u0026rsquo;re working with Ansible, it\u0026rsquo;s a good idea to work with project directories in bigger environments.\nWorking with project directories makes it easier to delegate tasks and have the right people responsible for the right things.\nEach project directory may have its own ansible.cfg file, inventory file, and playbooks.\nIf the project grows bigger, variable files and other include files may be used, and they are normally stored in subdirectories.\nAt the top-level directory, create the main playbook from which other playbooks are included. The suggested name for the main playbook is site.yml.\nUse group_vars/ and host_vars/ to set host-related variables and do not define them in inventory.\nConsider using different inventory files to differentiate between production and staging phases.\nUse roles to standardize common tasks.\nWhen you are working with roles, some additional recommendations apply:\nUse a version control repository to maintain roles in a consistent way. Git is commonly used for this purpose.\nSensitive information should never be included in roles. Use Ansible Vault to store sensitive information in an encrypted way.\nUse ansible-galaxy init to create the role base structure. Remove files and directories you don\u0026rsquo;t use.\nDon\u0026rsquo;t forget to provide additional information in the role\u0026rsquo;s README.md and meta/main.yml files.\nKeep roles focused on a specific function. It is better to use multiple roles to perform multiple tasks.\nTry to develop roles in a generic way, such that they can be used for multiple purposes.\nLab 9-1 # Create a playbook that starts the Nginx web server on ansible1, according to the following requirements: • A requirements file must be used to install the Nginx web server. Do NOT use the latest version of the Galaxy role, but instead use the version before that. • The same requirements file must also be used to install the latest version of postgresql. • The playbook needs to ensure that neither httpd nor mysql is currently installed.\nLab 9-2 # Use the RHEL SELinux System Role to manage SELinux properties according to the following requirements:\n• A Boolean is set to allow SELinux relabeling to be automated using cron. • The directory /var/ftp/uploads is created, permissions are set to 777, and the context label is set to public_content_rw_t. • SELinux should allow web servers to use port 82 instead of port 80. • SELinux is in enforcing state. Subjects: ansible-playbook timesync.yaml to run the playbook. Observe its output. Notice that some messages in red are shown, but these can safely be ignored.\n5. Use ansible ansible2 -a \u0026quot;timedatectl show\u0026quot; and notice that the timezone variable is set to UTC.\nLab 9-1 # Create a playbook that starts the Nginx web server on ansible1, according to the following requirements: • A requirements file must be used to install the Nginx web server. Do NOT use the latest version of the Galaxy role, but instead use the version before that. • The same requirements file must also be used to install the latest version of postgresql. ansible-galaxy install -r roles/requirements.yml\ncat roles/requirements.yml\n- src: geerlingguy.nginx version: \u0026#34;3.1.4\u0026#34; - src: geerlingguy.postgresql • The playbook needs to ensure that neither httpd nor mysql is currently installed.\n--- - name: ensure conflicting packages are not installed hosts: web1 tasks: - name: remove packages yum: name: - mysql - httpd state: absent - name: nginx web server hosts: web1 roles: - geerlingguy.nginx - geerlingguy.postgresql (Had to add a variable file for redhat 10 into the role. )\nLab 9-2 # Use the RHEL SELinux System Role to manage SELinux properties according to the following requirements:\n• A Boolean is set to allow SELinux relabeling to be automated using cron. • The directory /var/ftp/uploads is created, permissions are set to 777, and the context label is set to public_content_rw_t. • SELinux should allow web servers to use port 82 instead of port 80. • SELinux is in enforcing state.\nvim lab92.yml\n--- - name: manage ftp selinux properties hosts: ftp1 vars: selinux_booleans: - name: cron_can_relabel state: true persistent: true selinux_state: enforcing selinux_ports: - ports: 82 proto: tcp setype: http_port_t state: present local: true tasks: - name: create /var/ftp/uploads/ file: path: /var/ftp/uploads state: directory mode: 777 - name: set selinux context sefcontext: target: \u0026#39;/var/ftp/uploads(/.*)?\u0026#39; setype: public_content_rw_t ftype: d state: present notify: run restorecon - name: Execute the role and reboot in a rescue block block: - name: Include selinux role include_role: name: rhel-system-roles.selinux rescue: - name: \u0026gt;- Fail if failed for a different reason than selinux_reboot_required fail: msg: \u0026#34;role failed\u0026#34; when: not selinux_reboot_required - name: Restart managed host reboot: - name: Wait for managed host to come back wait_for_connection: delay: 10 timeout: 300 - name: Reapply the role include_role: name: rhel-system-roles.selinux handlers: - name: run restorecon command: restorecon -v /var/ftp/uploads Ansible Galaxy # Ansible Galaxy is a public library of Ansible content provided by community members. It can be browsed on the web at https://galaxy.ansible.com.\nThe content includes collections and roles. Collections are groups of modules, roles, playbooks, and plugins the accomplish a certain task. Like setting up a web server.\nYou can search for roles on the site and see the date edited and the amount of times the role or collection has been downloaded.\nTags to make identifying Galaxy roles easier. They provide information about a role and are used to make searching easier.\nDownload roles directly from the Ansible Galaxy website or use the ansible-galaxy command.\nThe ansible-galaxy Command # Search the name and description of roles with a given keyword.\nansible-galaxy search\nGet more information about a role.\n`ansible-galaxy info\nSome handy command-line options: # --platforms\nOperating system platform to search for --author GitHub username of the author --galaxy-tags Additional tags to filter by Managing Ansible Galaxy Roles # Install a role into the ~/.ansible/roles directory. (Or path specified in the role_path setting in ansible.cfg.) ansible-galaxy install\nYou can also manually specify the path instead with the -p option.\nAnsible requirements file # A requirements file is YAML file that has a list of required roles and collections needed to run your infrastructure or role. You can include it when using the ansible-roles command.\n--- collections: - name: ansible.posix - name: community.general - name: awx.awx - name: community.proxmox - name: geerlingguy.php_roles - name: community.mysql - src: geerlingguy.nginx version: \u0026#34;2.7.0\u0026#34; You can add roles from a git repository or from a tarball. You must specify the full URL or path using the src option.\nFor a git repository, the scm keyword is also required and must be set to git.\nInstall a role using the requirements file:\nansible-galaxy install -r roles/requirements.yml\nGet a list of currently installed roles:\nansible-galaxy list\nRemove a role: ansible-galaxy remove\nExamples # ansible-galaxy search --author geerlingguy --platforms EL ansible-galaxy search nginx --author geerlingguy --platforms EL ansible-galaxy info geerlingguy.nginx. ansible-galaxy install -r listing96.yaml ansible-galaxy list\nUsing RHEL System Roles # Allows for a uniform approach while managing multiple RHEL versions Red Hat provides RHEL System Roles. RHEL System Roles make managing different parts of the operating system easy. RHEL System Roles:\nrhel-system-roles.kdump\nConfigures the kdump crash recovery service rhel system-roles.network Configures network interfaces rhel system-roles.postfix Configures hosts as a Mail Transfer Agent using Postfix rhel system-roles.selinux Manages SELinux settings rhel system-roles.storage Configures storage rhel system-roles.timesync Configures time synchronization Understanding RHEL System Roles # RHEL System Roles are based on the community Linux System Roles Provide a uniform interface to make configuration tasks easier where significant differences may exist between versions of the managed operating system. RHEL System Roles can be used to manage Red Hat Enterprise Linux 6.10 and later, as well as RHEL 7.4 and later, and all versions of RHEL 8. Linux System Roles are not supported by RHEL technical support. Installing RHEL System Roles # To use RHEL System Roles, you need to install the rhel-system-roles package on the control node by using sudo yum install rhel-system-roles.\nThis package can be found in the RHEL 8 AppStream repository.\nAfter installation, the roles are copied to the /usr/share/ansible/roles directory, a directory that is a default part of the Ansible roles_path setting.\nIf a modification to the roles_path setting has been made in ansible.cfg, the roles are applied to the first directory listed in the roles_path.\nWith the roles, some very useful documentation is installed also; you can find it in the /usr/share/doc/rhel-system-roles directory.\nTo pass configuration to the RHEL System Roles, variables are important.\nIn the documentation directory, you can find information about variables that are required and used by the role.\nSome roles also contain a sample playbook that can be used as a blueprint when defining your own role.\nIt\u0026rsquo;s a good idea to use these as the basis for your own RHEL System Roles\u0026ndash;based configuration.\nThe next two sections describe the SELinux and the TimeSync System Roles, which provide nice and easy-to-implement examples of how you can use the RHEL System Roles.\nUsing the RHEL SELinux System Role # You learned earlier how to manage SELinux settings using task definitions in your own playbooks.\nUsing the RHEL SELinux System Role provides an easy-to-use alternative.\nTo use this role, start by looking at the documentation, which is in the /usr/share/doc/rhel-system-roles/selinux directory.\nA good file to start with is the README.md file, which provides lists of all the ingredients that can be used.\nThe SELinux System Role also comes with a sample playbook file.\nThe most important part of this file is the vars: section, which defines the variables that should be applied by SELinux.\nVariable Definition in the SELinux System Role:\n--- - hosts: all become: true become_method: sudo become_user: root vars: selinux_policy: targeted selinux_state: enforcing selinux_booleans: - { name: ’samba_enable_home_dirs’, state: ’on’ } - { name: ’ssh_sysadm_login’, state: ’on’, persistent: ’yes’ } selinux_fcontexts: - { target: ’/tmp/test_dir(/.*)?’, setype: ’user_home_dir_t’, ftype: ’d’ } selinux_restore_dirs: - /tmp/test_dir selinux_ports: - { ports: ’22100’, proto: ’tcp’, setype: ’ssh_port_t’, state: ’present’ } selinux_logins: - { login: ’sar-user’, seuser: ’staff_u’, serange: ’s0-s0:c0.c1023’, state: ’present’ } SELinux Variables Overview\nselinux_policy\nPolicy to use, usually set to targeted selinux_state\nSELinux state, as managed with setenforce selinux_booleans\nList of Booleans that need to be set selinux_fcontext\nList of file contexts that need to be set, including the target file or directory to which they should be applied. selinux_restore_dir\nList of directories at which the Linux restorecon command needs to be executed to apply new context. selinux_ports\nList of ports and SELinux port types selinux_logins\nA list of SELinux user and roles that can be created\nMost of the time while configuring SELinux, you need to apply the correct state as well as file context.\nTo set the appropriate file context, you first need to define the selinux_fcontext variable.\nNext, you have to define selinux_restore_dirs also to ensure that the desired context is applied correctly.\nLab: Sets the httpd_sys_content_t context type to the /web directory. # Sample doc is used and unnecessary lines are removed and the values of two variables have been set When you use the RHEL SELinux System Role, some changes require the managed host to be rebooted. To take care of this, a block structure is used, where the System Role runs in the block. When a change that requires a reboot is applied, the SELinux System Role sets the variable selinux_reboot_required and fails. As a result, the rescue section in the playbook is executed. This rescue section first makes sure that the playbook fails because of the selinux_reboot_required variable being set to true. If that is the case, the reboot module is called to reboot the managed host. While rebooting, playbook execution waits for the rebooted host to reappear, and when that happens, the RHEL SELinux System Role is called again to complete its work. --- - hosts: ansible2 vars: selinux_policy: targeted selinux_state: enforcing selinux_fcontexts: - { target: ’/web(/.*)?’, setype: ’httpd_sys_content_t’, ftype: ’d’ } selinux_restore_dirs: - /web # prepare prerequisites which are used in this playbook tasks: - name: Creates directory file: path: /web state: directory - name: execute the role and catch errors block: - include_role: name: rhel-system-roles.selinux rescue: # Fail if failed for a different reason than selinux_reboot_required. - name: handle errors fail: msg: \u0026#34;role failed\u0026#34; when: not selinux_reboot_required - name: restart managed host shell: sleep 2 \u0026amp;\u0026amp; shutdown -r now \u0026#34;Ansible updates triggered\u0026#34; async: 1 poll: 0 ignore_errors: true - name: wait for managed host to come back wait_for_connection: delay: 10 timeout: 300 - name: reapply the role include_role: name: rhel-system-roles.selinux Using the RHEL TimeSync System Role # timesync_ntp_servers variable\nmost important setting\nspecifies attributes to indicate which time servers should be used.\nThe hostname attribute identifies the name of IP address of the time server.\nThe iburst option is used to enable or disable fast initial time synchronization using the timesync_ntp_servers variable.\nThe System Role finds out which version of RHEL is used, and according to the currently used version, it either configures NTP or Chronyd.\nLab: Using an RHEL System Role to Manage Time Synchronization # 1. Copy the sample timesync playbook to the current directory: cp /usr/share/doc/rhel-system-roles/timesync/example-single-pool-playbook.yml timesync.yaml\n2. Add the target host, NTP hostname pool.ntp.org, and remove pool true in the file timesync.yaml:\n--- - name: Configure NTP hosts: \u0026#34;{{ host }}\u0026#34; vars: timesync_ntp_servers: - hostname: pool.ntp.org iburst: true roles: - rhel-system-roles.timesync 3. Add the timezone module and the timezone variable to the playbook to set the timezone as well. The complete playbook should look like the following:\n--- - hosts: ansible2 vars: timesync_ntp_servers: - hostname: pool.ntp.org iburst: yes timezone: UTC roles: - rhel-system-roles.timesync tasks: - name: set timezone timezone: name: \u0026#34;{{ timezone }}\u0026#34; 4. Use ansible-playbook timesync.yaml to run the playbook. Observe its output. Notice that some messages in red are shown, but these can safely be ignored.\n5. Use ansible ansible2 -a \u0026quot;timedatectl show\u0026quot; and notice that the timezone variable is set to UTC.\n","externalUrl":null,"permalink":"/tech/ansible/getting-started-with-ansible-roles/","section":"Teches","summary":"\u003cp\u003eAnsible roles are fully-featured solutions to accomplish a task. Roles can be downloaded from Ansible Galaxy, built from scratch, or vendor created like RHEL System Roles.\u003c/p\u003e\n\u003cp\u003eRoles provide a reusable way to build a webserver, a database server, and more. That way you can have a role handle tasks the same way on all of your server.\u003c/p\u003e","title":"Getting Started with Ansible Roles","type":"tech"},{"content":" Using Handlers # A task that is triggered and is executed by a successful task. Working with Handlers # Define a notify statement at the level where the task is defined. The notify statement should list the name of the handler that is to be executed Handlers are listed at the end of the play. Make sure the name of the handler matches the name of the item that is called in the notify statement, because that is what the handler is looking for. Handlers can be specified as a list, so one task can call multiple handlers. Lab # Define the file index.html on localhost. Use this file in the second play to set up the web server. The handler is triggered from the task where the copy module is used to copy the index.html file. If this task is successful, the notify statement calls the handler. A second task is defined, which is intended to fail. --- - name: create file on localhost hosts: localhost tasks: - name: create index.html on localhost copy: content: \u0026#34;welcome to the webserver\u0026#34; dest: /tmp/index.html - name: set up web server hosts: all tasks: - name: install httpd yum: name: httpd state: latest - name: copy index.html copy: src: /tmp/index.html dest: /var/www/html/index.html notify: - restart_web - name: copy nothing - intended to fail copy: src: /tmp/nothing dest: /var/www/html/nothing.html handlers: - name: restart_web service: name: httpd state: restarted All tasks up to copy index.html run successfully. However, the task copy nothing fails, which is why the handler does not run. The solution seems easy: the handler doesn\u0026rsquo;t run because the task that copies the file /tmp/nothing fails as the source file doesn\u0026rsquo;t exist.\nCreate the source file using touch /tmp/nothing on the control host and run the task again.\nAfter creating the source file and running the playbook again, the handler still doesn\u0026rsquo;t run.\nHandlers run only if the task that triggers them gives a changed status.\nRun an ad hoc command to remove the /var/www/html/index.html file on the managed hosts and run the playbook again: ansible ansible2 -m file -a \u0026quot;name=/var/www/html/index.html state=absent\u0026quot;\nRun the playbook again and you\u0026rsquo;ll see the handler runs.\nUnderstanding Handler Execution and Exceptions # When a task fails, none of the following tasks run. How does that make handlers different? A handler runs only on the success of a task, but the next task in the list also runs only if the previous task was successful. What, then, is so special about handlers?\nThe difference is in the nature of the handler.\nHandlers are meant to perform an extra action when a task makes a change to a host. Handler should be considered an extension to the regular task. A conditional task that runs only upon the success of a previous task. Two methods to get Handlers to run even if a subsequent task fails:\nforce_handlers: true (More specific and preferred)\nUsed in the play header to ensure that the handler will run even if a task fails. ignore_errors: true\nUsed in the play header to accomplish the same thing. • Handlers are specified in a handlers section at the end of the play. • Handlers run in the order they occur in the handlers section and not in the order as triggered. • Handlers run only if the task calling them generates a changed status. • Handlers by default will not run if any task in the same play fails, unless force_handlers or ignore_errors are used. • Handlers run only after all tasks in the play where the handler is activated have been processed. You might want to define multiple plays to avoid this behavior.\nLab: Working with Handlers # 1. Open a playbook with the name exercise73.yaml.\n2. Define the play header:\n--- - name: update the kernel hosts: all force_handlers: true tasks: 3. Add a task that updates the current kernel:\n--- - name: update the kernel hosts: all force_handlers: true tasks: - name: update kernel yum: name: kernel state: latest notify: reboot_server 4. Add a handler that reboots the server in case the kernel was successfully updated:\n--- - name: update the kernel hosts: all force_handlers: true tasks: - name: update kernel yum: name: kernel state: latest notify: reboot_server handlers: - name: reboot_server command: reboot 5. Run the playbook using ansible-playbook exercise73.yaml andobserve its result. Notice that the handler runs only if the kernel was updated. If the kernel already was at the latest version, nothing has changed and the handler does not run. Also notice that it wasn\u0026rsquo;t really necessary to use force_handlers in the play header, but by using it anyway, at least you now know where to use it.\nDealing with Failures # Understanding Task Execution # Tasks in Ansible playbooks are executed in the order they are specified. If a task in the playbook fails to execute on a host, the task generates an error and the play does not further execute on that specific host. This also goes for handlers: if any task that follows the task that triggers a handler fails, the handlers do not run. In both of these cases, it is important to know that the tasks that have run successfully still generate their result. Because this can give an unexpected result, it is important to always restore the original situation if that happens. any_errors_fatal\nUsed in the play header or on a block. Stop executing on all hosts when a failing task is encountered Managing Task Errors # Generically, tasks can generate three different types of results. ok\nThe tasks has run successfully but no changes were applied changed The task has run successfully and changes have been applied failed While running the task, a failure condition was encountered ignore_errors: yes\nKeep running the playbook even if a task fails force_handlers. If\ncan be used to ensure that handlers will be executed, even if a failing task was encountered. Lab: ignore_errors # --- - name: restart sshd only if crond is running hosts: all tasks: - name: get the crond server status command: /usr/bin/systemctl is-active crond ignore_errors: yes register: result - name: restart sshd based on crond status service: name: sshd state: restarted when: result.rc == 0 Lab: Forcing Handlers to Run # --- - name: create file on localhost hosts: localhost tasks: - name: create index.html on localhost copy: content: \u0026#34;welcome to the webserver\u0026#34; dest: /tmp/index.html - name: set up web server hosts: all force_handlers: yes tasks: - name: install httpd yum: name: httpd state: latest - name: copy index.html copy: src: /tmp/index.html dest: /var/www/html/index.html notify: - restart_web - name: copy nothing - intended to fail copy: src: /tmp/nothing dest: /var/www/html/nothing.html handlers: - name: restart_web service: name: httpd state: restarted Specifying Task Failure Conditions # failed_when\nconditional used to evaluate some expression. Set a failure condition on a task Lab: failed_when # --- - name: demonstrating failed_when hosts: all tasks: - name: run a script command: echo hello world ignore_errors: yes register: command_result failed_when: \u0026#34;’world’ in command_result.stdout\u0026#34; - name: see if we get here debug: msg: second task executed fail module\nspecify when a task fails. Using this module makes sense only if when is used to define the exact condition when a failure should occur. Lab: Using the fail Module # --- - name: demonstrating the fail module hosts: all ignore_errors: yes tasks: - name: run a script command: echo hello world register: command_result - name: report a failure fail: msg: the command has failed when: \u0026#34;’world’ in command_result.stdout\u0026#34; - name: see if we get here debug: msg: second task executed The ignore_errors statement has movedfrom the task definition to the play header. Without this move, the message \u0026ldquo;second task executed\u0026rdquo; would never be shown because the fail module always generates a failure message. The main advantage of using the fail module instead of using failed_when is that the fail module can easily be used to set a clear failure message, which is not possible when using failed_when. Managing Changed Status # In Ansible, there are commands that change something and commands that don\u0026rsquo;t. Some commands, however, are not very obvious in reporting their status.\nLab: Change status # --- - name: demonstrate changed status hosts: all tasks: - name: check local time command: date register: command_result - name: print local time debug: var: command_result.stdout Reports a changed status, even if nothing really was changed!\nManaging the changed status can be useful in avoiding unexpected results while running a playbook.\nchanged_when\nIf you set changed_when to false, the playbook reports only an ok or failed status and never reports a changed status. Lab: Using changed_when # --- - name: demonstrate changed status hosts: all tasks: - name: check local time command: date register: command_result changed_when: false - name: print local time debug: var: command_result.stdout Using Blocks # Useful when working with conditional statements. A group of tasks to which a when statement can be applied. As a result, if a single condition is true, multiple tasks can be executed. To do so, between the tasks: statement in the play header and the actual tasks that run the specific modules, you can insert a block: statement. Lab: Using Blocks # --- - name: simple block example hosts: all tasks: - name: setting up http block: - name: installing http yum: name: httpd state: present - name: restart httpod service: name: httpd state: started when: ansible_distribution == \u0026#34;CentOS\u0026#34; The when statement is applied at the same level as the block definition. When you define it this way, the tasks in the block are executed only if the when statement is true. Using Blocks with rescue and always Statements # Blocks can be used for simple error handling as well, in such a way that if any task that is defined in the block statement fails, the tasks that are defined in the rescue section are executed. Besides that, an always section can be used to define tasks that should always run, regardless of the success or failure of the tasks in the block. Lab: Using Blocks, rescue, and always # - name: using blocks hosts: all tasks: - name: intended to be successful block: - name: remove a file shell: cmd: rm /var/www/html/index.html - name: printing status debug: msg: block task was operated rescue: - name: create a file shell: cmd: touch /tmp/rescuefile - name: printing rescue status debug: msg: rescue task was operated always: - name: always write a message to logs shell: cmd: logger hello - name: always printing this message debug: msg: this message is always printed Run this twice to see the rescue. (The file is already created so a task in the block fails) command_warnings=False\nSetting in ansible.cfg to avoid seeing command module warning message.\nyou cannot use a block on a loop.\nIf you need to iterate over a list of values, think of using a different solution.\nLab Install and Enable a Webserver\n","externalUrl":null,"permalink":"/tech/ansible/handlers/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eUsing Handlers\n    \u003cdiv id=\"using-handlers\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-handlers\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA task that is triggered and is executed by a successful task.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 class=\"relative group\"\u003eWorking with Handlers\n    \u003cdiv id=\"working-with-handlers\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#working-with-handlers\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a \u003cstrong\u003enotify\u003c/strong\u003e statement at the level where the task is defined.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003enotify\u003c/strong\u003e statement should list the name of the handler that is to be executed\u003c/li\u003e\n\u003cli\u003eHandlers are listed at the end of the play.\u003c/li\u003e\n\u003cli\u003eMake sure the name of the handler matches the name of the item that is called in the \u003cstrong\u003enotify\u003c/strong\u003e statement, because that is what the handler is looking for.\u003c/li\u003e\n\u003cli\u003eHandlers can be specified as a list, so one task can call multiple handlers.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eLab\n    \u003cdiv id=\"lab\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#lab\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDefine the file index.html on localhost. Use this file in the second play to set up the web server.\u003c/li\u003e\n\u003cli\u003eThe handler is triggered from the task where the copy module is used to copy the index.html file.\u003c/li\u003e\n\u003cli\u003eIf this task is successful, the \u003cstrong\u003enotify\u003c/strong\u003e statement calls the handler.\u003c/li\u003e\n\u003cli\u003eA second task is defined, which is intended to fail.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-yml\" data-lang=\"yml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e---\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ecreate file on localhost\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ehosts\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003elocalhost\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003etasks\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ecreate index.html on localhost\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"nt\"\u003ecopy\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"nt\"\u003econtent\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;welcome to the webserver\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"nt\"\u003edest\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e/tmp/index.html\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eset up web server\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ehosts\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eall\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003etasks\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003einstall httpd\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"nt\"\u003eyum\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ehttpd\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003elatest\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ecopy index.html\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"nt\"\u003ecopy\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003esrc\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e/tmp/index.html\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003edest\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e/var/www/html/index.html\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"nt\"\u003enotify\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e- \u003cspan class=\"l\"\u003erestart_web\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ecopy nothing - intended to fail\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"nt\"\u003ecopy\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003esrc\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e/tmp/nothing\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003edest\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e/var/www/html/nothing.html\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ehandlers\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003erestart_web\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e          \u003c/span\u003e\u003cspan class=\"nt\"\u003eservice\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ehttpd\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e            \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003erestarted\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAll tasks up to \u003cstrong\u003ecopy index.html\u003c/strong\u003e run successfully. However, the task \u003cstrong\u003ecopy nothing\u003c/strong\u003e fails, which is why the handler does not run. The solution seems easy: the handler doesn\u0026rsquo;t run because the task that copies the file /tmp/nothing fails as the source file doesn\u0026rsquo;t exist.\u003c/p\u003e","title":"Handlers","type":"tech"},{"content":"Working with host name patterns\nWorking with Host Name Patterns # If you want to use an IP address in a playbook, the IP address must be specified as such in the inventory.\nYou cannot use IP addresses that are based only on DNS name resolving.\nSo specifying an IP address in the playbook but not in the inventory file\u0026mdash;assuming DNS name resolution is going to take care of the IP address resolving\u0026mdash;doesn\u0026rsquo;t work.\napart from the specified groups, there are the implicit host groups all and ungrouped.\nhost name wildcards may be used.\nansible -m ping 'ansible\\*' match all hosts that have a name starting with ansible. Must put the pattern between single quotes or it will fail with a no matching hosts error. Can be used at any place in the host name. ansible -m ping '\\*ble1' When you use wildcards to match host names, Ansible doesn\u0026rsquo;t distinguish between IP addresses, host names, or hosts; it just matches anything.\n'web\\*' Matches all servers that are members of the group \u0026lsquo;webservers\u0026rsquo;, but also hosts \u0026lsquo;web1\u0026rsquo; and \u0026lsquo;web2\u0026rsquo;. To address multiple hosts:\nYou specify a comma-separated list of targets to address multiple hosts: ansible -m ping ansible1,192.168.4.202 Can be a mix of host names, IP addresses, and host group names. Operators:\nCan specify a logical AND condition by including an ampersand (\u0026amp;), and a logical NOT by using an exclamation point (!). web,\u0026amp;file applies to hosts only if they are members of the web and file groups web,!webserver1 applies to all hosts in the web group, except host webserver1. When you use the logical AND operator, the position of the ampersand doesn\u0026rsquo;t matter. web,\u0026amp;file as \u0026amp;web,file also. You can use a colon (:) instead of a comma (,), but using a comma is better to avoid confusion when using IPv6 addresses. ","externalUrl":null,"permalink":"/tech/ansible/host-name-patterns/","section":"Teches","summary":"\u003cp\u003eWorking with host name patterns\u003c/p\u003e\n\n\u003ch4 class=\"relative group\"\u003eWorking with Host Name Patterns\n    \u003cdiv id=\"working-with-host-name-patterns\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#working-with-host-name-patterns\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIf you want to use an IP address in a playbook, the IP address must be specified as such in the inventory.\u003c/p\u003e","title":"Host Name Patterns","type":"tech"},{"content":"Word Press is great, but it is probably a lot more bloated then you need for a personal website. Enter Hugo, it has less server capacity and storage needs than Word Press. Hugo is a static site generator than takes markdown files and converts them to html.\nHosting your own website is also a lot cheaper than having a provider like Bluehost do it for you. Instead of $15 per month, I am currently paying $10 per year.\nThis guide will walk through building a website step-by-step.\nSetting up a Virtual Private Server (VPS) Registering a domain name Pointing the domain to your server Setting up hugo on your local PC Syncing your Hugo generate site with your server Using nginx to serve your site Enable http over SSL Setting up a Virtual Private Server (VPS) # I use Vultr as my VPS. When I signed up they had a $250 credit towards a new account. If you select the cheapest server (you shouldn\u0026rsquo;t need anything else for a basic site) that comes out to about $6 a month. Of course the $250 credit goes towards that which equates to around 41 months free.\nHead to vultr.com. Create and account and Select the Cloud Compute option.\nUnder CPU \u0026amp; Storage Technology, select \u0026ldquo;Regular Performance\u0026rdquo;. Then under \u0026ldquo;Server Location, select the server closest to you. Or closest to where you think your main audience will be.\nUnder Server image, select the OS you are most comfortable with. This guide uses Debian.\nUnder Server Size, slect the 10GB SSD. Do not select the \u0026ldquo;IPv6 ONLY\u0026rdquo; option. Leave the other options as default and enter your server hostname.\nOn the products page, click your new server. You can find your server credentials and IPv4 address here. You will need these to log in to your server.\nLog into your sever via ssh to test. From a Linux terminal run:\nssh username@serveripaddress Then, enter your password when prompted.\nRegistering a Domain Name # I got my domain perfectdarkmode.com from Cloudflare.com for about $10 per year. You can check to see available domains there. You can also check https://www.namecheckr.com/ to see iof that name is available on various social media sites.\nIn CloudFlare, just click \u0026ldquo;add a site\u0026rdquo; and pick a domain that works for you. Next, you will need your server address from earlier.\nUnder domain Registration, click \u0026ldquo;Manage Domains\u0026rdquo;, click \u0026ldquo;manage\u0026rdquo; on your domain. One the sidebar to the right, there is a qucik actions menu. Click \u0026ldquo;update DNS configuration\u0026rdquo;.\nClick \u0026ldquo;Add record\u0026rdquo;. Type is an \u0026ldquo;A\u0026rdquo; record. Enter the name and the ip address that you used earlier for your server. Uncheck \u0026ldquo;Proxy Status\u0026rdquo; and save.\nYou can check to see if your DNS has updated on various DNS severs at https://dnschecker.org/. Once those are up to date (after a couple minutes) you should be able to ping your new domain.\n$ ping perfectdarkmode.com PING perfectdarkmode.com (104.238.140.131) 56(84) bytes of data. 64 bytes from 104.238.140.131.vultrusercontent.com (104.238.140.131): icmp_seq=1 ttl=53 time=33.2 ms 64 bytes from 104.238.140.131.vultrusercontent.com (104.238.140.131): icmp_seq=2 ttl=53 time=28.2 ms 64 bytes from 104.238.140.131.vultrusercontent.com (104.238.140.131): icmp_seq=3 ttl=53 time=31.0 ms Now, you can use the same ssh command to ssh into your vultr serverusing your domain name.\nssh username@domain.com Setting up hugo on your local PC # Hugo is a popular open-source static site generator. It allows you to take markdown files, and builds them into and html website. To start go to https://gohugo.io/installation/ and download Hugo on your local computer. (I will show you how to upload the site to your server later.)\nPick a theme The theme I use is here https://themes.gohugo.io/themes/hugo-theme-hello-friend-ng/\nYou can browse your own themes as well. Just make sure to follow the installation instructions. Let\u0026rsquo;s create a new Hugo site. Change into the directory where you want your site to be located in. Mine rests in ~/Documents/.\ncd ~/Documents/ Create your new Hugo site.\nhugo new site site-name This will make a new folder with your site name in the ~/Documents directory. This folder will have a few directories and a config file in it.\narchetypes config.toml content data layouts public resources static themes For this tutorial, we will be working with the config.toml file and the content, public, static, and themes. Next, load the theme into your site directory. For the Hello Friend NG theme:\ngit clone https://github.com/rhazdon/hugo-theme-hello-friend-ng.git themes/hello-friend-ng Now we will load the example site into our working site. Say yes to overwrite.\ncp -a themes/hello-friend-ng/exampleSite/* . The top of your new config.toml site now contains:\nbaseURL = \u0026#34;https://example.com\u0026#34; title = \u0026#34;Hello Friend NG\u0026#34; languageCode = \u0026#34;en-us\u0026#34; theme = \u0026#34;hello-friend-ng\u0026#34; Replace your baseURL with your site name and give your site a title. Set the enableGlobalLanguageMenu option to false if you want to remove the language swithcer option at the top. I also set enableThemeToggle to true so users could set the theme to dark or light.\nYou can also fill in the links to your social handles. Comment out any lines you don\u0026rsquo;t want with a \u0026ldquo;#\u0026rdquo; like so:\n[params.social](params.social) name = \u0026#34;twitter\u0026#34; url = \u0026#34;https://twitter.com/\u0026#34; [params.social](params.social) name = \u0026#34;email\u0026#34; url = \u0026#34;mailto:nobody@example.com\u0026#34; [params.social](params.social) name = \u0026#34;github\u0026#34; url = \u0026#34;https://github.com/\u0026#34; [params.social](params.social) name = \u0026#34;linkedin\u0026#34; url = \u0026#34;https://www.linkedin.com/\u0026#34; # [params.social](params.social) # name = \u0026#34;stackoverflow\u0026#34; # url = \u0026#34;https://www.stackoverflow.com/\u0026#34; You may also want to edit the footer text to your liking. I commented out the second line that comes with the example site:\n[params.footer] trademark = true rss = true copyright = true author = true topText = [] bottomText = [ # \u0026#34;Powered by \u0026lt;a href=\\\u0026#34;http://gohugo.io\\\u0026#34;\u0026gt;Hugo\u0026lt;/a\u0026gt;\u0026#34;, # \u0026#34;Made with \u0026amp;#10084; by \u0026lt;a href=\\\u0026#34;https://github.com/rhazdon\\\u0026#34;\u0026gt;Djordje Atlialp\u0026lt;/a\u0026gt;\u0026#34; ] Now, move the contents of the example contents folder over to your site\u0026rsquo;s contents folder (giggidy):\ncp -r ~/Documents/hugo/themes/hello-friend-ng/exampleSite/content/* ~/Documents/hugo/content/ Let\u0026rsquo;s clean up a little bit. Cd into ~/Documents/hugo/content/posts. Rename the file to the name of your first post. Also, delete all of the other files here:\ncd ~/Documents/hugo/contents/posts mv goisforlovers.md newpostnamehere.md find . ! -name \u0026#39;newpostnamehere.md\u0026#39; -type f -exec rm -f {} + Open the new post file and delete everything after this:\n+++ title = \u0026#34;Building a Minimalist Website with Hugo\u0026#34; description = \u0026#34;\u0026#34; type = [\u0026#34;posts\u0026#34;,\u0026#34;post\u0026#34;] tags = [ \u0026#34;hugo\u0026#34;, \u0026#34;nginx\u0026#34;, \u0026#34;ssl\u0026#34;, \u0026#34;http\u0026#34;, \u0026#34;vultr\u0026#34;, ] date = \u0026#34;2023-03-26\u0026#34; categories = [ \u0026#34;tools\u0026#34;, \u0026#34;linux\u0026#34;, ] series = [\u0026#34;tools\u0026#34;] [ author ] name = \u0026#34;David Thomas\u0026#34; +++ You will need to fill out this header information for each new post you make. This will allow you to give your site a title, tags, date, categories, etc. This is what is called a TOML header. TOML stands for Tom\u0026rsquo;s Obvious Minimal Language. Which is a minimal language used for parsing data. Hugo uses TOML to fill out your site.\nSave your doc and exit. Next, there should be an about.md page now in your ~/Documents/hugo/Contents folder. Edit this to edit your about page for your site. You can use this Markdown Guide if you need help learning markdown language. https://www.markdownguide.org/\nServe your website locally # Let\u0026rsquo;s test the website by serving it locally and accessing it at localhost:1313 in your web browser. Enter the command:\nhugo serve Hugo will now be generating your website. You can view it by entering localhost:1313 in your webbrowser.\nYou can use this to test new changes before uploading them to your server. When you svae a post or page file such as your about page, hugo will automatically update the changes to this local page if the local server is running.\nPress \u0026ldquo;Ctrl + c\u0026rdquo; to stop this local server. This is only for testing and does not need to be running to make your site work.\nBuild out your public directory # Okay, your website is working locally, how do we get it to your server to host it online? We are almost there. First, we will use the hugo command to build your website in the public folder. Then, we will make a copy of our public folder on our server using rsync. I will also show you how to create an alias so you do not have to remember the rsync command every time.\nFrom your hugo site folder run:\nhugo Next, we will put your public hugo folder into /var/www/ on your server. Here is how to do that with an alias. Open ~/.bashrc.\nvim ~/.bashrc Add the following line to the end of the file, making sure to replace the username and server name:\n# My custom aliases alias rsyncp=\u0026#39;rsync -rtvzP ~/Documents/hugo/public/ username@myserver.com:/var/www/public\u0026#39; Save and exit the file. Then tell bash to update it\u0026rsquo;s source config file.\nsource ~/.bashrc Now your can run the command by just using the new alias any time. Your will need to do this every time you update your site locally.\nrsyncp Set up nginx on your server # Install nginx\napt update apt upgrade apt install nginx create an nginx config file in /etc/nginx/sites-available/\nvim /etc/nginx/sites-available/public You will need to add the following to the file, update the options, then save and exit:\nserver { listen 80 ; listen [::]:80 ; server_name example.org ; root /var/www/mysite ; index index.html index.htm index.nginx-debian.html ; location / { try_files $uri $uri/ =404 ; } } Enter your domain in \u0026ldquo;server_name\u0026rdquo; line in place of \u0026ldquo;example.org\u0026rdquo;. Also, point \u0026ldquo;root\u0026rdquo; to your new site file from earlier. (/var/www/public). Then save and exit.\nLink this site-available config file to sites-enabled to enable it. Then restart nginx:\nln -s /etc/nginx/sites-available/public /etc/nginx/sites-enabled systemctl reload nginx Access Permissions # We will need to make sure nginx has permissions to your site folder so that it can access them to serve your site. Run:\nchmod 777 /var/www/public Firewall Permissions # You will need to make sure your firewall allows port 80 and 443. Vultr installs the ufw program by default. But your can install it if you used a different provider. Beware, enabling a firewalll could block you from accessing your vm, so do your research before tinkering outside of these instructions.\nufw allow 80 ufw allow 443 Nginx Security # We will want to hide your nginx version number on error pages. This will make your site a bit harder for hackers to find exploits. Open your Nginx config file at /etc/nginx/nginx.conf and remove the \u0026ldquo;#\u0026rdquo; before \u0026ldquo;server_tokens off;\u0026rdquo;\nEnter your domain into your browser. Congrats! You now have a running website!\nUse Certbot to enable HTTPS # Right now, our site uses the unencrypted http. We want it to use the encrypted version HTTPS (HTTP over SSL). This will increase user privacy, hide usernames and passwords used on your site, and you get the lock symbol by your URL name instead of \u0026ldquo;!not secure\u0026rdquo;.\nInstall Certbot and It\u0026rsquo;s Nginx Module # apt install python3-certbot-nginx Run certbot # certbot --nginx Fill out the information, certbot asks for your emaill so it can send you a reminder when the certs need to be renewed every 3 months. You do not need to consent to giving your email to the EFF. Press 1 to select your domain. And 2 too redirect all connections to HTTPS.\nCertbot will build out some information in your site\u0026rsquo;s config file. Refresh your site. You should see your new fancy lock icon.\nSet Up a Cronjob to automatically Renew certbot certs # crontab -e Select a text editor and add this line to the end of the file. Then save and exit the file:\n0 0 1 * * certbot --nginx renew You now have a running website. Just make new posts locally, the run \u0026ldquo;hugo\u0026rdquo; to rebuild the site. And use the rsync alias to update the folder on your server. I will soon be making tutorials on making an email address for your domain, such as david@perfectdarkmode.com on my site. I will also be adding a comments section, RSS feed, email subscription, sidebar, and more.\nFeel free to reach out with any questions if you get stuck. This is meant to be an all encompassing guide. So I want it to work.\nExtras # Optimizing images # Create assets folder in main directory.\nCreate images folder in /assets\nAccess image using hugo pipes\n{{ $image := resources.Get \u0026#34;images/test-image.jpg\u0026#34; }} \u0026lt;img src=\u0026#34;{{ ( $image.Resize \u0026#34;500x\u0026#34; ).RelPermalink }}\u0026#34; /\u0026gt; https://gohugo.io/content-management/image-processing/\n","externalUrl":null,"permalink":"/tech/tools/how-to-build-a-website-with-hugo/","section":"Teches","summary":"\u003cp\u003eWord Press is great, but it is probably a lot more bloated then you need for a personal website. Enter Hugo, it has less server capacity and storage needs than Word Press. Hugo is a static site generator than takes markdown files and converts them to html.\u003c/p\u003e","title":"How to Build a website With Hugo","type":"tech"},{"content":"Here are my highlights pulled up in Vim:\nAs you can see, Bookfusion gives you a lot of extra information when you export highlights. First, let\u0026rsquo;s get rid of the lines that begin with ##\nEnter command mode in Vim by pressing esc. Then type :g/^##/d and press enter.\nMuch better.\nNow let\u0026rsquo;s get rid of the color references:`\n:g/^Color/d To get rid of the timestamps, we must find a different commonality between the lines. In this case, each line ends with \u0026ldquo;UTC\u0026rdquo;. Let\u0026rsquo;s match that:\n:g/UTC$/d Where $ matches the end of the line.\nNow, I want to get rid of the \u0026gt; on each line: %s/\u0026gt; //g\nAlmost there, you\u0026rsquo;ll notice there are 6 empty lines in between each highlight. Let\u0026rsquo;s shrink those down into one:\n:%s/\\(\\n\\)\\{3,}/\\r\\r/g The command above matches newline character n 3 or more times and replaces them with two newline characters /r/r.\nAs we scroll down, I see a few weird artifacts from the book conversion to markdown.\nNow, I want to get rid of any carrot brackets in the file. Let\u0026rsquo;s use the substitute command again here:\n%s/\u0026lt;//g Depending on your book and formatting. You may have some other stuff to edit.\n","externalUrl":null,"permalink":"/tech/tools/process-bookfusion-highlights/","section":"Teches","summary":"\u003cp\u003eHere are my highlights pulled up in Vim:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"\"\n    src=\"../../img/Pasted%20image%2020240414034031.png\"\n    \u003e\u003c/figure\u003e\n\u003cp\u003eAs you can see, Bookfusion gives you a lot of extra information when you export highlights. First, let\u0026rsquo;s get rid of the lines that begin with \u003ccode\u003e##\u003c/code\u003e\u003c/p\u003e","title":"How to Process Bookfusion Highlights with Vim","type":"tech"},{"content":" Hugo Setup # Adding a module as a theme # Make sure Go is installed\ngo version Create a new site\nhugo new site sitename cd sitename Initialize your site as a module\nhugo mod init sitename Confirm\ncat go.mod Add the module as a dependency using it\u0026rsquo;s git link\nhugo mod get github.com/McShelby/hugo-theme-relearn Confirm\ncat go.mod add the theme to config.toml\n# add this line to config.toml and save theme = [\u0026#34;github.com/McShelby/hugo-theme-relearn\u0026#34;] Confirm by viewing site\nhugo serve # visit browser at http://localhost:1313/ to view site Adding a new \u0026ldquo;chapter\u0026rdquo; page\nhugo new --kind chapter Chapter/_index.md Add a home page\nhugo new --kind home _index.md Add a default page\nhugo new \u0026lt;chapter\u0026gt;/\u0026lt;name\u0026gt;/_index.md or\nhugo new \u0026lt;chapter\u0026gt;/\u0026lt;name\u0026gt;.md You will need to change some options in _index.md\n+++ # is this a \u0026#34;chaper\u0026#34;? chapter=true archetype = \u0026#34;chapter\u0026#34; # page title name title = \u0026#34;Linux\u0026#34; # The \u0026#34;chapter\u0026#34; number weight = 1 +++ Adding a \u0026ldquo;content page\u0026rdquo; under a category\nhugo new basics/first-content.md Create a sub directory:\nhugo new basics/second-content/_index.md change draft = true to draft = false in the content page to make a page render. Global site parameters # Add these to your config.toml file and edit as you please\n[params] # This controls whether submenus will be expanded (true), or collapsed (false) in the # menu; if no setting is given, the first menu level is set to false, all others to true; # this can be overridden in the pages frontmatter alwaysopen = true # Prefix URL to edit current page. Will display an \u0026#34;Edit\u0026#34; button on top right hand corner of every page. # Useful to give opportunity to people to create merge request for your doc. # See the config.toml file from this documentation site to have an example. editURL = \u0026#34;\u0026#34; # Author of the site, will be used in meta information author = \u0026#34;\u0026#34; # Description of the site, will be used in meta information description = \u0026#34;\u0026#34; # Shows a checkmark for visited pages on the menu showVisitedLinks = false # Disable search function. It will hide search bar disableSearch = false # Disable search in hidden pages, otherwise they will be shown in search box disableSearchHiddenPages = false # Disables hidden pages from showing up in the sitemap and on Google (et all), otherwise they may be indexed by search engines disableSeoHiddenPages = false # Disables hidden pages from showing up on the tags page although the tag term will be displayed even if all pages are hidden disableTagHiddenPages = false # Javascript and CSS cache are automatically busted when new version of site is generated. # Set this to true to disable this behavior (some proxies don\u0026#39;t handle well this optimization) disableAssetsBusting = false # Set this to true if you want to disable generation for generator version meta tags of hugo and the theme; # don\u0026#39;t forget to also set Hugo\u0026#39;s disableHugoGeneratorInject=true, otherwise it will generate a meta tag into your home page disableGeneratorVersion = false # Set this to true to disable copy-to-clipboard button for inline code. disableInlineCopyToClipBoard = false # A title for shortcuts in menu is set by default. Set this to true to disable it. disableShortcutsTitle = false # If set to false, a Home button will appear below the search bar on the menu. # It is redirecting to the landing page of the current language if specified. (Default is \u0026#34;/\u0026#34;) disableLandingPageButton = true # When using mulitlingual website, disable the switch language button. disableLanguageSwitchingButton = false # Hide breadcrumbs in the header and only show the current page title disableBreadcrumb = true # If set to true, hide table of contents menu in the header of all pages disableToc = false # If set to false, load the MathJax module on every page regardless if a MathJax shortcode is present disableMathJax = false # Specifies the remote location of the MathJax js customMathJaxURL = \u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026#34; # Initialization parameter for MathJax, see MathJax documentation mathJaxInitialize = \u0026#34;{}\u0026#34; # If set to false, load the Mermaid module on every page regardless if a Mermaid shortcode or Mermaid codefence is present disableMermaid = false # Specifies the remote location of the Mermaid js customMermaidURL = \u0026#34;https://unpkg.com/mermaid/dist/mermaid.min.js\u0026#34; # Initialization parameter for Mermaid, see Mermaid documentation mermaidInitialize = \u0026#34;{ \\\u0026#34;theme\\\u0026#34;: \\\u0026#34;default\\\u0026#34; }\u0026#34; # If set to false, load the Swagger module on every page regardless if a Swagger shortcode is present disableSwagger = false # Specifies the remote location of the RapiDoc js customSwaggerURL = \u0026#34;https://unpkg.com/rapidoc/dist/rapidoc-min.js\u0026#34; # Initialization parameter for Swagger, see RapiDoc documentation swaggerInitialize = \u0026#34;{ \\\u0026#34;theme\\\u0026#34;: \\\u0026#34;light\\\u0026#34; }\u0026#34; # Hide Next and Previous page buttons normally displayed full height beside content disableNextPrev = true # Order sections in menu by \u0026#34;weight\u0026#34; or \u0026#34;title\u0026#34;. Default to \u0026#34;weight\u0026#34;; # this can be overridden in the pages frontmatter ordersectionsby = \u0026#34;weight\u0026#34; # Change default color scheme with a variant one. Eg. can be \u0026#34;auto\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34; or an array like [ \u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34; ]. themeVariant = \u0026#34;auto\u0026#34; # Change the title separator. Default to \u0026#34;::\u0026#34;. titleSeparator = \u0026#34;-\u0026#34; # If set to true, the menu in the sidebar will be displayed in a collapsible tree view. Although the functionality works with old browsers (IE11), the display of the expander icons is limited to modern browsers collapsibleMenu = false # If a single page can contain content in multiple languages, add those here additionalContentLanguage = [ \u0026#34;en\u0026#34; ] # If set to true, no index.html will be appended to prettyURLs; this will cause pages not # to be servable from the file system disableExplicitIndexURLs = false # For external links you can define how they are opened in your browser; this setting will only be applied to the content area but not the shortcut menu externalLinkTarget = \u0026#34;_blank\u0026#34; Syntax highlighting # Supports a variety of [Code Syntaxes] To select the syntax, wrap the code in backticks and place the syntax by the first set of backticks.\n```bash echo hello \\``` Adding tags # Tags are displayed in order at the top of the page. They will also display using the menu shortcut made further down.\nAdd tags to a page:\n+++ tags = [\u0026#34;tutorial\u0026#34;, \u0026#34;theme\u0026#34;] title = \u0026#34;Theme tutorial\u0026#34; weight = 15 +++ Choose a default color theme # Add to config.toml with the chosen theme for the \u0026ldquo;style\u0026rdquo; option:\n[markup] [markup.highlight] # if `guessSyntax = true`, there will be no unstyled code even if no language # was given BUT Mermaid and Math codefences will not work anymore! So this is a # mandatory setting for your site if you want to use Mermaid or Math codefences guessSyntax = false # choose a color theme or create your own style = \u0026#34;base16-snazzy\u0026#34; Add Print option and search output page. # add the following to config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;PRINT\u0026#34;, \u0026#34;SEARCH\u0026#34;] section = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;PRINT\u0026#34;] page = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;PRINT\u0026#34;] Customization # This theme has a bunch of editable customizations called partials. You can overwrite the default partials by putting new ones in /layouts/partials/.\nto customize \u0026ldquo;partials\u0026rdquo;, create a \u0026ldquo;partials\u0026rdquo; directory under site/layouts/\ncd layouts mkdir partials cd partials You can find all of the partials available for this theme here\nChange the site logo using the logo.html partial # Create logo.html in /layouts/partials\nvim logo.html Add the content you want in html. This can be an img html tag referencing an image in the static folder. Or even basic text. Here is the basic syntax of an html page, adding \u0026ldquo;Perfect Dark Mode\u0026rdquo; as the text to display:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h3\u0026gt;Perfect Dark Mode\u0026lt;/h3\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Add a favicon to your site # This is pasted from the relearn site. Add Favicon and edit * If your favicon is a SVG, PNG or ICO, just drop off your image in your local static/images/ folder and name it favicon.svg, favicon.png or favicon.ico respectively. If no favicon file is found, the theme will lookup the alternative filename logo in the same location and will repeat the search for the list of supported file types.\nIf you need to change this default behavior, create a new file in layouts/partials/ named favicon.html. Then write something like this:\n\u0026lt;link rel=\u0026#34;icon\u0026#34; href=\u0026#34;/images/favicon.bmp\u0026#34; type=\u0026#34;image/bmp\u0026#34;\u0026gt; Changing theme colors # In your config.toml file edit the themeVariant option under [params]\nthemeVariant = \u0026#34;relearn-dark\u0026#34; There are some options to choose from or you can custom make your theme colors by using this stylesheet generator\nMenu Shortcuts Add a [[menu.shortcuts]] entry for each link\n[[menu.shortcuts]] name = \u0026#34;\u0026lt;i class=\u0026#39;fab fa-fw fa-github\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; GitHub repo\u0026#34; identifier = \u0026#34;ds\u0026#34; url = \u0026#34;https://github.com/McShelby/hugo-theme-relearn\u0026#34; weight = 10 [[menu.shortcuts]] name = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-fw fa-camera\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Showcases\u0026#34; url = \u0026#34;more/showcase/\u0026#34; weight = 11 [[menu.shortcuts]] name = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-fw fa-bookmark\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Hugo Documentation\u0026#34; identifier = \u0026#34;hugodoc\u0026#34; url = \u0026#34;https://gohugo.io/\u0026#34; weight = 20 [[menu.shortcuts]] name = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-fw fa-bullhorn\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Credits\u0026#34; url = \u0026#34;more/credits/\u0026#34; weight = 30 [[menu.shortcuts]] name = \u0026#34;\u0026lt;i class=\u0026#39;fas fa-fw fa-tags\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; Tags\u0026#34; url = \u0026#34;tags/\u0026#34; weight = 40 Extras\nMenu button arrows. (Add to page frontmatter)\nmenuPre = \u0026#34;\u0026lt;i class=\u0026#39;fa-fw fas fa-caret-right\u0026#39;\u0026gt;\u0026lt;/i\u0026gt; \u0026#34; ","externalUrl":null,"permalink":"/tech/tools/how-to-set-up-hugo-relearn-theme/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003eHugo Setup\n    \u003cdiv id=\"hugo-setup\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#hugo-setup\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\n\u003ch2 class=\"relative group\"\u003eAdding a module as a theme\n    \u003cdiv id=\"adding-a-module-as-a-theme\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#adding-a-module-as-a-theme\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eMake sure Go is installed\u003c/p\u003e","title":"How to Set Up Hugo Relearn Theme","type":"tech"},{"content":" It took me a whopping 2 years to finish my CCNA! I kept giving up and quitting my studies for months at a time. Why? Because I couldn\u0026rsquo;t remember the massive amount of content covered in the CCNA. It felt hopeless. I could have done it in 6 month (or faster) if I knew how to study.\nI hadn\u0026rsquo;t taken a test in 10 years before this. So I had completely forgotten how to learn. This post is about the mistakes I made studying for the CCNA and how to avoid them.\nYou will also learn, as I did, about spaced repetition. I\u0026rsquo;ve also included a 6 month CCNA spaced repetition calendar.\nMy Mistakes, So You Don\u0026rsquo;t Make Them # Mistake #1 Didn\u0026rsquo;t start flashcards until the final 30 days # I wish I would have started flashcards from day 1. This would have helped a crap ton. Remembering all of the little details is not only useful for taking the test. It embeds the concepts in your brain and keeps you processing how things work .\nIf there is anything you take from this list. You should definitely be doing some flashcards every day.\nMistake #2 Not enough labs as I went. # While studying the OCG and video courses. I did some labs. But I also skipped a ton of labs because it wasn\u0026rsquo;t convenient at the time. Then I was forced to lab every single topic in the final 30 days. A lot of cramming was done..\nMake sure to do all of the labs as you go. Make up your own labs as well. This is very important to building job worthy skills.\nMistake #3 Didn\u0026rsquo;t have a plan or stick with it. # When your plan consists of, \u0026ldquo;just read everything and watch the videos and take the test when you feel ready\u0026rdquo;, you tend to procrastinate and put things off. Make a study schedule and a solid plan. (See below)\nHaving a set date for when you will take the test was pretty motivating. I did not find this out until about 30 days until my test.\nSpaced Repetition # If you are using Anki flashcards for your studies, you may already be using spaced repetition. Spaced repetition is repeatedly reviewing with the time in between reviews getting longer each time you review it.\nHere is an excellent article about our learning curves and why spaced repetition helps us remember things https://fs.blog/spacing-effect/\nHow to set up a spaced repetition calendar for CCNA. # Step 1. Plan how long your studies will take\nFigure out how long you need. It usually takes around 240 hours of studying for CCNA. (Depending on experience). Then figure out how many hours per day that you can spend on studying. This example is based on a 6 month study calendar.\nYou can use this 6 month excel calendar to plan and track your progress. You. can still use this method If you have already been studying CCNA. Just edit your calendar for how much time you have left.\nThe calendar is also based on Wendel Odom\u0026rsquo;s Official Cert Guide. You will also want to mix your other resources into your reviews.\nDecide what your review sessions will be\nPlan to review each chapter 3-4 times. here is what I did for review sessions to pass the exam.\nReview 1 Read and highlight (and flashcards)\nRead the chapter. Highlight key information that you want to remember. Do a lab for the material you studied (if applicable) Answer DIKTA questions Start Chapter 1 Anki Flascards Review 2 Copy highlights over to OneNote (keep doing flashcards)\nCopy your highlights over to OneNote. (using copy and paste if you have the digital book) Read your highlights and make sure you understand everything. lab and continue doing flashcards. (just go through Anki suggested flashcards, not just ones for the specific chapter.) Review 3 Labs and Highlight your notes (and flashcards)\nMore labs! Go over your notes. Color coding everything. (You can find my jumbled note mess here) Green: Read again Teal: Very important Learn this/ lab it. Red/ purple: make extra flashcards out of this. Review 4 Practice questions and review\nGo through and answer the DIKTA questions again. Review any missed answers. Lab anything you aren\u0026rsquo;t quite sure of. The final 30 days # I HIGHLY recommend Boson ExSim for your final 30 days of studying. ExSim comes with 3 exams (A,B, and C). Start with exam A in test simulation mode. Leave about a week in between each practice exam so you can go over your answers and Boson\u0026rsquo;s explanations for each answer.\nOne week before your test, (after you\u0026rsquo;ve completed exams A,B, and C). Do a random exam. Make sure you do the timed version that don\u0026rsquo;t show your score as you go.\nYou should be scoring above 900 by your 3rd and 4th exam if you have been reviewing Boson\u0026rsquo;s answer explanations.\nSchedule your exam\nPearson view didn\u0026rsquo;t let me schedule the exam past 30 days out from when I wanted to take it. I\u0026rsquo;m not sure if this is the case all the time. But by the time you are 30 days out you should have your test scheduled. This will light the fire under you. Great motivation for the home stretch.\nIf your exam is around June during Cisco Live, Cisco usually offers a 50% discount for an exam voucher. You probably won’t find any other discounts unless you pay for Cisco’s specific CCNA training.\nFinal word on labs # You can technically pass the CCNA without doing many labs. But this will leave you at a HUGE disadvantage in the job market. Labs are crucial for really understanding networking. Knowing your way around the CLI and being able to troubleshoot networking issues will make you stand out from those who crammed for the exam.\nIf you’ve made it this far I really appreciate you taking the time to read this post. I really hope it helps at least one person.\n","externalUrl":null,"permalink":"/tech/tools/how-to-study-for-the-ccna-exam/","section":"Teches","summary":"\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"CCNA Study Calendar\"\n    src=\"../../img/CCNA.png\"\n    \u003e\u003c/figure\u003e\n\u003cp\u003eIt took me a whopping 2 years to finish my CCNA! I kept giving up and quitting my studies for months at a time. Why? Because I couldn\u0026rsquo;t remember the massive amount of content covered in the CCNA. It felt hopeless. I could have done it in 6 month (or faster) if I knew how to study.\u003c/p\u003e","title":"How to Study for the CCNA Exam","type":"tech"},{"content":"When content is included, it is dynamically processed at the moment that Ansible reaches that content.\nIf content is imported, Ansible performs the import operation before starting to work on the tasks in the playbook. Files can be included and imported at different levels:\n• Roles: Roles are typically used to process a complete set of instructions provided by the role. Roles have a specific structure as well.\n• Playbooks: Playbooks can be imported as a complete playbook. You cannot do this from within a play. Playbooks can be imported only at the top level of the playbook.\n• Tasks: A task file is just a list of tasks and can be imported or included in another task.\n• Variables: Variables can be maintained in external files and included in a playbook. This makes managing generic multipurpose variables easier.\nImporting Playbooks # Importing playbooks is common in a setup where one master playbook is used, from which different additional playbooks are included. According to the Ansible Best Practices Guide (which is a part of the Ansible documentation), the master playbook could have the name site.yaml, and it can be used to include playbooks for each specific set of servers, for instance.\nWhen a playbook is imported, this replaces the entire play. You cannot import a playbook at a task level; it needs to happen at a play level. Listing 10-4 gives an example of the playbook imported in Listing 10-5. In Listing 10-6, you can see the result of running the ansible-playbook listing105.yaml command. Listing 10-4 Sample Playbook to Be Imported\n::: pre_1 - hosts: all tasks: - debug: msg: running the imported play :::\nListing 10-5 Importing a Playbook\n::: pre_1 \u0026mdash; - name: run a task hosts: all tasks: - debug: msg: running task1\n- name: importing a playbook import_playbook: listing104.yaml :::\nListing 10-6 Running ansible-playbook listing105.yaml Result\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing105.yaml\nPLAY [run a task] ************************************************************** TASK [Gathering Facts] ********************************************************* ok: [ansible2] ok: [ansible1] ok: [ansible3] ok: [ansible4] TASK [debug] ******************************************************************* ok: [ansible1] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running task1\u0026quot; } ok: [ansible2] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running task1\u0026quot; } ok: [ansible3] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running task1\u0026quot; } ok: [ansible4] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running task1\u0026quot; } PLAY [all] ********************************************************************* TASK [Gathering Facts] ********************************************************* ok: [ansible2] ok: [ansible1] ok: [ansible3] ok: [ansible4] TASK [debug] ******************************************************************* ok: [ansible1] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running the imported play\u0026quot; } ok: [ansible2] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running the imported play\u0026quot; } ok: [ansible3] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running the imported play\u0026quot; } ok: [ansible4] =\u0026gt; { \u0026quot;msg\u0026quot;: \u0026quot;running the imported play\u0026quot; } PLAY RECAP ********************************************************************* ansible1 : ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible2 : ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible3 : ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible4 : ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 :::\nImporting and Including Task Files # import_tasks\ntasks are statically imported while executing the playbook. include_tasks\ntasks are dynamically included at the moment they are needed. Dynamically including task files is recommended when the task file is used in a conditional statement. If task files are mainly used to make development easier by working with separate task files, they can be statically imported. There are a few considerations when working with import_tasks to statically import tasks:\n• Loops cannot be used with import_tasks.\n• If a variable is used to specify the name of the file to import, this cannot be a host or group inventory variable.\n• When you use a when statement on the entire import_tasks file, the conditional statements are applied to each task that is involved.\nAs an alternative, include_tasks can be used to dynamically include a task file. This approach also comes with some considerations:\n• When you use the ansible-playbook --list-tasks command, tasks that are in the included tasks are not displayed.\n• You cannot use ansible-playbook --start-at-task to start a playbook on a task that comes from an included task file.\n• You cannot use a notify statement in the main playbook to trigger a handler that is in the included tasks file.\n::: note\nTip\nWhen you use includes and imports to work with task files, the recommendation is to store the task files in a separate directory. Doing so makes it easier to delegate task management to specific users.\n:::\nUsing Variables When Importing and Including Files # The main goal to work with imported and included files is to make working with reusable code easy. To make sure you reach this goal, the imported and included files should be as generic as possible. That means it\u0026rsquo;s a bad idea to include names of specific items that may change when used in a different context. Think, for instance, of the names of packages, users, services, and more.\nTo deal with include files in a flexible way, you should define specific items as variables. Within the include_tasks file, for instance, you refer to {{ package }}, and in the main playbook from which the include files are called, you can define the variables. Obviously, you can use this approach with a straight variable definition or by using host variable or group variable include files.\n::: note\nExam tip\nIt\u0026rsquo;s always possible to configure items in a way that is brilliant but quite complex. On the exam it\u0026rsquo;s not a smart idea to go for complex. Just keep your solution as easy as possible. The only requirement on the exam is to get things working, and it doesn\u0026rsquo;t matter exactly how you do that.\n:::\nIn Listings 10-7 through 10-10, you can see how include and import files are used to work on one project. The main playbook, shown in Listing 10-9, defines the variables to be used, as well as the names of the include and import files. Listings 10-7 and 10-8 show the code from the include files, which use the variables that are defined in Listing 10-9. The result of running the playbook in Listing 10-9 can be seen in Listing 10-10.\nListing 10-7 The Include Tasks File tasks/service.yaml Used for Services Definition\n::: pre_1 - name: install {{ package }} yum: name: \u0026ldquo;{{ package }}\u0026rdquo; state: latest - name: start {{ service }} service: name: \u0026ldquo;{{ service }}\u0026rdquo; enabled: true state: started :::\nThe sample tasks file in Listing 10-7 is straightforward; it uses the yum module to install a package and the service module to start and enable the package. The variables this file refers to are defined in the main playbook in Listing 10-9.\nListing 10-8 The Import Tasks File tasks/firewall.yaml Used for Firewall Definition\n::: pre_1 - name: install the firewall package: name: \u0026ldquo;{{ firewall_package }}\u0026rdquo; state: latest - name: start the firewall service: name: \u0026ldquo;{{ firewall_service }}\u0026rdquo; enabled: true state: started - name: open the port for the service firewalld: service: \u0026ldquo;{{ item }}\u0026rdquo; immediate: true permanent: true state: enabled loop: \u0026ldquo;{{ firewall_rules }}\u0026rdquo; :::\nIn the sample firewall file in Listing 10-8, the firewall service is installed, defined, and configured. In the configuration of the firewalld service, a loop is used on the variable firewall_rules. This variable obviously is defined in Listing 10-9, which is the file where site-specific contents such as variables are defined.\nListing 10-9 Main Playbook Example\n::: pre_1 \u0026mdash; - name: setup a service hosts: ansible2 tasks: - name: include the services task file include_tasks: tasks/service.yaml vars: package: httpd service: httpd when: ansible_facts[’os_family’] == ’RedHat’ - name: import the firewall file import_tasks: tasks/firewall.yaml vars: firewall_package: firewalld firewall_service: firewalld firewall_rules: - http - https :::\nThe main playbook in Listing 10-9 shows the site-specific configuration. It performs two main tasks: it defines variables, and it calls an include file and an import file. The variables that are defined are used by the include and import files. The include_tasks statement is executed in a when statement. Notice that the firewall_rules variable contains a list as its value, which is used by the loop that is defined in the import file.\nListing 10-10 Running ansible-playbook listing109.yaml\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing109.yaml\nPLAY [setup a service] ********************************************************* TASK [Gathering Facts] ********************************************************* ok: [ansible2] TASK [include the services task file] ****************************************** included: /home/ansible/rhce8-book/tasks/service.yaml for ansible2 TASK [install httpd] *********************************************************** ok: [ansible2] TASK [start httpd] ************************************************************* changed: [ansible2] TASK [install the firewall] **************************************************** changed: [ansible2] TASK [start the firewall] ****************************************************** ok: [ansible2] TASK [open the port for the service] ******************************************* changed: [ansible2] =\u0026gt; (item=http) changed: [ansible2] =\u0026gt; (item=https) PLAY RECAP ********************************************************************* ansible2 : ok=7 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 :::\nThe interesting thing in the Listing 10-10 output is that the include file is dynamically included while running the playbook. This is not the case for the statically imported file. In Exercise 10-3 you practice working with include files.\n::: box Exercise 10-3 Using Includes and Imports\nIn this exercise you create a simple master playbook that installs a service. The name of the service is defined in a variable file, and the specific tasks are included through task files.\n1. Open the file exercise103-vars.yaml and define three variables as follows:\npackagename: vsftpd servicename: vsftpd firewalld_servicename: ftp 2. Create the exercise103-ftp.yaml file and give it the following contents to install, enable, and start the vsftpd service and also to make it accessible in the firewall:\n- name: install {{ packagename }} yum: name: \u0026#34;{{ packagename }}\u0026#34; state: latest - name: enable and start {{ servicename }} service: name: \u0026#34;{{ servicename }}\u0026#34; state: started enabled: true - name: open the service in the firewall firewalld: service: \u0026#34;{{ firewalld_servicename }}\u0026#34; permanent: yes state: enabled 3. Create the exercise103-copy.yaml file that manages the /var/ftp/pub/README file and make sure it has the following contents:\n- name: copy a file copy: content: \u0026#34;welcome to this server\u0026#34; dest: /var/ftp/pub/README 4. Create the master playbook exercise103.yaml that includes all of them and give it the following contents:\n--- - name: install vsftpd on ansible2 vars_files: exercise103-vars.yaml hosts: ansible2 tasks: - name: install and enable vsftpd import_tasks: exercise103-ftp.yaml - name: copy the README file import_tasks: exercise103-copy.yaml 5. Run the playbook and verify its output\n6. Run an ad hoc command to verify the /var/ftp/pub/README file has been created: ansible ansible2 -a \u0026ldquo;cat /var/ftp/pub/README\u0026rdquo;.\nEnd-of-Chapter Lab # In the end-of-chapter lab with this chapter, you reorganize a playbook to work with several different files instead of one big file. Do this according to the instructions in Lab 10-1.\nLab 10-1 # The lab82.yaml file, which you can find in the GitHub repository that goes with this course, is an optimal candidate for optimization. Optimize this playbook according to the following requirements:\n• Use includes and import to make this a modular playbook where different files are used to distinguish between the different tasks.\n• Optimize this playbook such that it will run on no more than two hosts at the same time and completes the entire playbook on these two hosts before continuing with the next host.\n","externalUrl":null,"permalink":"/tech/ansible/including-and-importing-files/","section":"Teches","summary":"\u003cp\u003eWhen content is included, it is dynamically processed at the moment that Ansible reaches that content.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf content is imported, Ansible performs the import operation before starting to work on the tasks in the playbook.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFiles can be included and imported at different levels:\u003c/p\u003e","title":"Including and importing Files","type":"tech"},{"content":" Chapter 1 RHCSA Notes - Installation # About RHEL9 # Kernel 5.14 Released May 2019 Built along side of Fedora 34 Installer program = Anaconda Default Bootloader = GRUB2 Default automatic partitioning = /boot, /, swap Default desktop environment = GNOME Installation Logs # /root/anaconda-ks.cfg Configuration entered\n/var/log/anaconda/anaconda.log Contains informational, debug, and other general messages\n/var/log/anaconda/journal.log Stores messages generated by many services and components during system installation\n/var/log/anaconda/packaging.log Records messages generated by the dnf and rpm commands during software installation\n/var/log/anaconda/program.log Captures messages generated by external programs\n/var/log/anaconda/storage.log Records messages generated by storage modules\n/var/log/anaconda/syslog Records messages related to the kernel\n/var/log/anaconda/X.log Stores X Window System information\nNote: Logs are created in /tmp then transferred over to /var/log/anaconda once the install is finished. 6 Virtual Consoles # Monitor the installation process. View diagnostic messages. Discover and fix any issues encountered. Information displayed on the console screens is captured in installation log files. Console 1 (Ctrl+Alt+F1)\nMain screen Select language Then switches default console to 6 Console 2 (Ctrl_Alt+F2)\nShell interface for root user Console 3 (Ctrl_Alt+F3)\nDisplays install messages Stores them in /tmp/anaconda.log Info on detected hardware, etc. Console 4 (Ctrl_Alt+F4)\nShows storage messages Stores them in /tmp/storage.log Console 5 (Ctrl_Alt+F5)\nProgram messages Stores them in /tmp/program.log Console 6 (Ctrl_Alt+F6)\nDefault Graphical configuration and installation console screen Console 1 Brings you to the log in screen. Console 2 does nothing. Console 3-6 all bring you to this log in screen\nLab Setup\nVM1\nserver1.example.om 192.168.0.110 Memory: 2GB Storage: 1x20GB 2 vCPUs VM2\nserver2.exmple.om 192.168.0.120 Memory: 2048 Storage: 1x20GB 4x250 MB data disk 1x5GB data disk 2 vCPUs Setting up VM1 # Download the disc iso on Redhat\u0026rsquo;s website: https://access.redhat.com/downloads/content/rhel\nName RHEL9-VM1 Accept defaults.\nSet drive to 20 gigs\npress \u0026ldquo;spe\u0026rdquo; to hlt utooot\nSelet instll\nselet lnguge\nonfigure timezone under time \u0026amp; dte\ngo into instlltion destintion nd li \u0026ldquo;done\u0026rdquo;\nNetwor nd hostnme settings\nhnge the hostnme to server1.exmple.om go to IPv4 settings in networ nd host nd set to mnul ddress: 192.168.0.110 netms 24 gtewy 192.168.0.1 then sve slide the on/off swith in the min menu to on Set root pssword\nChnge the oot order\npower off the vm Set oot sequene to hrd dis first then optil, remove floppy Accept license terms and rete user\nssh from host os with putty\nIssue these Commnds after set up\nwhoami hostname pwd logout or ctrl+d Using cockpit # Web gui for managing RHEL system Comes pre-installed if not then install with: sudo dnf install cockpit must enable cockpit socket sudo systemctl enable --now cockpit.socket https://yourip:9090 Labs # Lab: # Enable cockpit.socket:\nsudo systemctl enable --now cockpit.socket In a web browser, go to https://\u0026lt;your-ip\u0026gt;:9090\n","externalUrl":null,"permalink":"/tech/linux/installation/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003eChapter 1 RHCSA Notes - Installation\n    \u003cdiv id=\"chapter-1-rhcsa-notes---installation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#chapter-1-rhcsa-notes---installation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\n\u003ch2 class=\"relative group\"\u003eAbout RHEL9\n    \u003cdiv id=\"about-rhel9\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#about-rhel9\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eKernel 5.14\u003c/li\u003e\n\u003cli\u003eReleased May 2019\u003c/li\u003e\n\u003cli\u003eBuilt along side of Fedora 34\u003c/li\u003e\n\u003cli\u003eInstaller program = Anaconda\u003c/li\u003e\n\u003cli\u003eDefault Bootloader = GRUB2\u003c/li\u003e\n\u003cli\u003eDefault automatic partitioning = /boot, /, swap\u003c/li\u003e\n\u003cli\u003eDefault desktop environment = GNOME\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eInstallation Logs\n    \u003cdiv id=\"installation-logs\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#installation-logs\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e/root/anaconda-ks.cfg\u003c/strong\u003e Configuration entered\u003c/p\u003e","title":"Installing RHEL Linux","type":"tech"},{"content":"Looking to get started using Fedora or Red Hat operating systems?\nThis guide with get you started with the RHEL Graphical environment, file system, and essential commands to get started using Fedora, Red Hat, or other RHEL based systems.\nRedHat (RHEL9) Graphical Environment (Wayland) # Redhat runs a graphical environment called Wayland. This is the foundation for running GUI apps. Wayland is a client/server display protocol. Which just means that the user (the client) requests a resource and the display manager (the server) serves those resources.\nWayland is slowly replaced and older display protocol called \u0026ldquo;X\u0026rdquo;. And has better graphics capabilities, features, and performance than X. And consists of a Display or Login manager and a Desktop environment.\nThe Display/ Login manager presents the login screen for users to log in. Once you log in, you get to the pre-configured desktop manager or Desktop Environment (DE). The GNOME Display Manager. (GDM)\nFile System and Directory Hierarchy # The standard for the Linux filesystem is the Filesystem Hierarchy Standard (FHS). Which describes locations, names, and permissions for a variety of file types and directories.\nThe directory structure starts at the root. Which is notated by a \u0026ldquo;/\u0026rdquo;. The top levels of the directory can be viewed by running the ls command on the root of the directory tree.\nSize of the root file system is automatically determined by the installer program based on the available disk space when you select the default partitioning (it may be altered). Here is a listing of the contents of /:\n$ ls / afs bin boot dev etc home lib lib64 lost+found media mnt opt proc root run sbin snap srv sys tmp usr var Some of these directories hold static data such as commands, configuration files, kernel and device files, etc. And some hold dynamic data such as log and status files.\nThere are three major categories of file systems. They are:\ndisk-based network-based memory-based Disk-based files systems are physical media such as a hard drive or a USB flash drive and store information persistently. The root and boot file systems and both disk-based and created automatically when you select the default partitioning.\nNetwork-Based file systems are disk-based file systems that are shared over the network for remote access. (Also stored persistently)\nMemory-Based filesystems are virtual. And are created automatically at system startup and destroyed when the system goes down.\nKey Directories in / # /etc (extended text configuration) # This directory contains system configuration files for systemd, LVM, and user shell startup template files.\ndavid@fedora:$ ls /etc abrt dhcp gshadow- locale.conf openldap request-key.d sysctl.conf adjtime DIR_COLORS gss localtime opensc.conf resolv.conf sysctl.d aliases DIR_COLORS.lightbgcolor gssproxy login.defs opensc-x86_64.conf rpc systemd alsa dleyna-server-service.conf host.conf logrotate.conf openvpn rpm system-release alternatives dnf hostname logrotate.d opt rpmdevtools system-release-cpe anaconda dnsmasq.conf hosts lvm os-release rpmlint tcsd.conf anthy-unicode.conf dnsmasq.d hp machine-id ostree rsyncd.conf terminfo apk dracut.conf httpd magic PackageKit rwtab.d thermald appstream.conf dracut.conf.d idmapd.conf mailcap pam.d rygel.conf timidity++.cfg asound.conf egl ImageMagick-7 makedumpfile.conf.sample paperspecs samba tmpfiles.d audit environment init.d man_db.conf passwd sane.d tpm2-tss authselect ethertypes inittab mcelog passwd- sasl2 Trolltech.conf avahi exports inputrc mdevctl.d passwdqc.conf security trusted-key.key bash_completion.d exports.d ipp-usb mercurial pinforc selinux ts.conf bashrc favicon.png iproute2 mime.types pkcs11 services udev bindresvport.blacklist fedora-release iscsi mke2fs.conf pkgconfig sestatus.conf udisks2 binfmt.d filesystems issue modprobe.d pki sgml unbound bluetooth firefox issue.d modules-load.d plymouth shadow updatedb.conf brlapi.key firewalld issue.net mono pm shadow- UPower brltty flatpak java motd polkit-1 shells uresourced.conf brltty.conf fonts jvm motd.d popt.d skel usb_modeswitch.conf ceph fprintd.conf jvm-common mtab ppp sos vconsole.conf chkconfig.d fstab kdump mtools.conf printcap speech-dispatcher vdpau_wrapper.cfg chromium fuse.conf kdump.conf my.cnf profile ssh vimrc chrony.conf fwupd kernel my.cnf.d profile.d ssl virc cifs-utils gcrypt keys nanorc protocols sssd vmware-tools containers gdbinit keyutils ndctl pulse statetab.d vpl credstore gdbinit.d krb5.conf ndctl.conf.d qemu subgid vpnc credstore.encrypted gdm krb5.conf.d netconfig qemu-ga subgid- vulkan crypto-policies geoclue ld.so.cache NetworkManager rc0.d subuid wgetrc crypttab glvnd ld.so.conf networks rc1.d subuid- whois.conf csh.cshrc gnupg ld.so.conf.d nfs.conf rc2.d subversion wireplumber csh.login GREP_COLORS libaudit.conf nfsmount.conf rc3.d sudo.conf wpa_supplicant cups groff libblockdev nftables rc4.d sudoers X11 cupshelpers group libibverbs.d nilfs_cleanerd.conf rc5.d sudoers.d xattr.conf dbus-1 group- libnl npmrc rc6.d swid xdg dconf grub2.cfg libreport nsswitch.conf rc.d swtpm-localca.conf xml debuginfod grub2-efi.cfg libssh nvme reader.conf.d swtpm-localca.options yum.repos.d default grub.d libuser.conf odbc.ini redhat-release swtpm_setup.conf zfs-fuse depmod.d gshadow libvirt odbcinst.ini request-key.conf sysconfig As you can see, there is a lot of stuff here.\n/root # This is the default home directory for the root user.\n/mnt # /mnt is used to temporarily mount a file system.\n/boot (Disk-Based) # This directory contains the Linux Kernel, as well as boot support and configuration files.\nThe size of /boot is determined by the installer program based on the available disk space when you select the default partitioning. It may be set to a different size during or after the installation.\n/home # This is used to store user home directories and other user contents.\n/opt (Optional) # This directory holds additional software that may need to be installed on the system. A sub directory is created for each installed software.\n/usr (UNIX System Resources) # Holds most of the system files such as:\n/usr/bin # Binary directory for user executable commands\n/usr/sbin # System binaries required at boot and system administration commands not intended for execution by normal users. This directory is not included in the default search path for normal users.\n/usr/lib and /usr/lib64 # Contain shared library routines required by many commands/programs located in /usr/bin and /usr/sbin. These are used by kernel and other applications and programs for their successful installation and operation.\n/usr/lib directory also stores system initialization and service management programs. /usr/lib64 contains 64-bit shared library routines.\n/usr/include # Contains header files for the C programming language.\n/usr/local: # This is a system administrator repository for storing commands and tools. These commands not generally included with the original Linux distribution.\nDirectory Contains /usr/local/bin ecutables /usr/local/etc configuration files /usr/local/lib and /usr/local/lib64 library routines /usr/share manual pages, documentation, sample templates, configuration files /usr/src: # This directory is used to store source code.\nVariable Directory (/var) # For data that frequently changes while the system is operational. Such as log, status, spool, lock, etc.\nCommon sub directories in /var:\n/var/log # Contains most system log files. Such as boot logs, user logs, failed user logs, installation logs, cron logs, mail logs, etc.\n/var/opt # Log, status, etc. for software installed in /opt.\n/var/spool # Queued files such as print jobs, cron jobs, mail messages, etc.\n/var/tmp # For large or longer term temporary files that need to survive system reboots. These are deleted if they are not accessed for a period of 30 days.\n/tmp (Temporary) # Temporary files that survive system reboots. These are deleted after 10 days if they are not accessed. Programs may need to create temporary files in order to run.\n/dev (Devices) # Contains Device nodes for physical and virtual devices. Linux kernel talks to devices through these nodes. Device nodes are automatically created and deleted by the udevd service. Which dynamically manages devices.\nThe two types of device files are character (or raw) and block.\nCharacter devices # Accessed serially. Console, serial printers, mice, keyboards, terminals, etc. Block devices # Accessed in a parallel fashion with data exchanged in blocks. Data on block devices is accessed randomly. Hard disk drives, optical drives, parallel printers, etc. Procfs File System (/proc) # Config and status info on: Kernel, CPU, memory, disks, partitioning, file systems, networking, running processes, etc. Zero-length pseudo files point to data maintained by the kernel in the memory. Interface to interact with kernel-maintained information. Contents created in memory at system boot time, updated during runtime, and destroyed at system shutdown. Runtime File System (/run) # Data for processes running on the system. /run/media Used to automatically mount external file systems (CD, DVD, flash USB.) Contents deleted at shutdown. The System File System (/sys) # Info about hardware devices, drivers, and some kernel features. Used by the kernel to load necessary support for devices, create device nodes in /dev, and configure devices. Auto-maintained. Essential System Commands # tree command # List hierarchy of directories and files. Column 2 Size. Column 3 Full path. Options. tree -a :: Include hidden files in the output. tree -d :: Exclude files from the output. tree -h :: Displays file sizes in human-friendly format. tree -f :: Prints the full path for each file. tree -p :: Includes file permissions in the output\nLabs # List only the directories (-d) in the root user’s home directory (/root). # tree -d /root List files in the /etc/sysconfig directory along with their permissions, sizes in human-readable format, and full path. # tree -phf /etc/sysconfig View tree man pages. # man tree Prompt Symbols # Hash sign (#) for root user. Dollar sign ($) for normal users. Linux Commands # Two types of commands:\nUser General purpose. For any user. System Management Superuser. Require elevated privileges. Command Mechanics # Basic Syntax\ncommand option(s) argument(s) Many commands have preconfigured default options and arguments. An option that starts with a single hyphen character (-la, for instance) ::: Short-option format.\nTwo hyphen characters (\u0026ndash;all, for instance) ::: Long-option format. Listing Files and Directories # ls # ll :: shortcut for ls -l Flags ls -l ::: View long listing format. ls -d ::: View info on the specified directory. ls -h ::: Human readable format. ls -a ::: List all files, including the hidden files. ls -t ::: Sort output by date and time with the newest file first. ls -R ::: List contents recursively. ls -i ::: View inode information.\nlabs: # Show the long listing of only /usr without showing its contents. # ls -ld /usr Display all files in the current directory with their sizes in human-friendly format. # ls -lh List all files, including the hidden files, in the current directory with detailed information. # ls -la Sort output by date and time with the newest file first. # ls -lt List contents of the /etc directory recursively. # ls -R /etc List directory info and the contents of a directory recursively. # ls -lR /etc View ls manpage. # man ls Printing Working Directory (pwd) command # Returns the absolute path to a file or directory. Navigating Directories # Absolute path (full path or a fully qualified pathname) :: Points to a file or directory in relation to the top of the directory tree. It always starts with the forward slash (/).\nRelative path :: Points to a file or directory in relation to your current location.\nLabs: # Go one level up into the parent directory using the relative path # cd .. cd into /etc/sysconfig using the absolute path (/etc/sysconfig), or the relative path (etc/sysconfig) # cd /etc/sysconfig cd / cd etc/sysconfig Change into the /usr/bin directory from /etc/sysconfig using relative or absolute path # cd /usr/bin or\ncd ../usr/bin Return to your home directory # cd or\ncd ~ Use the absolute path to change into the home directory of the root user from /etc/sysconfig # cd ../../root Switch between the current and previous directories # cd .. use the cd command to print the home directory of the current user # cd - Terminal Device Files # Unique pseudo (or virtual) numbered device files that represent terminal sessions opened by users. Used to communicate with individual sessions. Stored in the /dev/pts/ (pseudo terminal session). Created when a user opens a new terminal session. Removed when a session closes. tty command # Identify current terminal session. Displays filename and location. Example: /dev/pts/0 Inspecting System’s Uptime and Processor Load # uptime command # Displays: System’s current time. System up time. Number of users currently logged in. Average % CPU load over the past 1, 5, and 15 minutes. 0.00 and 1.00 represent no load and full load. Greater than 1.00 signifies excess load (over 100%). clear command # Clears the terminal screen and places the cursor at the top left of the screen. Can also use Ctrl+l for this command. clear Determining Command Path # Tools for identifying the absolute path of the command that will be executed when you run it without specifying its full path.\nwhich, whereis, and type\nshow the full location of the ls command:\nwhich command # Show command aliases and location. [root@server1 bin]# which ls alias ls=\u0026#39;ls --color=auto\u0026#39; /usr/bin/ls whereis command # Locates binary, source, and manual files for specified command name. [root@server1 bin]# whereis ls ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz /usr/share/man/man1p/ls.1p.gz\u0026gt;) type command # Find whether the given command is an alias, shell built-in, file, function, or keyword. type ls Viewing System Information # uname command # Show system operating system name. [root@server1 bin]# uname Linux Flags uname -s ::: Show kernel name. uname -n ::: Show hostname. uname -r ::: Show kernel release. uname -v ::: Show kernel build date. uname -m ::: Show machine hardware name. uname -p ::: Show processor type. uname -i ::: Show hardware platform. uname -o ::: Show OS name. uname -a ::: Show kernel name, nodename, release, version, machine, and os.\nuname uname -a Linux = Kernel name server1.example.com = Hostname of the system 4.18.0-80.el8.x86_64 = Kernel release #1 SMP Wed Mar 13 12:02:46 UTC 2019 = Date and time of the kernel built x86_64 = Machine hardware name x86_64 = Processor type x86_64 = Hardware platform GNU/Linux = Operating system name Viewing CPU Specs # lscpu command # Shows CPU: Architecture. Operating modes. Vendor. Family. Model. Speed. Cache memory. Virtualization support type. lscpu architecture of the CPU (x86_64) supported modes of operation (32-bit and 64-bit) sequence number of the CPU on this system (1) threads per core (1) cores per socket (1) number of sockets (1) vendor ID (GenuineIntel) CPU model (58) model name (Intel …) speed (2294.784 MHz) amount and levels of cache memory (L1d, L1i, L2, and L3) Getting Help # Manual pages # Informational pages stored in /usr/share/man for each program. See Using Man Pages for more.\nman command # Flags: -k\nPerform a keyword search on manual pages. Must build the database with mandb first. -f\nEquivalent to whatis. Commands to find information/help about programs. # apropos whatis info pinfo /usr/share/doc/\nDirectory with additional program documentation. man passwd line at the bottom indicates the line number of the manual page.\nMan page navigation # h ::: Help on navigation. q ::: Quit the man page. Up arrow key ::: Scroll up one line. Enter or Down arrow key ::: Scroll down one line. f / Spacebar / Page down ::: Move forward one page. b / Page up ::: Move backward one page. d / u ::: Move down/up half a page. g / G ::: Move to the beginning / end of the man pages. :f ::: Display line number and bytes being viewed. /pattern ::: Searches forward for the specified pattern. ?pattern ::: Searches backward for the specified pattern. n / N ::: Find the next / previous occurrence of a pattern.\nHeadings in the Manual # NAME\nName of the command or file with a short description. SYNOPSIS Syntax summary. DESCRIPTION Overview of the command or file. OPTIONS Options available for use. EXAMPLES Some examples to explain the usage. FILES A list of related files. SEE ALSO Reference to other manual pages or topics. BUGS Any reported bugs or issues. AUTHOR Contributor information. Manual Sections # Manual information is split into nine sections for organization and clarity. Man searches through each section until it finds a match. Starts at section 1, then section 2, etc. Some commands in Linux also have a configuration file with an identical name. Ex: passwd command in /usr/bin and the passwd file in /etc. Specify the section to find that page only. Ex: man 5 passwd Section number is located at the top (header) of the page. Section 1\nRefers to user commands. Section 4 Contains special files. Section 5 Describes file formats for many system configuration files. Section 8 Documents system administration and privileged commands designed for the root user. Run man man for more details.\nSearching by Keyword # apropos command # Search all sections of the manual pages and show a list of all entries matching the specified keyword in their names or descriptions. Must mandb command in order to build an indexed database of the manual pages prior to using. mandb mandb command # Build an indexed database of the manual pages. Lab: Find a forgotten XFS administration command. # man -k xfs or apropos xfs Lab: Show a brief list of options and a description. # passwd --help or passwd -? whatis command # Same output as man -f Display one-line manual page descriptions. info and pinfo Commands # Display command detailed documentation. Divided into sections called nodes. Header: Name of the file being displayed. Names of the current, next, and previous nodes. Almost identical to each other. info ls u navigate efficiently.\ninfo page Navigation # Down / Up arrows\nMove forward / backward one line. Spacebar / Del Move forward / backward one page. q Quit the info page. t Go to the top node of the document. s Search Documentation in /usr/share/doc/ # /usr/share/doc/\nStores general documentation for installed packages under subdirectories that match their names. ls -l /usr/share/doc/gzip Online RHEL Documentation # docs.redhat.com Release notes and guides on planning, installation, administration, security, storage management, virtualization, etc. access.redhat.com Labs # Lab 2: Navigate Linux Directory Tree # Check your location in the directory tree.\npwd Show file permissions in the current directory including the hidden files.\nls -la Change directory into /etc and confirm the directory change.\ncd /etc pwd Switch back to the directory where you were before, and run pwd again to verify.\ncd - pwd Lab: Miscellaneous Tasks # Identify the terminal device file.\ntty Open a couple of terminal sessions. Compare the terminal numbers.\ntty /dev/pts/1 Execute the uptime command and analyze the system uptime and processor load information.\nuptime Use three commands to identify the location of the vgs command.\nwhich vgs whereis vgs type vgs Lab: Identify System and Kernel Information # Analyze the basic information about the system and kernel reported. uname -a Examine the key items relevant to the processor.\nlscpu Lab: Man # View man page for uname.\nman uname View the 5 man page section for the shadow.\nman 5 shadow ","externalUrl":null,"permalink":"/tech/linux/interaction/","section":"Teches","summary":"\u003cp\u003eLooking to get started using Fedora or Red Hat operating systems?\u003c/p\u003e\n\u003cp\u003eThis guide with get you started with the RHEL Graphical environment, file system, and essential commands to get started using Fedora, Red Hat, or other RHEL based systems.\u003c/p\u003e","title":"Interaction","type":"tech"},{"content":"All hosts act like they always have, with one default router setting that never has to change.\nThe default routers share a virtual IP address in the subnet, defined by the FHRP.\nHosts use the FHRP virtual IP address as their default router address.\nThe routers exchange FHRP protocol messages so that both agree as to which router does what work at any point in time.\nWhen a router fails or has some other problem, the routers use the FHRP to choose which router\ntakes over responsibilities from the failed router.\nThe Three Solutions for First-Hop Redundancy\nFirst Hop Redundancy Protocol does not name any one protocol. Instead, it names a family of\nprotocols that fill the same role\nHSRP Concepts\noperates with an active/standby model(also more generally called active/passive\nallows two (or more) routers to cooperate\nHSRP Failover\nTo make the switches change their MAC address table entries for VMAC1, R2 sends an Ethernet frame with VMAC1 as the source MAC address. he frame is also a LAN broadcast, so all the switches learn a MAC table entry for VMAC1 that leads toward R2. HSRP Load Balancing\nyou canconfigure multiple instances of HSRP in the same subnet (called multiple HSRP groups),\npreferring one router to be active in one group and the other router to be preferred as active in another.\nFHRPs are needed on any device that acts as a default router, includes both traditional routers and Layer 3 switches. Simple Network Management Protocol # SNMPv2c and SNMPv3 application layer protocol\nprovides a message format for communication between what are termed managers and agents\nmanager\na network management application running on a PC or server\ntypically being called a Network Management Station (NMS)\nuses SNMP protocols to communicate with each SNMP agent.\nCisco Primeseries of management products (www.cisco.com/go/prime) use SNMP (and\nother protocols) to manage networks.\nagents\nexist in the network, one per device that is managed.\nsoftware running inside each device (router, switch, and so on), with knowledge of all the\nvariables on that device that describe the device’s configuration, status, and counters.\nkeeps a operations of the device.database of variables that make up the parameters, status, and counters for the This database, called the Management Information Base (MIB)\nIOS on routers and switches include an SNMP agent, with builtwith the configuration shown later -in MIB, that can be enabled\nSNMP Variable Reading and Writing: SNMP Get and Set\nNMS typically polls the SNMP agent on each device\nNMS can notify the human user in front of the PC or send emails, texts, and so on to notify\nthe network operations staff of any issues identified by the data found by polling the devices. You can even reconfigure the device through these SNMP variables in the MIB if\nyou permit this level of control.\nNMS uses the SNMPGet messages)to ask for information from an agent.Get, GetNext, and GetBulk messages(together referenced simply as\nNMS sends an SNMP Set message to write variables on the SNMP agent as a means to change the configuration of the device.\nchange the configuration of the device. messages come in pairs, with, for instance, a Get Request asking the agent for the contents of a variable, and the Get Response supplying that information NMS can analyze various statistical facts such as averages, minimums, and maximums NMS can set thresholds for certain key variables, telling the NMS to send a notification (email, text, and so on) when a threshold is passed. SNMP Notifications: Traps and Informs\nSNMP agents can initiate communications to the NMS.\ngenerally called notifications, use two specific SNMP messages: Trap and Inform\nSNMP agents MIB variables when those variables reach a certain state.send a Trap or Inform SNMP message to the NMS to list the state of certain\nSNMP Traps and Inform messages have the exact same purpose but differ in the protocol mechanisms trap The SNMP agent transport protocol as with all SNMP messages,sends the Trap to the IP address of the NMS, with UDP as the less overhead than inform Inform Like Trap messages but with reliability added Added to the protocol with SNMP Version 2 (SNMPv2), use UDP but add application layer reliability NMS must acknowledge receipt of the Inform with an SNMP Response message, or the SNMP agent will time out and resend the Inform. The Management Information Base\nEvery SNMP agent has its own Management Information Base\ndefines variables whose values are set and updated by the agent\nenable the management software to monitor/control the network device.\ndefineseach variable as an object ID (OID)\norganizes the OIDs based in part on RFC standards, and in part with vendor-proprietary\nvariables\norganizes all the variables into a hierarchy of OIDs, usually shown as a tree\nEach node in the tree can be described based on the tree structure sequence, either by name or by number.\nyou could use an SNMP manager and type MIB variable 1.3.6.1.4.1.9.2.1.58.0 and click a\nbutton to get that variable, to see the current CPU usage percentage from a Cisco router\nSecuring SNMP\nSecuring SNMP\nuse ACLs to limit SNMP messages to those from known servers only.\ncan configure an IPv4 ACL to filter incoming SNMP messages that arrive in IPv4 packets and an IPv6 ACL to filter SNMP messages that arrive in IPv6 packets.\nall versions of SNMP support a basic clear-text password mechanism,\nSNMPv1 defined clear-text passwords called SNMP communities.\nSNMP agent and the SNMP manager need prior knowledge of the same SNMP community value (called a community string)\nGet messages and the Set message include the appropriate community string value, in clear text.\nNMS sends a Get or Set with the correct community string, as configured on the\nSNMP agent, the agent processes the message.\nSNMPv1 defines both a read-only community and a read-write community.\nread-only (RO) community allows Get messages, and the read-write (RW)\ncommunity allows both reads and writes (Gets and Sets).\nSNMPv2, and the related Community-based SNMP Version 2 (SNMPv2c) The original specifications for SNMPv2 did not include SNMPv1 communities SNMPv3 security had arrived with the powerful network management protocol. away with communities and replaces them with the following features:SNMPv3 does Message integrity:This mechanism, applied to all SNMPv3 messages, confirms whether or not each message has been changed during transit. Authentication:and password,withThis optional feature the password never sent as clear text. Instead, it uses a adds authentication with both a username hashing method like many other modern authentication processes. Encryption (privacy): This optional feature encrypts the contents of SNMPv3 messages so that attackers who intercept the messages cannot read their contents. FTP and TFTP # Opaque:and commandsTo represent logical internal file systems for the convenience of internal functions Network: To represent external file systems found on different types of servers for the convenience of reference in different IOS commands Disk: For flash Usbflash: For USB flash NVRAM: A special type for NVRAM memory, the default location of the startup-config file the command more flash0:/wotemp/fred woulddisplay the contents of file fred in directory /wotemp in the first flash memory slot in the router. many commands use a keyword that indirectly refers to a formal filename, to reduce typing.For example: example:\nshow running-config command:Refers to file system:running-config\nshow startup-config command: Refers to file nvram:startup-config\nshow flash command: Refers to default flash IFS (usually flash0:)\nUpgrading IOS Images # process to upgrade an IOS image into flash memory, using the following steps: Step 1.Obtain the IOS image from Cisco, usually by downloading the IOS image from Cisco.com using HTTP or FTP. Step 2. Place the IOS image someplace that the router can reach. Locations include TFTP or FTP servers in the network or a USB flash drive that is then inserted into the router. Step 3. Issue the copy command from the router, copying the file into the flash memory that usually remains with the router on a permanent basis. (Routers usually cannot boot from the IOS image in a USB flash drive.) Copying a New IOS Image to a Local IOS File System Using TFTP copy command copy command\nworks through these kinds of questions:\nWhat is the IP address or host name of the TFTP server?\nWhat is the name of the file?\nAsk the server to learn the size of the file, and then check the local router’s flash\nto ask whether enough space is available for this file in flash memory.\nDoes the server actually have a file by that name?\nDo you want the router to erase any old files in flash?\nAfterward\nverifies that the checksum for the file shows that no errors occurred\nview the contents of the flash file system\nshow flash\nshows the files in the default flash file system (flash0:)\ndir flash0 : commandlists the contents of that same file system, with similar information.IFS.) (You can use the dircommand to display the contents of any local show flash(bytes used plus bytes free).lists the bytes used, whereas the know which command lists which particular total.dir command lists the total bytes Verifying IOS Code Integrity with MD5\nwhen Cisco builds a new IOS image, it calculates and publishes an MD5 hash value for\nthat specific IOS file.\nIOS verify command.\nwill list the MD5 hash as recalculated on your router. If both MD5 hashes are\nequal, the file has not changed.\nverify /md5 commandgenerates the MD5 hash on your router, you can include the hash value computed by Cisco as the last parameter (as shown in the example), or leave it offlocally computed value matches what you copied into the command. If you. If you include it, IOS will tell you if the leave it out, the verify command lists the locally computed MD5 hash, and you leave it out, the verify command lists the locally computed MD5 hash, and you have to do the picky character-by-character check of the values yourself. Copying Images with FTP\ncopy ftp flash can configure the FTP username and password on the router so that you do not have to include them in the copy command. For instance, the global have to include them in the configuration commands ip ftp username wendellcopy command. For instance, the global and ip ftp password odom\nwould have configured those values.\nThe FTP and TFTP Protocols\ncopycommand, when using the tftp or ftp keyword, makes the command act as a client\nFTP ProtocolBasics\nuses TCP\nTCP port 21 and in some cases also well-known port 20.\nFTP uses a client/server model for file transfer\nFTP control connection—define the kinds of functions supported by FTP allow the client to navigate around the directory structures of the server, list files, and then transfer files from the server (FTP GET) or to the server (FTP PUT). summary of some of the FTP actions: Navigate directories : List the current directory, change the current directory to a new directory, go back to the home directory, all on both the server and client side of the connection. Add/remove directories: Create new directories and remove existing directories on both the client and server. List files : List files on both the client and server. File transfer: Get (client gets a copy of the file from the server), Put (client takes a file that exists on the client and puts a copy of the FTP server). FTP Active and Passive Modes may impact whether the TCP client can or cannot connect to the server and perform normal functions user at the FTP client can choose which mode to use FTP passive mode may be the more likely option to work. FTP uses two types of TCP connections: FTP uses two types of TCP connections:\nControl Connection: Used to exchange FTP commands\nData Connection: Used for sending and receiving data, both for file transfers and for output to display to a user\nwhen a client connects to an FTP server, the client first creates the FTP control connection\nserver listens for new control connections on its well-known port 21\nclient allocates any new dynamic port (49222 in this case) and creates a TCP connection to the server\nThe FTP client allocates a currently unused dynamic port and starts listening on that port. The client identifies that port (and its IP address) to the FTP server by sending an FTP PORT command to the server. The server, because it also operates in active mode, expects the PORT command; the server reacts and initiates the FTP data connection to the client’s address (192.168.1.102) and port (49333). Passive mode helps solve the firewall restrictions by having the FTP client\ninitiate the FTP data connection to the server.\npassive mode does not simply cause the FTP client to connect to a well-known\nport on the server;\nThe FTP client changes to use FTP passive mode, notifying the server using the FTP PASV command.\nThe server chooses a port to listen on for the upcoming new TCP\nThe server chooses a port to listen on for the upcoming new TCP connection, in this case TCP port 49444. The FTP notifies the FTP client of its IP address and chosen port with the FTP PORT command. The FTP client opens the TCP data connection to the IP address and port learned at the previous step. FTP over TLS (FTP Secure)\nFTPS encrypts both the control and data connections with TLS, including the exchange of the usernames and passwords\nFTPS explicit mode process:\nThe client creates the FTP control TCP connection to server well-known port 21.\nThe client initiates the use of TLS in the control connection with the FTP AUTH command.\nWhen the user takes an action that requires an FTP data connection, the client\ncreates an FTP data TCP connection to server well-known port 21.\nThe client initiates the use of TLS in the data connection with the FTP AUTH\ncommand.\nimplicit mode process begins with a required TLS connection, with no need for an FTP AUTH command, using wellthe data connection).-known ports 990 (for the control connection) and 989 (for SFTP uses SSH to encrypt file transfers over an SSH connection. However, the acronym SFTP does not refer to a secure version of FTP. TFTP Protocol Basics\nTrivial File Transfer Protocol uses UDP well-known port 69. Because it uses UDP, TFTP adds a\nfeature to check each file for transmission errors by using a checksum process on each file after the transfer completes.\nthe code requires less space to install, which can be useful for devices with limited memory.\ncan Get and Put files, but it includes no commands to change directories, create/remove directories, or even to list files on the server.\ndoes not support even simple clearit should accept requests from any TFTP client.-text authentication. In effect, if a TFTP server is running,\n1.2 Describe characteristics of network topology architectures 1.2.a 2 tier 1.2.b 3 tier 1.2.e Small office/home office (SOHO) 1.3 Compare physical interface and cabling types 1.3.c Concepts of PoE Two-Tier Campus Design (Collapsed Core) access, distribution, and core. Access switches connect directly to end users, Distribution switches provide a path through which the access switches can forward traffic to each other each of the access switches connects to at least one distribution switch, typically to two distribution switches for redundancy. most designs use at least two uplinks to two different distribution switches (as shown in Figure 13 - 1) for redundancy. Provides a place to connect end-user devices (the access layer, with access switches) Connects the switches with a reasonable number of cables and switch ports by connecting all 40 access switches to two distribution switches ","externalUrl":null,"permalink":"/tech/networking/ip-services/","section":"Teches","summary":"\u003cp\u003eAll hosts act like they always have, with one default router setting that never has to change.\u003cbr\u003e\nThe default routers share a virtual IP address in the subnet, defined by the FHRP.\u003cbr\u003e\nHosts use the FHRP virtual IP address as their default router address.\u003cbr\u003e\nThe routers exchange FHRP protocol messages so that both agree as to which router does what work at any point in time.\u003cbr\u003e\nWhen a router fails or has some other problem, the routers use the FHRP to choose which router\u003cbr\u003e\ntakes over responsibilities from the failed router.\u003cbr\u003e\nThe Three Solutions for First-Hop Redundancy\u003cbr\u003e\nFirst Hop Redundancy Protocol does not name any one protocol. Instead, it names a family of\u003cbr\u003e\nprotocols that fill the same role\u003c/p\u003e","title":"IP Services","type":"tech"},{"content":"example All open on one computer: Port 80 Web ServerPort 800 Ad Server Port 9876 Wire Application Socket\nIncludes IP address, transport protocol, and port number (10.1.1.2, TCP, port 80) Multiplexing\n0 to 1023 Well Known (System) Ports:\n1024 to 49151 User (Registered) Ports:\n49152 to 65535, not assigned uses the same port number for all connections. For example, web server with 100 clients would have only one socket (one port number) server looks at source port of received TCP segments. Servers use well-known ports (or user ports), whereas clients use dynamic ports Ephemeral (Dynamic, Private) Ports:\nPopular TCP/IP Applications\nSimple Network Management Protocol (SNMP)\nquery, compile, store, and display information about a network’s operationCisco Prime software\nFTP/ TFTP FTP allows many more features TFTPis very simple, good tools for embedded parts of networking devices.\nSMTP/ POP3 Simple Mail Transfer Protocol (SMTP) and Post Office Protocol version 3 (POP3) both used for transferring mail (TCP).\nPort numbers and protocols Port numbers and protocols\nFTP Data TCP 20FTP Control TCP 21 SSH TCP 22Telnet TCP 23 SMTP TCP 25 DNS UDP/TCP 53DHCP Server UDP 67 DHCP Client UDP 68TFTP UDP 69 HTTP TCP 80POP2 TCP 110 SNMP UDP 161 SSL TCP 443Syslog UDP 514 Connection Establishment and Termination\n▪▪ SYN, DPORT=80, SPORT=49145SYN ACK , DPORT= 49145, SPORT=80 ▪ ACK DPORT=80, SPORT=49145 initializing Sequence and Acknowledgment fields agreeing on the port numbers 2 bits inside the flag fields of the TCP header. Called the SYN and ACK flags TCP connection establishment (3 way handshake) occurs 1st TCP connection termination. (four-way) ▪▪ ACK, FIN \u0026gt;\u0026lt; ACK ▪▪ ACK, FIN \u0026lt;\u0026lt; ACK uses an additional flag, called the FIN bit Error Recovery and Reliability\nreliability in both directionsSequence Number field of one direction and Acknowledgment field in the other direction - 1000 bytes, Seq = 1000 \u0026gt; - - 1000 bytes, Seq = 2000 \u0026gt;1000 bytes, Seq = 3000 \u0026gt; - \u0026lt; no data, ACK = 4000\n▪ received all data with sequence numbers up through one less than 4000\n▪ ready to receive your byte 4000 next. ack by listing the next expected byte (forward acknowledgment)sequence number field identifies the data (sender) forward acknowledgments acknowledge the data (receiver)\nforward acknowledgments acknowledge the data (receiver)\nask the sending host to resendacknowledge that the re-sent data arrived Sequence and Acknowledgment fields let the receiving host can notice lost data\n1000 bytes, SEQ 1000 \u0026gt;1000 bytes, SEQ 2000 X\u0026gt; 1000 bytes, SEQ 3000 \u0026gt;\n□ (received 1000 -1999 and 3000 -3999, asking for 2000) \u0026lt; no data, ACK = 2000 1000 bytes, SEQ 2000 \u0026gt; \u0026lt; no data, ACK 4000 Sender may wait a few moments to make sure no other acknowledgments arrive Retransmission timer Flow Control Using Windowing\nSliding window (dynamic window- Receiver slides the window size up and down\n\u0026lt; ACK=1000, Window=3000SEQ=1000, SEQ=2000, SEQ=3000 \u0026gt; \u0026lt; ACK=4000, Window=4000\nUser Datagram Protocol connectionless no reliability, no windowing, no reordering of the received data, andno segmentation of large chunks of data into the right size for transmission provides data transfer and multiplexing using port numbersLess overhead and processing than TCP. no reordering or recovery DNS requests use UDP, user will retry an operation if the DNS resolution failsNetwork File System (NFS), a remote file system application, performs recovery with\napplication layer code, so UDP features are acceptable to NFS. - # Source port, Destination PortLength, Checksum 8 byte header Uniform Resource Identifiers (URI) clicking a link and typing a URIreferred to as web address or Universal (uniform) Resource Locator [URL]—refer to a URI three key components before the :// identifies the protocol between the // and / identifies the server by nameafter the / identifies the web page. TCP/IP Applications\nhttp:// (Scheme) http://www.certskills.com/blog (path) (Authority) \u0026lt; Name Resolution Request (IP Header, UDP Header, DNS Request)Name resolution Reply (Ip Header, UDP Header, DNS Request (IP address) \u0026gt; \u0026lt; TCP connection to requested web server DNS requests can be cached by hosts and serversLocal DNS may need to ask for help Sends repeated DNS messages to find the authoritative DNS server. ➢➢ Root DNS .com TLD DNS ➢ Authoritative cisco.com DNS Recursive DNS lookup- host \u0026gt; Enterprise DNS \u0026gt; The enterprise DNS acts as a recursive DNS server DNS # HTTP GET request lists file it needs HTTP GET response from server with a Server may issue areturn code of 404, (file not found)return code of 200 (meaning OK) and file’s contents. Web pages consist of multiple files, called objects Objects are stored as different files on the web serverWeb browser gets the first file which can include references to other URIs that the browser\nalso requests - # \u0026lt; HTTP GET /go/ccnaHTTP OK data \u0026gt; \u0026lt; HTTP GET /graphics/logo1.gif HTTP OK data \u0026gt;\u0026lt; HTTP GET /graphics/ad1.gif HTTP OK data \u0026gt; Flow over one or more TCP connection between the client and the server. Transferring Files with HTTP\nIdentifying the Correct Receiving Application\ntracks which port opened which request Fields that identify next header\n\u0026lt; Ethernet (Type) (0x0800)\n\u0026lt; IPv4 (Protocol) (6)\u0026lt; TCP (Destination port) (49124)\n","externalUrl":null,"permalink":"/tech/networking/ip-transport-and-applications/","section":"Teches","summary":"\u003cp\u003eexample\nAll open on one computer:\nPort 80 Web ServerPort 800 Ad Server\nPort 9876 Wire Application\nSocket\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIncludes IP address, transport protocol, and port number\u003c/li\u003e\n\u003cli\u003e(10.1.1.2, TCP, port 80)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMultiplexing\u003c/p\u003e","title":"IP Transport and Applications","type":"tech"},{"content":"Endpackets if the host is in a different subnet.-user hosts need to know the IPv6 address of a default router, to which the host sends IPv6 IPv6 routers de-encapsulate and re-encapsulate each IPv6 packet when routing the packet. IPv6 routers make routing decisions by comparing the IPv6 packet’s destination address to the router’s IPv6 routing table; the matched route lists directions of where to send the IPv6 packet next. Note You could take the preceding list and replace every instance of IPv6 with IPv4, and all the statements would be true of IPv4 as well.\nthe router must look at a protocol type field in the data-link header, which identifies the type of packet inside the dataIPv6 packet. -link frame. Today, most data-link frames carry either an IPv4 packet or an To route an IPv6 packet, a router must use its IPv6 routing table instead of the IPv4 routing table. the process works like IPv4, except that the IPv6 packet lists IPv6 addresses, and the IPv6 routing table lists routing information for IPv6 subnets (called prefixes). (The migration strategy of running both IPv4 and IPv6 is called dual stack.) All you have to do is configure the router to route IPv6 packets, in addition to the existing configuration for routing IPv4 packets.\nRouting Information Protocol (RIP), Open Shortest Path First (OSPF), Enhanced Interior Gateway Routing Protocol (EIGRP), and Border Gateway Protocol (BGP) were all updated to support IPv6.\nIPv6 Routing Protocols these routing protocols also follow the same interior gateway protocol (IGP) and exterior gateway protocol (EGP) conventions as their IPv4 cousins. RIPng, EIGRPv6, and OSPFv3 act as interior gateway protocols, advertising IPv6 routes inside an enterprise. Representing the Prefix Length of an Address IPv6 uses a mask concept, called the prefix length, similar to IPv4 subnet masks. the IPv6 prefix length is written as a /, followed by a decimal number. The prefix length defines how many bits of the IPv6 address define the IPv6 prefix, which is basically the same concept as the IPv4 subnet ID. When writing an IPv6 address and prefix length in documentation, you can choose to leave a space before the /, or not, as shown in the next two examples. 2222:1111:0:1:A:B:C:D/642222:1111:0:1:A:B:C:D /64 by zeroing out the last 64 bits (16 digits) of the address, you find the following prefix value: 2000\u0026#x1f522;5678:9ABC:0000:0000:0000:0000/64 This value can be abbreviated, with four quartets of all 0s at the end, as follows: 2000\u0026#x1f522;5678:9ABC::/64\nFinding the IPv6 Prefix This chapter covers the following exam topics: 1.0 Network Fundamentals 1.8 Configure and verify IPv6 addressing and prefix 1.9 Compare and contrast IPv6 address types 1.9.a Global unicast 1.9.b Unique local IPv6 does not use any concept like the classful network concept used by IPv4reserve some IPv6 address ranges for specific purposes.. However, IANA does still Public and Private IPv6 Addresses global unicast addresses as the public IPv6 address space. Global unicast: Addresses that work like public IPv4 addresses. The organization that needs IPv6 addresses asks for a registered IPv6 address block, which is assigned as a global routing prefix. After that, only that organization uses the addresses inside that block of addressesaddresses that begin with the assigned prefix. —that is, the Unique local: Works somewhat like private IPv4 addresses, with the possibility that multiple organizations use the exact same addresses, and with no requirement for registering with any numbering authority. That reserved block of IPv6 addressescalled aglobal routing prefix. —a set of addresses that only one company can use—is Address Ranges for Global Unicast Addresses unique local unicast addresses, discussed later in this chapter, all start with hex FD. any address ranges that are not specifically reserved, for now, are considered to be global unicast addresses.\n","externalUrl":null,"permalink":"/tech/networking/ipv6/","section":"Teches","summary":"\u003cp\u003eEndpackets if the host is in a different subnet.-user hosts need to know the IPv6 address of a default router, to which the host sends IPv6\nIPv6 routers de-encapsulate and re-encapsulate each IPv6 packet when routing the packet.\nIPv6 routers make routing decisions by comparing the IPv6 packet’s destination address to the router’s IPv6 routing table; the matched route lists directions of where to send the IPv6 packet\nnext.\nNote\nYou could take the preceding list and replace every instance of IPv6 with IPv4, and all the statements would be true of IPv4 as well.\u003c/p\u003e","title":"IPv6","type":"tech"},{"content":"Friday, August 27, 2021 3:17 PM\nIPv6 Subnetting Using Global Unicast Addresses\nMost everyone uses the easiest possible IPv6 prefix length: /64.\ntheright side of the IPv6, formally called the interface ID(short for interface identifier), acts like\nthe IPv4 host field.\nthe prefix length of the global routing prefix is often between /32 and /48, or possibly as long as\n/56.\nwith the commonly used 64being the length of the global routing prefix.-bit interface ID field, the subnet field is typically 64–P bits, with P\nAll subnet IDs begin with the global routing prefix. Use a different value in the subnet field to identify each different subnet. All subnet IDs have all 0s in the interface ID. The IPv6 subnet ID, more formally called the subnet router anycast address, is reserved and should not be used as an IPv6 address for any host. Assigning Addresses to Hosts in a Subnet\nhosts can learn these same settings dynamically, using either Dynamic Host Configuration\nProtocol (DHCP) or a built(SLAAC). -in IPv6 mechanism called Stateless Address Autoconfiguration\nUnique Local Unicast Addresses\nUnique local unicast addresses act as private IPv6 addresses.\nThe biggest difference lies in the literal number with the administrative process: the unique local prefixes are not registered with any numbering (unique local addresses begin with hex FD) and\nauthority and can be used by multiple organizations.\nauthority and can be used by multiple organizations. Use FD as the first two hex digits. Choose a unique 40-bit global ID. Append the global ID to FD to create a 48-bit prefix, used as the prefix for all your addresses. Use the next 16 bits as a subnet field. Note that the structure leaves a convenient 64-bit interface ID field. IANA actually reserves prefix FC00::/7, and not FD00::/8, for these addresses. FC00::/7 includes\nall addresses that begin with hex FC and FD. However, an RFC (4193) requires the eighth bit of these addresses to be set to 1, which means that in practice today, the unique local addresses all\nbegin with their first two digits as FD.\nSubnetting with Unique Local IPv6 Addresses\nWith unique local, you create that prefix locally, and the prefix begins with /48, with the first 8\nbits set and the next 40 bits randomly chosen.\nimagine you chose a 10-hex-digit value of hex 00 0001 0001, prepend a hex FD, making the\nentire prefix be FD00:0001:0001::/48, or FD00:1:1::/48 when abbreviated.\nThe Need for Globally Unique Local Addresses\nRFC stresses the importance of choosing your global ID in a way to make it statistically unlikely to be used by other companies.\nyour prefix.in a real network, plan on using the random number generator logic listed in RFC 4193 to create One of the big reasons to attempt to use a unique prefix, rather than everyone using the same easyanother company.-to-remember prefixes, is to be ready for the day that your company merges with or buys With IPv6 unique local addresses, if both companies did the right thing and randomly chose a\nprefix, they will most likely be using completely different prefixes, making the merger much simpler\nThis chapter covers the following exam topics: 1.0 Network Fundamentals 1.9 Compare and contrast IPv6 address types 1.9.a Global unicast 1.9.b Unique local 1.9.c Link local 1.9.d Anycast 1.9.e Multicast 1.9.f Modified EUI 64 Configuring the Full 128-Bit Address To statically configure the full 128-bit unicast address—either global unicast or unique local—the router needs anThe address can be an abbreviated IPv6 address or the full 32 ipv6 address address/prefix-length interface subcommand on each interface. -digit hex address.The command includes theprefix length value, at the end, with no space between the address and prefix length. ","externalUrl":null,"permalink":"/tech/networking/ipv6addressingandsubnetting/","section":"Teches","summary":"\u003cp\u003eFriday, August 27, 2021 3:17 PM\u003c/p\u003e\n\u003cp\u003eIPv6 Subnetting Using Global Unicast Addresses\u003cbr\u003e\nMost everyone uses the easiest possible IPv6 prefix length: /64.\u003cbr\u003e\ntheright side of the IPv6, formally called the interface ID(short for interface identifier), acts like\u003cbr\u003e\nthe IPv4 host field.\u003cbr\u003e\nthe prefix length of the global routing prefix is often between /32 and /48, or possibly as long as\u003cbr\u003e\n/56.\u003cbr\u003e\nwith the commonly used 64being the length of the global routing prefix.-bit interface ID field, the subnet field is typically 64–P bits, with P\u003c/p\u003e","title":"IPv6 Addressing and Subnetting","type":"tech"},{"content":"Monday, August 30, 2021 9:19 AM\nboth abbreviated and unabbreviated addresses, and both lowercase and uppercase hex digits,\nshowing that all are allowed.\nEnabling IPv6 Routing\nIPv6 routing is not enabled by default. routing —whichenables IPv6 routing on the router.The solution takes only a single command— ipv6 unicast-\nA router address) must enable IPv6 globally before the router will attempt to route IPv6 packets in and out an interface (ipv6 unicast-routing ) and enable IPv6 on the interface .If you omit (ipv6\ntheroute any received IPv6 packets, but the router will act as an IPv6 host ipv6 unicast-routing command but configure interface IPv6 addresses, the router will not. If you include the ipv6\nunicastroute IPv6 packets but have no interfaces that have IPv6 enabled, effectively disabling IPv6 -routingcommand but omit all the interface IPv6 addresses, the router will be ready to\nrouting\nVerifying the IPv6 Address Configuration\nThe show ipv6 interface brief command gives you interface IPv6 address info, but not prefix\nlength info, similar to the IPv4 show ip interface brief command.\nthis command lists IPv6 addresses, but not the prefix length or prefixes.\nThe show ipv6 interface commandgives the details of IPv6 interface settings, much like the show\nip interface command does for IPv4.\nthe IPv6 show interfaces. So, to see IPv6 interface addresses, use commands that begin withcommand still lists the IPv4 address and mask but tells us nothing about show ipv6\nIPv6. So, to see IPv6 interface addresses, use commands that begin with show ipv6\nGenerating a Unique Interface ID Using Modified EUI- 64\nThe router then uses EUI-64 rules to create the interface ID part of the address, as follows:\nKey Topic.\nSplit the 6-byte (12-hex-digit) MAC address in two halves (6 hex digits each).\nInsert FFFE in between the two, making the interface ID now have a total of 16 hex digits\n(64 bits).\nInvert the seventh bit of the interface ID.\nTable 24-2 lists some practice problems, ipv6 address address/prefix-length eui- 64 interface subcommand. mac-address command under R1’s G0/0 interface, which causes IOS to use the configured MAC address instead of the universal (burned-in) MAC address for interfaces that do not have a MAC address, like serial interfaces, the router uses the MAC of the lowest-numbered router interface that does have a MAC. if you mistakenly type the full address and still use the euicommand and converts the address to the matching prefix before putting the command -64 keyword, IOS accepts the into the running config file. For example, IOS converts ipv6 address 2000:1:1:1::1/64 euito ipv6 address 2000:1:1:1::/64 eui-64. - 64 routers can be configured to use dynamically learned IPv6 addresses. These can be useful for routers connecting to the Internet through some types of Internet access technologies, like DSL and cable modems. Stateful DHCP Stateless Address Autoconfiguration (SLAAC) two ways for the router interface to dynamically learn an IPv6 address to use: Dynamic Unicast Address Configuration\nSpecial Addresses Used by Routers\nAfter you configure thefunction of IPv6 routing, the addition of a unicast IPv6 address on an interface causes the router ipv6 unicast-routing global configuration command, to enable the\nto do the following:\nGives the interface a unicast IPv6 address\nEnables the routing of IPv6 packets in/out that interface\nDefines the IPv6 prefix (subnet) that exists off that interface\nTells the router to add a connected IPv6 route for that prefix, to the IPv6 routing table,\nwhen that interface is up/up\nthe same ideas happen for IPv4 when you configure an IPv4 address on a router interface.\nLink-Local Addresses\na special kind of unicast IPv6 address.\nnot used for normal IPv6 packet flows that contain data for applications\nused by some overhead protocols and for routing.\npackets sent to any link-local address should not be forwarded by any router to another subnet. For example,link-local addresses.Neighbor Discovery Protocol (NDP), which replaces the functions of IPv4’s ARP, uses Routers also use link-local addresses as the next-hop IP addresses in IPv6 routes, IPv6 hosts also use a default router (default gateway) concept, like IPv4, but instead of the router address being in the same subnet, hosts refer to the router’s link-local address. The show ipv6 route command lists the linkunicast or unique local unicast address.-local address of the neighboring router, rather than the global Link-Local Address Concepts\nkey facts about link-local addresse\nUnicast (not multicast): Link-local addresses represent a single host,\nForwarding scope is the local link only:local data link because routers do not forward packets with linkPackets sent to a link-local address do not leave the -local destination\naddresses.\nAutomatically generated: Every IPv6 host interface (and router interface) can create its own\nlink-local address automatically,\nCreating Link-Local Addresses on Routers\nall link-local addresses start with the same prefix,(FE80,FE90,FEA0,FEB0)as shown on the left side\nof Figure 24-9.\ntFE8, FE9, FEA, or FEB. he first 10 bits must match prefix FE80::/10, meaning that the first three hex digits will be either the next 54 bits should be binary 0, so the link-local address should always start with FE80:0000:0000:0000 as the first four unabbreviated quartets. The second half of the linkrandomly generated, or even configured.-local address, in practice, can be formed using EUI-64 rules, can be IOS creates a linkaddress using the ipv6 address command (global unicast or unique local). To see the link-local address for any interface that has configured at least one other unicast -local address, just use the usual commands that also list the unicast IPv6 address: and show ipv6 interface brief. show ipv6 interface both addresses have the same interface ID value\nIOS chooses the link-local address for the interface based on the following rules:\nIf configured, the router uses the value in the ipv6 address address link-local interface\nsubcommand. Note that the configured linkrange for link-local addresses; that is, an address from prefix FE80::/10. In other words, the -local address must be from the correct address\naddress must begin with FE8, FE9, FEA, or FEB.\nIf not configured, the IOS calculates the linkand demonstrated in and around Example 24-local address using EUI-7. The calculation uses EUI-64 rules, as discussed -64 rules even if\nthe interface unicast address does not use EUI-64.\nRouting IPv6 with Only Link-Local Addresses on an Interface\nipv6 address address/prefix-length : Static configuration of a specific address\nipv6 address prefix/prefix-length eui- 64 : Static configuration of a specific prefix and prefix\nlength, with the router calculating the interface ID using EUI-64 rules\nipv6 address dhcp: Dynamic learning on the address and prefix length using DHCP\nipv6 address autoconfig : Dynamic learning of the prefix and prefix length, with the router\ncalculating the interface ID using EUI-64 rules (SLAAC)\nipv6 enable : Enables IPv6 processing and adds a link-local address, but adds no other unicast IPv6\naddresses.\nsome links, particularly WAN links, do not need a global unicast address\nthe routers do not need to have global unicast (or unique local) addresses on the WAN links for routing to work. IPv6 routing protocols use link-local addresses as the next-hop address\nwhen dynamically building IPv6 routes.\nstatic routes, as discussed in Chapter 25, “Implementing IPv6 Routing,” can use link-local\nstatic routes, as discussed in Chapter 25, “Implementing IPv6 Routing,” can use linkaddresses for the next-hop address. -local creating a WAN link with no global unicast (or unique local) addresses works. As a result, you would not even need to assign an IPv6 subnet to each WAN link. Then to configure the WAN interfaces, use the ipv6 enable command, enabling IPv6 and giving each interface a generated link-local IPv6 address. To use the command, just configure the ipv6 enable command on the interfaces on both ends of the WAN link. IANA defines the range FF30::/12 (all IPv6 addresses that begin with FF3) as the range of addresses to be used for some types of multicast applications. different IPv6 RFCs reserve multicast addresses for specific purposes. For instance, FF02::5andFF02::6 as the all-OSPF-routers and all-DR-Routers multicast addresses, OSPFv3 uses OSPFv2 uses IPv4 addresses 224.0.0.5 and 224.0.0.6 for the equivalent purposes. IPv6 Multicast Addresses\nReserved Multicast Addresses\nIPv6, instead of using Layer 3 and Layer 2 broadcasts, instead uses Layer 3 multicast addresses, which in turn cause Ethernet frames to use Ethernet multicast addresses. As a result:\nAll the hosts that should receive the message receive the message, which is necessary for the protocols to work. However\u0026hellip;\n\u0026hellip;Hosts that do not need to process the message can make that choice with much less\nprocessing as compared to IPv4.\nOSPFv3 uses IPv6 multicast addresses FF02::5 and FF02::6. In a subnet, the listen for packets sent to those addresses. However, all the endpoint hosts do not use OSPFv3 OSPFv3 routers will\nand should ignore those OSPFv3 messages\nthe most common reserved IPv6 multicast addresses.\nshow ipv6 interface command to show the multicast addresses used by Router R1 on its G0/0 interface. Each scope defines a different set of rules about whether routers should or should not forward a packet, and how far routers should forward packets, based on those scopes. Multicast Address Scopes\nrouters can predict the boundaries of some scopes, like linkknow the boundaries of other scopes, for instance, organization-local, but they need configuration to -local.)\nLink-local address: An IPv6 address that begins FE80. This serves as a unicast address for an\ninterface to which devices apply a linkaddresses using EUI-64 rules. A more complete term for comparison would be -local scope.Devices often create their own linklink-local unicast -local\naddress.\naddress. Linkmulticast address to which devices apply a link-local multicast address: An IPv6 address that begins with FF02. This -local scope. serves as a reserved Linkrouters should not forward packets sent to an address in this scope.-local scope: A reference to the scope itself, rather than an address. This scope defines that Solicited-Node Multicast Addresses\nIPv6 Neighbor Discovery Protocol (NDP) replaces IPv4 ARP,\nNDP improves the MACprocessed by the correct host but discarded with less processing by the rest of the hosts in the -discovery process by sending IPv6 multicast packets that can be\nsubnet\nFigure 24unicast address- 12 shows how to determine the solicited node multicast address associated with a. Start with the predefined /104 prefix (26 hex digits) shown in Figure 24-12. In\nother words, all the solicitedthe last 24 bits (6 hex digits), copy the last 6 hex digits of the unicast address into the solicited-node multicast addresses begin with the abbreviated FF02::1:FF. In -\nnode address.\na host or router calculates a matching solicited node multicast address for every unicast address on an interface the router interface has a unicast address of 2001:DB8:1111:1::1/64, and a linkFE80::AA:AAAA. As a result, the interface has two solicited node multicast addresses, shown at -local address of the end of the output. all IPv6 hosts can use two additional special addresses: The unknown (unspecified) IPv6 address, ::, or all 0s The loopback IPv6 address, ::1, or 127 binary 0s with a single 1 A host can use the unknown address (::) when its own IPv6 address is not yet known or when the host wonders if its own IPv6 address might have problems. hosts use the unknown address during the early stages of dynamically discovering their IPv6 address. When a host does not yet know what IPv6 address to use, it can use the :: address as its source IPv6 address IPv6 loopback address gives each IPv6 host a way to test its own protocol stack. Just like the IPv4 127.0.0.1 loopback address, packets sent to ::1 do not leave the host but are instead simply delivered down the stack to IPv6 and back up the stack to the application on the local host Miscellaneous IPv6 Addresses\nAnycast Addresses\nservice works best when implemented on several routers\nHosts can send just one packet to an IPv6 address, and the routers will forward the packet to the nearest router that supports that service by virtue of supporting that destination IPv6 address.\nStep 1.Two routers configure the exact same IPv6 address, designated as an anycast\naddress, to support some service.\nStep 2.routers simply route the packet to the nearest router that supports the address.In the future, when any router receives a packet for that anycast address, the other\nthe routers implementing the anycast address must be configured and then advertise a route for the anycast address. The addresses do not come from a special reserved range of addresses;\nthey are from the unicast address range. Often, the address is configured with a /128 prefix so\nthat the routers advertise a host route for that one anycast address.\nthe routing protocol advertises the route just like any other IPv6 route; the other routers cannot tell the difference the actual address (2001:1:1:2::99) looks like any other unicast address note the different anycast keyword on the ipv6 address command, telling the local router that the address has a special purpose as an anycast address the ipv6 show ipv6 interface interface brief command does not.command does identify the address as an anycast address, but the show IPv6 Addressing Configuration Summary\n3.0 IP Connectivity\n3.3 Configure and verify IPv4 and IPv6 static routing\n3.3.a Default route\n3.3.b Network route\n3.3.c Host route\n3.3.d Floating static\nConnected and Local IPv6 Routes\na router adds IPv6 routes based on the following:\nThe configuration of IPv6 addresses on working interfaces (connected and local routes)\nThe direct configuration of a static route (static routes)\nThe configuration of a routing protocol, like OSPFv3, on routers that share the same data link (dynamic routes)\nFirstthe ipv6 address , the router looks for any configured unicast addresses on any interfaces by looking for command. if the interface is working—if the interface has a “line status is up, protocol status is up” notice in the output of theand local route. show interfaces command—the router adds both a connected Routers do not create connected or local IPv6 routes for link-local addresses. Theroute connected route is a host route for only the specific IPv6 address configured on the interface.represents the subnet connected to the interface,whereas thelocal Routers create IPv6 routes based on each unicast IPv6 address on an interface, as configured with the ipv6 address command, as follows: The router creates a route for the subnet (a connected route). The router creates a host route (/128 prefix length) for the router IPv6 address (a local route). Routers do not create routes based on the link-local addresses associated with the interface. Routers remove the connected and local routes for an interface if the interface fails, and they re-add these routes when the interface is again in a working (up/up) state Rules for Connected and Local Routes ","externalUrl":null,"permalink":"/tech/networking/ipv6configuration/","section":"Teches","summary":"\u003cp\u003eMonday, August 30, 2021 9:19 AM\u003c/p\u003e\n\u003cp\u003eboth abbreviated and unabbreviated addresses, and both lowercase and uppercase hex digits,\u003cbr\u003e\nshowing that all are allowed.\u003cbr\u003e\nEnabling IPv6 Routing\u003cbr\u003e\nIPv6 routing is not enabled by default. \u003cstrong\u003erouting\u003c/strong\u003e —whichenables IPv6 routing on the router.The solution takes only a single command— \u003cstrong\u003eipv6 unicast-\u003c/strong\u003e\u003cbr\u003e\nA router \u003cstrong\u003eaddress)\u003c/strong\u003e must enable IPv6 globally before the router will attempt to route IPv6 packets in and out an interface \u003cstrong\u003e(ipv6 unicast-routing\u003c/strong\u003e ) and enable IPv6 on the interface .If you omit \u003cstrong\u003e(ipv6\u003c/strong\u003e\u003cbr\u003e\ntheroute any received IPv6 packets, but the router will act as an IPv6 host \u003cstrong\u003eipv6 unicast-routing\u003c/strong\u003e command but configure interface IPv6 addresses, the router will not. If you include the ipv6\u003cbr\u003e\nunicastroute IPv6 packets but have no interfaces that have IPv6 enabled, effectively disabling IPv6 -routingcommand but omit all the interface IPv6 addresses, the router will be ready to\u003cbr\u003e\nrouting\u003cbr\u003e\nVerifying the IPv6 Address Configuration\u003cbr\u003e\nThe \u003cstrong\u003eshow ipv6 interface brief\u003c/strong\u003e command gives you interface IPv6 address info, but not prefix\u003cbr\u003e\nlength info, similar to the IPv4 \u003cstrong\u003eshow ip interface\u003c/strong\u003e brief command.\u003cbr\u003e\nthis command lists IPv6 addresses, but not the prefix length or prefixes.\u003cbr\u003e\nThe \u003cstrong\u003eshow ipv6 interface\u003c/strong\u003e commandgives the details of IPv6 interface settings, much like the \u003cstrong\u003eshow\u003cbr\u003e\nip interface\u003c/strong\u003e command does for IPv4.\u003cbr\u003e\nthe IPv6 \u003cstrong\u003eshow interfaces\u003c/strong\u003e. So, to see IPv6 interface addresses, use commands that begin withcommand still lists the IPv4 address and mask but tells us nothing about \u003cstrong\u003eshow ipv6\u003c/strong\u003e\u003c/p\u003e","title":"IPv6 Configuration","type":"tech"},{"content":"Tuesday, August 31, 2021 2:58 PM\nthey re-add these routes when the interface is again in a working (up/up) state Examples of Local IPv6 Routes\nshow ipv6 route local command\nStatic IPv6 Routes\nstatic routes require direct configuration with the ipv6 route command\nthe ipv6 route command begins with the prefix and prefix length. Then the respective commands\nthe list the directions of how this router should forward packets toward that destination subnet or ipv6 route command begins with the prefix and prefix length. Then the respective commands\nprefix by listing the outgoing interface or the address of the next-hop router.\nA static route on R1, for this subnet, will begin witheither the outgoing interface (S0/0/0) or the next-hop IPv6 address, or both. ipv6 route 2001:DB8:1111:2::/64, followed by\nwhen the command references an interface, the interface is a local interface show ipv6 route command will list all the IPv6 routes. show ipv6 route static command, whichlists only static routes directly connected” might mislead you to think this is a connected route; trust the “S” code. show ipv6 route 2001:DB8:1111:2::22 route that the router would use when forwarding packets to that particular address.command. This command asks the router to list the Example 25-7 shows an example. Static Routes Using the Outgoing Interface\nStatic IPv6 routes that refer to a next-hop address have two options: the unicast address on the neighboring router (global unicast or unique local) or the linkneighboring router -local address of that same Static Routes Using Next-Hop IPv6 Address\nExample Static Route with a Link-Local Next-Hop Address\nipv6 route the link-local address does not, by itself, tell the local router which outgoing interface to command cannot simply refer to a link-local next-hop address by itself because\nuse.\nWith a linkoutgoing interface must also be configured.-local next-hop address, a router cannot work through this same logic, so the\nshow commands also list both the next-hop (link-local) address and the outgoing interface Static Routes over Ethernet Links\nTo configure a static route that uses an Ethernet interface, the parameters should always include a next-hop IPv6 address. IOS allows you to configure the ipv6 ipv6 route command’s forwarding\nroute command using only the outgoingThe router will accept the command; however, -interface parameter, without listing a nextif that outgoing interface happens to be an -hop address.\nEthernet interface, the router cannot successfully forward IPv6 packets using the route.\nTo configure the ipv6 route correctly when directing packets out an Ethernet interface, the configuration should use one of these styles:\nRefer to the next-hop global unicast address (or unique local address) only\nRefer to both the outgoing interface and nextaddress) -hop global unicast address (or unique local\nRefer to both the outgoing interface and next-hop link-local address\nBranch routers could use default routes instead of a routing protocol. use a specific value to note the route as a default route: ::/0 Static Default Routes\nStatic IPv6 Host Routes\na route to a single host IP address. With IPv4, those routes use a /32 mask, which identifies a\nsingle IPv4 address in thethe ipv6 route command. ip route command; with IPv6, a /128 mask identifies that single host in\nFloating Static IPv6 Routes\nIOS considers static routes better than OSPFdistance. IOS uses the same administrative distance concept and default values for IPv6 as it does -learned routes by default due to administrative\nfor IPv4\nIPv6 floating static route floats or moves into and out of the IPv6 routing table depending on\nwhether the better (lower) administrative distance route learned by the routing protocol happens to exist currently.\nTo implement an IPv6 floating static route, just override the default administrative distance on the static route, making the value larger than the default administrative distance of the routing\nprotocol.\nipv6 route 2001:db8:1111:7::/64 2001:db8:1111:9::3 130 that, setting the static route’s administrative distance to 130. command on R1 would do exactly\nTroubleshooting Incorrect Static Routes That Appear in the IPv6 Routing Table\nIOS cannot tell if you choose the incorrect outgoing interface, incorrect nextincorrect prefix/prefix-length in a static route. -hop address, or\nIOS puts the route into the IP routing tablepoorly chosen parameters. —even though the route may not work because of the\nR1 cannot use its own IPv6 address as a next-hop address.\nroutes may have incorrect parameters. Check for these types of mistakes:\nStep 1. Prefix/Length: Does the ipv6 route command reference the correct subnet ID (prefix) and mask (prefix length)?\nStep 2. If using a next-hop IPv6 address that is a link-local address:\nA. Is the link-local address an address on the correct neighboring router? (It should be\nan address on another router on a shared link.)\nB. Does the ipv6 route command also refer to the correct outgoing interface on the\nlocal router?\nStep 3. If using a nextaddress the correct unicast address of the neighboring router?-hop IPv6 address that is a global unicast or unique local address, is the\nStep 4. If referencing an outgoing interface, does the ipv6 route command reference the interface on the local router (that is, the same router where the static route is configured)?\nThe Static Route Does Not Appear in the IPv6 Routing Table\nIOS makes the following checks before adding the route;\nFor ipv6 route commands that list an outgoing interface, that interface must be in an up/up state.\nFor ipv6 route commands that list a global unicast or unique local next-hop IP address (that\nFor ipv6 route commands that list a global unicast or unique local nextis, not a link-local address), the local router must have a route to reach that next-hop IP address (that -hop address. If another IPv6 route exists for that exact same prefix/prefixhave a better (lower) administrative distance. -length, the static route must The Neighbor Discovery Protocol\nwith 1pv4 ARP works as a separate protocol; with IPv6, thepart of ICMPv6,performs the same functions. Neighbor Discovery Protocol (NDP), a\nfew of the functions of the NDP protocol (RFC 4861). Some of those NDP functions are\nNeighbor MAC Discovery:of other hosts in the same subnet. NDP replaces IPv4’s ARP, providing messages that An IPv6 LAN-based host will need to learn the MAC address\nreplace the ARP Request and Reply messages.\nRouter Discoverysame subnet. : Hosts learn the IPv6 addresses of the available IPv6 routers in the\nSLAACmessages to learn the subnet (prefix) used on the link plus the prefix length.: When using Stateless Address Auto Configuration (SLAAC), the host uses NDP\nDAD: Before using an IPv6 address, hosts use NDP to perform a Duplicate Address\nDetection (DAD) process, to ensure no other host uses the same IPv6 address before attempting to use it.\nDiscovering Neighbor Link Addresses with NDP NS and NA\nusing a pair of matched solicitation and advertisement messages:(NS)and Neighbor Advertisement (NA)messages. the Neighbor Solicitation\nthe send back a replyNS acts like an IPv4 ARP request. The NA message acts like an IPv4 ARP Reply, listing that host’s MAC , asking the host with a particular unicast IPv6 address to\naddress.\nNeighbor Solicitation (NS)(the target address) to reply with an NA message that lists its MAC address. : This message asks the host with a particular IPv6 address The NS\nmessage is sent to the solicitedaddress, so the message is processed only by hosts whose last six hex digits match the -node multicast address associated with the target\naddress that is being queried.\nNeighbor Advertisement (NA): This message lists the sender’s IPv6 and MAC\naddresses. IPv6 unicast address of the host that sent the original NS messageIt can be sent in reply to an NS message, and if so, the packet is sent to the .A host can also\nsend an unsolicited NA, announcing its IPv6 and MAC addresses, in which case the message is sent to the all-IPv6-hosts local-scope multicast address FF02::1.\nTo view a host’s NDP neighbor table, use these commands: (Windows) netsh interface ipv6\nshow neighbors; (Linux) ip -6 neighbor show; (Mac OS) ndp -an.\nDiscovering Routers with NDP RS and RA\nNDP defines two messages that allow any host to discover all routers in the subnet:\nRouter Solicitation (RS):This message is sent to the “all-IPv6-routers” local-scope multicast\naddress of FF02::2 so that the message asks all routers, on the local link only, to identify themselves.\nRouter Advertisement (RA): link-local IPv6 address of the router. When sent in response to an RS message, it flows back This message, sent by the router, lists many facts, including the\nto either the unicast address of the host that sent the RS or to the allFF02::1. Routers also send RA messages without being asked, sent to the all-IPv6-hosts address -IPv6-hosts local-\nscope multicast address of FF02::1.\nIPv6 allows multiple prefixes and multiple default routers to be listed in the RA message; Figure 25-9 just shows one of each for simplicity’s sake.\nalso periodically send unsolicited RA messages\nthe RA messages flow to the FF02::1 all-nodes IPv6 multicast address.\nUsing SLAAC with NDP RS and RA\nIPv6 supports an alternative method for IPv6 hosts to dynamically choose an unused IPv6 address to use The process goes by the nameStateless Address Autoconfiguration (SLAAC). Learn the IPv6 prefix used on the link, from any router, using NDP RS/RA messages. Build an address from the prefix plus an interface ID, chosen either by using EUI-64 rules or as a random value. Before using the address, first use DAD to make sure that no other host is already using the same address. Discovering Duplicate Addresses Using NDP NS and NA\nIPv6 uses the Duplicate Address Detection (DAD) process before using a unicast address to make sure that no other node on that link is already using the address. Hosts use DAD not only at the\nend of the SLAAC process, but also any time that a host interface initializes, no matter whether using SLAAC, DHCP, or static address configuration. When performing DAD, if another host already\nuses that address, the first host simply does not use the address until the problem is resolved.\na host sends an NS message for its own IPv6 address. No other host should be using that address,\nso no other host should send an NDP NA in reply. However, if another host already uses that address, that host will reply with an NA, identifying a duplicate use of the address.\nHosts do the DAD check for each of their unicast addresses, link-local addresses included, both\nwhen the address is first used and each time the host’s interface comes up.\nNDP Summary\nNDP does more than what is listed in this chapter, and the protocol allows for addition of other\nfunctions, so NDP might continue to grow over time. For now, use Table 25for the four NDP features discussed here. -3 as a study reference\nThis chapter covers the following exam topics:\n1.0 Network Fundamentals\n1.1 Explain the role and function of network components\n1.1.d Access Points\n1.11 Describe wireless principles\n1.11.a Nonoverlapping Wi-Fi channels\n1.11.b SSID\n1.11.c RF\nComparing Wired and Wireless Networks\nWireless devices must adhere to a common standard (IEEE 802.11).\nWireless coverage must exist in the area where devices are expected to use it.\nWireless LAN Topologies\nIEEE 802.11 WLANs are always half duplex because transmissions between stations use the same frequency or channel.\nBasic Service Set\nbefore a device can participate, it must advertise its capabilities and then be granted permission to join. The 802.11 standard calls this a basic service set (BSS). At the heart of every BSS is a\nwireless access point (AP),\nThe AP operates ininfrastructure mode,which meansit offers the services that are necessary to\nform the infrastructure of a wireless networkchannel. The AP and the members of the BSS must all use the same channel to communicate. The AP also establishes its BSS over a single wireless\nproperly.\n","externalUrl":null,"permalink":"/tech/networking/ipv6routing/","section":"Teches","summary":"\u003cp\u003eTuesday, August 31, 2021 2:58 PM\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ethey re-add these routes when the interface is again in a working (up/up) state\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eExamples of Local IPv6 Routes\u003cbr\u003e\n\u003cstrong\u003eshow ipv6 route local\u003c/strong\u003e command\u003c/p\u003e\n\u003cp\u003eStatic IPv6 Routes\u003cbr\u003e\nstatic routes require direct configuration with the \u003cstrong\u003eipv6 route\u003c/strong\u003e command\u003cbr\u003e\nthe \u003cstrong\u003eipv6 route\u003c/strong\u003e command begins with the prefix and prefix length. Then the respective commands\u003c/p\u003e","title":"IPv6 Routing","type":"tech"},{"content":" Using Jinja2 Templates # A template is a configuration file that contains variables and, based on the variables, is generated on the managed hosts according to host-specific requirements. Using templates allows for a structural way to generate configuration files, which is much more powerful than changing specific lines from specific files. Ansible uses Jinja2 to generate templates. Jinja2 is a generic templating language for Python developers. It is used in Ansible templates, but Jinja2-based approaches are also found in other parts of Ansible. For instance, the way variables are referred to is based on Jinja2. In a Jinja2 template, three elements can be used. data\nsample text comment {# sample text #} variable {{ ansible_facts['default_ipv4']['address'] }} expression {% for myhost in groups[\u0026#39;web\u0026#39;] %} {{ myhost }} {% endfor %} To work with a template, you must create a template file, written in Jinja2. Template file must be included in an Ansible playbook that uses the template module. Sample Template:\n# {{ ansible_managed }} \u0026lt;VirtualHost *:80\u0026gt; ServerAdmin webmaster@{{ ansible_facts[\u0026#39;fqdn\u0026#39;] }} ServerName {{ ansible_facts[\u0026#39;fqdn\u0026#39;] }} ErrorLog logs/{{ ansible_facts[\u0026#39;hostname\u0026#39;] }}-error.log CustomLog logs/{{ ansible_facts[\u0026#39;hostname\u0026#39;] }}-common.log common DocumentRoot /var/www/vhosts/{{ ansible_facts[\u0026#39;hostname\u0026#39;] }}/ \u0026lt;Directory /var/www/vhosts/{{ ansible_facts[\u0026#39;hostname\u0026#39;] }}\u0026gt; Options +Indexes +FollowSymlinks +Includes Require all granted \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; starts with # {{ ansible_managed }}.\nThis string is commonly used to identify that a file is managed by Ansible so that administrators are not going to change file contents by accident.\nWhile processing the template, this string is replaced with the value of the ansible_managed variable.\nThis variable can be set in ansible.cfg.\nFor instance, you can use ansible_managed = This file is managed by Ansible to substitute the variable with its value while generating the template.\ntemplate file is just a text file that uses variables to substitute specific variables to their values.\nCalling a template:\n--- - name: installing a template file hosts: ansible1 tasks: - name: install httpd yum: name: httpd state: latest - name: start and enable httpd service: name: httpd state: started enabled: true - name: install vhost config file template: src: listing813.j2 dest: /etc/httpd/conf.d/vhost.conf owner: root group: root mode: 0644 - name: restart httpd service: name: httpd state: restarted Applying Control Structures in Jinja2 Using for # Control structures can be used to dynamically generate contents. A for statement can be used to iterate over all elements that exist as the value of a variable. {% for node in groups[\u0026#39;all\u0026#39;] %} host_port={{ node }}:8080 {% endfor %} variable with the name host_ports is defined on the second line (which is the line that will be written to the target file). To produce its value, the host group all is processed in the for statement on the first line. While processing the host group, a temporary variable with the name node is defined. This value of the node variable is replaced with the name of the host while it is processed, and after the host name, the string :8080 is copied, which will result in a separate line for each host that was found. As the last element, {% endfor %} is used to close the for loop. LAB: Generating a Template with a Conditional Statement # --- - name: generate host list hosts: ansible2 tasks: - name: template loop template: src: listing815.j2 dest: /tmp/hostports.txt To verify, you can use the ad hoc command ansible ansible2 -a \u0026quot;cat /tmp/hostports.txt\u0026quot;\nUsing Conditional Statements with if # The for statement can be used in templates to iterate over a series of values. The if statement can be used to include text only if a variable contains a specific value or evaluates to a Boolean true. Template Example with if if.j2\n{% if apache_package == \u0026#39;apache2\u0026#39; %} Welcome to Apache2 {% else %} Welcome to httpd {% endif %} --- - name: work with template file vars: apache_package: \u0026#39;httpd\u0026#39; hosts: ansible2 tasks: - template: src: if.j2 dest: /tmp/httpd.conf [ansible@control ~]$ ansible ansible2 -a \u0026#34;cat /tmp/httpd.conf\u0026#34; ansible2 | CHANGED | rc=0 \u0026gt;\u0026gt; Welcome to httpd Using Filters # In Jinja2 templates, you can use filters. Filters are a way to perform an operation on the value of a template expression, such as a variable. The filter is included in the variable definition itself, and the result of the variable and its filter is used in the file that is generated. Common filters {{ myvar | to_json }}\nwrites the contents of myvar in JSON format {{ myvar || to_yaml }} writes the contents of myvar in YAML format {{ myvar | ipaddr }} tests whether myvar contains an IP address From https://docs.ansible.com: # How do I loop over a list of hosts in a group, inside of a template? # A pretty common pattern is to iterate over a list of hosts inside of a host group, perhaps to populate a template configuration file with a list of servers. To do this, you can just access the “$groups” dictionary in your template, like this:\n{% for host in groups[\u0026#39;db_servers\u0026#39;] %} {{ host }} {% endfor %} If you need to access facts about these hosts, for example, the IP address of each hostname, you need to make sure that the facts have been populated. For example, make sure you have a play that talks to db_servers:\n- hosts: db_servers tasks: - debug: msg=\u0026#34;doesn\u0026#39;t matter what you do, just that they were talked to previously.\u0026#34; Then you can use the facts inside your template, like this:\n{% for host in groups[\u0026#39;db_servers\u0026#39;] %} {{ hostvars[host][\u0026#39;ansible_eth0\u0026#39;][\u0026#39;ipv4\u0026#39;][\u0026#39;address\u0026#39;] }} {% endfor %} Lab: Working with Conditional Statements in Templates # 1. Use your editor to create the file exercise83.j2. Include the following line to open the Jinja2 conditional statement:\n{% for host in groups[\u0026#39;all\u0026#39;] %} 2. This statement defines a variable with the name host. This variable iterates over the magic variable groups, which holds all Ansible host groups as defined in inventory. Of these groups, the all group (which holds all inventory host names) is processed.\n3. Add the following line (write it as one line; it will wrap over two lines, but do not press Enter to insert a newline character):\n{{ hostvars[host][\u0026#39;ansible_default_ipv4\u0026#39;][\u0026#39;address\u0026#39;] }} {{ hostvars[host][\u0026#39;ansible_fqdn\u0026#39;] }} {{ hostvars[host][\u0026#39;ansible_hostname\u0026#39;] }} This line writes a single line for each inventory host, containing three items. To do so, you use the magic variable hostvars, which can be used to identify Ansible facts that were discovered on the inventory host. The \\[host\\] part is replaced with the name of the current host, and after that, the specific facts are referred to. As a result, for each host a line is produced that holds the IP address, the FQDN, and next the host name. 4. Add the following line to close the for loop:\n{% endfor %} 5. Verify that the complete file contents look like the following and write and quit the file:\n{% for host in groups[\u0026#39;all\u0026#39;] %} {{ hostvars[host][\u0026#39;ansible_default_ipv4\u0026#39;][\u0026#39;address\u0026#39;] }} {{ hostvars[host][\u0026#39;ansible_fqdn\u0026#39;] }} {{ hostvars[host][\u0026#39;ansible_hostname\u0026#39;] }} {% endfor %} 6. Use your editor to create the file exercise83.yaml. It should contain the following lines:\n--- - name: generate /etc/hosts file hosts: all tasks: - name: template: src: exercise83.j2 dest: /tmp/hosts 7. Run the playbook by using ansible-playbook exercise83.yaml\n8. Verify the /tmp/hosts file was generated by using ansible all -a \u0026quot;cat /tmp/hosts\u0026quot;\nThis lab only worked if every host in the inventory file was reachable.\nLab: Generate an /etc/hosts File # Write a playbook that generates an /etc/hosts file on all managed hosts. Apply the following requirements:\n• All hosts that are defined in inventory should be added to the /etc/hosts file.\n[ansible@control ~]$ cat hostfile.yaml --- - name: generate /etc/hosts hosts: all gather_facts: yes tasks: - name: Generate hosts file with template template: src: hosts.j2 dest: /etc/hosts [ansible@control ~]$ cat hosts.j2 {% for host in groups[\u0026#39;all\u0026#39;] %} {{ hostvars[host][\u0026#39;ansible_default_ipv4\u0026#39;][\u0026#39;address\u0026#39;] }} {{ hostvars[host][\u0026#39;ansible_fqdn\u0026#39;] }} {{ hostvars[host][\u0026#39;ansible_hostname\u0026#39;] }} {% endfor %}} Lab: Manage a vsftpd Service # Write a playbook that uses at least two plays to install a vsftpd service configure the vsftpd service using templates configure permissions as well as SELinux. Install, start, and enable the vsftpd service. open a port in the firewall to make it accessible. Use the /etc/vsftpd/vsftpd.conf file to generate a template. In this template, you should use the following variables to configure specific settings. Replace these settings with the variables and leave all else unmodified: Anonymous_enable: yes Local_enable: yes Write_enable: yes Anon_upload_enable: yes Set permissions on the /var/ftp/pub directory to mode 0777. Configure the ftpd_anon_write Boolean to allow anonymous user writes. Set the public_content_rw_t SELinux context type to the /var/ftp/pub directory. If any additional tasks are required to get this done, take care of them. vim vsftpd.yaml\n--- - name: manage vsftpd hosts: ansible1 vars: anonymous_enable: yes local_enable: yes write_enable: yes Anon_upload_enable: yes tasks: - name: install vsftpd dnf: name: vsftpd state: latest - name: configure vsftpd configuration file template: src: vsftpd.j2 dest: /etc/vsftpd/vsftpd.conf - name: apply permissions hosts: ansible1 tasks: - name: set folder permissions to /var/ftp/pub file: path: /var/ftp/pub mode: 0777 - name: set ftpd_anon_write boolean seboolean: name: ftpd_anon_write state: yes persistent: yes - name: set public_content_rw_t SELinux context type to /var/ftp/pub directory sefcontext: target: \u0026#39;/var/ftp/pub(/.*)?\u0026#39; setype: public_content_rw_t state: present notify: restore selinux contexts - name: firewall stuff firewalld: service: ftp state: enabled permanent: true immediate: true - name: start and enable fsftpd service: name: vsftpd state: started enabled: yes handlers: - name: restore selinux contexts command: restorecon -v /var/ftp/pub vsftpd.j2\n{{ ansible_managed }} anonymous_enable={{ anonymous_enable }} local_enable={{ local_enable }} write_enable={{ write_enable }} Anon_upload_enable{{ Anon_upload_enable }} local_umask=022 dirmessage_enable=YES xferlog_enable=YES connect_from_port_20=YES xferlog_std_format=YES listen=NO listen_ipv6=YES pam_service_name=vsftpd userlist_enable=YES ","externalUrl":null,"permalink":"/tech/ansible/jinja2-templates/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eUsing Jinja2 Templates\n    \u003cdiv id=\"using-jinja2-templates\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-jinja2-templates\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA template is a configuration file that contains variables and, based on the variables, is generated on the managed hosts according to host-specific requirements.\u003c/li\u003e\n\u003cli\u003eUsing templates allows for a structural way to generate configuration files, which is much more powerful than changing specific lines from specific files.\u003c/li\u003e\n\u003cli\u003eAnsible uses Jinja2 to generate templates.\u003c/li\u003e\n\u003cli\u003eJinja2 is a generic templating language for Python developers.\u003c/li\u003e\n\u003cli\u003eIt is used in Ansible templates, but Jinja2-based approaches are also found in other parts of Ansible. For instance, the way variables are referred to is based on Jinja2.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn a Jinja2 template, three elements can be used.\n\u003cstrong\u003edata\u003c/strong\u003e\u003c/p\u003e","title":"Jinja2 templates","type":"tech"},{"content":" Connection Methods # Factory default login: # User: root\nNo password\nFxpo # Ethernet management interface\nSSH, FTP. Telnet, http(s) # Cannot route traffic and is used for management purposes only.\nInitial Login # Logging for the First Time\n• Nonroot users are placed into the CLI automatically\n• Root user SSH login requires explicit config\nrouter (ttyu0)\nSerial console\nlogin :\nuser\nPassword:\nJUNOS 15.1X49-DIOO.6 built 2017-06-28 07:33:31 UTC\n• The root user must start the CLI from the shell\n• Remember to exit the root shell after logging out of the CLI!\nrouter (ttyuO) JUNOS 15.1X49-DIOO\n2017-06-28\nUTC\nlogin:\nPassword:\nroot@router\u0026gt;\n.6 built\ncli\nShell Prompt\nCLI Prompt \u0026ldquo;\u0026gt; CLI Modes # configure\nConfigure mode . New candidate config file\nconfigure private (best practice) # Configure mode with a private candidate file\nOther users logged in will not make changes to this file\nPrivate Files comitted are merged into active config\nWhoever commits last wins if there are matching commands\nCan\u0026rsquo;t commit until you are at the top of the configuration (in private mode)\nconfigure exclusive # Locks config database\nCan be killed by admin\nNo other user can edit config while you are in this mode\n(edit) top # Goes back to the top of the configuration tree\nCandidate Config Files\ncommit # Turns candidate config file into active\nWarning will show if candidate config is already being edited\nCommiting Configurations\nRollback files are last three Active configurations and stored in /config/(the current active are stored here as well)\n4-49 are stores in /var/config/\nShows timestamp for the last time the file was active\nrollback 1 # Places rollback file one into the candidate config, must commit to make it active\nCLI Help, Auto complete\nCan type ? To show available commands\n#\u0026gt; Show version brief\nShow version info, hostname, and model\n#\u0026gt;Configure\ngoes into configure mode\nset system host-name hostname # set\u0026rsquo;s hostname\ndelete system host-name # deletes set hostname\nedit routing-options static # edit routing options mode\nexit # exit\nJunos will let you know that config hasn\u0026rsquo;t been committed and ask if you want to commit\nrollback 0 # throwaway all changes to active candidate\n#\u0026gt; help topic routing-options static\nshows info page for topic specified\n#\u0026gt; help references routing-options static\nsyntax and hierarchy of commands\nKeyboard Shortcuts\nCommand completion\nSpace\nauto complete commands built into system, Does not autocomplete things you named\ntab\nautocomplete user defined commands in the system\n?\nwill show user defined options for autocomplete as well\nNavigating Configuration Mode\nWhen you go into config mode the running config is copied into a candidate file that you will be working on\nshow # if in configure mode, displays the entire candidate configuration\nedit # similar to cd\nedit protocols ospf # goes to the protocols/ospf heirarchy config mode\nif you run show commend it will show the contents of hierarchy from wherever you are.\ntop # goes to the top of the hierarchy. Like cd to / in Linux\nmust be at the top to commit changes\nshow protocols ospf # selects which part of the hierarchy to show\nwill only see this if you are above the option you want to show in the hierarchy\ncan bypass this with:\ntop show routing-options static # same thing happens with the edit command\ntop edit routing-options # same fix\nEditing, Renaming, and Comparing Configuration\nup # moves up one level in the hierarchy\nthere is a protion in this video wioth vlan and interface configuration, come back if this isn\u0026rsquo;t covered elsewhere\nup 2 # jump up 2 levels\nrollback ? # shows all the rollback files on the system\nrun show system uptime # run is like \u0026ldquo;do\u0026rdquo; in cisco, can run command from anywhere\nrollback 1 # rolls config back to rollback 1 file\nshow | compare # show things to be removed added with - or +\nexit # Also brings you to the top of config file\nReplace, Copy, or annotate Configuration\ncopy ge-0/0/1 to ge-0/0/2 # makes a copy of the config\nshow ge-0/0/0 # edit ge-0/0/0 # Edit interfaces mode\n#(int) replace pattern 0.101 with 200.102\nReplaces the pattern of the ip address\n#(int) replace pattern /24 with /25\nReplace mask\nIf using replace commands don\u0026rsquo;t commit the config without running the #top show | compare command to verify. You may have run the compare command from one place.\ntop edit protocols ospf # Go into ospf edit\ndeactivate interface ge-0/0/0.0 # Remove interface from ospf\nannotate interface ge-0/0/0 \u0026ldquo;took down due to flapping\u0026rdquo; # C style programming comment\nLoad merge Configuration\nrun file list # ls -l basically\nrun file show top-int-config # Display contents of top-int-config\nPaste Config on a Juniper Switch # cli top delete configure load set terminal ctrl+shift +D to exit commit check commit and-quit Juniper command equivalent to Cisco commands # Basic CLI and Systems Management # Commands\nclock set \u0026gt; set date\nreload \u0026gt; request system reboot\nshow history \u0026gt; show cli history\nshow logging \u0026gt; show log messages | last show processes \u0026gt; show system processes\nshow running config \u0026gt; show configuration\nshow users \u0026gt; show system users\nshow version \u0026gt; show version | show chassis hardware trace \u0026gt; traceroute\nSwitching Commands # show ethernet-switching interfaces\nshow spanning-tree \u0026gt; show spanning-tree bridge\nshow mac address-table \u0026gt; show ethernet-switching table\nOSPF Commands # show ip ospf database \u0026gt; show ospf database\nshow ip ospf interface \u0026gt; show ospf interface\nshow ip ospf neighbor \u0026gt; show ospf neighbor\nRouting Protocol-Independent Commands # clear arp-cache \u0026gt; clear arp\nshow arp \u0026gt; show arp\nshow ip route \u0026gt; show route\nshow ip route summary \u0026gt; show route summary\nshow route-map \u0026gt; show policy | policy-name\nshow tcp \u0026gt; show system connections\nInterface Commands # clear counters \u0026gt; clear interface statistics\nshow interfaces \u0026gt; show interfaces\nshow interfaces detail \u0026gt; show interfaces extensive\nshow ip interface brief \u0026gt; show interfaces terse\n","externalUrl":null,"permalink":"/tech/tools/juniper_cli_basics/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eConnection Methods\n    \u003cdiv id=\"connection-methods\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#connection-methods\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\n\u003ch4 class=\"relative group\"\u003eFactory default login:\n    \u003cdiv id=\"factory-default-login\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#factory-default-login\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cp\u003eUser: root\u003cbr\u003e\nNo password\u003c/p\u003e","title":"Juniper CLI Basics","type":"tech"},{"content":" Configuring Storage Advanced # To work on this exercise, you need managed machines with an additional disk device: add a 10 GB second disk to host ansible2 and a 5 GB second disk to host ansible3. The exercise assumes the name of the second disk is /dev/sdb; if a different disk name is used in your configuration, change this according to your specifications.\nExercise 15-3 Setting Up an Advanced Storage Solution\nIn this exercise you need to set up a storage solution that meets the following requirements:\n• Tasks in this playbook should be executed only on hosts where the device /dev/sdb exists.\n• If no device /dev/sdb exists, the playbook should print \u0026ldquo;device sdb not present\u0026rdquo; and stop executing tasks on that host.\n• Configure the device with one partition that includes all available disk space.\n• Create an LVM volume group with the name vgfiles.\n• If the volume group is bigger than 5 GB, create an LVM logical volume with the name lvfiles and a size of 6 GB. Note that you must check the LVM volume group size and not the /dev/sdb1 size because in theory you could have multiple block devices in a volume group.\n• If the volume group is equal to or smaller than 5 GB, create an LVM logical volume with the name lvfiles and a size of 3 GB.\n• Format the volume with the XFS file system.\n• Mount it on the /files directory.\n1. Check the size of the volume group. You can, however, write a test that works on a default volume group, and that is what you\u0026rsquo;re going to do first, using the name of the default volume group on CentOS 8, which is \u0026ldquo;cl\u0026rdquo;. The purpose is to test the constructions, which is why it doesn\u0026rsquo;t really matter that the two tasks have overlapping when statements. So create a file with the name exercise153-dev1.yaml and give it the following contents:\n--- - name: get vg sizes hosts: all tasks: - name: find small vgroup sizes debug: msg: volume group smaller than or equal to 20G when: - ansible_facts[’lvm’][’vgs’][’cl’] is defined - ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00 - name: find large vgroup size debug: msg: volume group larger than or equal to 19G when: - ansible_facts[’lvm’][’vgs’][’cl’] is defined - ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026gt;= 19.00 2. Run the playbook by using ansible-playbook exercise153-dev1.yaml. You\u0026rsquo;ll notice that it fails with the error shown in Listing 15-12.\nListing 15-12 exercise153-dev1.yaml Failure Message\nTASK [find small vgroups sizes] *************************************************** fatal: [ansible1]: FAILED! =\u0026gt; \\{\\\u0026#34;msg\u0026#34;: \u0026#34;The conditional check ’ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \\\\\u0026lt;= 20.00’ failed. The error was: Unexpected templating type error occurred on ({% if ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00 %} True {% else %} False {% endif %}): ’\u0026lt;=’ not supported between instances of ’AnsibleUnsafeText’ and ’float’\\n\\nThe error appears to be in ’/home/ansible/rhce8-book/exercise153-dev1.yaml’: line 5, column 5, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n tasks:\\n - name: find small vgroups sizes\\n ^ here\\n\u0026#34;} fatal: [ansible2]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;The conditional check ’ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00’ failed. The error was: Unexpected templating type error occurred on ({% if ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00 %} True {% else %} False {% endif %}): ’\u0026lt;=’ not supported between instances of ’AnsibleUnsafeText’ and ’float’\\n\\nThe error appears to be in ’/home/ansible/rhce8-book/exercise153-dev1.yaml’: line 5, column 5, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n tasks:\\n - name: find small vgroups sizes\\n ^ here\\n\u0026#34;} fatal: [ansible3]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;The conditional check ’ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00’ failed. The error was: Unexpected templating type error occurred on ({% if ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00 %} True {% else %} False {% endif %}): ’\u0026lt;=’ not supported between instances of ’AnsibleUnsafeText’ and ’float’\\n\\nThe error appears to be in ’/home/ansible/rhce8-book/exercise153-dev1.yaml’: line 5, column 5, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n tasks:\\n - name: find small vgroups sizes\\n ^ here\\n\u0026#34;} fatal: [ansible4]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;The conditional check ’ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00’ failed. The error was: Unexpected templating type error occurred on ({% if ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] \u0026lt;= 20.00 %} True {% else %} False {% endif %}): ’\u0026lt;=’ not supported between instances of ’AnsibleUnsafeText’ and ’float’\\n\\nThe error appears to be in ’/home/ansible/rhce8-book/exercise153-dev1.yaml’: line 5, column 5, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n tasks:\\n - name: find small vgroups sizes\\n ^ here\\n\u0026#34;} skipping: [ansible5] skipping: [ansible6] TASK [find large vgroups sizes] *************************************************** skipping: [ansible5] skipping: [ansible6] :::\n3. As you can see in the errors in Listing 15-12, there are two problems in the playbook. The first problem is that there is no ignore_errors in the failing play, which means that only hosts that haven\u0026rsquo;t failed will reach the next task. The second error is the \u0026ldquo;Unexpected templating error\u0026rdquo;. The playbook in its current form is trying to perform a logical test to compare the value of two variables that have an incompatible variable type. The Ansible fact has the type \u0026ldquo;AnsibleUnsafeText\u0026rdquo;, and the value of 20.00 is a float, not an integer. To make this test work, you must force the type of both variables to be set to an integer. Now write exercise153-dev2.yaml where this is happening; notice the use of the filter int, which is essential for the success of this playbook:\n--- - name: get vg sizes ignore_errors: yes hosts: all tasks: - name: set vgroup sizes in variables set_fact: vgsize: \u0026#34;{{ ansible_facts[’lvm’][’vgs’][’cl’][’size_g’] | int }}\u0026#34; - name: debug this debug: msg: the value of vgsize is {{ vgsize }} - name: testing big vgsize value debug: msg: the value of vgsize is bigger than 5 when: vgsize | int \u0026gt; 5 - name: testing small vgsize value debug: msg: the value of vgsize is smaller than 5 when: vgsize | int \u0026lt;= 5 4. Run this playbook. You\u0026rsquo;ll notice it skips and ignores some tasks but doesn\u0026rsquo;t fail anywhere, which means that this playbook\u0026mdash;although absolutely not perfect\u0026mdash;is usable as an example to test the size of the vgfiles volume group later in this exercise.\n5. Now that you\u0026rsquo;ve tested the most complex part of the assignment, you can start writing the rest of the playbook. Do this in a new file with the name exercise153.yaml. Because this playbook has quite a few tasks to accomplish, it might be smart to define the rough structure and ensure that all elements that are needed later are at least documented so that you can later work out the details. So let\u0026rsquo;s start with the first part, where the play header is defined, as well as the rough structure. This is the part where you still have the global overview of all the tasks in this requirement, so you need to make sure you won\u0026rsquo;t forget about them later, which is a real risk if you\u0026rsquo;ve been into the details too much for too long.\n--- - name: set up hosts that have an sdb device hosts: all tasks: - name: getting out with a nice failure message if there is no second disk # fail: debug: msg: write a nice failure message and a when test here # when: something - name: create a partition #parted debug: msg: creating the partition - name: create a volume group #lvg: debug: msg: creating the volume group - name: get the vg size and store it in a variable #set_fact: debug: msg: storing variable as an integer - name: create an LVM on big volume groups #lvol: debug: msg: use when statement to create 6g lvol if vsize \u0026gt; 5 - name: create an LVM on small volume groups #lvol: debug: msg: use when statement to create 3g lvol if vsize \u0026lt;= 5 - name: formatting the XFS filesystem # filesystem debug: msg: creating the filesystem - name: mounting /dev/vgfiles/lvfiles # mount: debug: msg: mounting the volume 6. The advantage of a generic structure like the one you just defined is that you can run a test at any moment. Now it\u0026rsquo;s time to fill it in. Start with the play header and then check whether /dev/sdb is present on the managed system:\n--- - name: setup up hosts that have an sdb device hosts: all tasks: - name: getting out with a nice failure message if there is no second disk fail: msg: there is no second disk when: ansible_facts[’devices’][’sdb’] is not defined 7. At this point I recommend you run a test to see that the playbook really does skip all hosts that don\u0026rsquo;t have a second disk device. Use ansible-playbook exercise153.yaml to do so and observe that you see a lot of skipping messages in the output.\n8. If all is well so far, you can continue to create the partition and create the logical volume group as well. Here are the tasks you need to enter. Notice that no size is specified at any point, which means that the partition and the volume group will be allowed to grow up to the maximum size.\n- name: create a partition parted: device: /dev/sdb number: 1 state: present - name: create a volume group lvg: pvs: /dev/sdb1 vg: vgfiles 9. At this point you can insert the part where you save the volume group size into a variable, which can be used in the when statement that will occur in one of the next tasks. Also, because it\u0026rsquo;s good to check a lot while you are writing a complex playbook, use the debug module to verify the results.\n- name: get vg size and convert to integer in new variable set_fact: vgsize: \u0026#34;{{ ansible_facts[’lvm’][’vgs’][’vgfiles’][’size_g’] | int }}\u0026#34; - name: show vgsize value debug: var: \u0026#34;{{ vgsize }}\u0026#34; 10. After this important step, it\u0026rsquo;s time to run a test. If you need it, you can find a sample playbook of the state so far named exercise153-step9.yaml in the GitHub repository at https://github.com/sandervanvugt/rhce8-book, but it\u0026rsquo;s obviously much better and recommended to run your own code! So use ansible-playbook exercise153.yaml to verify what you\u0026rsquo;ve got so far. Notice that you must make sure to run it on hosts that don\u0026rsquo;t have any configuration yet. If a configuration already exists, that will most likely give you false positives! If you want to make sure all is clean, use ansible all -a \u0026ldquo;dd if=/dev/zero of=/dev/sdb bs=1M count=10\u0026rdquo; to wipe the /dev/sdb devices on your managed hosts, followed by ansible all -m reboot to reboot all of them before you test. The purpose of all this is that at this point you see the error message shown in Listing 15-13. Before moving on to the next step, try to understand what is going wrong.\nListing 15-13 Error Message After Exercise 15-3 Step 10\n::: pre_1\nTASK [get vg size and convert to integer in new variable] ****************************** fatal: [ansible2]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;The task includes an option with an undefined variable. The error was: ’dict object’ has no attribute ’vgfiles’\\n\\nThe error appears to be in ’/home/ansible/rhce8-book/exercise153-step9.yaml’: line 18, column 5, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n vg: vgfiles\\n - name: get vg size and convert to integer in new variable\\n ^ here\\n\u0026#34;} fatal: [ansible3]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;The task includes an option with an undefined variable. The error was: ’dict object’ has no attribute ’vgfiles’\\n\\nThe error appears to be in ’/home/ansible/rhce8-book/exercise153-step9.yaml’: line 18, column 5, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n vg: vgfiles\\n - name: get vg size and convert to integer in new variable\\n ^ here\\n\u0026#34;} :::\n11. As you can see, the variable that you are trying to use has no value yet. And that is for the simple reason that fact gathering is required to set the variable, and fact gathering is happening at the beginning of the playbook. At this point, you need to add a task that runs the setup module right after creating the volume group, and then you can try again. In the output you have to look at the \\[show vgsize value\\] task, which should look all right now, and everything after that can be ignored. See exercise153-step11.yaml in the GitHub repository if you need the complete example.\n# skipping first part of the playbook in this listing - name: create a volume group lvg: pvs: /dev/sdb1 vg: vgfiles - name: run the setup module so that we can use updated facts setup: - name: get vg size and convert to integer in new variable set_fact: vgsize: \u0026#34;{{ ansible_facts[’lvm’][’vgs’][’vgfiles’][’size_g’] | int }}\u0026#34; - name: show vgsize value debug: var: \u0026#34;{{ vgsize }}\u0026#34; 12. Assuming that all went well, you can now add the two conditional tests, where according to the vgsize value, the lvol module is used to create the logical volumes:\n- name: create an LVM on big volume groups lvol: vg: vgfiles lv: lvfiles size: 6g when: vgsize | int \u0026gt; 5 - name: create an LVM on small volume groups lvol: vg: vgfiles lv: lvfiles size: 3g when: vgsize | int \u0026lt;= 5 13. Add the tasks to format the volumes with the XFS file system and mount them:\n- name: formatting the XFS filesystem filesystem: dev: /dev/vgfiles/lvfiles fstype: xfs - name: mounting /dev/vgfile/lvfiles mount: path: /file state: mounted src: /dev/vgfiles/lvfiles fstype: xfs 14. That\u0026rsquo;s all! The playbook is now ready for use. Run it by using ansible-playbook exercise153.yaml and verify its output.\n15. Use the ad hoc command ansible ansible2,ansible3 -a \u0026ldquo;lvs\u0026rdquo; to show LVM logical volume sizes on the machines with the additional hard drive. You should see that all has worked out well and you are done! :::\n","externalUrl":null,"permalink":"/tech/ansible/lab-configuring-storage/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eConfiguring Storage Advanced\n    \u003cdiv id=\"configuring-storage-advanced\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#configuring-storage-advanced\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eTo work on this exercise, you need managed machines with an additional disk device: add\na 10 GB second disk to host ansible2 and a 5 GB second disk to host ansible3. The exercise assumes the name of the second disk is /dev/sdb; if a different disk name is used in your configuration, change this according to your specifications.\u003c/p\u003e","title":"Lab Configuring Storage","type":"tech"},{"content":"It\u0026rsquo;s time to work on an advanced scenario now. Exercise 13-4 includes a step-by-step procedure that guides you through the process of setting up a complex playbook. In this procedure I tried to give you practical guidelines on how to approach such a complex task on the exam, including the part where you may change your mind because you have realized there is a more efficient method. It is important to read the steps carefully because some improvements will be applied while working on this procedure.\n::: note\nWarning\nThis exercise is written such that you can learn from errors that are made. In early steps, some configuration is created that will be optimized later. I purposely used this approach, and you are advised to closely follow the steps in the exercise before investigating the final solution in the exercise134.yaml playbook.\n:::\n::: box Exercise 13-4 Setting Up Ansible Users\nIn this exercise you create a few Ansible users. The users need to be created on the Ansible control host as well as on the managed hosts, and after running the playbook, any user created on the localhost must be able to log in using SSH keys to the corresponding user account on the remote host without having to enter a password. Make sure that the setup meets the following requirements:\n• Create users sharon, blair, ashley, and ahmed.\n• Users sharon and blair are members of the group admins; users ashley and ahmed are members of the group students.\n• On the managed hosts, members of the group admins should have sudo privileges to run any command they want.\n• All users must be configured with the default password \u0026ldquo;password\u0026rdquo;.\n1. This time you\u0026rsquo;re going to use a different approach and set up the framework of the playbook first. This is a good approach to start the development of more complex playbooks and minimizes chances that you miss anything in the playbook. To do so, use your editor to create a file with the name exercise134.yaml, and define the play headers and the names and modules you intend to use for each of the tasks according to the following example:\n--- - name: create users on localhost hosts: localhost tasks: - name: create groups groups: - name: create users users: - name: create users on managed hosts hosts: ansible4 tasks: - name: create groups groups: - name: create users users: - name: copy authorized keys authorized_key: - name: modify sudo configuration template 2. Now that you have defined the global structure, you can start filling it in with details. Begin with the first play, which should start with the creation of the user accounts. In this play, users and groups need to be created. To start with, focus on the basic setup and fill it in with additional details later:\n--- - name: create users on localhost hosts: localhost vars: users: - username: sharon groups: admins - username: blair groups: admins - username: ashley groups: students - username: ahmed groups: students tasks: - name: create groups groups: name: \u0026#34;{{ item.groups }}\u0026#34; state: present loop: \u0026#34;{{ users }}\u0026#34; - name: create users user: name: \u0026#34;{{ item.username }}\u0026#34; groups: \u0026#34;{{ item.groups }}\u0026#34; loop: \u0026#34;{{ users }}\u0026#34; 3. Because you\u0026rsquo;re in for a big project this time, now is a good moment to give it a try. To do so, temporarily comment out the entire second play and run the playbook in check mode by using ansible-playbook -C exercise134.yaml. If you typed the exact text listed in step 2, you get an error at the line where the groups module is referred to. That\u0026rsquo;s right\u0026mdash;there is no groups module; the name is group. Correct this and run the playbook again in check mode. Notice that in check mode you might get false errors. Just double-check, and if you\u0026rsquo;re convinced you\u0026rsquo;ve done it right, ignore the error. Notice that it also doesn\u0026rsquo;t really hurt if you just run the playbook. Any later modifications will be added to the configuration anyway.\n4. Now you\u0026rsquo;re ready to complete the first play by adding the remaining tasks to it. To do so, you still have to do two things, all of which must be done on the user module: you need to set the user password, and you need to create an SSH key pair. To generate the password, you need to generate an encrypted string that can be used as an argument to the user module password argument. To generate this string, use an ad hoc command: ansible localhost -m debug -a \u0026ldquo;msg={{ \u0026lsquo;password\u0026rsquo; | password_hash(\u0026lsquo;sha512\u0026rsquo;, \u0026lsquo;mysalt\u0026rsquo;) }}\u0026rdquo;. Just copy the crypto string this generates (it starts with $6$) and use that in the next step.\n5. Complete the user task in the first play with the generate_ssh_key and password arguments. The complete task looks as follows:\n- name: create users user: name: \u0026#34;{{ item.username }}\u0026#34; groups: \u0026#34;{{ item.groups }}\u0026#34; generate_ssh_key: yes password: $6$mysalt$khiuhihrb8y349hwbohbuoehr8bhqohoibhro8bohoiheoi loop: \u0026#34;{{ users }}\u0026#34; tags: setuplocal Notice the use of tags: setuplocal on the last line; this tag makes it easier to run specific parts of the playbook only, which can be convenient later in the setup procedure. You might want to run the playbook now by using ansible-playbook exercise134.yaml --tags=setuplocal.\n6. At this point the local part of the setup seems to be done, so you can work on the second play. You should start by observing what you\u0026rsquo;re trying to do. In the second play, a couple of tasks are exactly the same as in the first play. Because just repeating the same stuff again wouldn\u0026rsquo;t be very efficient, you can create some imports instead and move the existing code to a file that will be imported. To start with, create the exercise134-vars.yaml file and give it the following contents:\n--- users: - username: sharon groups: admins - username: blair groups: admins - username: ashley groups: students - username: ahmed groups: students 7. Create the exercise134-tasks.yaml file and give it the following contents:\n- name: create groups group: name: \u0026#34;{{ item.groups }}\u0026#34; state: present loop: \u0026#34;{{ users }}\u0026#34; - name: create users user: name: \u0026#34;{{ item.username }}\u0026#34; groups: \u0026#34;{{ item.groups }}\u0026#34; generate_ssh_key: yes ssh_key_comment: \u0026#34;{{ item.username }}@{{ ansible_fqdn }}\u0026#34; password: $6$mysalt$khiuhihrb8y349hwbohbuoehr8bhqohoibhro8bohoiheoi loop: \u0026#34;{{ users }}\u0026#34; 8. Now it\u0026rsquo;s time to rewrite the playbook so that the entire playbook looks as follows (note that the last two tasks still need to be defined):\n--- - name: create users on localhost hosts: localhost vars_files: - exercise134-vars.yaml tasks: - name: include user and group setup import_tasks: exercise134-tasks.yaml tags: setuplocal - name: create users on managed hosts hosts: ansible4 vars_files: - exercise134-vars.yaml tasks: - name: include user and group setup import_tasks: exercise134-tasks.yaml - name: copy authorized keys authorized_key: - name: modify sudo configuration 9. You can work on the copy authorized keys tasks at this point. Because the users were created on localhost and each user has its own SSH key pair, this step appears to be fairly easy. The challenge in this task is the use of the lookup plug-in. Complete the authorized_key task such that the second play looks as follows:\n- name: create users on managed hosts hosts: ansible4 vars_files: - exercise134-vars.yaml tasks: - name: include user and group setup import_tasks: exercise134-tasks.yaml - name: copy authorized keys authorized_key: user: \u0026#34;{{ item.username }}\u0026#34; key: \u0026#34;{{ lookup(‘file’, ‘/home/’+ item.username + ‘/.ssh/id_rsa.pub’) }}\u0026#34; loop: \u0026#34;{{ users }}\u0026#34; - name: modify sudo configuration tags: setupremote 10. Because you can easily make an error while using the lookup plug-in, it\u0026rsquo;s a good idea to run the second play by using ansible-playbook exercise134.yaml --tags=setupremote. Notice that this play works only if the first play has been executed successfully. And oops! That doesn\u0026rsquo;t work out well! You can see the error shown in Listing 13-15. This error is generated because the authorized_keys module cannot access the id_rsa.pub file directly from the hidden .ssh directory in the user home directory.\nListing 13-15 Task 10 Error Output\n::: pre_1\nTASK [copy authorized keys] ******************************************************* [WARNING]: Unable to find ‘/home/laksmi/id_rsa.pub’ in expected paths (use -vvvvv to see paths) fatal: [ansible3]: FAILED! =\u0026gt; {\u0026#34;msg\u0026#34;: \u0026#34;An unhandled exception occurred while running the lookup plugin ‘file’. Error was a \u0026lt;class ‘ansible.errors.AnsibleError’\u0026gt;, original message: could not locate file in lookup: /home/laksmi/id_rsa.pub\u0026#34;} :::\n11. To fix the error that occurred in step 10, you must rewrite the first play with the solution discussed in the earlier section \u0026ldquo;Managing SSH Connections.\u0026rdquo; The following code shows the entire first play, with the modifications you need to make applied after the import_tasks: statement:\n--- - name: create users on localhost hosts: localhost vars_files: - exercise134-vars.yaml tasks: - name: include user and group setup import_tasks: exercise134-tasks.yaml - name: create a directory to store the key file file: name: \u0026#34;{{ item.username }}\u0026#34; state: directory loop: \u0026#34;{{ users }}\u0026#34; - name: copy the local user ssh key to temporary {{ item.username }} key shell: ‘cat /home/{{ item.username }}/.ssh/id_rsa.pub \u0026gt; {{ item.username }}/id_rsa.pub’ loop: \u0026#34;{{ users }}\u0026#34; - name: verify that file exists command: ls -l {{ item.username }}/ loop: \u0026#34;{{ users }}\u0026#34; tags: setuplocal 12. Now it\u0026rsquo;s time to configure the sudo file in the /etc/sudoers.d/ directory. While you\u0026rsquo;ve been setting up the rough structure of the playbook so far, using the template module has been suggested. But the fact is that the file that needs to be created is simple and straightforward, and just needs to contain the line %admins ALL=(ALL:ALL) NOPASSWD:ALL. Because this is a simple task, you don\u0026rsquo;t need to use the template module. Just use the copy module instead, such that after the authorized_key task, only the following task is included:\n- name: copy sudoers file copy: content: ‘%admins ALL=(ALL:ALL) NOPASSWD:ALL’ dest: /etc/sudoers.d/admins 13. Before running the playbook, you may verify what you have typed with the sample playbook in the GitHub repository at https://github.com/sandervanvugt/rhce8-book/exercise134.yaml.\n14. At this point, you can run the playbook by using ansible-playbook exercise134.yaml, and you should encounter no errors.\n15. To verify that all works well, on the control host, type sudo su - ahmed, and once in a shell as user ahmed, type ssh ansible2. Ansible should let the user in without asking for a password. :::\nLab 13-1 # Write a playbook that creates users according to the following specifications:\n• Create users kim, christina, kelly, and bill.\n• Users kim and kelly must be members of the profs group; users christina and bill are members of the students group.\n• While creating the users, set the encrypted password to \u0026ldquo;password\u0026rdquo;.\n• Ensure that members of the group profs have sudo privileges. :::\n[]{#ch14.xhtml}\n::: {#ch14.xhtml#sbo-rt-content}\n","externalUrl":null,"permalink":"/tech/ansible/lab-managing-users/","section":"Teches","summary":"\u003cp\u003eIt\u0026rsquo;s time to work on an advanced scenario now. \u003ca\n  href=\"#ch13.xhtml#exe13_4\"\u003eExercise\n13-4\u003c/a\u003e includes a step-by-step procedure that guides\nyou through the process of setting up a complex playbook. In this\nprocedure I tried to give you practical guidelines on how to approach\nsuch a complex task on the exam, including the part where you may change\nyour mind because you have realized there is a more efficient method. It\nis important to read the steps carefully because some improvements will\nbe applied while working on this procedure.\u003c/p\u003e","title":"Lab Managing Users","type":"tech"},{"content":" Lab: Configure a playbook that works with custom facts # Requirements: • Use the project directory chapter6. • Create an inventory file where ansible1 is member of the host group named file and ansible2 is member of the host group named lamp. • Create a custom facts file that contains a section named packages and set the following variables:\nsmb_package = samba ftp_package = vsftpd db_package = mariadb web_package = httpd firewall_package = firewalld • Create another custom facts file that contains a section named services and set the following variables:\nsmb_service = smbd ftp_service = vsftpd db_service = mariadb web_service = httpd firewall_service = firewalld • Create a playbook with the name copy_facts.yaml that copies these facts to all managed hosts. In this playbook\nDefine a variable remote_dir to specify the directory the fact files should be copied to. Use the variable fact_file to copy the fact files to the appropriate directories. Run the playbook and verify whether it works. Lab 6-2 After copying over the facts files, create a playbook that uses the facts to set up the rest of the environment. # Requirements: • Use a variable inclusion file with the name allvars.yaml and set the following variables:\nweb_root = /var/www/html ftp_root = /var/ftp • Create a playbook that sets up the file services and the web services. Also ensure the playbook opens the firewalld firewall to provide access to these servers.\n• Make sure the webservice provides access to a file index.html, which contains the text \u0026ldquo;Welcome to the Ansible Web Server.\u0026rdquo;\n• Run the playbook and use ad hoc commands to verify that the services have been started.\n","externalUrl":null,"permalink":"/tech/ansible/lab-configure-playbook-with-custom-facts/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eLab: Configure a playbook that works with custom facts\n    \u003cdiv id=\"lab-configure-a-playbook-that-works-with-custom-facts\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#lab-configure-a-playbook-that-works-with-custom-facts\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eRequirements:\n• Use the project directory chapter6.\n• Create an inventory file where ansible1 is member of the host group named \u003cstrong\u003efile\u003c/strong\u003e and ansible2 is member of the host group named \u003cstrong\u003elamp\u003c/strong\u003e.\n• Create a custom facts file that contains a section named packages and set the following variables:\u003c/p\u003e","title":"Lab: Configure Playbook With Custom Facts","type":"tech"},{"content":"Write a playbook that meets the following requirements. Use multiple plays in a way that makes sense.\n• Write a first play that installs the httpd and mod_ssl packages on host ansible1. • Use variable inclusion to define the package names in a separate file. • Use a conditional to loop over the list of packages to be installed. • Install the packages only if the current operating system is CentOS or RedHat (but not Fedora) version 8.0 or later. If that is not the case, the playbook should fail with the error message \u0026ldquo;Host hostname does not meet minimal requirements,\u0026rdquo; where hostname is replaced with the current host name. • On the Ansible control host, create a file /tmp/index.html. This file must have the contents \u0026ldquo;welcome to my webserver\u0026rdquo;. • If the file /tmp/index.html is successfully copied to /var/www/html, the web server process must be restarted. If copying the package fails, the playbook should show an error message. • The firewall must be opened for the http as well as the https services.\npackages\nweb: - httpd - mod_ssl web.yaml\n--- - name: set up a webserver vars_files: packages hosts: ansible1 become: yes tasks: - name: block block: - name: install web packages yum: name: \u0026#34;{{ item }}\u0026#34; state: present loop: \u0026#34;{{ web }}\u0026#34; when: \u0026gt; ( ansible_facts[\u0026#39;distribution\u0026#39;] == \u0026#34;AlmaLinux\u0026#34; ) or ( ansible_facts[\u0026#39;distribution\u0026#39;] == \u0026#34;RedHat\u0026#34; ) and ( ansible_facts[\u0026#39;distribution_version\u0026#39;] \u0026gt; \u0026#34;8\u0026#34; ) and (ansible_facts[\u0026#39;distribution\u0026#39;] != \u0026#34;Fedora\u0026#34; ) rescue: - name: distribution is invalid debug: msg: Host {{ ansible_facts[\u0026#39;hostname\u0026#39;] }} does not meet minimal requirements\u0026#34; - name: copy file to webserver copy: src: /tmp/index.html dest: /var/www/html/ notify: restart_httpd - name: open http ansible.posix.firewalld: service: http state: enabled permanent: true immediate: true - name: open https ansible.posix.firewalld: service: https state: enabled permanent: true immediate: true handlers: - name: restart_httpd service: name: httpd state: restarted ","externalUrl":null,"permalink":"/tech/ansible/lab-install-and-enable-a-webserver/","section":"Teches","summary":"\u003cp\u003eWrite a playbook that meets the following requirements. Use multiple\nplays in a way that makes sense.\u003c/p\u003e\n\u003cp\u003e• Write a first play that installs the httpd and mod_ssl packages on host ansible1.\n• Use variable inclusion to define the package names in a separate file.\n• Use a conditional to loop over the list of packages to be installed.\n• Install the packages only if the current operating system is CentOS or\nRedHat (but not Fedora) version 8.0 or later. If that is not the case,\nthe playbook should fail with the error message \u0026ldquo;Host \u003cem\u003ehostname\u003c/em\u003e does\nnot meet minimal requirements,\u0026rdquo; where \u003cem\u003ehostname\u003c/em\u003e is replaced with the\ncurrent host name.\n• On the Ansible control host, create a file /tmp/index.html. This file\nmust have the contents \u0026ldquo;welcome to my webserver\u0026rdquo;.\n• If the file /tmp/index.html is successfully copied to /var/www/html,\nthe web server process must be restarted. If copying the package fails,\nthe playbook should show an error message.\n• The firewall must be opened for the http as well as the https\nservices.\u003c/p\u003e","title":"Lab: Install and Enable a Webserver","type":"tech"},{"content":"The advanced exercise, 14-3, is a relatively easy task this time. You are guided through the procedure of creating a playbook that runs a command before the reboot, schedules a cron job at the next reboot, and, using that cron job, ensures that after rebooting a specific command is used as well. To make sure you see what happens when, you work with a temporary file to which lines are added.\n::: box Exercise 14-3 Managing the Boot Process and Services\n1. Use your editor to create the file exercise143.yaml and write the playbook header as follows:\n--- - name: exercise143 hosts: ansible2 tasks: 2. Write the first task. This task is not really functional but enables you to check what is happening in the remaining tasks in the playbook. In this task, you use the lineinfile module to add a line to the end of the check file /tmp/rebooted. Notice how the time, including a second indicator, is written using two Ansible facts. It\u0026rsquo;s required to operate this way because not one single fact has the time in an hh:mm:ss format. Write this task code as follows:\n- name: add a line to file before rebooting lineinfile: create: true state: present path: /tmp/rebooted insertafter: EOF line: rebooted at {{ ansible_facts[’date_time’][’time’] }}:{{ ansible_facts[’date_time’][’second’] }} 3. At this point, you can add a cron job that runs at reboot. The job that runs will add a message to the /tmp/rebooted file, and to make sure that it is working correctly, you use bash shell command substitution to print the results of the Linux date command. Notice that this is possible in the cron module because commands are executed by a bash shell, but it\u0026rsquo;s not possible in the previous task that uses the lineinfile module because in that task the commands are not processed by a shell. Now add the task as follows:\n- name: run a cron job on reboot cron: name: \u0026#34;run on reboot\u0026#34; state: present special_time: reboot job: \u0026#34;echo rebooted at $(date) \u0026gt;\u0026gt; /tmp/rebooted\u0026#34; 4. Add another task that uses the reboot module to reboot the managed host:\n- name: reboot managed host reboot: msg: reboot initiated by Ansible test_command: whoami - name: show reboot success debug: msg: just rebooted successfully 5. Now that the playbook is complete, you can run it by using ansible-playbook exercise143.yaml. Notice that it needs a minute because it has to wait until the target host is back. When it is back, use ansible ansible2 -a \u0026ldquo;cat /tmp/rebooted\u0026rdquo;. You then see that both reboot messages are written, and there is about 30 seconds between the pre-reboot and the post-reboot commands. :::\nLab 14-1 # Write a playbook according to the following specifications:\n• The cron module must be used to restart your managed servers at 2 a.m. each weekday.\n• After rebooting, a message must be written to syslog, with the text \u0026ldquo;CRON initiated reboot just completed.\u0026rdquo;\n• The default systemd target must be set to multi-user.target.\n• The last task should use service facts to show the current version of the cron process.\n","externalUrl":null,"permalink":"/tech/ansible/lab-managing-the-boot-process-and-services/","section":"Teches","summary":"\u003cp\u003eThe advanced exercise, 14-3, is a relatively easy task this time. You\nare guided through the procedure of creating a playbook that runs a\ncommand before the reboot, schedules a cron job at the next reboot, and,\nusing that cron job, ensures that after rebooting a specific command is\nused as well. To make sure you see what happens when, you work with a\ntemporary file to which lines are added.\u003c/p\u003e","title":"Lab: Managing the Boot Process and Services","type":"tech"},{"content":"Star: A design in which one central device connects to several others, so that if you drew the links out in all directions, the design would look like a star with light shining in all directions. Topology Terminology Seen Within a Two-Tier Design Full mesh: For any set of network nodes, a design that connects a link between each pair of nodes. Partial mesh: For any set of network nodes, a design that connects a link between some pairs of nodes, but not all. In other words, a mesh that is not a full mesh. Hybrid: A design that combines topology design concepts into a larger (typically more complex) design. twoand a partial mesh at the distribution layer.-tier design is indeed a hybrid design that uses both a star topology at the access layer The distribution layer creates a partial mesh. none of the access layer switches connect to each other. Three-Tier Campus Design (Core)\nThree-Tier Campus Design (Core)\ncollapsed core refers to the fact that the two-tier design does not have a third tier, the core tier\nSometimes the center of the network uses a full mesh, sometimes a partial mesh, depending on the availability of cables between the buildings. provide one function: to connect the distribution switches. Access: Provides a connection point (access) for endbetween two other access switches under normal circumstances.-user devices. Does not forward frames Distribution: Provides an aggregation point for access switches, providing connectivity to the rest of the devices in the LAN, forwarding frames between switches, but not connecting directly to end-user devices. Core: Aggregates distribution switches in very large campus LANs, providing very high forwarding\nrates for the larger volume of traffic due to the size of the network.\nTopology Design Terminology\nthe right side of Figure 13-6 combines the star topology of the access layer with the partial mesh of the distribution layer. So you might hear these designs that combine concepts called a hybrid design. Small Office/Home Office # that one wireless router acts like separate devices you would find in an enterprise campus: Key Topic. An Ethernet switch, for the wired Ethernet connections A wireless access point (AP), to communicate with the wireless devices and forward the frames to/from the wired network A router, to route IP packets to/from the LAN and WAN (Internet) interfaces A firewallbut not vice versa, which often defaults to allow only clients to connect to servers in the Internet, Power over Ethernet (PoE) # a LAN switch, acts as the Power Sourcing Equipment (PSE supplies DC power so the device does not need an AC/DC converter. A device that has the capability to be powered over the Ethernet cable is called the Powered Device (PD) PoE Operation PoE must (and does) have processes in place to determine if PoE is needed, and for how much power, before applying any potentially harmful power levels to the circuit. autonegotiation mechanisms need to work before the PD has booted, By using these IEEE autonegotiation messages and watching for the return signal levels, PoE can determine whether the device on the end of the cable requires power (that is, it is a PD) and how much power to supply Step 1. the device needs power.Do not supply power on a PoE-capable port unless negotiation identifies that Step 2. Use Ethernet autonegotiation techniques, sending low power signals and monitoring the return signal, to determine the PoE power class, which determines how much power to supply to the device. Step 3allows the device to boot.. If the device is identified as a PD, supply the power per the power class, which Step 4. Monitor for changes to the power class, both with autonegotiation and listening for CDP and LLDP messages from the PD. listening for CDP and LLDP messages from the PD. Step 5.If a new power class is identified, adjust the power level per that class. PDs signaling how many watts of power they would like to receive from the PSon the specific PoE standard, the PSE will then supply the power, either over two pairs or E. Depending four pairs, as noted in Table 13-2. PoE and LAN Design\nsome of the key points to consider when planning a LAN design that includes PoE:\nPowered Devices: power requirements.Determine the types of devices and specific models, along with their\nPower Requirements : Plan the numbers of different types of PDs to connect into each\nwiring closet to build a power budget. That power budget can then be processed to determine the amount of PoE power to make available through each switch.\nSa subset of ports. Research the various switch models so that you purchase enough PoE witch Ports : Some switches support PoE standards on all ports, some on no ports, some on -\ncapable ports for the switches planned for each wiring closet.\nSwitch Power Supplies so that it delivers enough power to power the switch itself. With PoE, the switch acts as a : Without PoE, when purchasing a switch, you choose a power supply\ndistributor of electrical power, so the switch power supply must deliver many more watts than it needs to run the switch itself. You will need to create a power budget per switch,\nbased on the number of connected PDs, and purchase power supplies to match those\nrequirements.\nPoE Standards versus Actual standards they support, the standards supported by the PDs, and how much power they : Consider the number of PoE switch ports needed, the\nconsume. For instance, a PD and a switch port may both support PoE+, which supports up to 30 watts supplied by the PSE. However, that powered device may need at most 9 watts\nto 30 watts supplied by the PSE. However, that powered device may need at most 9 watts to operate, so your power budget needs to reserve less power than the maximum for those\ndevices.\n","externalUrl":null,"permalink":"/tech/networking/lan-architecture/","section":"Teches","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eStar: A design in which one central device connects to several others, so that if you\ndrew the links out in all directions, the design would look like a star with light shining in all directions.\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eTopology Terminology Seen Within a Two-Tier Design\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eFull mesh: For any set of network nodes, a design that connects a link between each pair of nodes.\nPartial mesh: For any set of network nodes, a design that connects a link between\nsome pairs of nodes, but not all. In other words, a mesh that is not a full mesh.\nHybrid: A design that combines topology design concepts into a larger (typically more\ncomplex) design.\ntwoand a partial mesh at the distribution layer.-tier design is indeed a hybrid design that uses both a star topology at the access layer\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eThe distribution layer creates a partial mesh.\nnone of the access layer switches connect to each other.\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThree-Tier Campus Design (Core)\u003c/p\u003e","title":"LAN Architecture","type":"tech"},{"content":" Manual Layer 2 EtherChannel # Add the correct channel-group configuration command to each physical interface on each switch, all with the on keyword, and all with the same number. The on keyword tells the switches to place a physical interface into an EtherChannel, and The number identifies the PortChannel interface number that the interface should be a part of. EtherChannel, PortChannel, and Channel-group basically mean the same thing. channel-group number on the neighboring switch can differ. \u0026ldquo;Po1\u0026rdquo; is short for PortChannel1 Channel-group command cannot override the channel-protocol command Create the port channel:\nint port-channel 1 Add an Ip address to the port channel:\nip address 10.0.0.5 255.255.255.0 Add the channel-group to each interface that should be in the channel:\nchannel-group 1 mode on Display port channel status:\nshow etherchannel Dynamic EtherChannels # Cisco switches also support two different configuration options that then use a dynamic protocol to negotiate whether a particular link becomes part of an EtherChannel or not. Basically, the configuration enables a protocol for a particular channel-group number. At that point, the switch can use the protocol to send messages to/from the neighboring switch and discover whether their configuration settings pass all checks. If a given physical link passes, the link is added to the EtherChannel and used; if not, it is placed in a down state, and not used, until the configuration inconsistency can be resolved.\nPort Aggregation Protocol (PAgP) (Cisco) # support 8 links in a channel uses \u0026ldquo;desirable\u0026rdquo; and \u0026ldquo;auto\u0026rdquo; At least one of the two sides must be set to \u0026ldquo;desirable\u0026rdquo; Link Aggregation Control Protocol (LACP) # Support 16 links in a channel Only 8 can be active at a time inactive ports wait for any of the active ports to fail Uses \u0026ldquo;active\u0026rdquo; and \u0026ldquo;passive\u0026rdquo; At least one of the two sides must be set to \u0026ldquo;active\u0026rdquo; negotiate so that only links that pass the configuration checks are actually used in an EtherChannel.\nTo configure either protocol, a switch uses the channel-group configuration commands on each switch, but with a keyword that either means “use this protocol and begin negotiations” or “use this protocol and wait for the other switch to begin negotiations.”\nNote:\nDo not use the on parameter on one end, and either auto or desirable (or for LACP, active or passive) on the neighboring switch. The on option uses neither PAgP nor LACP, so a configuration that uses on, with PAgP or LACP options on the other end, would prevent the EtherChannel from working. Show port channel 1 information:\nshow etherchannel 1 port-channel Matching Physical Interface Settings # Physical port configuration must match other ports in the channel or it will not be used. Switches either use PAgP or LACP (if already in use) or use Cisco Discovery Protocol (CDP) if using manual configuration to check neighbors All settings except the STP settings must match. Shutdown/no shutdown the port channel interface to recover from err-disabled state. (This restarts the physical interfaces as well) Things that must match:\nSpeed Duplex Access or Trunk setting VLAN for access ports Allowed VLAN list for trunk ports Native VLAN for trunk ports STP interface settings Etherchannel Load Distribution # MAC addresses of the PortChannel interfaces are used instead of the physical ports. The switch must decide out which specific physical port to use to forward the frame. (EtherChannel load distribution or load balancing) MAC or IP based load balancing methods depending on the switch Messages in a single application flow use the same link in the channel integrates ASIC so that load distribution works as quick as forwarding any other frame Uses all the active links in the EtherChannel, adjusting to the addition and removal of active links over time. Balance the traffic across active links. Balanced based on low order bits Define which method is used for load balancing:\nport-channel load-balance method Test which ports specific types of traffic would use based on source/destination mac address:\ntest etherchannel load-balance ","externalUrl":null,"permalink":"/tech/networking/l2-etherchannel/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eManual Layer 2 EtherChannel\n    \u003cdiv id=\"manual-layer-2-etherchannel\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#manual-layer-2-etherchannel\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAdd the correct channel-group configuration command to each physical interface on each switch, all with the on keyword, and all with the same number.\u003c/li\u003e\n\u003cli\u003eThe on keyword tells the switches to place a physical interface into an EtherChannel, and\u003c/li\u003e\n\u003cli\u003eThe number identifies the PortChannel interface number that the interface should be a part of.\u003c/li\u003e\n\u003cli\u003eEtherChannel, PortChannel, and Channel-group basically mean the same thing.\u003c/li\u003e\n\u003cli\u003echannel-group number on the neighboring switch can differ.\u003c/li\u003e\n\u003cli\u003e\u0026ldquo;Po1\u0026rdquo; is short for PortChannel1\u003c/li\u003e\n\u003cli\u003eChannel-group command cannot override the channel-protocol command\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCreate the port channel:\u003c/p\u003e","title":"Layer 2 Etherchannel","type":"tech"},{"content":"Using Monkey Type to get my first speedtest results:\n02/24/2025 55 WPM Learning to type with: https://www.typing.com/ https://www.typingclub.com/\nGoing to be practicing on these sites: https://10fastfingers.com/ https://www.keybr.com/ https://play.typeracer.com/\ntry ctrl+backspace to delete entire word\nDay 1 (1 hour) Typing club Lessons 1-41\nDay 2 (1 hour) Typing club Lessons 1-41\nDay 3 (1 hour) Typing club Lessons 1-50\nDay 4 (1 hour) Typing club Lessons 2-55\nDay 5 (1 hour) Typing club Lessons 2-62\nDay 6 (1 hour) Typing club Lessons 25-46 Lessons 2-10 Above 50 WPM at 100% Hands very cold and lack of sleep today.\nDay 7 (1 hour) Typing Club Lessons 53 - 81\nDay 8 (skipped) Busy with sick child\nDay 9 (1.5 hours) Typing Club Lessons 55-107\nDay 10 (30 minutes) Typing Club Lessons 108-119\nDay 11 (1 hour) Typing Club Lessons 120-141\nDay 12 (30 minutes) Typing Club Lessons 142-151\nDay 13 (30 minutes) Typing Club Lessons 151-164\n","externalUrl":null,"permalink":"/tech/tools/learning-touch-typing/","section":"Teches","summary":"\u003cp\u003eUsing \u003ca\n  href=\"https://monkeytype.com/\"\n    target=\"_blank\"\n  \u003eMonkey Type\u003c/a\u003e to get my first speedtest results:\u003c/p\u003e\n\u003cp\u003e02/24/2025\n55 WPM\nLearning to type with:\n\u003ca\n  href=\"https://www.typing.com/\"\n    target=\"_blank\"\n  \u003ehttps://www.typing.com/\u003c/a\u003e\n\u003ca\n  href=\"https://www.typingclub.com/\"\n    target=\"_blank\"\n  \u003ehttps://www.typingclub.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGoing to be practicing on these sites:\n\u003ca\n  href=\"https://10fastfingers.com/\"\n    target=\"_blank\"\n  \u003ehttps://10fastfingers.com/\u003c/a\u003e\n\u003ca\n  href=\"https://www.keybr.com/\"\n    target=\"_blank\"\n  \u003ehttps://www.keybr.com/\u003c/a\u003e\n\u003ca\n  href=\"https://play.typeracer.com/\"\n    target=\"_blank\"\n  \u003ehttps://play.typeracer.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etry ctrl+backspace to delete entire word\u003c/p\u003e","title":"Learning Touch Typing","type":"tech"},{"content":" File Systems and File System Types # File systems\nCan be optimized, resized, mounted, and unmounted independently. Must be connected to the directory hierarchy in order to be accessed by users and applications. Mounting may be accomplished automatically at system boot or manually as required. Can be mounted or unmounted using their unique identifiers, labels, or device files. Each file system is created in a discrete partition, VDO volume, or logical volume. A typical production RHEL system usually has numerous file systems. During OS installation, only two file systems\u0026mdash; / and /boot \u0026mdash;are created in the default disk layout, but you can design a custom disk layout and construct separate containers to store dissimilar information. Typical additional file systems that may be created during an installation are /home, /opt, /tmp, /usr, and /var. / and /boot\u0026mdash;are required for installation and booting. Storing disparate data in distinct file systems versus storing all data in a single file system offers the following advantages:\nMake any file system accessible (mount) or inaccessible (unmount) to users independent of other file systems. This hides or reveals information contained in that file system. Perform file system repair activities on individual file systems Keep dissimilar data in separate file systems Optimize or tune each file system independently Grow or shrink a file system independent of other file systems 3 types of file systems:\ndisk-based, network-based, and memory-based. Disk-based\nTypically created on physical drives using SATA, USB, Fibre Channel, and other technologies. store information persistently Network-based\nEssentially disk-based file systems shared over the network for remote access. store information persistently Memory-based\nVirtual Created at system startup and destroyed when the system goes down. data saved in virtual file systems does not survive across system reboots. Ext3\nDisk based The third generation of the extended filesystem. Metadata journaling for faster recovery Superior reliability Creation of up to 32,000 subdirectories supports larger file systems and bigger files than its predecessor Ext4\nDisk based Successor to Ext3. Supports all features of Ext3 in addition to: Larger file system size Bigger file size Unlimited number of subdirectories Metadata and quota journaling Extended user attributes XFS\nDisk based Highly scalable and high-performing 64-bit file system. Supports: Metadata journaling for faster crash recovery Online defragmentation, expansion, quota journaling, and extended user attributes default file system type in RHEL 9. VFAT\nDisk based Used for post-Windows 95 file system formats on hard disks, USB drives, and floppy disks. ISO9660\nDisk based Used for optical file systems such as CD and DVD. NFS - (Network File System.)\nNetwork based Shared directory or file system for remote access by other Linux systems. AutoFS (Auto File System)\nNetwork based NFS file system set to mount and unmount automatically on remote client systems. Extended File Systems # First generation is obsolete and is no longer supported Second, third, and fourth generations are currently available and supported. Fourth generation is the latest in the series and is superior in features and enhancements to its predecessors. Structure is built on a partition or logical volume at the time of file system creation. Structure is divided into two sets: first set holds the file system\u0026rsquo;s metadata and it is very tiny. Superblock keeps vital file system structural information: type size status of the file system number of data blocks it contains automatically replicated and maintained at various known locations throughout the file system. primary superblock superblock at the beginning of the file system backup superblocks. I used to supplant the corrupted or lost primary superblock to bring the file system back to its normal state. Copy of the primary Inode table maintains a list of index node (inode) numbers. Each file is assigned an inode number at the time of its creation, and the inode number holds the file\u0026rsquo;s attributes such as: type permissions ownership owning group size last access/modification time holds and keeps track of the pointers to the actual data blocks where the file contents are located. second set stores the actual data, and it occupies almost the entire partition or the logical volume (VDO and LVM) space.\\ journaling\nSupported by Ext3 and Ext4\nRecover swiftly after a system crash.\nkeep track of recent changes in their metadata in a journal (or log).\nEach metadata update is written in its entirety to the journal after completion.\nThe system peruses the journal of each extended file system following the reboot after a crash to determine if there are any errors\nLets the system recover the file system rapidly using the latest metadata information stored in its journal.\nExt3 that supports file systems up to 16TiB and files up to 2TiB,\nExt4 supports very large file systems up to 1EiB (ExbiByte) and files up to 16TiB (TebiByte).\nUses a series of contiguous physical blocks on the hard disk called extents, resulting in improved read and write performance with reduced fragmentation. Supports extended user attributes, metadata and quota journaling, etc. XFS File System # High-performing 64-bit extent-based journaling file system type. Allows the creation of file systems and files up to 8EiB (ExbiByte). Does not run file system checks at system boot Relies on you to use the xfs_repair utility to manually fix any issues. Sets the extended user attributes and certain mount options by default on new file systems. Enables defragmentation on mounted and active file systems to keep as much data in contiguous blocks as possible for faster access. Inability to shrink. Uses journaling for metadata operations, guaranteeing the consistency of the file system against abnormal or forced unmounting. Journal information is read and any pending metadata transactions are replayed when the XFS file system is remounted. Speedy input/output performance. Can be snapshot in a mounted, active state. VFAT File System # Extension to the legacy FAT file system (FAT16) Supports 255 characters in filenames including spaces and periods Does not differentiate between lowercase and uppercase letters. Primarily used on removable media, such as floppy and USB flash drives, for exchanging data between Linux and Windows. ISO9660 File System # For removable optical disc media such as CD/DVD drives File System Management # File System Administration Commands # Some are limited to their operations on the Extended, XFS, or VFAT file system type. Others are general and applicable to all file system types. Extended File System Management Commands # e2label\nModifies the label of a file system tune2fs\nTunes or displays file system attributes XFS Management Commands # xfs_admin Tunes file system attributes xfs_growfs Extends the size of a file system xfs_info Exhibits information about a file system General File System Commands # blkid Displays block device attributes including their UUIDs and labels df Reports file system utilization du Calculates disk usage of directories and file systems fsadm Resizes a file system. This command is automatically invoked when the lvresize command is run with the -r switch. lsblk Lists block devices and file systems and their attributes including their UUIDs and labels mkfs Creates a file system. Use the -t option and specify ext3, ext4, vfat, or xfs file system type. mount Mount a file system for user access. Display currently mounted file systems. umount Unmount a file system. Mounting and Unmounting File Systems # File system must be connected to the directory structure at a desired attachment point, (mount point) A mount point in essence is any empty directory that is created and used for this purpose. Use the mount command to view information about xfs mounted file systems:\n[root@server2 ~]# mount -t xfs /dev/mapper/rhel-root on / type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota) /dev/sda1 on /boot type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota) Mount command # -t option type. Mount a file system to a mount point. Performed with the root user privileges. Requires the absolute pathnames of the file system block device and the mount point name. Accepts the UUID or label of the file system in lieu of the block device name. Mount all or a specific type of file system. Upon successful mount, the kernel places an entry for the file system in the /proc/self/mounts file. A mount point should be empty when an attempt is made to mount a file system on it, otherwise the content of the mount point will hide. The mount point must not be in use or the mount attempt will fail. auto (noauto)\nMounts (does not mount) the file system when the -a option is specified defaults\nMounts a file system with all the default values (async, auto, rw, etc.) _netdev\nUsed for a file system that requires network connectivity in place before it can be mounted. NFS is an example. remount\nRemounts an already mounted file system to enable or disable an option ro (rw)\nMounts a file system read-only read/write) umount Command # Detach a file system from the directory hierarchy and make it inaccessible to users and applications. Expects the absolute pathname to the block device containing the file system or its mount point name in order to detach it. Unmount all or a specific type of file system. Kernel removes the corresponding file system entry from the /proc/self/mounts file after it has been successfully disconnected. Determining the UUID of a File System # Extended and XFS file systems have a 128-bit (32 hexadecimal characters) UUID (Universally Unique IDentifier) assigned to it at the time of its creation.\nUUIDs assigned to vfat file systems are 32-bit (8 hexadecimal characters) in length.\nAssigning a UUID makes the file system unique among many other file systems that potentially exist on the system.\nPersistent across system reboots.\nUsed by default in RHEL 9 in the /etc/fstab file for any file system that is created by the system in a standard partition.\nRHEL attempts to mount all file systems listed in the /etc/fstab file at reboots.\nEach file system has an associated device file and UUID, but may or may not have a corresponding label.\nThe system checks for the presence of each file system\u0026rsquo;s device file, UUID, or label, and then attempts to mount it.\nDetermine the UUID of /boot\n[root@server2 ~]# lsblk | grep boot ├─sda1 8:1 0 1G 0 part /boot [root@server2 ~]# sudo xfs_admin -u /dev/sda1 UUID = 630568e1-608f-4603-9b97-e27f82c7d4b4 [root@server2 ~]# sudo blkid /dev/sda1 /dev/sda1: UUID=\u0026#34;630568e1-608f-4603-9b97-e27f82c7d4b4\u0026#34; TYPE=\u0026#34;xfs\u0026#34; PARTUUID=\u0026#34;7dcb43e4-01\u0026#34; [root@server2 ~]# sudo lsblk -f /dev/sda1 NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS sda1 xfs 630568e1-608f-4603-9b97-e27f82c7d4b4 616.1M 36% /boot For extended file systems, you can use the tune2fs, blkid, or lsblk commands to determine the UUID.\nA UUID is also assigned to a file system that is created in a VDO or LVM volume; however, it need not be used in the fstab file, as the device files associated with the logical volumes are always unique and persistent.\nLabeling a File System # A unique label may be used instead of a UUID to keep the file system association with its device file exclusive and persistent across system reboots. A label is limited to a maximum of 12 characters on the XFS file system 16 characters on the Extended file system. By default, no labels are assigned to a file system at the time of its creation. The /boot file system is located in the /dev/sda1 partition and its type is XFS. You can use the xfs_admin or the lsblk command as follows to determine its label:\n[root@server2 ~]# sudo xfs_admin -l /dev/sda1 label = \u0026#34;\u0026#34; [root@server2 ~]# sudo lsblk -f /dev/sda1 NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS sda1 xfs 630568e1-608f-4603-9b97-e27f82c7d4b4 616.1M 36% /boot Not needed on a file system if you intend to use its UUID or if it is created in a logical volume You can still apply one using the xfs_admin command with the -L option. Labeling an XFS file system requires that the target file system be unmounted. unmount /boot, set the label \u0026ldquo;bootfs\u0026rdquo; on its device file, and remount it:\n[root@server2 ~]# sudo umount /boot [root@server2 ~]# sudo xfs_admin -L bootfs /dev/sda1 writing all SBs new label = \u0026#34;bootfs\u0026#34; Confirm the new label by executing sudo xfs_admin -l /dev/sda1 or sudo lsblk -f /dev/sda1.\nFor extended file systems, you can use the e2label command to apply a label and the tune2fs, blkid, and lsblk commands to view and verify.\nNow you can replace the UUID=\\\u0026quot;22d05484-6ae1-4ef8-a37d-abab674a5e35\u0026quot; for /boot in the fstab file with LABEL=bootfs, and unmount and remount /boot as demonstrated above for confirmation.\n[root@server2 ~]# mount /boot mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. A label may also be applied to a file system created in a logical volume; however, it is not recommended for use in the fstab file, as the device files for logical volumes are always unique and remain persistent across system reboots.\nAutomatically Mounting a File System at Reboots # /etc/fstab # File systems defined in the /etc/fstab file are mounted automatically at reboots. Must contain proper and complete information for each listed file system. An incomplete or inaccurate entry might leave the system in an undesirable or unbootable state. Only need to specify one of the four attributes Block device name UUID label mount point The mount command obtains the rest of the information from this file. Only need to specify one of these attributes with the umount command to detach it from the directory hierarchy. Contains entries for file systems that are created at the time of installation. [root@server2 ~]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Sun Feb 25 12:11:47 2024 # # Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk/\u0026#39;. # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info. # # After editing this file, run \u0026#39;systemctl daemon-reload\u0026#39; to update systemd # units generated from this file. # /dev/mapper/rhel-root / xfs defaults 0 0 LABEL=bootfs /boot xfs defaults 0 0 /dev/mapper/rhel-swap none swap defaults 0 0 EXAM TIP: Any missing or invalid entry in this file may render the system unbootable. You will have to boot the system in emergency mode to fix this file. Ensure that you understand each field in the file for both file system and swap entries.\nThe format of this file is such that each row is broken out into six columns to identify the required attributes for each file system to be successfully mounted. Here is what the columns contain:\nColumn 1:\nphysical or virtual device path where the file system is resident, or its associated UUID or label. can be entries for network file systems here as well. Column 2:\nIdentifies the mount point for the file system. swap partitions, use either \u0026ldquo;none\u0026rdquo; or \u0026ldquo;swap\u0026rdquo;. Column 3:\nType of file system such as Ext3, Ext4, XFS, VFAT, or ISO9660. For swap, the type \u0026ldquo;swap\u0026rdquo; is used. may use \u0026ldquo;auto\u0026rdquo; instead to leave it up to the mount command to determine the type of the file system. Column 4:\nIdentifies one or more comma-separated options to be used when mounting the file system. Consult the manual pages of the mount command or the fstab file for additional options and details. Column 5:\nUsed by the dump utility to ascertain the file systems that need to be dumped. Value of 0 (or the absence of this column) disables this check. This field is applicable only on Extended file systems; XFS does not use it. Column 6:\nSequence number in which to run the e2fsck (file system check and repair utility for Extended file system types) utility on the file system at system boot.\nBy default, 0 is used for memory-based, remote, and removable file systems, 1 for /, and 2 for /boot and other physical file systems. 0 can also be used for /, /boot, and other physical file systems you don\u0026rsquo;t want to be checked or repaired.\nApplicable only on Extended file systems;\nXFS does not use it.\n0 in columns 5 and 6 for XFS, virtual, remote, and removable file system types has no meaning. You do not need to add them for these file system types.\nLab: Create and Mount Ext4, VFAT, and XFS File Systems in Partitions (server2) # Create 2 x 100MB partitions on the /dev/sdb disk, initialize them separately with the Ext4 and VFAT file system types, define them for persistence using their UUIDs, create mount points called /ext4fs1 and /vfatfs1, attach them to the directorystructure verify their availability and usage you will use the disk /dev/sdc and repeat the above procedure to establish an XFS file system in it and mount it on /xfsfs1. 1. Apply the label \u0026ldquo;msdos\u0026rdquo; to the sdb disk using the parted command:\n[root@server20 ~]# sudo parted /dev/sdb mklabel msdos Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? y Information: You may need to update /etc/fstab. 2. Create 2 x 100MB primary partitions on sdb with the parted command:\n[root@server20 ~]# sudo parted /dev/sdb mkpart primary 1 101m Information: You may need to update /etc/fstab. [root@server20 ~]# sudo parted /dev/sdb mkpart primary 102 201m Information: You may need to update /etc/fstab. 3. Initialize the first partition (sdb1) with Ext4 file system type using the mkfs command:\n[root@server20 ~]# sudo mkfs -t ext4 /dev/sdb1 mke2fs 1.46.5 (30-Dec-2021) /dev/sdb1 contains a LVM2_member file system Proceed anyway? (y,N) y Creating filesystem with 97280 1k blocks and 24288 inodes Filesystem UUID: 73db0582-7183-42aa-951d-2f48b7712597 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729 Allocating group tables: done Writing inode tables: done Creating journal (4096 blocks): done Writing superblocks and filesystem accounting information: done 4. Initialize the second partition (sdb2) with VFAT file system type using the mkfs command:\n[root@server20 ~]# sudo mkfs -t vfat /dev/sdb2 mkfs.fat 4.2 (2021-01-31) 5. Initialize the whole disk (sdc) with the XFS file system type using the mkfs.xfs command. Add the -f flag to force the removal of any old partitioning or labeling information from the disk.\n[root@server20 ~]# sudo mkfs.xfs /dev/sdc -f Filesystem should be larger than 300MB. Log size should be at least 64MB. Support for filesystems like this one is deprecated and they will not be supported in future releases. meta-data=/dev/sdc isize=512 agcount=4, agsize=16000 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=64000, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=1368, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 6. Determine the UUIDs for all three file systems using the lsblk command:\n[root@server2 ~]# lsblk -f /dev/sdb /dev/sdc NAME FSTYPE FSVER LABEL UUID FSAVAIL FSUSE% MOUNTPOINTS sdb ├─sdb1 ext4 1.0 0bdd22d0-db53-40bb-8cc7-36efc9184196 └─sdb2 vfat FAT16 FB3A-6572 sdc xfs 91884326-9686-4569-96fa-9adb02c1f6f4\u0026gt;) 7. Open the /etc/fstab file, go to the end of the file, and append entries for the file systems for persistence using their UUIDs:\nUUID=0bdd22d0-db53-40bb-8cc7-36efc9184196 /ext4fs1 ext4 defaults 0 0 UUID=FB3A-6572 /vfatfs1 vfat defaults 0 0 UUID=91884326-9686-4569-96fa-9adb02c1f6f4 /xfsfs1 xfs defaults 0 0 8. Create mount points /ext4fs1, /vfatfs1, and /xfsfs1 for the three file systems using the mkdir command: [root@server2 ~]# sudo mkdir /ext4fs1 /vfatfs1 /xfsfs1\n9. Mount the new file systems using the mount command. This command will fail if there are any invalid or missing information in the file.\n[root@server2 ~]# sudo mount -a mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. 10. View the mount and availability status as well as the types of all three file systems using the df command:\n[root@server2 ~]# df -hT Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 4.0M 0 4.0M 0% /dev tmpfs tmpfs 888M 0 888M 0% /dev/shm tmpfs tmpfs 356M 5.1M 351M 2% /run /dev/mapper/rhel-root xfs 17G 2.0G 15G 12% / /dev/sda1 xfs 960M 344M 617M 36% /boot tmpfs tmpfs 178M 0 178M 0% /run/user/0 /dev/sdb1 ext4 84M 14K 77M 1% /ext4fs1 /dev/sdb2 vfat 95M 0 95M 0% /vfatfs1 /dev/sdc xfs 245M 15M 231M 6% /xfsfs1 Lab: Create and Mount Ext4 and XFS File Systems in LVM Logical Volumes (server2) # Create a volume group called vgfs comprised of a 172MB physical volume created in a partition on the /dev/sdd disk. The PE size for the volume group should be set at 16MB. Create two logical volumes called ext4vol and xfsvol of sizes 80MB each and initialize them with the Ext4 and XFS file system types. Ensure that both file systems are persistently defined using their logical volume device filenames. Create mount points called /ext4fs2 and /xfsfs2, Mount the file systems. Verify their availability and usage. 1. Create a 172MB partition on the sdd disk using the parted command:\n[root@server2 ~]# sudo parted /dev/sdd mkpart pri 1 172m Information: You may need to update /etc/fstab. 2. Initialize the sdd1 partition for use in LVM using the pvcreate command:\n[root@server2 ~]# sudo pvcreate /dev/sdd1 Device /dev/sdb2 has updated name (devices file /dev/sdd2) Device /dev/sdb1 has no PVID (devices file brKVLFEG3AoBzhWoso0Sa1gLYHgNZ4vL) Physical volume \u0026#34;/dev/sdd1\u0026#34; successfully created. 3. Create the volume group vgfs with a PE size of 16MB using the physical volume sdd1:\n[root@server2 ~]# sudo vgcreate -s 16 vgfs /dev/sdd1 Volume group \u0026#34;vgfs\u0026#34; successfully created The PE size is not easy to alter after a volume group creation, so ensure it is defined as required at creation.\n4. Create two logical volumes ext4vol and xfsvol of size 80MB each in vgfs using the lvcreate command:\n[root@server2 ~]# sudo lvcreate -n ext4vol -L 80 vgfs Logical volume \u0026#34;ext4vol\u0026#34; created. [root@server2 ~]# sudo lvcreate -n xfsvol -L 80 vgfs Logical volume \u0026#34;xfsvol\u0026#34; created. 5. Format the ext4vol logical volume with the Ext4 file system type using the mkfs.ext4 command:\n[root@server2 ~]# sudo mkfs.ext4 /dev/vgfs/ext4vol mke2fs 1.46.5 (30-Dec-2021) Creating filesystem with 81920 1k blocks and 20480 inodes Filesystem UUID: 4ed1fef7-2164-485b-8035-7f627cd59419 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729 Allocating group tables: done Writing inode tables: done Creating journal (4096 blocks): done Writing superblocks and filesystem accounting information: done You can also use sudo mkfs -t ext4 /dev/vgfs/ext4vol.\n6. Format the xfsvol logical volume with the XFS file system type using the mkfs.xfs command:\n[root@server2 ~]# sudo mkfs.xfs /dev/vgfs/xfsvol Filesystem should be larger than 300MB. Log size should be at least 64MB. Support for filesystems like this one is deprecated and they will not be supported in future releases. meta-data=/dev/vgfs/xfsvol isize=512 agcount=4, agsize=5120 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=20480, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=1368, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 You may also use sudo mkfs -t xfs /dev/vgfs/xfsvol instead.\n7. Open the /etc/fstab file, go to the end of the file, and append entries for the file systems for persistence using their device files:\n/dev/vgfs/ext4vol /ext4fs2 ext4 defaults 0 0 /dev/vgfs/xfsvol /xfsfs2 xfs defaults 0 0 8. Create mount points /ext4fs2 and /xfsfs2 using the mkdir command: [root@server2 ~]# sudo mkdir /ext4fs2 /xfsfs2\n9. Mount the new file systems using the mount command. This command will fail if there is any invalid or missing information in the file.\n[root@server2 ~]# sudo mount -a mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. 10. View the mount and availability status as well as the types of the new LVM file systems using the lsblk and df commands:\n[root@server2 ~]# lsblk /dev/sdd NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sdd 8:48 0 250M 0 disk └─sdd1 8:49 0 163M 0 part ├─vgfs-ext4vol 253:2 0 80M 0 lvm /ext4fs2 └─vgfs-xfsvol 253:3 0 80M 0 lvm /xfsfs2 [root@server2 ~]# df -hT | grep fs2 /dev/mapper/vgfs-ext4vol ext4 70M 14K 64M 1% /ext4fs2 /dev/mapper/vgfs-xfsvol xfs 75M 4.8M 70M 7% /xfsfs2 Lab: Resize Ext4 and XFS File Systems in LVM Logical Volumes (server 2) # Grow the size of the vgfs volume group that was created in the last lab by adding the whole sde disk to it. Extend the ext4vol logical volume along with the file system it contains by 40MB using two separate commands. Extend the xfsvol logical volume along with the file system it contains by 40MB using a single command. Verify the new extensions. 1. Initialize the sde disk and add it to the vgfs volume group:\nsde had a gpt partition table with no partitions ran the following to reset it:\n[root@server2 ~]# dd if=/dev/zero of=/dev/sde bs=1M count=2 conv=fsync 2+0 records in 2+0 records out 2097152 bytes (2.1 MB, 2.0 MiB) copied, 0.0102036 s, 206 MB/s [root@server2 ~]# sudo partprobe /dev/sde [root@server2 ~]# sudo pvcreate /dev/sde Physical volume \u0026#34;/dev/sde\u0026#34; successfully created. [root@server2 ~]# sudo pvcreate /dev/sde Physical volume \u0026#34;/dev/sde\u0026#34; successfully created. [root@server2 ~]# sudo vgextend vgfs /dev/sde Volume group \u0026#34;vgfs\u0026#34; successfully extended 2. Confirm the new size of vgfs using the vgs and vgdisplay commands:\n[root@server2 ~]# sudo vgs VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 vgfs 2 2 0 wz--n- 400.00m 240.00m [root@server2 ~]# vgdisplay vgfs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 PVID qeP9dCevNnTy422I8p18NxDKQ2WyDodU last seen on /dev/sdf1 not found. --- Volume group --- VG Name vgfs System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 400.00 MiB PE Size 16.00 MiB Total PE 25 Alloc PE / Size 10 / 160.00 MiB Free PE / Size 15 / 240.00 MiB VG UUID amDADJ-I4dH-jQUF-RFcE-58iL-jItl-5ti6LS There are now two physical volumes in the volume group and the total size increased to 400MiB.\n3. Grow the logical volume ext4vol and the file system it holds by 40MB using the lvextend and fsadm command pair. Make sure to use an uppercase L to specify the size. The default unit is MiB. The plus sign (+) signifies an addition to the current size.\n[root@server2 ~]# sudo lvextend -L +40 /dev/vgfs/ext4vol Rounding size to boundary between physical extents: 48.00 MiB. Size of logical volume vgfs/ext4vol changed from 80.00 MiB (5 extents) to 128.00 MiB (8 extents). Logical volume vgfs/ext4vol successfully resized. [root@server2 ~]# sudo fsadm resize /dev/vgfs/ext4vol resize2fs 1.46.5 (30-Dec-2021) Filesystem at /dev/mapper/vgfs-ext4vol is mounted on /ext4fs2; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 1 The filesystem on /dev/mapper/vgfs-ext4vol is now 131072 (1k) blocks long. The resize subcommand instructs the fsadm command to grow the file system to the full length of the specified logical volume.\n4. Grow the logical volume xfsvol and the file system (-r) it holds by (+) 40MB using the lvresize command:\n[root@server2 ~]# sudo lvresize -r -L +40 /dev/vgfs/xfsvol Rounding size to boundary between physical extents: 48.00 MiB. Size of logical volume vgfs/xfsvol changed from 80.00 MiB (5 extents) to 128.00 MiB (8 extents). File system xfs found on vgfs/xfsvol mounted at /xfsfs2. Extending file system xfs to 128.00 MiB (134217728 bytes) on vgfs/xfsvol... xfs_growfs /dev/vgfs/xfsvol meta-data=/dev/mapper/vgfs-xfsvol isize=512 agcount=4, agsize=5120 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=20480, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=1368, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 20480 to 32768 xfs_growfs done Extended file system xfs on vgfs/xfsvol. Logical volume vgfs/xfsvol successfully resized. 5. Verify the new extensions to both logical volumes using the lvs command. You may also issue the lvdisplay or vgdisplay command instead.\n[root@server2 ~]# sudo lvs | grep vol Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 PVID qeP9dCevNnTy422I8p18NxDKQ2WyDodU last seen on /dev/sdf1 not found. ext4vol vgfs -wi-ao---- 128.00m xfsvol vgfs -wi-ao---- 128.00m 6. Check the new sizes and the current mount status for both file systems using the df and lsblk commands:\n[root@server2 ~]# df -hT | grep -E \u0026#39;ext4vol|xfsvol\u0026#39; /dev/mapper/vgfs-xfsvol xfs 123M 5.4M 118M 5% /xfsfs2 /dev/mapper/vgfs-ext4vol ext4 115M 14K 107M 1% /ext4fs2 [root@server2 ~]# lsblk /dev/sdd /dev/sde NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sdd 8:48 0 250M 0 disk └─sdd1 8:49 0 163M 0 part ├─vgfs-ext4vol 253:2 0 128M 0 lvm /ext4fs2 └─vgfs-xfsvol 253:3 0 128M 0 lvm /xfsfs2 sde 8:64 0 250M 0 disk ├─vgfs-ext4vol 253:2 0 128M 0 lvm /ext4fs2 └─vgfs-xfsvol 253:3 0 128M 0 lvm /xfsfs2 Lab: Create and Mount XFS File System in LVM VDO Volume # Create an LVM VDO volume called lvvdo of virtual size 20GB on the 5GB sdf disk in a volume group called vgvdo1. Initialize the volume with the XFS file system type. Define it for persistence using its device files. Create a mount point called /xfsvdo1, attach it to the directory structure. verify its availability and usage.\\ 1. Initialize the sdf disk using the pvcreate command:\n[root@server2 ~]# sudo pvcreate /dev/sdf WARNING: adding device /dev/sdf with idname t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 which is already used for missing device. Physical volume \u0026#34;/dev/sdf\u0026#34; successfully created. 2. Create vgvdo1 volume group using the vgcreate command:\n[root@server2 ~]# sudo vgcreate vgvdo1 /dev/sdf WARNING: adding device /dev/sdf with idname t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 which is already used for missing device. Volume group \u0026#34;vgvdo1\u0026#34; successfully created 3. Display basic information about the volume group:\nroot@server2 ~]# sudo vgdisplay vgvdo1 Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 PVID qeP9dCevNnTy422I8p18NxDKQ2WyDodU last seen on /dev/sdf1 not found. --- Volume group --- VG Name vgvdo1 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size \u0026lt;5.00 GiB PE Size 4.00 MiB Total PE 1279 Alloc PE / Size 0 / 0 Free PE / Size 1279 / \u0026lt;5.00 GiB VG UUID b9u8Ng-m3BF-Jz2b-sBu8-gEG1-bBGQ-sBgrt0 4. Create a VDO volume called lvvdo1 using the lvcreate command. Use the -l option to specify the number of logical extents (1279) to be allocated and the -V option for the amount of virtual space (20GB).\n[root@server2 ~]# sudo lvcreate -n lvvdo -l 1279 -V 20G --type vdo vgvdo1 WARNING: vdo signature detected on /dev/vgvdo1/vpool0 at offset 0. Wipe it? [y/n]: y Wiping vdo signature on /dev/vgvdo1/vpool0. The VDO volume can address 2 GB in 1 data slab. It can grow to address at most 16 TB of physical storage in 8192 slabs. If a larger maximum size might be needed, use bigger slabs. Logical volume \u0026#34;lvvdo\u0026#34; created. 5. Display detailed information about the volume group including the logical volume and the physical volume:\n[root@server2 ~]# sudo vgdisplay -v vgvdo1 Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 PVID qeP9dCevNnTy422I8p18NxDKQ2WyDodU last seen on /dev/sdf1 not found. --- Volume group --- VG Name vgvdo1 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size \u0026lt;5.00 GiB PE Size 4.00 MiB Total PE 1279 Alloc PE / Size 1279 / \u0026lt;5.00 GiB Free PE / Size 0 / 0 VG UUID b9u8Ng-m3BF-Jz2b-sBu8-gEG1-bBGQ-sBgrt0 --- Logical volume --- LV Path /dev/vgvdo1/vpool0 LV Name vpool0 VG Name vgvdo1 LV UUID nTPKtv-3yTW-J7Cy-HVP1-Aujs-cXZ6-gdS2fI LV Write Access read/write LV Creation host, time server2, 2024-07-01 12:57:56 -0700 LV VDO Pool data vpool0_vdata LV VDO Pool usage 60.00% LV VDO Pool saving 100.00% LV VDO Operating mode normal LV VDO Index state online LV VDO Compression st online LV VDO Used size \u0026lt;3.00 GiB LV Status NOT available LV Size \u0026lt;5.00 GiB Current LE 1279 Segments 1 Allocation inherit Read ahead sectors auto --- Logical volume --- LV Path /dev/vgvdo1/lvvdo LV Name lvvdo VG Name vgvdo1 LV UUID Z09BdK-ETJk-Gi53-m8Cg-mnTd-RYug-Z9nV0L LV Write Access read/write LV Creation host, time server2, 2024-07-01 12:58:02 -0700 LV VDO Pool name vpool0 LV Status available # open 0 LV Size 20.00 GiB Current LE 5120 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:6 --- Physical volumes --- PV Name /dev/sdf PV UUID WKc956-Xp66-L8v9-VA6S-KWM5-5e3X-kx1v0V PV Status allocatable Total PE / Free PE 1279 / 0 6. Display the new VDO volume creation using the lsblk command:\n[root@server2 ~]# sudo lsblk /dev/sdf NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sdf 8:80 0 5G 0 disk └─vgvdo1-vpool0_vdata 253:4 0 5G 0 lvm └─vgvdo1-vpool0-vpool 253:5 0 20G 0 lvm └─vgvdo1-lvvdo 253:6 0 20G 0 lvm The output shows the virtual volume size (20GB) and the underlying disk size (5GB).\n7. Initialize the VDO volume with the XFS file system type using the mkfs.xfs command. The VDO volume device file is /dev/mapper/vgvdo1-lvvdo as indicated in the above output. Add the -f flag to force the removal of any old partitioning or labeling information from the disk.\n[root@server2 mapper]# sudo mkfs.xfs /dev/mapper/vgvdo1-lvvdo meta-data=/dev/mapper/vgvdo1-lvvdo isize=512 agcount=4, agsize=1310720 blks = sectsz=4096 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=5242880, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=16384, version=2 = sectsz=4096 sunit=1 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 Discarding blocks...Done. (lab said vgvdo1-lvvdo1 but it didn\u0026rsquo;t exist for me.)\n8. Open the /etc/fstab file, go to the end of the file, and append the following entry for the file system for persistent mounts using its device file:\n/dev/mapper/vgvdo1-lvvdo /xfsvdo1 xfs defaults 0 0 9. Create the mount point /xfsvdo1 using the mkdir command:\n[root@server2 mapper]# sudo mkdir /xfsvdo1 10. Mount the new file system using the mount command. This command will fail if there are any invalid or missing information in the file.\n[root@server2 mapper]# sudo mount -a mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. The mount command with the -a flag is a validation test for the fstab file. It should always be executed after updating this file and before rebooting the server to avoid landing the system in an unbootable state.\n11. View the mount and availability status as well as the type of the VDO file system using the lsblk and df commands:\n[root@server2 mapper]# lsblk /dev/sdf NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sdf 8:80 0 5G 0 disk └─vgvdo1-vpool0_vdata 253:4 0 5G 0 lvm └─vgvdo1-vpool0-vpool 253:5 0 20G 0 lvm └─vgvdo1-lvvdo 253:6 0 20G 0 lvm /xfsvdo1 [root@server2 mapper]# df -hT /xfsvdo1 Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/vgvdo1-lvvdo xfs 20G 175M 20G 1% /xfsvdo1 Monitoring File System Usage # df (disk free) command # reports usage details for mounted file systems. reports the numbers in KBs unless the -m or -h option is specified to view the sizes in MBs or human-readable format. Let\u0026rsquo;s run this command with the -h option on server2:\n[root@server2 ~]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 4.0M 0 4.0M 0% /dev tmpfs 888M 0 888M 0% /dev/shm tmpfs 356M 5.1M 351M 2% /run /dev/mapper/rhel-root 17G 2.0G 15G 12% / tmpfs 178M 0 178M 0% /run/user/0 /dev/sda1 960M 344M 617M 36% /boot Column 1:\nfile system device file or type Columns 2, 3, 4, 5, 6\ntotal, used, and available spaces in and the usage percentage and mount point Useful flags\n-T Add the file system type to the output (example: df -hT) -x\nExclude the specified file system type from the output (example: df -hx tmpfs) -t Limit the output to a specific file system type (example: df -t xfs) -i\nshow inode information (example: df -hi) Calculating Disk Usage # du command # reports the amount of space a file or directory occupies. -m or -h option to view the output in MBs or human-readable format. In addition, you can view a usage summary with the -s switch and a grand total with -c. Run this command on the /usr/bin directory to view the usage summary:\n[root@server2 ~]# du -sh /usr/bin 151M\t/usr/bin Add a \u0026ldquo;total\u0026rdquo; row to the output and with numbers displayed in KBs:\n[root@server2 ~]# du -sc /usr/bin 154444\t/usr/bin 154444\ttotal [root@server2 ~]# du -sch /usr/bin 151M\t/usr/bin 151M\ttotal Try this command with different options on the /usr/sbin/lvm file and observe the results.\nSwap and its Management # Move pages of idle data between physical memory and swap.\nSwap areas act as extensions to the physical memory.\nMay be activated or deactivated independent of swap spaces located in other partitions and volumes.\nThe system splits the physical memory into small logical chunks called pages and maps their physical locations to virtual locations on the swap to facilitate access by system processors.\nThis physical-to-virtual mapping of pages is stored in a data structure called page table, and it is maintained by the kernel.\nWhen a program or process is spawned, it requires space in the physical memory to run and be processed.\nAlthough many programs can run concurrently, the physical memory cannot hold all of them at once.\nThe kernel monitors the memory usage.\nAs long as the free memory remains above a high threshold, nothing happens.\nWhen the free memory falls below that threshold, the system starts moving selected idle pages of data from physical memory to the swap space to make room to accommodate other programs.\nThis piece in the process is referred to as page out.\nSince the system CPU performs the process execution in around-robin fashion, when the system needs this paged-out data for execution, the CPU looks for that data in the physical memory and a pagefault occurs, resulting in moving the pages back to the physical memory from the swap.\nThis return of data to the physical memory is referred to as page in.\nThe entire process of paging data out and in is known as demand paging.\nRHEL systems with less physical memory but high memory requirements can become over busy with paging out and in.\nWhen this happens, they do not have enough cycles to carry out other useful tasks, resulting in degraded system performance.\nThe excessive amount of paging that affects the system performance is called thrashing.\nWhen thrashing begins, or when the free physical memory falls below a low threshold, the system deactivates idle processes and prevents new processes from being launched.\nThe idle processes are only reactivated, and new processes are only allowed to be started when the system discovers that the available physical memory has climbed above the threshold level and thrashing has ceased.\nDetermining Current Swap Usage # Size of a swap area should not be less than the amount of physical memory. Depending on workload requirements, it may be twice the size or larger. It is also not uncommon to see systems with less swap than the actual amount of physical memory. This is especially witnessed on systems with a huge physical memory size. free command # View memory and swap space utilization. view how much physical memory is installed (total), used (used), available (free), used by shared library routines (shared), holding data before it is written to disk (buffers), and used to store frequently accessed data (cached) on the system. The -h list the values in human-readable format, -k for KB, -m for MB, -g for GB, -t display a line with the \u0026ldquo;total\u0026rdquo; at the bottom of the output. [root@server2 mapper]# free -ht total used free shared buff/cache available Mem: 1.7Gi 783Mi 714Mi 5.0Mi 440Mi 991Mi Swap: 2.0Gi 0B 2.0Gi Total: 3.7Gi 783Mi 2.7Gi Try free -hts 3 and free -htc 2 to refresh the output every three seconds (-s) and to display the output twice (-c).\nReads memory and swap information from the /proc/meminfo file to produce the report. The values are shown in KBs by default, and they are slightly off from what is shown above with free. Here are the relevant fields from this file: [root@server2 mapper]# cat /proc/meminfo | grep -E \u0026#39;Mem|Swap\u0026#39; MemTotal: 1818080 kB MemFree: 731724 kB MemAvailable: 1015336 kB SwapCached: 0 kB SwapTotal: 2097148 kB SwapFree: 2097148 kB Prioritizing Swap Spaces # You may find multiple swap areas configured and activated to meet the workload demand. The default behavior of RHEL is to use the first activated swap area and move on to the next when the first one is exhausted. The system allows us to prioritize one area over the other by adding the option \u0026ldquo;pri\u0026rdquo; to the swap entries in the fstab file. This flag supports a value between -2 and 32767 with -2 being the default. A higher value of \u0026ldquo;pri\u0026rdquo; sets a higher priority for the corresponding swap region. For swap areas with an identical priority, the system alternates between them. Swap Administration Commands # In order to create and manage swap spaces on the system, the mkswap, swapon, and swapoff commands are available. Use mkswap to initialize a partition for use as a swap space. Once the swap area is ready, you can activate or deactivate it from the command line with the help of the other two commands, Can also set it up for automatic activation by placing an entry in the fstab file. The fstab file accepts the swap area\u0026rsquo;s device file, UUID, or label. Lab: Create and Activate Swap in Partition and Logical Volume (server 2) # Create one swap area in a new 40MB partition called sdb3 using the mkswap command. Create another swap area in a 140MB logical volume called swapvol in vgfs. Add their entries to the /etc/fstab file for persistence. Use the UUID and priority 1 for the partition swap and the device file and priority 2 for the logical volume swap. Activate them and use appropriate tools to validate the activation. EXAM TIP: Use the lsblk command to determine available disk space.\n1. Use parted print on the sdb disk and the vgs command on the vgfs volume group to determine available space for a new 40MB partition and a 144MB logical volume:\n[root@server2 mapper]# sudo parted /dev/sdb print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 101MB 99.6MB primary ext4 2 102MB 201MB 99.6MB primary fat16 [root@server2 mapper]# sudo vgs vgfs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 PVID qeP9dCevNnTy422I8p18NxDKQ2WyDodU last seen on /dev/sdf1 not found. VG #PV #LV #SN Attr VSize VFree vgfs 2 2 0 wz--n- 400.00m 144.00m The outputs show 49MB (250MB minus 201MB) free space on the sdb disk and 144MB free space in the volume group.\n2. Create a partition called sdb3 of size 40MB using the parted command:\n[root@server2 mapper]# sudo parted /dev/sdb mkpart primary 202 242 Information: You may need to update /etc/fstab. 3. Create logical volume swapvol of size 144MB in vgs using the lvcreate command:\n[root@server2 mapper]# sudo lvcreate -L 144 -n swapvol vgfs Logical volume \u0026#34;swapvol\u0026#34; created. 4. Construct swap structures in sdb3 and swapvol using the mkswap command:\n[root@server2 mapper]# sudo mkswap /dev/sdb3 Setting up swapspace version 1, size = 38 MiB (39841792 bytes) no label, UUID=a796e0df-b1c3-4c30-bdde-dd522bba4fff [root@server2 mapper]# sudo mkswap /dev/vgfs/swapvol Setting up swapspace version 1, size = 144 MiB (150990848 bytes) no label, UUID=88196e73-feaf-4137-8743-f9340296aeec 5. Edit the fstab file and add entries for both swap areas for auto-activation on reboots. Obtain the UUID for partition swap with lsblk -f /dev/sdb3 and use the device file for logical volume. Specify their priorities.\nUUID=a796e0df-b1c3-4c30-bdde-dd522bba4fff swap swap pri=1 0 0 /dev/vgfs/swapvol swap swap pri=2 0 0 EXAM TIP: You will not be given any credit for this work if you forget to add entries to the fstab file.\n6. Determine the current amount of swap space on the system using the swapon command:\n[root@server2]# sudo swapon NAME TYPE SIZE USED PRIO /dev/dm-1 partition 2G 0B -2 There is one 2GB swap area on the system and it is configured at the default priority of -2.\n7. Activate the new swap regions using the swapon command:\n[root@server2]# sudo swapon -a 8. Confirm the activation using the swapon command or by viewing the /proc/swaps file:\n[root@server2 mapper]# sudo swapon NAME TYPE SIZE USED PRIO /dev/dm-1 partition 2G 0B -2 /dev/sdb3 partition 38M 0B 1 /dev/dm-7 partition 144M 0B 2 [root@server2 mapper]# cat /proc/swaps Filename\tType\tSize\tUsed\tPriority /dev/dm-1 partition\t2097148\t0\t-2 /dev/sdb3 partition\t38908\t0\t1 /dev/dm-7 partition\t147452\t0\t2 #dm is device mapper 9. Issue the free command to view the reflection of swap numbers on the Swap and Total lines:\n[root@server2 mapper]# free -ht total used free shared buff/cache available Mem: 1.7Gi 793Mi 706Mi 5.0Mi 438Mi 981Mi Swap: 2.2Gi 0B 2.2Gi Total: 3.9Gi 793Mi 2.9Gi Local Filesystems and Swap DIY Labs # Lab: Create VFAT, Ext4, and XFS File Systems in Partitions and Mount Persistently # Create three 70MB primary partitions on one of the available 250MB disks (lsblk) by invoking the parted utility directly at the command prompt. [root@server2 mapper]# parted /dev/sdc mklabel msdos Information: You may need to update /etc/fstab. [root@server2 mapper]# parted /dev/sdc mkpart primary 1 70m Information: You may need to update /etc/fstab. root@server2 mapper]# parted /dev/sdb print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 70.3MB 69.2MB primary parted) mkpart primary 71MB 140MB Warning: The resulting partition is not properly aligned for best performance: 138671s % 2048s != 0s Ignore/Cancel? Ignore/Cancel? ignore (parted) mkpart primary 140MB 210MB Warning: The resulting partition is not properly aligned for best performance: 273438s % 2048s != 0s Ignore/Cancel? ignore (parted) print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 70.3MB 69.2MB primary 2 71.0MB 140MB 69.0MB primary 3 140MB 210MB 70.0MB primary Apply label \u0026ldquo;msdos\u0026rdquo; if the disk is new. Initialize partition 1 with VFAT, partition 2 with Ext4, and partition 3 with XFS file system types. [root@server2 mapper]# sudo mkfs -t vfat /dev/sdc1 mkfs.fat 4.2 (2021-01-31) [root@server2 mapper]# sudo mkfs -t ext4 /dev/sdc2 mke2fs 1.46.5 (30-Dec-2021) Creating filesystem with 67380 1k blocks and 16848 inodes Filesystem UUID: 43b590ff-3330-4b88-aef9-c3a97d8cf51e Superblock backups stored on blocks: 8193, 24577, 40961, 57345 Allocating group tables: done Writing inode tables: done Creating journal (4096 blocks): done Writing superblocks and filesystem accounting information: done [root@server2 mapper]# sudo mkfs -t xfs /dev/sdc3 Filesystem should be larger than 300MB. Log size should be at least 64MB. Support for filesystems like this one is deprecated and they will not be supported in future releases. meta-data=/dev/sdb3 isize=512 agcount=4, agsize=4273 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=17089, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=1368, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 Create mount points /vfatfs5, /ext4fs5, and /xfsfs5, and mount all three manually. [root@server2 mapper]# mkdir /vfatfs5 /ext4fs5 /xfsfs5 [root@server2 mapper]# mount /dev/sdc1 /vfatfs5 mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. [root@server2 mapper]# mount /dev/sdc2 /ext4fs5 mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. [root@server2 mapper]# mount /dev/sdc3 /xfsfs5 mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. [root@server2 mapper]# mount /dev/sdb1 on /vfatfs5 type vfat (rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=ascii,shortname=mixed,errors=remount-ro) /dev/sdb2 on /ext4fs5 type ext4 (rw,relatime,seclabel) /dev/sdb3 on /xfsfs5 type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota) Determine the UUIDs for the three file systems, and add them to the fstab file. [root@server2 mapper]# blkid /dev/sdc1 /dev/sdc2 /dev/sdc3 \u0026gt;\u0026gt; /etc/fstab [root@server2 mapper]# vim /etc/fstab Unmount all three file systems manually, and execute mount -a to mount them all. umount /dev/sdb1 /dev/sdb2 /dev/sdb3 Run df -h for verification. Lab: Create XFS File System in LVM VDO Volume and Mount Persistently # Ensure that VDO software is installed. sudo dnf install kmod-kvdo\nCreate a volume vdo5 with a logical size 20GB on a 5GB disk (lsblk) using the lvcreate command.\n[root@server2 ~]# sudo lvcreate -n vdo5 -l 1279 -V 20G --type vdo vgvdo1 WARNING: vdo signature detected on /dev/vgvdo1/vpool0 at offset 0. Wipe it? [y/n]: y Wiping vdo signature on /dev/vgvdo1/vpool0. The VDO volume can address 2 GB in 1 data slab. It can grow to address at most 16 TB of physical storage in 8192 slabs. If a larger maximum size might be needed, use bigger slabs. Logical volume \u0026#34;vdo5\u0026#34; created. Initialize the volume with XFS file system type. [root@server2 mapper]# sudo mkfs.xfs /dev/mapper/vgvdo1-vdo5 meta-data=/dev/mapper/vgvdo1-vdo5 isize=512 agcount=4, agsize=1310720 blks = sectsz=4096 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=5242880, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=16384, version=2 = sectsz=4096 sunit=1 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 Discarding blocks...Done. Create mount point /vdofs5, and mount it manually. [root@server2 mapper]# mkdir /vdofs5 [root@server2 mapper]#mount /dev/mapper/vgvdo1-vdo5 /vdofs5)/etc/fstab [root@server2 mapper]# umount /dev/mapper/vgvdo1-vdo5 Unmount the file system manually and execute mount -a to mount it back. [root@server2 mapper]# blkid /dev/mapper/vgvdo1-vdo5 \u0026gt;\u0026gt; /etc/fstab [root@server2 mapper]# vim /etc/fstab Run df -h to confirm. Lab: Create Ext4 and XFS File Systems in LVM Volumes and Mount Persistently # Initialize an available 250MB disk for use in LVM (lsblk). [root@server2 mapper]# parted /dev/sdc mklabel msdos Warning: The existing disk label on /dev/sdc will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? y Information: You may need to update /etc/fstab. [root@server2 mapper]# parted /dev/sdc mkpart primary 1 100% Information: You may need to update /etc/fstab. Create volume group vg with PE size 8MB and add the physical volume. [root@server2 ~]# sudo pvcreate /dev/sdc1 Devices file /dev/sdc is excluded: device is partitioned. WARNING: adding device /dev/sdc1 with idname t10.ATA_VBOX_HARDDISK_VB6894bac4-590d5546 which is already used for /dev/sdc. Physical volume \u0026#34;/dev/sdc1\u0026#34; successfully created. [root@server2 ~]# vgcreate -s 8 vg /dev/sdc1 Devices file /dev/sdc is excluded: device is partitioned. WARNING: adding device /dev/sdc1 with idname t10.ATA_VBOX_HARDDISK_VB6894bac4-590d5546 which is already used for /dev/sdc. Volume group \u0026#34;vg\u0026#34; successfully created Create two logical volumes lv200 and lv300 of sizes 120MB and 100MB. [root@server2 ~]# lvcreate -n lv200 -L 120 vg Devices file /dev/sdc is excluded: device is partitioned. Logical volume \u0026#34;lv200\u0026#34; created. [root@server2 ~]# lvcreate -n lv300 -L 100 vg Rounding up size to full physical extent 104.00 MiB Logical volume \u0026#34;lv300\u0026#34; created. Use the vgs, pvs, lvs, and vgdisplay commands for verification. Initialize the volumes with Ext4 and XFS file system types. [root@server2 ~]# mkfs.ext4 /dev/vg/lv200 mke2fs 1.46.5 (30-Dec-2021) Creating filesystem with 122880 1k blocks and 30720 inodes Filesystem UUID: 52eac2ee-b5bd-4025-9e40-356b38d21996 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729 Allocating group tables: done Writing inode tables: done Creating journal (4096 blocks): done Writing superblocks and filesystem accounting information: done [root@server2 ~]# mkfs.xfs /dev/vg/lv300 Filesystem should be larger than 300MB. Log size should be at least 64MB. Support for filesystems like this one is deprecated and they will not be supported in future releases. meta-data=/dev/vg/lv300 isize=512 agcount=4, agsize=6656 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=26624, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=1368, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 Create mount points /lvmfs5 and /lvmfs6, and mount them manually. [root@server2 ~]# mkdir /lvmfs5 /lvmfs6 [root@server2 ~]# mount /dev/vg/lv200 /lvmfs5 mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. [root@server2 ~]# mount /dev/vg/lv300 /lvmfs6 mount: (hint) your fstab has been modified, but systemd still uses the old version; use \u0026#39;systemctl daemon-reload\u0026#39; to reload. Add the file system information to the fstab file using their device files. [root@server2 ~]# blkid /dev/vg/lv200 \u0026gt;\u0026gt; /etc/fstab [root@server2 ~]# blkid /dev/vg/lv300 \u0026gt;\u0026gt; /etc/fstab [root@server2 ~]# vim /etc/fstab Unmount the file systems manually, and execute mount -a to mount them back. Run df -h to confirm. [root@server2 ~]# umount /dev/vg/lv200 /dev/vg/lv300 [root@server2 ~]# mount -a Lab 14-4: Extend Ext4 and XFS File Systems in LVM Volumes # initialize an available 250MB disk for use in LVM (lsblk). [root@server2 ~]# pvcreate /dev/sdb Devices file /dev/sdc is excluded: device is partitioned. WARNING: dos signature detected on /dev/sdb at offset 510. Wipe it? [y/n]: y Wiping dos signature on /dev/sdb. WARNING: adding device /dev/sdb with idname t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f which is already used for missing device. Physical volume \u0026#34;/dev/sdb\u0026#34; successfully created. Add the new physical volume to volume group vg200. [root@server2 ~]# vgextend vg /dev/sdb Devices file /dev/sdc is excluded: device is partitioned. WARNING: adding device /dev/sdb with idname t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f which is already used for missing device. Volume group \u0026#34;vg\u0026#34; successfully extended Expand logical volumes lv200 and lv300 along with the underlying file systems to 200MB and 250MB. [root@server2 ~]# lvextend -L 200m /dev/vg/lv200 Size of logical volume vg/lv200 changed from 120.00 MiB (15 extents) to 200.00 MiB (25 extents). Logical volume vg/lv200 successfully resized. [root@server2 ~]# lvextend -L 250m /dev/vg/lv200 Rounding size to boundary between physical extents: 256.00 MiB. Size of logical volume vg/lv200 changed from 200.00 MiB (25 extents) to 256.00 MiB (32 extents). Logical volume vg/lv200 successfully resized. Use the vgs, pvs, lvs, vgdisplay, and df commands for verification. Lab 14-5: Create Swap in Partition and LVM Volume and Activate Persistently # Create two 100MB partitions on an available 250MB disk (lsblk) by invoking the parted utility directly at the command prompt. Apply label \u0026ldquo;msdos\u0026rdquo; if the disk is new. [root@localhost ~]# parted /dev/sdd mklabel msdos Information: You may need to update /etc/fstab. [root@localhost ~]# parted /dev/sdd mkpart primary 1 100MB Information: You may need to update /etc/fstab. [root@localhost ~]# parted /dev/sdd mkpart primary 101 201 Information: You may need to update /etc/fstab. Initialize one of the partitions with swap structures. [root@localhost ~]# sudo mkswap /dev/sdd1 Setting up swapspace version 1, size = 94 MiB (98562048 bytes) no label, UUID=40eea6c2-b80c-4b25-ad76-611071db52d5 Apply label swappart to the swap partition, and add it to the fstab file. [root@localhost ~]# swaplabel -L swappart /dev/sdd1 [root@localhost ~]# blkid /dev/sdd1 \u0026gt;\u0026gt; /etc/fstab [root@localhost ~]# vim /etc/fstab UUID=\u0026#34;40eea6c2-b80c-4b25-ad76-611071db52d5\u0026#34; swap swap pri=1 0 0 Execute swapon -a to activate it.\nRun swapon -s to confirm activation.\nInitialize the other partition for use in LVM.\n[root@localhost ~]# pvcreate /dev/sdd2 Physical volume \u0026#34;/dev/sdd2\u0026#34; successfully created. Expand volume group vg (Lab 14-3) by adding this physical volume to it. [root@localhost ~]# vgextend vg /dev/sdd2 Volume group \u0026#34;vg200\u0026#34; successfully extended Create logical volume swapvol of size 180MB. [root@localhost ~]# lvcreate -L 180 -n swapvol vg Logical volume \u0026#34;swapvol\u0026#34; created. Use the vgs, pvs, lvs, and vgdisplay commands for verification. Initialize the logical volume for swap. [root@localhost vg200]# mkswap /dev/vg/swapvol Setting up swapspace version 1, size = 180 MiB (188739584 bytes) no label, UUID=a4b939d0-4b53-4e73-bee5-4c402aff6f9b Add an entry to the fstab file for the new swap area using its device file. [root@localhost vg200]# vim /etc/fstab /dev/vg200/swapvol swap swap pri=2 0 0 Execute swapon -a to activate it. Run swapon -s to confirm activation. ","externalUrl":null,"permalink":"/tech/linux/local-file-systems-and-swap/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eFile Systems and File System Types\n    \u003cdiv id=\"file-systems-and-file-system-types\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#file-systems-and-file-system-types\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eFile systems\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCan be optimized, resized, mounted, and unmounted independently.\u003c/li\u003e\n\u003cli\u003eMust be connected to the directory hierarchy in order to be accessed by users and applications.\u003c/li\u003e\n\u003cli\u003eMounting may be accomplished automatically at system boot or manually as required.\u003c/li\u003e\n\u003cli\u003eCan be mounted or unmounted using their unique identifiers, labels, or device files.\u003c/li\u003e\n\u003cli\u003eEach file system is created in a discrete partition, VDO volume, or logical volume.\u003c/li\u003e\n\u003cli\u003eA typical production RHEL system usually has numerous file systems.\u003c/li\u003e\n\u003cli\u003eDuring OS installation, only two file systems\u0026mdash; / and /boot \u0026mdash;are created in the default disk layout, but you can design a custom disk layout and construct separate containers to store dissimilar information.\u003c/li\u003e\n\u003cli\u003eTypical additional file systems that may be created during an installation are /home, /opt, /tmp, /usr, and /var.\u003c/li\u003e\n\u003cli\u003e/ and /boot\u0026mdash;are required for installation and booting.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStoring disparate data in distinct file systems versus storing all data in a single file system offers the following advantages:\u003c/p\u003e","title":"Local File Systems and Swap","type":"tech"},{"content":" Using Ansible Modules to Manage Users and Groups # management of the user and group accounts and their direct properties. management of sudo privilege escalation Setting up SSH connections and setting user passwords Modules # user\nmanage users and their base properties group\nManage groups and their properties pamd\nManage advanced authentication configuration through linux pluggable authentication modules (PAM) known_hosts\nmanage ssh known hosts authorized_key\ncopy authorized key to a managed host lineinfile\nmodify config file Managing Users and Groups # --- - name: creating a user and group hosts: ansible2 tasks: - name: setup the group account group: name: students state: present - name: setup the user account user: name: anna create_home: yes groups: wheel,students append: yes generate_ssh_key: yes ssh_key_bits: 2048 ssh_key_file: .ssh/id_rsa group argument is\nused to specify the primary group of the user. groups argument is\nused to make the user a member of additional groups.\nWhile using the groups argument for existing users, make sure to include the append argument as well.\nWithout append, all current secondary group assignments are overwritten.\nAlso notice that the user module has some options that cannot normally be managed with the Linux useradd command. The module can also be used to generate an SSH key and specify its properties.\nManaging sudo # No Ansible module specifically targets managing a sudo configuration\ntwo options:\nYou can use the template module to create a sudo configuration file in the directory /etc/sudoers.d. Using such a file is recommended because the file is managed independently, and as such, there is no risk it will be overwritten by an RPM update. The alternative is to use the lineinfile module to manage the /etc/sudoers main configuration file directly. Users are created and added to a sudo file that is generated from a template:\n[ansible@control rhce8-book]$ cat vars/sudo sudo_groups: - name: developers groupid: 5000 sudo: false - name: admins groupid: 5001 sudo: true - name: dbas groupid: 5002 sudo: false - name: sales groupid: 5003 sudo: true - name: account groupid: 5004 sudo: false [ansible@control rhce8-book]$ cat vars/users users: - username: linda groups: sales - username: lori groups: sales - username: lisa groups: account - username: lucy groups: account vars/users file defines users and the groups they should be a member of. vars/sudo file defines new groups and, for each of these groups, sets a sudo parameter, which will be used in the template file: {% for item in sudo_groups %} {% if item.sudo %} %{{ item.name}} ALL=(ALL:ALL) NOPASSWD:ALL {% endif %} {% endfor %} a for loop is used to walk through all items that have been defined in the sudo_groups variable in the vars/sudo file. for each of these groups an if statement is used to check the value of the Boolean variable sudo. If this variable is set to the Boolean value true, the group is added as a sudo group to the /etc/sudoers.d/sudogroups file. Listing 13-4 Managing sudo\n--- - name: configure sudo hosts: ansible2 vars_files: - vars/sudo - vars/users tasks: - name: add groups group: name: \u0026#34;{{ item.name }}\u0026#34; loop: \u0026#34;{{ sudo_groups }}\u0026#34; - name: add users user: name: \u0026#34;{{ item.username }}\u0026#34; groups: \u0026#34;{{ item.groups }}\u0026#34; loop: \u0026#34;{{ users }}\u0026#34; - name: allow group members in sudo template: src: listing133.j2 dest: /etc/sudoers.d/sudogroups validate: ‘visudo -cf %s’ mode: 0440 Managing SSH Connections # How to provide for SSH keys for new users in such a way that users are provided with SSH keys without having to set them up themselves. To do this, you use the authorized_key module together with the generate_ssh_key argument to the user module. Understanding SSH Connection Management Requirements # How SSH keys are used in the communication process between a user and an SSH server:\nThe user initiates a session with an SSH server. The server sends back an identification token that is encrypted with the server private key to the user. The user uses the server’s public key fingerprint, which is stored in the ~/.ssh/known_hosts file to verify the identification token. If no public key fingerprint was stored yet in the ~/.ssh/known_hosts file, the user is prompted to store the remote server identity in the ~/.ssh/known_hosts file. At this point there is no good way to verify whether the user is indeed communicating with the intended server. After establishing the identity of the remote server, the user can either send over a password or generate an authentication token that is based on the user’s private key. If an authentication token that was based on the user’s private key is sent over, this token is received by the server, which tries to match it against the user’s public key that is stored in the ~/.ssh/authorized_keys file. After the incoming authentication token to the stored user public key in the authorized_keys file is matched, the user is authenticated. If this authentication fails and password authentication is allowed, password authentication is attempted next. In the authentication procedure, two key pairs play an important role. First, there is the server’s public/private key pair, which is used to establish a secure connection. To manage the host public key, you can use the Ansible known_hosts module. Next, there is the user’s public/private key pair, which the user uses to authenticate. To manage the public key in this key pair, you can use the Ansible authorized_key module.\nLookup Plug-in # Enables Ansible to access data from outside sources. Read the file system or contact external datastores and services. Ran on the Ansible control host. Results are usually stored in variables or templates. Set the value of a variable to the contents of a file:\n--- - name: simple demo with the lookup plugin hosts: localhost vars: file_contents: \u0026#34;{{lookup(‘file’, ‘/etc/hosts’)}}\u0026#34; tasks: - debug: var: file_contents Setting Up SSH User Keys # To use SSH to connect to a user account on a managed host you can copy over the local user public key to the remote user ~/.ssh/authorized_keys file. If the target authorized_keys file just has to contain one single key, you could use the copy module to copy it over. To manage multiple keys in the remote user authorized_keys file, you’re better off using the authorized_key module. authorized_key module\nCopy the authorized_key for a user /home/ansible/.ssh/id_rsa.pub is used as the source. lookup plug-in is used to refer to the file contents that should be used: --- - name: authorized_key simple demo hosts: ansible2 tasks: - name: copy authorized key for ansible user authorized_key: user: ansible state: present key: \u0026#34;{{ lookup(‘file’, ‘/home/ansible/.ssh/id_rsa.pub’) }}\u0026#34; Do the same for multiple users: vars/users\n--- users: - username: linda groups: sales - username: lori groups: sales - username: lisa groups: account - username: lucy groups: account vars/groups\n--- usergroups: - groupname: sales - groupname: account --- - name: configure users with SSH keys hosts: ansible2 vars_files: - vars/users - vars/groups tasks: - name: add groups group: name: \u0026#34;{{ item.groupname }}\u0026#34; loop: \u0026#34;{{ usergroups }}\u0026#34; - name: add users user: name: \u0026#34;{{ item.username }}\u0026#34; groups: \u0026#34;{{ item.groups }}\u0026#34; loop: \u0026#34;{{ users }}\u0026#34; - name: add SSH public keys authorized_key: user: \u0026#34;{{ item.username }}\u0026#34; key: \u0026#34;{{ lookup(‘file’, ‘files/’+ item.username + ‘/id_rsa.pub’) }}\u0026#34; loop: \u0026#34;{{ users }}\u0026#34; authorized_key module is set up to work on item.username, using a loop on the users variable.\nThe id_rsa.pub files that have to be copied over are expected to exist in the files directory, which exists in the current project directory.\nCopying over the user public keys to the project directory is a solution because the authorized_keys module cannot read files from a hidden directory.\nIt would be much nicer to use key: “{{ lookup(‘file’, ‘/home/’+ item.username + ‘.ssh/id_rsa.pub’) }}”, but that doesn’t work.\nIn the first task you create a local user, including an SSH key.\nBecause an SSH key should include the name of the user and host that it applies to, you need to use the generate_ssh_key argument, as well as the ssh_key_comment argument to write the correct comment into the public key.\nWithout this content, the key will have generic content and not be considered a valid key.\n- name: create the local user, including SSH key user: name: \u0026#34;{{ username }}\u0026#34; generate_ssh_key: true ssh_key_comment: \u0026#34;{{ username }}@{{ ansible_fqdn }}\u0026#34; After creating the SSH keys this way, you aren’t able to fetch the key directly from the user home directory. To fix that problem, you create a directory with the name of the user in the project directory and copy the user public key from there by using the shell module: - name: create a directory to store the file file: name: \u0026#34;{{ username }}\u0026#34; state: directory - name: copy the local user ssh key to temporary {{ username }} key shell: ‘cat /home/{{ username }}/.ssh/id_rsa.pub \u0026gt; {{ username }}/id_rsa.pub’ - name: verify that file exists command: ls -l {{ username }}/ Next, in the second play you create the remote user and use the authorized_key module to copy the key from the temporary directory to the new user home directory. Exercise 13-2 Managing Users with SSH Keys Steps\nCreate a user on localhost. Use the appropriate arguments to create the SSH public/private key pair according to the required format. Make sure the public key is copied to a directory where it can be accessed. Uses the user module to create the user, as well as the authorized_key module to fetch the key from localhost and copy it to the .ssh/authorized_keys file in the remote user home directory. Use the command ansible-playbook exercise132.yaml -e username=radha to create the user radha with the appropriate SSH keys. To verify it has worked, use sudo su - radha on the control host, and type the command ssh ansible1. You should able to log in without entering a password. --- - name: prepare localhost hosts: localhost tasks: - name: create the local user, including SSH key user: name: \u0026#34;{{ username }}\u0026#34; generate_ssh_key: true ssh_key_comment: \u0026#34;{{ username }}@{{ ansible_fqdn }}\u0026#34; - name: create a directory to store the file file: name: \u0026#34;{{ username }}\u0026#34; state: directory - name: copy the local user ssh key to temporary {{ username }} key shell: ‘cat /home/{{ username }}/.ssh/id_rsa.pub \u0026gt; {{ username }}/id_rsa.pub’ - name: verify that file exists command: ls -l {{ username }}/ - name: setup remote host hosts: ansible1 tasks: - name: create remote user, no need for SSH key user: name: \u0026#34;{{ username }}\u0026#34; - name: use authorized_key to set the password authorized_key: user: \u0026#34;{{ username }}\u0026#34; key: \u0026#34;{{ lookup(‘file’, ‘./’+ username +’/id_rsa.pub’) }}\u0026#34; Managing Encrypted Passwords # When managing users in Ansible, you probably want to set user passwords as well. The challenge is that you cannot just enter a password as the value to the password: argument in the user module because the user module expects you to use an encrypted string.\nUnderstanding Encrypted Passwords # When a user creates a password, it is encrypted. The hash of the encrypted password is stored in the /etc/shadow file, a file that is strictly secured and accessible only with root privileges. The string looks like $6$237687687/$9809erhb8oyw48oih290u09. In this string are three elements, which are separated by $ signs:\n• The hashing algorithm that was used\n• The random salt that was used to encrypt the password\n• The encrypted hash of the user password\nWhen a user sets a password, a random salt is used to prevent two users who have identical passwords from having identical entries in /etc/shadow. The salt and the unencrypted password are combined and encrypted, which generates the encrypted hash that is stored in /etc/shadow. Based on this string, the password that the user enters can be verified against the password field in /etc/shadow, and if it matches, the user is authenticated.\nGenerating Encrypted Passwords # When you\u0026rsquo;re creating users with the Ansible user module, there is a password option. This option is not capable of generating an encrypted password. It expects an encrypted password string as its input. That means an external utility must be used to generate an encrypted string. This encrypted string must be stored in a variable to create the password. Because the variable is basically the user password, the variable should be stored securely in, for example, an Ansible Vault secured file.\nTo generate the encrypted variable, you can choose to create the variable before creating the user account. Alternatively, you can run the command to create the variable in the playbook, use register to write the result to a variable, and use that to create the encrypted user. If you want to generate the variable beforehand, you can use the following ad hoc command:\nansible localhost -m debug -a \u0026quot;msg={{ ‘password’ | password_hash(‘sha512’,’myrandomsalt’) }}\u0026quot; This command generates the encrypted string as shown in Listing 13-11, and this string can next be used in a playbook. An example of such a playbook is shown in Listing 13-12.\nListing 13-11 Generating the Encrypted Password String\n::: pre_1 [ansible@control ~]$ ansible localhost -m debug -a \u0026ldquo;msg={{ ‘password’ | password_hash(‘sha512’,’myrandomsalt’) }}\u0026rdquo; localhost | SUCCESS =\u0026gt; { \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;$6$myrandomsalt$McEB.xAVUWe0./6XqZ8n/7k9VV/Gxndy9nIMLyQAiPnhyBoToMWbxX2vA4f.Uv9PKnPRaYUUc76AjLWVAX6U10\u0026rdquo; } :::\nListing 13-12 Sample Playbook That Creates an Encrypted User Password\n--- - name: create user with encrypted pass hosts: ansible2.example.com vars: password: \u0026#34;$6$myrandomsalt$McEB.xAVUWe0./6XqZ8n/7k9VV/Gxndy9nIMLyQAiPnhyBoToMWbxX2vA4f.Uv9PKnPRaYUUc76AjLWVAX6U10\u0026#34; tasks: - name: create the user user: name: anna password: \u0026#34;{{ password }}\u0026#34; The method that is used here works but is not elegant. First, you need to generate the encrypted password manually beforehand. Also, the encrypted password string is used in a readable way in the playbook. By seeing the encrypted password and salt, it\u0026rsquo;s possible to get to the original password, which is why the password should not be visible in the playbook in a secure environment.\nIn Exercise 13-3 you create a playbook that prompts for the user password and that uses the debug module, which was used in Listing 13-11 inside the playbook, together with register, so that the password no longer is readable in clear text. Before looking at Exercise 13-3, though, let\u0026rsquo;s first look at an alternative approach that also works.\nThe procedure to use encrypted passwords while creating user accounts is documented in the Frequently Asked Questions from the Ansible documentation. Because the documentation is available on the exam, make sure you know where to find this information! Search for the item \u0026ldquo;How do I generate encrypted passwords for the user module?\u0026rdquo;\nUsing an Alternative Approach # As has been mentioned on multiple occasions, in Ansible often different solutions exist for the same problem. And sometimes, apart from the most elegant solution, there\u0026rsquo;s also a quick-and-dirty solution, and that counts for setting a user-encrypted password as well. Instead of using the solution described in the previous section, \u0026ldquo;Generating Encrypted Passwords,\u0026rdquo; you can use the Linux command echo password | passwd --stdin to set the user password. Listing 13-13 shows how to do this. Notice this example focuses on how to do it, not on security. If you want to make the playbook more secure, it would be nice to store the password in Ansible Vault.\nListing 13-13 Setting the User Password: Alternative Solution\n--- - name: create user with encrypted password hosts: ansible3 vars: password: mypassword user: anna tasks: - name: configure user {{ user }} user: name: \u0026#34;{{ user }}\u0026#34; groups: wheel append: yes state: present - name: set a password for {{ user }} shell: ‘echo {{ password }} | passwd --stdin {{ user }}’ ::: box Exercise 13-3 Creating Users with Encrypted Passwords\n1. Use your editor to create the file exercise133.yaml.\n2. Write the play header as follows:\n--- - name: create user with encrypted password hosts: ansible3 vars_prompt: - name: passw prompt: which password do you want to use vars: user: sharon tasks: 3. Add the first task that uses the debug module to generate the encrypted password string and register to store the string in the variable mypass:\n- debug: msg: \u0026#34;{{ ‘{{ passw }}’| password_hash(‘sha512’,’myrandomsalt’) }}\u0026#34; register: mypass 4. Add a debug module to analyze the exact format of the registered variable:\n- debug: var: mypass 5. Use ansible-playbook exercise133.yaml to run the playbook the first time so that you can see the exact name of the variable that you have to use. This code shows that the mypass.msg variable contains the encrypted password string (see Listing 13-14).\nListing 13-14 Finding the Variable Name Using debug\n::: pre_1\nTASK [debug] ******************************************************************* ok: [ansible2] =\u0026gt; { \u0026#34;mypass\u0026#34;: { \u0026#34;changed\u0026#34;: false, \u0026#34;failed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;$6$myrandomsalt$Jesm4QGoCGAny9ebP85apmh0/uUXrj0louYb03leLoOWSDy/imjVGmcODhrpIJZt0rz.GBp9pZYpfm0SU2/PO.\u0026#34; } } :::\n6. Based on the output that you saw with the previous command, you can now use the user module to refer to the password in the right way. Add the following task to do so:\n- name: create the user user: name: \u0026#34;{{ user }}\u0026#34; password: \u0026#34;{{ mypass.msg }}\u0026#34; 7. Use ansible-playbook exercise133.yaml to run the playbook and verify its output. :::\n","externalUrl":null,"permalink":"/tech/ansible/manage-users-and-groups-with-ansible/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eUsing Ansible Modules to Manage Users and Groups\n    \u003cdiv id=\"using-ansible-modules-to-manage-users-and-groups\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-ansible-modules-to-manage-users-and-groups\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003emanagement of the user and group accounts and their direct properties.\u003c/li\u003e\n\u003cli\u003emanagement of sudo privilege escalation\u003c/li\u003e\n\u003cli\u003eSetting up SSH connections and setting user passwords\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 class=\"relative group\"\u003eModules\n    \u003cdiv id=\"modules\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#modules\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cp\u003euser\u003c/p\u003e","title":"Manage Users and Groups with Ansible","type":"tech"},{"content":" Using Modules to Manage Packages # Modules used to manage packages: Configuring Repository Access # The yum_repository module lets you work with yum repository files in the /etc/yum.repos.d/ directory.\n--- - name: setting up repository access hosts: all tasks: - name: connect to example repo yum_repository: name: example repo description: RHCE8 example repo file: examplerepo baseurl: ftp://control.example.com/repo/ gpgcheck: no yum_repository Key Arguments\nUse of the gpgcheck argument is recommended but not mandatory. Most repositories are provided with a GPG key to verify that packages in the repository have not been tampered with. However, if no GPG key is set up for the repository, the gpgcheck parameter can be set to no to skip checking the GPG key.\nManaging Software with yum # The yum module can be used to manage software packages. You use it to install and remove packages or to update packages. This can be done for individual packages, as well as package groups and modules. Let\u0026rsquo;s look at some examples that go beyond the mere installation or removal of packages, which was covered sufficiently in earlier chapters.\nListing 12-2 shows a module that will update all packages on this system.\nListing 12-2 Using yum to Perform a System Update\n::: pre_1 \u0026mdash; - name: updating all packages hosts: ansible2 tasks: - name: system update yum: name: ’*’ state: latest :::\nNotice the use of the name argument to the yum module. It has \u0026rsquo;*\u0026rsquo; as its argument. To prevent the wildcard from being interpreted by the shell, you must make sure it is placed between single quotes.\nListing 12-3 shows an example where yum package groups are used to install the Virtualization Host package group.\nListing 12-3 Installing Package Groups\n::: pre_1 \u0026mdash; - name: install or update a package group hosts: ansible2 tasks: - name: install or update a package group yum: name: ’@Virtualization Host’ state: latest :::\nWhen a yum package group instead of an individual package needs to be installed, the name of the package group needs to start with an at sign (@), and the entire package group name needs to be put between single quotes. Also notice the use of state: latest in Listing 12-3. This line ensures that the packages in the package group are installed if they have not been installed yet. If they have already been installed, they are updated to the latest version.\nA new feature in RHEL 8 is the yum AppStream module. Modules as listed by the Linux yum modules list command can be managed with the Ansible yum module also. Working with yum modules is similar to working with yum package groups. In the example in Listing 12-4, the main difference is that a version number and the installation profile are included in the module name.\nListing 12-4 Installing AppStream Modules with the yum Module\n::: pre_1 \u0026mdash; - name: installing an AppStream module hosts: ansible2 tasks: - name: install or update an AppStream module yum: name: ’@php:7.3/devel’ state: present :::\n::: note\nNote\nWhen using the yum module to install multiple packages, you can provide the name argument with a list of multiple packages. Alternatively, you can provide multiple packages in a loop. Of these solutions, using a list of multiple packages as the argument to name is always preferred. If multiple package names are provided in a loop, the module must execute a task for every single package. If multiple package names are provided as the argument to name, yum can install all these packages in one single task.\n:::\nManaging Package Facts # When Ansible is gathering facts, package facts are not included. To include package facts as well, you need to run a separate task; that is, you need to use the package_facts module. Facts that have been gathered about packages are stored to the ansible_facts.packages variable. The sample playbook in Listing 12-5 shows how to use the package_facts module.\nListing 12-5 Using the package_facts Module to Show Package Details\n::: pre_1 \u0026mdash; - name: using package facts hosts: ansible2 vars: my_package: nmap tasks: - name: install package yum: name: \u0026ldquo;{{ my_package }}\u0026rdquo; state: present - name: update package facts package_facts: manager: auto - name: show package facts for {{ my_package }} debug: var: ansible_facts.packages[my_package] when: my_package in ansible_facts.packages :::\nAs you can see, the package_facts module does not need much to do its work. The only argument used here is the manager argument, which specifies which package manager to communicate to. Its default value of auto automatically detects the appropriate package manager and uses that. If you want, you can specify the package manager manually, using any package manager such as yum or dnf. Listing 12-6 shows the output of running the Listing 12-5 playbook, where you can see details that are collected by the package_facts module.\nListing 12-6 Running ansible-playbook listing125.yaml Results\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing125.yaml\nPLAY [using package facts] ************************************************************** TASK [Gathering Facts] ****************************************************************** ok: [ansible2] TASK [install package] ****************************************************************** ok: [ansible2] TASK [update package facts] ************************************************************* ok: [ansible2] TASK [show package facts for my_package] ************************************************ ok: [ansible2] =\u0026gt; { \u0026quot;ansible_facts.packages[my_package]\u0026quot;: [ { \u0026quot;arch\u0026quot;: \u0026quot;x86_64\u0026quot;, \u0026quot;epoch\u0026quot;: 2, \u0026quot;name\u0026quot;: \u0026quot;nmap\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;5.el8\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;rpm\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;7.70\u0026quot; } ] } PLAY RECAP ****************************************************************************** ansible2 : ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 :::\nIn Exercise 12-1 you can practice working with the different tools Ansible provides for module management.\n::: box Exercise 12-1 Managing Software Packages\n1. Use your editor to create a new file with the name exercise121.yaml.\n2. Write a play header that defines the variable my_package and sets its value to virt-manager:\n--- - name: exercise121 hosts: ansible2 vars: my_package: virt-manager tasks: 3. Add a task that installs the package based on the name of the variable that was provided:\n- name: install package yum: name: \u0026#34;{{ my_package }}\u0026#34; state: present 4. Add a task that gathers facts about installed packages:\n- name: update package facts package_facts: manager: auto 5. As the last part of this exercise, add a task that shows facts about the package that you have just installed:\n- name: show package facts for {{ my_package }} debug: var: ansible_facts.packages[my_package] when: my_package in ansible_facts.packages 6. Run the playbook using ansible-playbook exercise121.yaml and verify its output. :::\nUsing Modules to Manage Repositories and Subscriptions # To work with software packages, you need to make sure that repositories are accessible and subscriptions are available. In the previous section you learned how to write a playbook that enables you to access an existing repository. In this section you learn how to set up the server part of a repository if that still needs to be done. Also, you learn how to manage RHEL subscriptions using Ansible.\nSetting Up Repositories # Most managed systems access the default distributions that are provided while installing the operating system. In some cases external repositories might not be accessible. If that happens, you need to set up a repository yourself. Before you can do that, however, it\u0026rsquo;s important to know what a repository is. A repository is a directory that contains RPM files, as well as the repository metadata, which is an index that allows the repository client to figure out which packages are available in the repository.\nAnsible does not provide a specific module to set up a repository. You must use a number of modules instead. Exactly which modules are involved depends on how you want to set up the repository. For instance, if you want to set up an FTP-based repository on the Ansible control host, you need to accomplish the following tasks:\n• Install the FTP package.\n• Start and enable the FTP server.\n• Open the firewall for FTP traffic.\n• Make sure the FTP shared repository directory is available.\n• Download packages to the repository directory.\n• Use the Linux createrepo command to generate the index that is required in each repository.\nThe playbook in Listing 12-7 provides an example of how this can be done.\nListing 12-7 Setting Up an FTP-based Repository\n::: pre_1 - name: install FTP to export repo hosts: localhost tasks: - name: install FTP server yum: name: - vsftpd - createrepo_c state: latest - name: start FTP server service: name: vsftpd state: started enabled: yes - name: open firewall for FTP firewalld: service: ftp state: enabled permanent: yes\n- name: setup the repo directory hosts: localhost tasks: - name: make directory file: path: /var/ftp/repo state: directory - name: download packages yum: name: nmap download_only: yes download_dir: /var/ftp/repo - name: createrepo command: createrepo /var/ftp/repo :::\nThe most significant tasks in setting up the repository are the download packages and createrepo tasks. In the download packages task, the yum module is used to download a single package. To do so, the download_only argument is used to ensure that the package is not installed but downloaded to a directory. When you use the download_only argument, you also must specify where the package needs to be installed. To do this, the task uses the download_dir argument.\nThere is one disadvantage in using this approach to download the package, though: it requires repository access. If repository access is not available, the fetch module can be used instead to download a file from a specific URL.\nManaging GPG Keys # To guarantee the integrity of packages, most repositories are set up with a GPG key. This enables the client to verify that packages have not been tampered with while transmitted between the repository server and client. For that reason, if packages are installed from a repository server on the Internet, you should always make sure that gpgcheck: yes is set while using the yum_repository module.\nHowever, if you want to make sure that a GPG check is performed, you need to make sure the client knows where to fetch the repository key. To help with that, you can use the rpm_key module. You can see how to do this in Listing 12-8. Notice that the playbook in this listing doesn\u0026rsquo;t work because no GPG-protected repository is available. Setting up GPG-protected repositories is complex and outside the scope of the EX294 objectives, and for that reason is not covered here.\nListing 12-8 Using rpm_key to Fetch an RPM Key\n::: pre_1 - name: use rpm_key in repository access hosts: all tasks: - name: get the GPG public key rpm_key: key: ftp://control.example.com/repo/RPM-GPG-KEY state: present - name: set up the repository client yum_repository: file: myrepo name: myrepo description: example repo baseurl: ftp://control.example.com/repo enabled: yes gpgcheck: yes state: present :::\nManaging RHEL Subscriptions # When you work with Red Hat Enterprise Linux, configuring repository access using the method described before is not enough. Red Hat Enterprise Linux works with subscriptions, and to be able to access software that is provided through your subscription entitlement, you need to set up managed systems to access these subscriptions.\n::: note\nTip\nFree developer subscriptions are available for RHEL as well as Ansible. Register yourself at https://developers.redhat.com and sign up for a free subscription if you want to test the topics described in this section on RHEL and you don\u0026rsquo;t have a valid subscription yet.\n:::\nTo understand how to use the Ansible modules to register a RHEL system, you need to understand how to use the Linux command-line utilities. When you are managing subscriptions from the Linux command line, multiple steps are involved.\n1. First, you use the subscription-manager register command to provide your RHEL credentials. Use, for instance, subscription-manager register --username=yourname --password=yourpassword.\n2. Next, you need to find out which pools are available in your account. A pool is a collection of software channels available to your account. Use subscription-manager list --available for an overview.\n3. Now you can connect to a specific pool using subscription-manager attach --pool=poolID. Note that if only one subscription pool is available in your account, you don\u0026rsquo;t have to provide the --pool argument.\n4. Next, you need to find out which additional repositories are available to your account by using subscription-manager repos --list.\n5. To register to use additional repositories, you use subscription-manager repos --enable \u0026ldquo;repos name\u0026rdquo;. Your system then has full access to its subscription and related repositories.\nTwo significant modules are provided by Ansible:\n• redhat_subscription: This module enables you to perform subscription and registration in one task.\n• rhsm_repository: This module enables you to add subscription manager repositories.\nListing 12-9 shows an example of a playbook that uses these modules to fully register a new RHEL 8 machine and add a new repository to the managed machine. Notice that this playbook is not runnable as such because important additional information needs to be provided. Exercise 12-3, later in the section titled \u0026ldquo;Implementing a Playbook to Manage Software,\u0026rdquo; will guide you to a scenario that shows how to use this code in production.\nListing 12-9 Using Subscription Manager to Set Up Ansible\n::: pre_1 \u0026mdash; - name: use subscription manager to register and set up repos hosts: ansible5 tasks: - name: register and subscribe ansible5 redhat_subscription: username: bob@example.com password: verysecretpassword state: present - name: configure additional repo access rhsm_repository: name: - rh-gluster-3-client-for-rhel-8-x86_64-rpms - rhel-8-for-x86_64-appstream-debug-rpms state: present :::\nIn the sample playbook in Listing 12-9, you can see how the redhat_subscription and rhsm_repository modules are used. Notice that redhat_subscription requires a password. In Listing 12-9 the username and password are provided as clear-text values in the playbook. From a security perspective, this is very bad practice. You should use Ansible Vault instead. Exercise 12-3 will guide you through a setup where this is done.\nIn Exercise 12-2 you are guided through the procedure of setting up your own repository and using it. This procedure consists of two distinct parts. In the first part you set up a repository server that is based on FTP. Because in Ansible you often need to configure topics that don\u0026rsquo;t have your primary attention, you set up the FTP server and also change its configuration. Next, you write a second playbook that configures the clients with appropriate repository access, and after doing so, install a package.\n::: box Exercise 12-2 Setting Up a Repository\n1. Use your editor to create the file exercise122-server.yaml.\n2. Define the play that sets up the basic FTP configuration. Because all its tasks should be familiar to you at this point, you can enter all the tasks at once:\n--- - name: install, configure, start and enable FTP hosts: localhost tasks: - name: install FTP server yum: name: vsftpd state: latest - name: allow anonymous access to FTP lineinfile: path: /etc/vsftpd/vsftpd.conf regexp: ’^anonymous_enable=NO’ line: anonymous_enable=YES - name: start FTP server service: name: vsftpd state: started enabled: yes - name: open firewall for FTP firewalld: service: ftp state: enabled immediate: yes permanent: yes 3. Set up a repository directory. Add the following play to the playbook. Notice the use of the download packages task, which uses the yum module to download a package without installing it. Also notice the createrepo task, which creates the repository metadata that converts the /var/ftp/repo directory into a repository.\n- name: setup the repo directory hosts: localhost tasks: - name: make directory file: path: /var/ftp/repo state: directory - name: download packages yum: name: nmap download_only: yes download_dir: /var/ftp/repo - name: install createrepo package yum: name: createrepo_c state: latest - name: createrepo command: createrepo /var/ftp/repo notify: - restart_ftp handlers: - name: restart_ftp service: name: vsftpd state: restarted 4. Use the command ansible-playbook exercise122-server.yaml to set up the FTP server on control.example.com. If you haven\u0026rsquo;t made any typos, you shouldn\u0026rsquo;t encounter any errors.\n5. Now that the repository server has been installed, it\u0026rsquo;s time to set up the repository client. Use your editor to create the file exercise122-client.yaml and write the play header as follows:\n--- - name: configure repository hosts: all vars: my_package: nmap tasks: 6. Add a task that uses the yum_repository module to configure access to the new repository:\n- name: connect to example repo yum_repository: name: exercise122 description: RHCE8 exercise 122 repo file: exercise122 baseurl: ftp://control.example.com/repo/ gpgcheck: no 7. After setting up the repository client, you also need to make sure that the clients know how to reach the repository server by addressing its name. Add the next task that writes a new line to /etc/hosts to make sure host name resolving on the clients is set up correctly:\n- name: ensure control is resolvable lineinfile: path: /etc/hosts line: 192.168.4.200 control.example.com control - name: install package yum: name: \u0026#34;{{ my_package }}\u0026#34; state: present 8. If you are using the package_facts module, you need to remember to update it after installing new packages. Add the following task to get this done:\n- name: update package facts package_facts: manager: auto 9. As the last task, just because it\u0026rsquo;s fun, use the debug module together with the package facts to get information about the newly installed package:\n- name: show package facts for {{ my_package }} debug: var: ansible_facts.packages[my_package] when: my_package in ansible_facts.packages 10. Use the command ansible-playbook exercise122-client.yaml -e my_package=redis. That\u0026rsquo;s right; this command overwrites the my_package variable that was set in the playbook\u0026mdash;just to remind you a bit about variable precedence. :::\n","externalUrl":null,"permalink":"/tech/ansible/managing-packages-repositories-and-subscriptions-with-ansible/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eUsing Modules to Manage Packages\n    \u003cdiv id=\"using-modules-to-manage-packages\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-modules-to-manage-packages\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eModules used to manage packages:\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"Image\"\n    src=\"../../../img/12tab02.jpg\"\n    \u003e\u003c/figure\u003e\n\u003c/p\u003e\n\n\u003ch4 class=\"relative group\"\u003eConfiguring Repository Access\n    \u003cdiv id=\"configuring-repository-access\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#configuring-repository-access\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eyum_repository\u003c/strong\u003e module lets you work with yum repository files in the /etc/yum.repos.d/ directory.\u003c/p\u003e","title":"Managing Packages, Repositories, and Subscriptions with Ansible","type":"tech"},{"content":" Discovering storage related facts # Table 15-2 Modules for Managing Storage\nTo make sure that your playbook is applied to the right devices, you first need to find which devices are available on your managed system.\nAfter you find them, you can use conditionals to make sure that tasks are executed on the right devices.\nUsing Storage-Related Facts # Ansible_facts related to storage\nansible_devices\nAvailable storage and device info ansible_device_links info on how to access storage and other device info ansible_mounts Mount point info ansible ansible1 -m setup -a 'filter=ansible_devices'\nFind generic information about storage devices.\nThe filter argument to the setup module uses a shell-style wildcard to search for matching items and for that reason can search in the highest level facts, such as ansible_devices, but it is incapable of further specifying what is searched for. For that reason, in the filter argument to the setup module, you cannot use a construction like ansible ansible1 -m setup -a \u0026quot;filter=ansible_devices.sda\u0026quot; which is common when looking up the variable in conditional statements.\nUsing Storage-Related Facts in Conditional Statements # Assert module\nshow an error message if a device does not exist and to perform a task if the device exists. For an easier solution, you can also use a when statement to look for the existence of a device. The advantage of using the assert module is that an error message can be printed if the condition is not met. Listing 15-2 Using assert to Run a Task Only If a Device Exists\n--- - name: search for /dev/sdb continue only if it is found hosts: all vars: disk_name: sdb tasks: - name: abort if second disk does not exist assert: that: - \u0026#34;ansible_facts[\u0026#39;devices\u0026#39;][\u0026#39;{{ disk_name }}\u0026#39;] is defined\u0026#34; fail_msg: second hard disk not found - debug: msg: \u0026#34;{{ disk_name }} was found, lets continue\u0026#34; Write a playbook that finds out the name of the disk device and puts that in a variable that you can work with further on in the playbook.\nThe set_fact argument comes in handy to do so.\nYou can use it in combination with a when conditional statement to store a detected device name in a variable.\nStoring the Detected Disk Device Name in a Variable\n--- - name: define variable according to diskname detected hosts: all tasks: - ignore_errors: yes set_fact: disk2name: sdb when: ansible_facts[’devices’][’sdb’] - name: Detect secondary disk name ignore_errors: yes set_fact: disk2name: vda when: ansible_facts[\u0026#39;devices\u0026#39;][\u0026#39;vda\u0026#39;] is defined - name: Search for second disk, continue only if it is found assert: that: - \u0026#34;ansible_facts[\u0026#39;devices\u0026#39;][disk2name] is defined\u0026#34; fail_msg: second hard disk not found - name: Debug detected disk debug: msg: \u0026#34;{{ disk2name }} was found. Moving forward.\u0026#34; ~ Managing Partitions and LVM # After detecting the disk device that needs to be used, you can move on and start creating partitions and logical volumes.\npartition a disk using the parted module, work with the lvg and lvol modules to manage LVM logical volumes, create file systems using the filesystem module and mount them using the mount module manage swap storage. Creating Partitions # Parted Module name:\nAssign unique name, required for GPT partitions label: type of partition table, msdos is default, gpt for gpt device: Device where you are creating partition number: partition number state: present or absent to add/remove part_start:\nStarting position expressed as an offset from the beginning of the disk part_end: Where to end the partition If these arguments are not used, the partition starts at 0% and ends at 100% of the available disk space. flags:\nSet specific partition properties such as LVM partition type. Required for LVM partition type - name: create new partition parted: name: files label: gpt device: /dev/sdb number: 1 state: present part_start: 1MiB part_end: 2GiB - name: create another new partition parted: name: swap label: gpt device: /dev/sdb number: 2 state: present part_start: 2GiB part_end: 4GiB flags: [ lvm ] Managing Volume Groups and LVM Logical Volumes # lvg module\nmanage LVM logical volumes managing LVM volume groups lvol module\nmanaging LVM logical volumes. Creating an LVM volume group\nvg argument to set the name of the volume group pvs argument to identify the physical volume (which is often a partition or a disk device) on which the volume group needs to be created. May need to specify the pesize to refer to the size of the physical extents. - name: create a volume group lvg: vg: vgdata pesize: \u0026#34;8\u0026#34; pvs: /dev/sdb1 After you create an LVM volume group, you can create LVM logical volumes.\nlvol Common Options: lv\nName of the LV pvs comma separated list of pvs, if it is a partition then it should have the lvm option set resizefs Indicates whether to resize filesystem when the lv is expanded size size of the lv snapshot specify name if this lv is a snapshot vg VG is which the lv should be created Creating an LVM Logical Volume\n- name: create a logical volume lvol: lv: lvdata size: 100%FREE vg: vgdata Creating and Mounting File Systems # filesystem module\nSupports creating as well as resizing file systems. Options: dev\nblock device name fstype filesystem type opts options passed to mkfs command resizefs Extends the filesystem if set to yes. Extended to the current block size Creating an XFS File System\n- name: create an XFS filesystem filesystem: dev: /dev/vgdata/lvdata fstype: xfs Mounting a filesystem # mount module.\nUsed to mount a filesystem Options: fstype\nFilesystem type is not automatically dedected. Used to specify filesystem type path directory to mount the filesystem to src device to be mounted state Current mount state mounted to mount device now present to set in /etc/fstab but not mount it now - name: mount the filesystem mount: src: /dev/vgdata/lvdata fstype: xfs state: mounted path: /mydir Configuring Swap Space # To set up swap space, you first must format a device as swap space and next mount the swap space.\nTo format a device as swap space, you use the filesystem module.\nThere is no specific Ansible module to activate theswap space, so you use the command module to run the Linux swapon command.\nBecause adding swap space is not always required, it can be done in a conditional statement.\nIn the statement, use the ansible_swaptotal_mb fact to discover how much swap is actually available.\nIf that amount falls below a specific threshold, the swap space can be created and activated.\nA conditional check is performed, and additional swap space is configured if the current amount of swap space is lower than 256 MiB.\n--- - name: configure swap storage hosts: ansible2 tasks: - name: setup swap block: - name: make the swap filesystem filesystem: fstype: swap dev: /dev/sdb1 - name: activate swap space command: swapon /dev/sdb1 when: ansible_swaptotal_mb \u0026lt; 256 Run an ad hoc command to ensure that /dev/sdb on the target host is empty:\nansible ansible2 -a \u0026#34;dd if=/dev/zero of=/dev/sdb bs=1M count=10\u0026#34; To make sure that you don\u0026rsquo;t get any errors about partitions that are in use, also reboot the target host:\nansible ansible2 -m reboot Lack of idempotency if the size is specified as 100%FREE, which is a relative value, not an absolute value. This value works the first time you run the playbook, but it does not the second time you run the playbook. Because no free space is available, the LVM layer interprets the task as if you wanted to create a logical volume with a size of 0 MiB and will complain about that. To ensure that plays are written in an idempotent way, make sure that you use absolute values, not relative values. ","externalUrl":null,"permalink":"/tech/ansible/managing-partitions-and-lvm/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003eDiscovering storage related facts\n    \u003cdiv id=\"discovering-storage-related-facts\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#discovering-storage-related-facts\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eTable 15-2\u003c/strong\u003e Modules for Managing Storage\u003c/p\u003e\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"Image\"\n    src=\"../../../img/15tab02.jpg\"\n    \u003e\u003c/figure\u003e\n\u003cp\u003eTo make sure that your playbook is applied to the right devices, you first need to find which devices are available on your managed system.\u003c/p\u003e","title":"Managing Partitions and LVM","type":"tech"},{"content":" Managing Services # Services can be managed in many ways. You can manage systemd services, but Ansible also allows for management of tasks using Linux cron and at. Apart from that, you can use Ansible to manage the desired systemd target that a managed system should be started in, and it can reboot running machines. Table 14-2 gives an overview of the most significant modules for managing services.\nTable 14-2 Modules Related to Service Management\nManaging Systemd Services # Throughout this book you have used the service module a lot. This module enables you to manage services, regardless of the init system that is used, so it works with System-V init, with Upstart, as well as systemd. In many cases, you can use the service module for any service-related task.\nIf systemd specifics need to be addressed, you must use the systemd module instead of the service module. Such systemd-specific features include daemon_reload and mask. The daemon_reload feature forces the systemd daemon to reread its configuration files, which is useful after applying changes (or after editing the service files directory, without using the Linux systemctl command). The mask feature marks a systemd service in such a way that it cannot be started, not even by accident. Listing 14-1 shows an example where the systemd module is used to manage services.\nListing 14-1 Using systemd Module Features\n::: pre_1 \u0026mdash; - name: using systemd module to manage services hosts: ansible2 tasks: - name: enable service httpd and ensure it is not masked systemd: name: httpd enabled: yes masked: no daemon_reload: yes :::\nGiven the large amount of functionality that is available in systemd, the functions that are offered by the systemd services are a bit limited, and for many specific features, you must use generic modules such as file and command instead. An example is setting the default target, which is done by creating a symbolic link using the file module.\nManaging cron Jobs # The cron module can be used to manage cron jobs. A Linux cron job is one that is periodically executed by the Linux crond daemon at a specific time. The cron module can manage jobs in different ways:\n• Write the job directly to a user\u0026rsquo;s crontab\n• Write the job to /etc/crontab or under the /etc/cron.d directory\n• Pass the job to anacron so that it will be run once an hour, day, week, month, or year without specifically defining when exactly\nIf you are familiar with Linux cron, using the Ansible cron module is straightforward. Listing 14-2 shows an example that runs the fstrim command every day at 4:05 and at 19:05.\nListing 14-2 Running a cron Job\n--- - name: run a cron job hosts: ansible2 tasks: - name: run a periodic job cron: name: \u0026quot;run fstrim\u0026quot; minute: \u0026quot;5\u0026quot; hour: \u0026quot;4,19\u0026quot; job: \u0026quot;fstrim\u0026quot; As a result of this playbook, a crontab file is created for user root. To create a crontab file for another user, you can use the user attribute. Notice that while managing cron jobs using the cron module, a name attribute is specified. This attribute is required for Ansible to manage the cron jobs and has no meaning for Linux crontab itself. If, for instance, you later want to remove a cron job, you must use the name of the job as an identifier.\nListing 14-3 shows a sample playbook that removes the job that was created in Listing 14-2. Notice that it just specifies state: absent as well as the name of the job that was previously created; no other parameters are required.\nListing 14-3 Removing a cron Job Using the name Attribute\n::: pre_1 \u0026mdash; - name: run a cron job hosts: ansible2 tasks: - name: run a periodic job cron: name: \u0026ldquo;run fstrim\u0026rdquo; state: absent :::\nManaging at Jobs # Whereas you use Linux cron to schedule tasks at a regular interval, you use Linux at to manage tasks that need to run once only. To interface with Linux at, the Ansible at module is provided. Table 14-3 gives an overview of the arguments it takes to specify how the task should be executed.\n::: group Table 14-3 at Module Arguments Overview\nThe most important point to understand when working with at is that it is used to defined how far from now a task has to be executed. This is done using count and units. If, for example, you want to run a task five minutes from now, you specify the job with the arguments count: 5 and units: minutes. Also notice the use of the unique argument. If set to yes, the task is ignored if a similar job is scheduled to run already. Listing 14-4 shows an example.\nListing 14-4 Running Commands in the Future with at\n::: pre_1 \u0026mdash; - name: run an at task hosts: ansible2 tasks: - name: run command and write output to file at: command: \u0026ldquo;date \u0026gt; /tmp/my-at-file\u0026rdquo; count: 5 units: minutes unique: yes state: present :::\nIn Exercise 14-1 you practice your skills working with the cron module.\n::: box Exercise 14-1 Managing cron Jobs\n1. Use your editor to create the playbook exercise141-1.yaml and give it the following contents:\n--- - name: run a cron job hosts: ansible2 tasks: - name: run a periodic job cron: name: \u0026#34;run logger\u0026#34; minute: \u0026#34;0\u0026#34; hour: \u0026#34;5\u0026#34; job: \u0026#34;logger IT IS 5 AM\u0026#34; 2. Use ansible-playbook exercise141-1.yaml to run the job.\n3. Use the command ansible ansible2 -a \u0026ldquo;crontab -l\u0026rdquo; to verify the cron job has been added. The output should look as follows:\nansible2 | CHANGED | rc=0 \u0026gt;\u0026gt; #Ansible: run logger 0 5 * * * logger IT IS 5 AM 4. Create a new playbook with the name exercise141-2 that runs a new cron job but uses the same name:\n--- - name: run a cron job hosts: ansible2 tasks: - name: run a periodic job cron: name: \u0026#34;run logger\u0026#34; minute: \u0026#34;0\u0026#34; hour: \u0026#34;6\u0026#34; job: \u0026#34;logger IT IS 6 AM\u0026#34; 5. Run this new playbook by using ansible-playbook exercise141-2.yaml. Notice that the job runs with a changed status.\n6. Repeat the command ansible ansible2 -a \u0026ldquo;crontab -l\u0026rdquo;. This shows you that the new cron job has overwritten the old job because it was using the same name. Here is something important to remember: all cron jobs should have a unique name!\n7. Write the playbook exercise141-3.yaml to remove the cron job that you just created:\n--- - name: run a cron job hosts: ansible2 tasks: - name: run logger cron: name: \u0026#34;run logger\u0026#34; state: absent 8. Use ansible-playbook exercise141-3.yaml to run the last playbook. Next, use ansible ansible2 -a \u0026ldquo;crontab -l\u0026rdquo; to verify that the cron job was indeed removed.\n","externalUrl":null,"permalink":"/tech/ansible/managing-services-with-ansible/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eManaging Services\n    \u003cdiv id=\"managing-services\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#managing-services\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eServices can be managed in many ways. You can manage systemd services,\nbut Ansible also allows for management of tasks using Linux cron and at.\nApart from that, you can use Ansible to manage the desired systemd\ntarget that a managed system should be started in, and it can reboot\nrunning machines. \u003ca\n  href=\"#ch14.xhtml#tab14_2\"\u003eTable 14-2\u003c/a\u003e gives an overview of\nthe most significant modules for managing services.\u003c/p\u003e","title":"Managing Services with Ansible","type":"tech"},{"content":" Implementing a Playbook to Manage Software # In the previous sections you learned how to use different modules to manage software. In this section you apply this knowledge in a more advanced playbook example. You\u0026rsquo;ll also find an advanced example like this in Chapters 13 through 15 so that you get the best possible preparation for the EX294 exam. To match the exam style of questions, the example is scenario based, and the assignment is formatted as a step-by-step exercise. As is the case for all exercises, the complete playbook discussed here is available in the GitHub repository at https://github.com/sandervanvugt/rhce8-book/exercise123.yaml.\nTo run this assignment on a RHEL 8 target, you need access to a valid RHEL 8 subscription. If you don\u0026rsquo;t have a current subscription, you can request it from https://developers.redhat.com.\n::: box Exercise 12-3 Configuring a New RHEL Managed Node\nCreate a playbook that will fully automate the setup of a new RHEL host. Write this playbook according to the following requirements:\n• Add the new host information to the inventory and /etc/hosts file on the control host.\n• Work with variables for the name of the host you want to set up.\n• Connect as user root to the new host. While running the playbook, run it with the appropriate option so that you will be prompted for the root password.\n• Set up the user ansible on the new host and make sure this user is a member of the group wheel.\n• Also, set a password for user ansible using the playbook.\n• Modify the sudoers file such that the user ansible can run root commands using sudo without having to enter a password.\n• Automatically register the host with the RHEL Subscription Manager.\n• Use Ansible Vault to provide the username and password in a secure way.\n• Add the new host to the rh-gluster-3-client-for-rhel-8-x86_64-rpms repository and the rhel-8-for-x86_64-appstream-debug-rpms repository.\n• Use tags so that you can run individual parts of the playbook.\n1. On the control host, use sudo yum install sshpass to install the sshpass package. This package enables you to work with SSH passwords in a noninteractive way, and you need it to automate working with SSH passwords from a playbook environment.\n2. To start, you need to set up the control host to include information about the new host. To make this playbook flexible, this playbook requires variables to be set from the command line because that is the only way to ensure that the variable is available in all of the plays. Using vars_prompt would have been an option, but variables that are set with vars_prompt apply only to the play in which they are set. To check that the variables were indeed passed as an argument to the ansible-playbook command, use the fail module as follows to check whether the variable newhost and the variable newhostip are provided as startup arguments. Create a file with the name exercise123.yaml as follows:\n- name: add host to inventory hosts: localhost tasks: - fail: msg: \u0026#34;add the options -e newhost=hostname -e newhostip=ip.ad.dr.ess and try again\u0026#34; when: (newhost is undefined) or (newhostip is undefined) 3. Write the tasks for this first play. In these tasks, you want to make sure that the local inventory file and the /etc/hosts file are modified. To do this, the lineinfile module provides good service. Notice the use of the line the second time the lineinfile module is called. The line contains only variables, and for that reason the entire variable string must be between double quotes. Also, at the end of the play, include the tags: addhost line, which makes it easy to skip this task after it has run successfully in case it is needed to run the playbook again. Make sure to add the following text to complete the first play:\n- name: add new host to inventory lineinfile: path: inventory state: present line: \u0026#34;{{ newhost }}\u0026#34; - name: add new host to /etc/hosts lineinfile: path: /etc/hosts state: present line: \u0026#34;{{ newhostip }} {{ newhost}}\u0026#34; tags: addhost 4. At this point it\u0026rsquo;s a good idea to test that all is going well so far. Use ansible-playbook -C exercise123.yaml and observe playbook output. It should fail because no arguments are provided on the command line. Use ansible-playbook -C exercise123.yaml -e newhost=ansible5 -e newhostip=192.168.4.205 and try again.\n5. The first play is ready at this point, so it\u0026rsquo;s time to configure the second play. This play is executed on the new host. The target host name is set to the variable newhost, which is defined while running the ansible-playbook command. Also notice that the remote_user and the become statements must be set because the default user ansible is not configured for privilege escalation yet. Write the play header for this second play as follows:\n- name: configure a new RHEL host hosts: \u0026#34;{{ newhost }}\u0026#34; remote_user: root become: false tasks: 6. Now it\u0026rsquo;s time to add the tasks, as well as the tag to this play. In the tasks you need to make sure a user ansible exists and is a member of the group wheel. Next, you use the shell module to set a password for the user ansible. It\u0026rsquo;s an ugly approach, but it works. In Chapter 13, \u0026ldquo;Managing Users,\u0026rdquo; you\u0026rsquo;ll learn how to do this in a much nicer way. As the next task you use the lineinfile module to modify the /etc/sudoers file and allow members of the group wheel to escalate permissions without entering a password. Add the tasks to do this as follows:\n- name: configure user ansible user: name: ansible groups: wheel append: yes state: present - name: set a password for this user shell: ’echo password | passwd --stdin ansible’ - name: enable sudo without passwords lineinfile: path: /etc/sudoers regexp: ’^%wheel’ line: ’%wheel ALL=(ALL) NOPASSWD: ALL’ validate: /usr/sbin/visudo -cf %s 7. With this part you have set up the user ansible on the managed host, but one element is still missing: the user cannot log in with an SSH public/private key pair yet. In Chapter 13 you\u0026rsquo;ll learn about a nice way to add the SSH public key to the remote user; for now you can do it in a quick-and-dirty way that will also work. Add the following lines to the playbook to conclude the second play:\n- name: create SSH directory in user ansible home file: path: /home/ansible/.ssh state: directory owner: ansible group: ansible - name: copy SSH public key to remote host copy: src: /home/ansible/.ssh/id_rsa.pub dest: /home/ansible/.ssh/authorized_keys tags: setuphost 8. Feel free to test the second play at this point; it\u0026rsquo;s better to filter out any errors now than to do it later. To test, use ansible-playbook -C -k exercise123.yaml -e newhost=ansible5 -e newhostip=192.168.4.205. Notice the use of the -k option, which prompts for the SSH password that user root in this play needs to set up the target host.\n9. At this point your RHEL host should be ready for use. The only thing that is still missing is that it has not been registered against Red Hat Subscription Manager. To do this, you need your Red Hat credentials. Because these credentials are sensitive information, you should use Ansible Vault. So let\u0026rsquo;s start creating the vault file and define the username and password variables in the Vault file. To create the Vault file, use ansible-vault create exercise123-vault.yaml and provide the following input, where you should use your real username and password and not XXXXXXXXX. Your rhsm_user name is the name (typically an email address) that you use to log in at redhat.com, and the rhsm_password is the password that you use with it. Also notice that for obvious security reasons, this file is NOT provided in the GitHub repository that comes with this book:\nrhsm_user: XXXXXXXXXXX rhsm_password: XXXXXXXXXXX 10. Now that you have created the Vault file, you can write the header for the third and last play in the file exercise123.yaml. The most important part of this header is the vars_files part, which refers to the Vault file:\n- name: use subscription manager to register and set up repos hosts: \u0026#34;{{ newhost }}\u0026#34; vars_files: - exercise123-vault.yaml tasks: 11. At this point you can complete the playbook and add the remaining tasks:\n- name: register and subscribe {{ newhost }} redhat_subscription: username: \u0026#34;{{ rhsm_user }}\u0026#34; password: \u0026#34;{{ rhsm_password }}\u0026#34; state: present - name: configure additional repo access rhsm_repository: name: - rh-gluster-3-client-for-rhel-8-x86_64-rpms - rhel-8-for-x86_64-appstream-debug-rpms state: present tags: registerhost 12. At this point the playbook is ready. Compare what you have written to the sample playbook exercise123.yaml that is in the GitHub repository and give it a try. To do so, use the ansible-playbook -k --ask-vault-pass exercise123.yaml -e newhost=ansible5 -e newhostip=192.168.4.205 command. Everything should be running smoothly, but because this is a large playbook and it is very difficult to write it without typos right from the beginning, you might have to do a little bit of troubleshooting. To do so, I recommend that you use the tags that have been provided with the plays. If, after running the first and second plays successfully, the third play generates an error, you can run just that play again, using ansible-playbook --tags=registerhost exercise123.yaml -e newhost=ansible5. (Notice that this command doesn\u0026rsquo;t use as many command-line options as the command you used just a minute ago because these parameters don\u0026rsquo;t apply to the registerhost tag.) :::\nLab 12-1 # Configure the control.example.com host as a repository server, according to the following requirements:\n• Create a directory with the name /repo, and in that directory copy all packages that have a name starting with nginx.\n• Generate the metadata that makes this directory a repository.\n• Configure the Apache web server to provide access to the repository server. You just have to make sure that the DocumentRoot in Apache is going to be set to the /repo directory.\nLab 12-2 # Write a playbook to configure all managed servers according to the following requirements:\n• All hosts can access the repository that was created in Lab 12-1.\n• Have the same playbook install the nginx package.\n• Do NOT start the service. Use the appropriate module to gather information about the installed nginx package, and let the playbook print a message stating the name of the nginx package as well as the version. :::\n[]{#ch13.xhtml}\n::: {#ch13.xhtml#sbo-rt-content}\n","externalUrl":null,"permalink":"/tech/ansible/lab-managing-software-with-ansible/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eImplementing a Playbook to Manage Software\n    \u003cdiv id=\"implementing-a-playbook-to-manage-software\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#implementing-a-playbook-to-manage-software\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eIn the previous sections you learned how to use different modules to\nmanage software. In this section you apply this knowledge in a more\nadvanced playbook example. You\u0026rsquo;ll also find an advanced example like\nthis in Chapters 13 through 15 so that you get the best possible\npreparation for the EX294 exam. To match the exam style of questions,\nthe example is scenario based, and the assignment is formatted as a\nstep-by-step exercise. As is the case for all exercises, the complete\nplaybook discussed here is available in the GitHub repository at\n\u003ca\n  href=\"https://github.com/sandervanvugt/rhce8-book/exercise123.yaml\"\n    target=\"_blank\"\n  \u003ehttps://github.com/sandervanvugt/rhce8-book/exercise123.yaml\u003c/a\u003e.\u003c/p\u003e","title":"Managing Software with Ansible","type":"tech"},{"content":"When you first download Fedora Workstation, it\u0026rsquo;s going to be a little hard to figure out how to make it usable. Especially if you\u0026rsquo;ve never tinkered with Linux before.\nThis is because Fedora with Gnome desktop is a blank canvas. The point is to let you customize it to your needs. When I first install Fedora, I pull my justfile to install most of the programs I use:\n`curl -sL https://raw.githubusercontent.com/linuxreader/dotfiles/main/dot_justfile -o ~/.justfile` Install just and run the justfile # To run the just file, I then install the just program and run it on the justfile:\ndnf install just\njust first-install\nThis is my current .justfile:\nfirst-install: # Install flatpacks flatpak install --noninteractive \\ flathub com.bitwarden.desktop \\ flathub com.brave.Browser \\ flathub org.gimp.GIMP \\ flathub org.gnome.Snapshot \\ flathub org.libreoffice.LibreOffice \\ flathub org.remmina.Remmina \\ flathub com.termius.Termius \\ flathub com.slack.Slack \\ flathub org.keepassxc.KeePassXC \\ flathub md.obsidian.Obsidian \\ flathub com.calibre_ebook.calibre \\ flathub org.mozilla.Thunderbird \\ flathub us.zoom.Zoom \\ flathub org.wireshark.Wireshark \\ flathub com.nextcloud.desktopclient.nextcloud \\ flathub com.google.Chrome \\ flathub io.github.shiftey.Desktop \\ flathub io.github.dvlv.boxbuddyrs \\ flathub com.github.tchx84.Flatseal \\ flathub io.github.flattool.Warehouse \\ flathub io.missioncenter.MissionCenter \\ flathub org.gnome.World.PikaBackup \\ flathub com.github.rafostar.Clapper \\ flathub com.mattjakeman.ExtensionManager \\ flathub com.jgraph.drawio.desktop \\ flathub org.adishatz.Screenshot \\ flatpak com.github.finefindus.eyedropper \\ flatpak com.github.johnfactotum.Foliate \\ flatpak com.usebottles.bottles \\ flatpak com.obsproject.Studio \\ flatpak net.lutris.Lutris \\ flatpak com.vivaldi.Vivaldi \\ flatpak com.vscodium.codium \\ flatpak io.podman_desktop.PodmanDesktop \\ flatpak org.kde.kdenlive # Install Homebrew sudo dnf -y groupinstall \\ \u0026#34;Development Tools\u0026#34; /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; echo \u0026#39;eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34; # Configure dnf for faster speeds sudo bash -c \u0026#39;echo \u0026#34;fastestmirror=True\u0026#34; \u0026gt;\u0026gt; /etc/dnf/dnf.conf\u0026#39; sudo bash -c \u0026#39;echo \u0026#34;max_parallel_downloads=10\u0026#34; \u0026gt;\u0026gt; /etc/dnf/dnf.conf\u0026#39; sudo bash -c \u0026#39;echo \u0026#34;defaultyes=True\u0026#34; \u0026gt;\u0026gt; /etc/dnf/dnf.conf\u0026#39; sudo bash -c \u0026#39;echo \u0026#34;keepcache=True\u0026#34; \u0026gt;\u0026gt; /etc/dnf/dnf.conf\u0026#39; # Other software, updates, etc. sudo dnf -y update sudo dnf install -y gnome-screenshot sudo dnf -y groupupdate core sudo flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo sudo dnf install -y wireguard-tools sudo dnf install gnome-tweaks sudo dnf -y install https://mirrors.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm \\ https://mirrors.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm sudo dnf -y update sudo dnf install gnome-themes-extra gsettings set org.gnome.desktop.interface gtk-theme \u0026#34;Adwaita-dark\u0026#34; sudo dnf install -y go echo \u0026#39;export PATH=$PATH:/usr/local/go/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc gsettings set org.gnome.mutter experimental-features \u0026#34;[\u0026#39;scale-monitor-framebuffer\u0026#39;]\u0026#34; homebrew: brew install \\ chezmoi \\ hugo \\ virt-manager Install Homebrew Stuff: # then run just homebrew after a reboot to install packages with brew\nvisudo config # Add to /etc/sudoers to make Vim default for visudo\nDefaults editor=/usr/bin/vim\nVirt Manager # sudo dnf install @virtualization sudo vi /etc/libvirt/libvirtd.conf Uncomment the line: unix_sock_group = \u0026quot;libvirt\u0026quot;\nAdjust the UNIX socket permissions for the R/W socket: unix_sock_rw_perms = \u0026quot;0770\u0026quot;\nStart the service: systemctl enable --now libvirtd\nAdd user to group:\nsudo usermod -a -G libvirt $(whoami) \u0026amp;\u0026amp; sudo usermod -a -G kvm $(whoami) use the Tweaks app to set the appearance of Legacy Applications to \u0026lsquo;adwaita-dark\u0026rsquo;.\nConfigure Howdy # Howdy is a tool for using an IR webcam for authentication:\nsudo dnf copr enable principis/howdy sudo dnf --refresh install -y howdy https://copr.fedorainfracloud.org/coprs/principis/howdy/ https://github.com/boltgolt/howdy\nSeahorse # I was using this to fix the Login Keyring error that is common with Fedora, but it no longer works. sudo dnf -y install seahorse \u0026amp;\u0026amp; seahorse\nApplications \u0026gt; Passwords and Keys \u0026gt; Passwords \u0026gt; Right-click Login \u0026gt; Change Password to blank.\nhttps://itsfoss.com/seahorse/\nInitialize Chezmoi # Chezmoi let\u0026rsquo;s you easy sync your dotfiles with Github and your other computers. Just init Chezmoi and add your Github username. This assumes your dotfiles in Github are saved in the proper format. `chezmoi init \u0026ndash;apply linuxreader\nAdd badname user (if needed) # If you need to use username with the format firstname.lastname, use the badname flag with the adduser command. You will have to create a normal user first, because you can\u0026rsquo;t do this during the initial install: $ adduser --badname firstname.lastname $ sudo usermod -aG wheel username\n# uncomment this line in the visudo file $ sudo visudo %wheel ALL=(ALL) ALL Delete the other user: $ userdel username\nAdditional DNF stuff: # Clear cache (do this occasionally): sudo dnf clean dbcache or sudo dnf clean all\nUpdate DNF: sudo dnf -y update\nAdditional DNF commands: https://docs.fedoraproject.org/en-US/fedora/latest/system-administrators-guide/package-management/DNF/\nSet up RPM Fusion # RPM Fusion give you more accessibility to various software packages.\nInstall:\nsudo dnf -y install https://mirrors.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm https://mirrors.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm https://rpmfusion.org/\nAppStream metadata # Use AppStream to enable users to install packages using Gnome Software/KDE Discover:\nsudo dnf -y groupupdate core Flatpaks # To enable Flatpaks (this may no longer be needed:\nflatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo https://flatpak.org/setup/Fedora\nSet a hostname # Set a hostname for the system. This will show after next reboot: sudo hostnamectl set-hostname \u0026quot;New_Custom_Name\u0026quot;\nOther Stuff # Here is some other stuff I install from the software center.\nInput Leap # Input Leap let\u0026rsquo;s you share a mouse and keyboard between two workstations.\nI don\u0026rsquo;t know what this is or why it is here: installing Git and a bunch of other stuff?\nsudo dnf install git cmake make gcc-c++ xorg-x11-server-devel \\ libcurl-devel avahi-compat-libdns_sd-devel \\ libXtst-devel qt5-qtbase qt5-qtbase-devel \\ qt5-qttools-devel libICE-devel libSM-devel \\ openssl-devel libXrandr-devel libXinerama-devel Virtual machine Manager # The best way to manage VMs on desktop.\nBox Buddy # For managing containers.\nWarehouse # Managing installed applications.\nMission Center # Task Manager like application.\nPika Backup # For backing up your desktop. Clapper # Video player.\nAnd some extensions installed through Extension Manager: Install Gnome Screenshot tool:\nInstall the extention: https://extensions.gnome.org/extension/1112/screenshot-tool/\nYou also need to install from DNF for some reason: dnf install -y gnome-screenshot\nAirpods not pairing Issue # If you ever have the issue where Airpods won\u0026rsquo;t pair. Remove them from the pairing list, force them in pairing mode, and pair them back. This can be made easy with bluetoothctl:\nJust in case, restart the bluetooth service:\nsudo systemctl restart bluetooth systemctl status bluetooth Show devices:\n# bluetoothctl [bluetooth] $ devices Device 42:42:42:42:42:42 My AirPods \u0026lt;-- grab the name here [bluetooth] $ remove 42:42:42:42:42:42 Now, make sure your Airpods are in the charging case, close the lid, wait 15 seconds, then open the lid. Press and hold the setup button on the case for up to 10 seconds. The status light should flash white, which means that your Airpods are ready to connect.\nPair them back:\n[bluetooth] $ pair 42:42:42:42:42:42 Enable Fractional Scaling # This lets you change display scaling in smaller increments. You\u0026rsquo;ll need to make sure Wayland is turned on.\nTurn on the feature then reboot: gsettings set org.gnome.mutter experimental-features \u0026quot;['scale-monitor-framebuffer']\u0026quot;\nreboot\nRemove pasted characters ^ # Kept having pasted format characters ^ mess up my groove. Here\u0026rsquo;s the fix.\nOpen your inputrc file and add the line below: vim ~/.inputrc\n\u0026#34;\\C-v\u0026#34;: \u0026#34;\u0026#34; Install gimp # sudo dnf install gimp enable in screenshot tool after install for gimp {f}\nInstall Arrows https://graphicdesign.stackexchange.com/questions/44797/how-do-i-insert-arrows-into-a-picture-in-gimp\nGo to your home folder Go to .config/GIMP Go to the folder with a version number (2.10 for me) Go to scripts Download the arrow.scm file and place it here. Don\u0026#39;t forget to unzip. Open GIMP and draw a path From Tools menu, select Arrow\n= h.265 main 10 profile media codec error =\nDistrobox # See distrobox\nZSH For Humans # GitHub - romkatv/zsh4humans: A turnkey configuration for Zsh\nInstall Starcraft on Fedora # https://www.youtube.com/watch?v=eefsL9K2w4k\nInstall your latest gpu driver https://github.com/lutris/docs/blob/master/InstallingDrivers.md I am just running off of built in AMD graphics. So we just need to install support for Vulkan API sudo dnf install vulkan-loader vulkan-loader.i686\nInstall Wine $ sudo dnf -y install wine\nInstall Lutris Install the Flatpak version in software center.\nFedora Hotkeys # Terminal # Close Terminal shift + c + q\nPrevious Tab c + Page Up\nNext Tab c + Page Down\nMove to Specific Tab Alt + #\nFull Screen F11\nNew Window Shift + Ctrl + t\nClose Tab Shift + Ctrl + w\nDesktop # Run a command super + F2\nSwitch Between Applications Alt + Esc\nMove Window to Left Monitor Shift + Super + \u0026lt;-\nMove Window to Right Monitor Shift + Super + -\u0026gt;\nMinimize Current Window Super + H\nClose Current Appllication Ctrl + Q\nBrowser # Firefox # Switch Between Tabs Ctrl + Tab\nSwitch Between Tabs in Reverse Ctrl + Shift + Tab\nDetach Tab Extension # https://addons.mozilla.org/en-US/firefox/addon/detach-tab/\nDetach Tab Ctrl _ Shift _ Space\nReattach Tab Ctrl + Shift + v\nSlack # Installing via package manager because of screen sharing issue.\nUpgrade dnf and download the slack rpm from the website.\nScreen Sharing in Slack:\nvim /usr/share/applications/slack.desktop Update the exec line to:\nExec=/usr/bin/slack --enable-features=WebRTCPipeWireCapturer %U Actual Budget # https://github.com/actualbudget/actual\nSet Vim to default editor for visudo # add Defaults editor=/usr/bin/vim to top of visudo file.su\n","externalUrl":null,"permalink":"/tech/tools/my-fedora-setup/","section":"Teches","summary":"\u003cp\u003eWhen you first download Fedora Workstation, it\u0026rsquo;s going to be a little hard to figure out how to make it usable. Especially if you\u0026rsquo;ve never tinkered with Linux before.\u003c/p\u003e\n\u003cp\u003eThis is because Fedora with Gnome desktop is a blank canvas. The point is to let you customize it to your needs. When I first install Fedora, I pull my justfile to install most of the programs I use:\u003c/p\u003e","title":"My Fedora Setup","type":" "},{"content":" NFS Basics and Configuration # Same tools for mounting and unmounting a filesystem.\nMounted and accessed the same way as local filesystems. Network protocol that allows file sharing over the network. Multi-platform Multiple clients can access a single share at the same time. Reduced overhead and storage cost. Give users access to uniform data. Consolidate scattered user home directories. May cause client to hang if share is not accessible. Share stays mounted until manually unmounted or the client shuts down. Does not support wildcard characters or environment variables. NFS Supported versions # RHEL 9 Supports versions 3,4.0,4.1, and 4.2 (default) NFSv3 supports: TCP and UDP. asynchronous writes. 64-bit files sizes. Access files larger than 2GB. NFSv4.x supports: All features of NFSv3. Transit firewalls and work on internet. Enhanced security and support for encrypted transfers and ACLs. Better scalability Better cross-platform Better system crash handling Use usernames and group names rather than UID and GID. Uses TCP by default. Can use UDP for backwards compatibility. Version 4.2 only supports TCP Network File System service # Export shares to mount on remote clients Exporting When the NFS server makes shares available. Mounting When a client mounts an exported share locally. Mount point should be empty before trying to mount a share on it. System can be both client and server. Entire directory tree of the share is shared. Cannot re-share a subdirectory of a share. A mounted share cannot be exported from the client. A single exported share is mounted on a directory mount point. Make sure to update the fstab file on the client. NFS Server and Client Configuration # How to export a share # Add entry of the share to /etc/exports using exportfs command Add firewall rule to allow access Mount a share from the client side # Use mount and add the filesystem to the fstab file. Lab: Export Share on NFS Server # Install nfs-utils sudo dnf -y install nfs-utils Create /common sudo mkdir /common Add full permissions sudo chmod 777 /common Add NFS service persistently to the firewalld configuration to allow NFS traffic and load the new rule: sudo firewall-cmd --permanent --add-service nfs sudo firewall-cmd --reload Start the NFS service and enable it to autostart at system reboots: sudo systemctl --now enable nfs-server Verify Operational Status of the NFS services: sudo systemctl status nfs-server Open /etc/exports and add entry for /common to export it to server10 with read/write: /common server10(rw) Export the entry defined in /etc/exports/. -a option exports all entries in the file. -v is verbose. sudo exportfs -av Unexport the share (-u): sudo exportfs -u server10:/common Re-export the share: sudo exportfs -av LAB: Mount share on NFS client # Install nfs-utils sudo dnf -y install nfs-utils Create /local mount point sudo mkdir /local Mount the share manually: sudo mount server20:/common /local Confirm using mount: (shows nfs version) mount | grep local Confirm using df: df -h | grep local Add to /fstab for persistence: server20:/common /local nfs _netdev 0 0 Note:\n_netdev option makes system wait for networking to come up before trying to mount the share. Unmount share manually using umount then remount to validate accuracy of the entry in /fstab: sudo umount /local sudo mount -a Verify: df -h Create a file in /local/ and verify: touch /local/nfsfile ls -l /local Confirm the sync on server 2 ls -l /common/ Update fstab ","externalUrl":null,"permalink":"/tech/linux/network-file-system-nfs/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eNFS Basics and Configuration\n    \u003cdiv id=\"nfs-basics-and-configuration\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#nfs-basics-and-configuration\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eSame tools for mounting and unmounting a filesystem.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMounted and accessed the same way as local filesystems.\u003c/li\u003e\n\u003cli\u003eNetwork protocol that allows file sharing over the network.\u003c/li\u003e\n\u003cli\u003eMulti-platform\u003c/li\u003e\n\u003cli\u003eMultiple clients can access a single share at the same time.\u003c/li\u003e\n\u003cli\u003eReduced overhead and storage cost.\u003c/li\u003e\n\u003cli\u003eGive users access to uniform data.\u003c/li\u003e\n\u003cli\u003eConsolidate scattered user home directories.\u003c/li\u003e\n\u003cli\u003eMay cause client to hang if share is not accessible.\u003c/li\u003e\n\u003cli\u003eShare stays mounted until manually unmounted or the client shuts down.\u003c/li\u003e\n\u003cli\u003eDoes not support wildcard characters or environment variables.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eNFS Supported versions\n    \u003cdiv id=\"nfs-supported-versions\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#nfs-supported-versions\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRHEL 9 Supports versions 3,4.0,4.1, and 4.2 (default)\u003c/li\u003e\n\u003cli\u003eNFSv3 supports:\n\u003cul\u003e\n\u003cli\u003eTCP and UDP.\u003c/li\u003e\n\u003cli\u003easynchronous writes.\u003c/li\u003e\n\u003cli\u003e64-bit files sizes.\u003c/li\u003e\n\u003cli\u003eAccess files larger than 2GB.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNFSv4.x supports:\n\u003cul\u003e\n\u003cli\u003eAll features of NFSv3.\u003c/li\u003e\n\u003cli\u003eTransit firewalls and work on internet.\u003c/li\u003e\n\u003cli\u003eEnhanced security and support for encrypted transfers and ACLs.\u003c/li\u003e\n\u003cli\u003eBetter scalability\u003c/li\u003e\n\u003cli\u003eBetter cross-platform\u003c/li\u003e\n\u003cli\u003eBetter system crash handling\u003c/li\u003e\n\u003cli\u003eUse usernames and group names rather than UID and GID.\u003c/li\u003e\n\u003cli\u003eUses TCP by default.\u003c/li\u003e\n\u003cli\u003eCan use UDP for backwards compatibility.\u003c/li\u003e\n\u003cli\u003eVersion 4.2 only supports TCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eNetwork File System service\n    \u003cdiv id=\"network-file-system-service\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#network-file-system-service\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eExport shares to mount on remote clients\u003c/li\u003e\n\u003cli\u003eExporting\n\u003cul\u003e\n\u003cli\u003eWhen the NFS server makes shares available.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMounting\n\u003cul\u003e\n\u003cli\u003eWhen a client mounts an exported share locally.\u003c/li\u003e\n\u003cli\u003eMount point should be empty before trying to mount a share on it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSystem can be both client and server.\u003c/li\u003e\n\u003cli\u003eEntire directory tree of the share is shared.\u003c/li\u003e\n\u003cli\u003eCannot re-share a subdirectory of a share.\u003c/li\u003e\n\u003cli\u003eA mounted share cannot be exported from the client.\u003c/li\u003e\n\u003cli\u003eA single exported share is mounted on a directory mount point.\u003c/li\u003e\n\u003cli\u003eMake sure to update the fstab file on the client.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eNFS Server and Client Configuration\n    \u003cdiv id=\"nfs-server-and-client-configuration\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#nfs-server-and-client-configuration\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\n\u003ch3 class=\"relative group\"\u003eHow to export a share\n    \u003cdiv id=\"how-to-export-a-share\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#how-to-export-a-share\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAdd entry of the share to \u003cem\u003e/etc/exports\u003c/em\u003e using \u003ccode\u003eexportfs\u003c/code\u003e command\u003c/li\u003e\n\u003cli\u003eAdd firewall rule to allow access\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eMount a share from the client side\n    \u003cdiv id=\"mount-a-share-from-the-client-side\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#mount-a-share-from-the-client-side\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUse \u003ccode\u003emount\u003c/code\u003e and add the filesystem to the fstab file.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eLab: Export Share on NFS Server\n    \u003cdiv id=\"lab-export-share-on-nfs-server\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#lab-export-share-on-nfs-server\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eInstall nfs-utils\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e sudo dnf -y install nfs-utils\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eCreate /common\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e sudo mkdir /common\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eAdd full permissions\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e sudo chmod 777 /common\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eAdd NFS service persistently to the firewalld configuration to allow NFS traffic and load the new rule:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo firewall-cmd --permanent --add-service nfs\nsudo firewall-cmd --reload\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eStart the NFS service and enable it to autostart at system reboots:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo systemctl --now enable nfs-server\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eVerify Operational Status of the NFS services:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo systemctl status nfs-server\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eOpen \u003cem\u003e/etc/exports\u003c/em\u003e and add entry for \u003cem\u003e/common\u003c/em\u003e to export it to server10 with read/write:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/common server10(rw)\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eExport the entry defined in \u003cem\u003e/etc/exports\u003c/em\u003e/. -a option exports all entries in the file. -v is verbose.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo exportfs -av\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eUnexport the share (-u):\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo exportfs -u server10:/common\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eRe-export the share:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo exportfs -av\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 class=\"relative group\"\u003eLAB: Mount share on NFS client\n    \u003cdiv id=\"lab-mount-share-on-nfs-client\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#lab-mount-share-on-nfs-client\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eInstall nfs-utils\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo dnf -y install nfs-utils\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eCreate /local mount point\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo mkdir /local\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eMount the share manually:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo mount server20:/common /local\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eConfirm using mount: (shows nfs version)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emount | grep local\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eConfirm using df:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edf -h | grep local\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003eAdd to /fstab for persistence:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eserver20:/common /local nfs _netdev 0 0\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNote:\u003c/p\u003e","title":"Network File System (NFS)","type":"tech"},{"content":" Hostname # \u0026ldquo;-\u0026rdquo;, \u0026ldquo;_ \u0026ldquo;, and \u0026ldquo;. \u0026quot; characters are allowed. Up to 253 characters. Stored in /etc/hostname. Can be viewed with several different commands, such as hostname, hostnamectl, uname, and nmcli, as well as by displaying the content of the /etc/hostname file. View the hostname:\nhostnamectl --static hostname uname -n cat /etc/hostname Lab: Change the Hostname # Server1\nOpen /etc/hostname and change the entry to server10.example.com Restart the systemd-hostnamed service daemon sudo systemctl restart systemd-hostnamed confirm hostname server2\nChange the hostname with hostnamectl: sudo hostnamectl set-hostname server21.example.com Log out and back in for the prompt to update\nChange the hostname using nmcli\nnmcli general hostname server20.example.com Hardware and IP Addressing # Ethernet Address # 48-bit address that is used to identify the correct destination node for data packets transmitted from the source node. The data packets include hardware addresses for the source and the destination node. Also referred to as the hardware, physical, link layer, or MAC address. List all network interfaces with their ethernet addresses:\nip addr | grep ether Subnetting # Network address space is divided into several smaller and more manageable logical subnetworks (subnets). Benefits: Reduced network traffic Improved network performance de-centralized and easier administration uses the node bits only Results in the reduction of usable addresses. All nodes in a given subnet have the same subnet mask. Each subnet acts as an isolated network and requires a router to talk to other subnets. The first and the last IP address in a subnet are reserved. The first address points to the subnet itself, and the last address is the broadcast address. IPv4 # View current ipv4 address:\nip addr Classful Network Addressing # See classfulipv4\nIPv6 Address # See ipv6\nThe ip addr command also shows IPv6 addresses for the interfaces:\n[root@server200 ~]# ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:b9:4e:ef brd ff:ff:ff:ff:ff:ff inet 172.16.1.155/20 brd 172.16.15.255 scope global dynamic noprefixroute enp0s3 valid_lft 79061sec preferred_lft 79061sec inet6 fe80::a00:27ff:feb9:4eef/64 scope link noprefixroute valid_lft forever preferred_lft forever Tools:\nping6 traceroute6 tracepath6 Protocols # Defined in /etc/protocols Well known ports are defined in /etc/services cat /etc/protocols\nTCP and UDP Protocols # See IP Transport and Applications and tcp_ip_basic\nICMP # Send two pings to server 20\nping -c2 192.168.0.120 Ping the server\u0026rsquo;s loopback interface:\nping 127.0.0.1 Send a traceroute to server 20\ntraceroute 192.168.0.120 Or:\ntracepath 192.168.0.120 ICMPv6 # IPv6 version of ICMP enabled by default Ping and ipv6 address:\nping6 Trace a route to an IPv6 address:\ntracepath6 traceroute6 Show IPv6 addresses:\nip addr | grep inet6 Network Manager Service # Default service in RHEL for network:\ninterface and connection configuration. Administration. Monitoring. NetworkManager daemon\nResponsible for keeping interfaces and connection up and active. Includes: nmcli nmtui (text-based) nm-connection-editor (GUI) Does not manage loopback interfaces. Interface Connection Profiles # Configuration file on each interface that defines IP assignments and other relevant parameters for it.\nThe networking subsystem reads this file and applies the settings at the time the connection is activated.\nConnection configuration files (or connection profiles) are stored in a central location under the /etc/NetworkManager/system-connections directory.\nThe filenames are identified by the interface connection names with nmconnection as the extension.\nSome instances of connection profiles are: enp0s3.nmconnection, ens160.nmconnection, and em1.nmconnection.\nOn server10 and server20, the device name for the first interface is enp0s3 with connection name enp0s3 and relevant connection information stored in the enp0s3.nmconnection file.\nThis connection was established at the time of RHEL installation. The current content of the file from server10 are presented below:\n[root@server200 system-connections]# cat /etc/NetworkManager/system-connections/enp0s3.nmconnection [connection] id=enp0s3 uuid=45d6a8ea-6bd7-38e0-8219-8c7a1b90afde type=ethernet autoconnect-priority=-999 interface-name=enp0s3 timestamp=1710367323 [ethernet] [ipv4] method=auto [ipv6] addr-gen-mode=eui64 method=auto [proxy] Each section defines a set of networking properties for the connection. Directives\nid\nAny description given to this connection. The default matches the interface name. uuid\nThe UUID associated with this connection type\nSpecifies the type of this connection autoconnect-priority\nIf the connection is set to autoconnect, connections with higher priority will be preferred. A higher number means higher priority. The range is between -999 and 999 with 0 being the default. interface_name\nSpecifies the device name for the network interface timestamp\nThe time, in seconds since the Unix Epoch that the connection was last activated successfully. This field is automatically populated each time the connection is activated. address1/method\nSpecifies the static IP for the connection if the method property is set to manual. /24 represents the subnet mask. addr-gen-mode/method\nGenerates an IPv6 address based on the hardware address of the interface. View additional directives:\nman nm-settings Naming rules for devices are governed by udevd service based on:\nDevice location Topology setting in firmware virtualization layer Understanding Hosts Table # See DNS and Time Synchronization\n/etc/hosts file\nTable used to maintain hostname to IP mapping for systems on the local network, allowing us to access a system by simply employing its hostname. Each row in the file contains an IP address in column 1 followed by the official (or canonical) hostname in column 2, and one or more optional aliases thereafter.\nEXAM TIP: In the presence of an active DNS with all hostnames resolvable, there is no need to worry about updating the hosts file.\nAs expressed above, the use of the hosts file is common on small networks, and it should be updated on each individual system to reflect any changes for best inter-system connectivity experience.\nNetworking DIY Challenge Labs # Lab: Update Hosts Table and Test Connectivity. # Add both server10 and server20\u0026rsquo;s interfaces to both server\u0026rsquo;s /etc/host files: 192.168.0.110 server10.example.com server10 \u0026lt;-- This is an alias 192.168.0.120 server20.example.com server20 172.10.10.110 server10s8.example.com server10s8 172.10.10.120 server20s8.example.com server20s8 Send 2 packets from server10 to server20\u0026rsquo;s IP address: ping -c2 192.168.0.120 Send 2 pings from server10 to server20\u0026rsquo;s hostname: ping -c2 server20 Lab 15-1: Add New Interface and Configure Connection Profile with nmcli # Add a third network interface to rhel9server40 in VirtualBox. As user1 with sudo on server40, run ip a and verify the addition of the new interface. Use the nmcli command and assign IP 192.168.0.40/24 and gateway 192.168.0.1 [root@server40 ~]# nmcli c a type Ethernet ifname enp0s8 con-name enp0s8 ip4 192.168.0.40/24 gw4 192.168.0.1 Deactivate and reactivate this connection manually. [root@server40 ~]# nmcli c d enp0s8 Connection \u0026#39;enp0s8\u0026#39; successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3) [root@server40 ~]# nmcli c s NAME UUID TYPE DEVICE enp0s3 6e75a5e4-869b-3ed1-bdc4-c55d2d268285 ethernet enp0s3 lo 66809437-d3fa-4104-9777-7c3364b943a9 loopback lo enp0s8 9a32e279-84c2-4bba-b5c5-82a04f40a7df ethernet -- [root@server40 ~]# nmcli c u enp0s8 Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/4) [root@server40 ~]# nmcli c s NAME UUID TYPE DEVICE enp0s3 6e75a5e4-869b-3ed1-bdc4-c55d2d268285 ethernet enp0s3 enp0s8 9a32e279-84c2-4bba-b5c5-82a04f40a7df ethernet enp0s8 lo 66809437-d3fa-4104-9777-7c3364b943a9 loopback lo Add entry server40 to server30\u0026rsquo;s hosts table. [root@server30 ~]# vim /etc/hosts [root@server30 ~]# ping server40 PING server40.example.com (192.168.0.40) 56(84) bytes of data. 64 bytes from server40.example.com (192.168.0.40): icmp_seq=1 ttl=64 time=3.20 ms 64 bytes from server40.example.com (192.168.0.40): icmp_seq=2 ttl=64 time=0.628 ms 64 bytes from server40.example.com (192.168.0.40): icmp_seq=3 ttl=64 time=0.717 ms ^C --- server40.example.com ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2009ms rtt min/avg/max/mdev = 0.628/1.516/3.204/1.193 ms Lab: Add New Interface and Configure Connection Profile Manually (server30) # Add a third network interface to RHEL9server30 in VirtualBox.\nrun ip a and verify the addition of the new interface.\nUse the nmcli command and assign IP 192.168.0.30/24 and gateway 192.168.0.1\nnmcli c a type Ethernet ifname enp0s8 con-name enp0s8 ip4 192.168.0.30/24 gw4 192.168.0.1 Deactivate and reactivate this connection manually. Add entry server30 to the hosts table of server 40\n[root@server30 system-connections]# nmcli c d enp0s8 Connection \u0026#39;enp0s8\u0026#39; successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3) [root@server30 system-connections]# nmcli c u enp0s8 Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/4) /etc/hosts 192.168.0.30 server30.example.com server30 ping tests to server30 from server 40\n[root@server40 ~]# ping server30 PING server30.example.com (192.168.0.30) 56(84) bytes of data. 64 bytes from server30.example.com (192.168.0.30): icmp_seq=1 ttl=64 time=1.59 ms 64 bytes from server30.example.com (192.168.0.30): icmp_seq=2 ttl=64 time=0.474 ms ^C --- server30.example.com ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 999ms rtt min/avg/max/mdev = 0.474/1.032/1.590/0.558 ms Or create the profile manually and restart network manager:\n[connection] id=enp0s8 type=ethernet interface-name=enp0s8 uuid=92db4c65-2f13-4952-b81f-2779b1d24a49 [ethernet] [ipv4] method=manual address1=10.1.13.3/24,10.1.13.1 [ipv6] addr-gen-mode=default method=auto [proxy] Administration Tools # ip Display monitor and manage network interfaces, routing, connections, traffic, etc. ifup\nBrings up an interface ifdown\nBrings down an interface nmcli\nCreates, updates, deletes, activates, and deactivates a connection profile. nmcli command # Create, view, modify, remove, activate, and deactivate network connections. Control and report network device status. Supports abbreviation of commands. Operates on 7 different object categories.\ngeneral networking connection (c)(con) device (d)(dev) radio monitor agent [root@server200 system-connections]# nmcli --help Usage: nmcli [OPTIONS] OBJECT { COMMAND | help } OPTIONS -a, --ask ask for missing parameters -c, --colors auto|yes|no whether to use colors in output -e, --escape yes|no escape columns separators in values -f, --fields \u0026lt;field,...\u0026gt;|all|common specify fields to output -g, --get-values \u0026lt;field,...\u0026gt;|all|common shortcut for -m tabular -t -f -h, --help print this help -m, --mode tabular|multiline output mode -o, --overview overview mode -p, --pretty pretty output -s, --show-secrets allow displaying passwords -t, --terse terse output -v, --version show program version -w, --wait \u0026lt;seconds\u0026gt; set timeout waiting for finishing operations OBJECT g[eneral] NetworkManager\\\u0026#39;s general status and operations n[etworking] overall networking control r[adio] NetworkManager radio switches c[onnection] NetworkManager\\\u0026#39;s connections d[evice] devices managed by NetworkManager a[gent] NetworkManager secret agent or polkit agent m[onitor] monitor NetworkManager changes 3. connection # Activates, deactivates, and administers network connections. Options:\nshow (list connections) up/down (Brings connection up or down) add(a) (adds a connection) edit (edit connection or add a new one) modify (modify properties of a connection) delete(d) (delete a connection) reload (re-read all connection profiles) load (re-read a connection profile) 4. Device # Options:\nstatus (Displays device status) show (Displays info about device(s) Show all connections, inactive or active:\nnmcli c s Deactivate the connection enp0s8:\nsudo nmcli c down enp0s8 Note:\nThe connection profile gets detached from the device, disabling the connection. Activate the connection enp0s8:\n$ sudo nmcli c up enp0s8 # connection profile re-attaches to the device. Display the status of all network devices:\nnmcli d s Lab: Add Network Devices to server10 and one to server20 using VirtualBox # Shut down your servers (follow each step for both servers) sudo shutdown now Add network interface in Virtualbox then power on the VMs Select machine \u0026gt; settings \u0026gt; Network \u0026gt; Adapter 2 \u0026gt; Enable Network Adapter \u0026gt; Internal Network \u0026gt; ok Verify the new interfaces: ip a Lab: Configure New Network Connection Using nmcli (server20) # Verify the interface that was added from virtualbox: nmcli d status | grep enp Add connection profile and attach it to the interface: sudo nmcli c a type Ethernet ifname enp0s8 con-name enp0s8 ip4 172.10.10.120/24 gw4 172.10.10.1 Confirm connection status nmcli d status | grep enp Verify ip address ip a Check the content of the connection profile cat /etc/NetworkManager/system-connections/enp0s8.nmconnection ","externalUrl":null,"permalink":"/tech/linux/networking-network-devices-and-network-connections/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eHostname\n    \u003cdiv id=\"hostname\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#hostname\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026ldquo;-\u0026rdquo;, \u0026ldquo;_ \u0026ldquo;, and \u0026ldquo;. \u0026quot; characters are allowed.\u003c/li\u003e\n\u003cli\u003eUp to 253 characters.\u003c/li\u003e\n\u003cli\u003eStored in \u003cem\u003e/etc/hostname\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eCan be viewed with several different commands, such as \u003ccode\u003ehostname\u003c/code\u003e, \u003ccode\u003ehostnamectl\u003c/code\u003e, \u003ccode\u003euname\u003c/code\u003e, and \u003ccode\u003enmcli\u003c/code\u003e, as well as by displaying the content of the \u003cem\u003e/etc/hostname\u003c/em\u003e file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eView the hostname:\u003c/p\u003e","title":"Networking Network Devices and Network Connections","type":"tech"},{"content":"I\u0026rsquo;m going to show you how to set up your own, self-hosted Nextcloud server using Alma Linux 9 and Apache.\nWhat is Nextcloud? # Nextcloud is so many things. It offers so many features and options, it deserves a bulleted list:\nFree and open source Cloud storage and syncing Email client Custom browser dashboard with widgets Office suite RSS newsfeed Project organization (deck) Notebook Calender Task manager Connect to decentralized social media (like Mastodon) Replacement for all of google\u0026rsquo;s services Create web forms or surveys It is also free and open source. This mean the source code is available to all. And hosting yourself means you can guarantee that your data isn\u0026rsquo;t being shared.\nAs you can see. Nextcloud is feature packed and offers an all in one solution for many needs. The set up is fairly simple.\nYou will need:\nDomain hosted through CloudFlare or other hosting. Server with Alma Linux 9 with a dedicated public ip address. Nextcloud dependencies:\nPHP 8.3 Apache sql database (This tutorial uses MariaDB) Official docs: https://docs.nextcloud.com/server/latest/admin_manual/installation/source_installation.html\nServer Specs # Hard drives: 120g main 500g data 250g backup\nOS: Alma 9 CPU: 4 sockets 8 cores RAM: 32768\nip 10.0.10.56/24 root: { password } davidt: { password }\nStorage setup # mkdir /var/www/nextcloud/ -p mkdir /home/databkup parted /dev/sdb mklabel gpt parted /dev/sdb mkpart primary 0% 100% parted /dev/sdc mklabel gpt parted /dev/sdc mkpart primary 0% 100% mkfs.xfs /dev/sdb1 mkfs.xfs /dev/sdc1 lsblk blkid /dev/sdb1 \u0026gt;\u0026gt; /etc/fstab blkid /dev/sdc1 \u0026gt;\u0026gt; /etc/fstab vim /etc/fstab mount -a [root@dt-lab2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 120G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 119G 0 part ├─almalinux-root 253:0 0 70G 0 lvm / ├─almalinux-swap 253:1 0 12G 0 lvm [SWAP] └─almalinux-home 253:2 0 37G 0 lvm /home sdb 8:16 0 500G 0 disk └─sdb1 8:17 0 500G 0 part /var/www/nextcloud sdc 8:32 0 250G 0 disk └─sdc1 8:33 0 250G 0 part /home/databkup Setting up dependencies # Install latest supported PHP # I used this guide to help get a supported php version. As php 2 installed from dnf repos by default: https://orcacore.com/php83-installation-almalinux9-rockylinux9/\nMake sure dnf is up to date:\nsudo dnf update -y sudo dnf upgrade -y Set up the epel repository:\nsudo dnf install epel-release -y Set up remi to manage php modules:\nsudo dnf install -y dnf-utils http://rpms.remirepo.net/enterprise/remi-release-9.rpm sudo dnf update -y Remove old versions of php:\nsudo dnf remove php* -y List available php streams:\nsudo dnf module list reset php -y Last metadata expiration check: 1:03:46 ago on Sun 29 Dec 2024 03:34:52 AM MST. AlmaLinux 9 - AppStream Name Stream Profiles Summary php 8.1 common [d], devel, minimal PHP scripting language php 8.2 common [d], devel, minimal PHP scripting language Remi\u0026#39;s Modular repository for Enterprise Linux 9 - x86_64 Name Stream Profiles Summary php remi-7.4 common [d], devel, minimal PHP scripting language php remi-8.0 common [d], devel, minimal PHP scripting language php remi-8.1 common [d], devel, minimal PHP scripting language php remi-8.2 common [d], devel, minimal PHP scripting language php remi-8.3 [e] common [d], devel, minimal PHP scripting language php remi-8.4 common [d], devel, minimal PHP scripting language Enable the correct stream:\nsudo dnf module enable php:remi-8.3 Now the default to install is version 8.3, install it like this:\nsudo dnf install php -y php -v Let\u0026rsquo;s install git, as it\u0026rsquo;s also needed in this setup: sudo dnf -y install git\nInstall Composer for managing php modules:\ncd \u0026amp;\u0026amp; curl -sS https://getcomposer.org/installer | php sudo mv composer.phar /usr/local/bin/composer Install needed PHP modules: sudo dnf -y install php-process php-zip php-gd php-mysqlnd php-ldap php-imagick php-bcmath php-gmp php-intl\nUpgrade php memory limit: sudo vim /etc/php.ini\nmemory_limit = 512M Apache setup # Add Apache config for vhost: sudo vim /etc/httpd/conf.d/nextcloud.conf\n\u0026lt;VirtualHost *:80\u0026gt; DocumentRoot /var/www/nextcloud/ ServerName cloud.{ site-name }.com \u0026lt;Directory /var/www/nextcloud/\u0026gt; Require all granted AllowOverride All Options FollowSymLinks MultiViews \u0026lt;IfModule mod_dav.c\u0026gt; Dav off \u0026lt;/IfModule\u0026gt; \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Set up the mysql database # Install: sudo dnf install mariadb-server -y\nEnable the service: sudo systemctl enable --now mariadb\nNextcloud needs some tables setup in order to store information in a database. First set up a secure sql database:\nsudo mysql_secure_installation Say “Yes” to the prompts and enter root password:\nSwitch to unix_socket authentication [Y/n]: Y Change the root password? [Y/n]: Y\t# enter password. Remove anonymous users? [Y/n]: Y Disallow root login remotely? [Y/n]: Y Remove test database and access to it? [Y/n]: Y Reload privilege tables now? [Y/n]: Y Sign in to your SQL database with the password you just chose:\nmysql -u root -p Create the database: # While signed in with the mysql command, enter the commands below one at a time. Make sure to replace the username and password. But leave localhost as is:\nCREATE DATABASE nextcloud; GRANT ALL ON nextcloud.* TO \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;{ password }\u0026#39;; FLUSH PRIVILEGES; EXIT; Nextcloud Install # Download nextcloud onto the server. Extract the contents to /var/www/nextcloud tar -xjf nextcloud-31.0.4.tar.bz2 -C /var/www/ --strip-components=1\nChange the nextcloud folder ownership to apache and add permissions: sudo chmod -R 755 /var/www/nextcloud sudo chown -R apache:apache /var/www/nextcloud\nSelinux:\nsudo semanage fcontext -a -t httpd_sys_content_t \u0026#34;/var/www/nextcloud(/.*)?\u0026#34; \u0026amp;\u0026amp; \\ sudo semanage fcontext -a -t httpd_sys_rw_content_t \u0026#34;/var/www/nextcloud/(config|data|apps)(/.*)?\u0026#34; \u0026amp;\u0026amp; \\ sudo semanage fcontext -a -t httpd_sys_rw_content_t \u0026#34;/var/www/nextcloud/data(/.*)?\u0026#34; sudo restorecon -Rv /var/www/nextcloud/ Now we can actually install Nextcloud. cd to the /var/www/nextcloud directory and run occ with these settings to install:\nsudo -u apache php occ maintenance:install \\ --database=\u0026#39;mysql\u0026#39; --database-name=\u0026#39;nextcloud\u0026#39; \\ --database-user=\u0026#39;root\u0026#39; --database-pass=\u0026#39;{ password }\u0026#39; \\ --admin-user=\u0026#39;admin\u0026#39; --admin-pass=\u0026#39;{ password }\u0026#39; Create a CNAME record for DNS. # Before you go any further, you will need to have a domain name set up for your server. I use Cloudflare to manage my DNS records. You will want to make a CNAME record for your nextcloud subdomain.\nJust add \u0026ldquo;nextcloud\u0026rdquo; as the name and \u0026ldquo;yourwebsite.com\u0026rdquo; as the content. This will make it so \u0026ldquo;nextcloud.yourwebsite.com\u0026rdquo; is the site for your nextcloud dashboard. Also, make sure to select \u0026ldquo;DNS Only\u0026rdquo; under proxy status.\nHere\u0026rsquo;s what my CloudFlare domain setup looks with this blog as the main site, and cloud.perfectdarkmode.com as the nextcloud site:\nThen you need to update trusted domains in /var/www/nextcloud/config/config.php:\n\u0026#39;trusted_domains\u0026#39; =\u0026gt; [ \u0026#39;cloud.{ site-name }.com\u0026#39;, \u0026#39;localhost\u0026#39; ], Install Apache # Install: sudo dnf -y install httpd\nEnable: systemctl enable --now httpd\nRestart httpd systemctl restart httpd\nFirewall rules:\nsudo firewall-cmd --add-service https --permanent sudo firewall-cmd --add-service http --permanent sudo firewall-cmd --reload Install SSL with Certbot # Install certbot: sudo dnf install certbot python3-certbot-apache -y\nObtain an SSL certificate. (See my Obsidian site setup post for information about Certbot and Apache setup.)\nsudo certbot -d {subdomain}.{domain}.com Now log into nextcloud with your admin account using the DNS name you set earlier:\nI recommend setting up a normal user account instead of doing everything as \u0026ldquo;admin\u0026rdquo;. Just hit the \u0026ldquo;A\u0026rdquo; icon at the top right and go to \u0026ldquo;Accounts\u0026rdquo;. Then just select \u0026ldquo;New Account\u0026rdquo; and create a user account with whatever privileges you want.\nI may make a post about which Nextcloud apps I recommend and customize the setup a bit. Let me know if that\u0026rsquo;s something you\u0026rsquo;d like to see. That\u0026rsquo;s all for now.\nMake log dir: # mkdir /var/log/nextcloud touch /var/log/nextcloud.log chown apache:apache -R /var/log/nextcloud Change apps to read only # semanage fcontext -a -t httpd_sys_content_t \u0026#34;/var/www/nextcloud/apps(/.*)?\u0026#34; restorecon -R /var/www/nextcloud/apps Allow outbound network: # sudo setsebool -P httpd_can_network_connect 1 sudo setsebool -P httpd_graceful_shutdown 1 sudo setsebool -P httpd_can_network_relay 1 sudo ausearch -c \u0026#39;php-fpm\u0026#39; --raw | audit2allow -M my-phpfpm sudo semodule -X 300 -i my-phpfpm.pp Backup # mkdir /home/databkup chown -R apache:apache /home/databkup\nvim /root/cleanbackups.sh\n#!/bin/bash find /home/backup -type f -mtime +5 -exec rm {} \\; chmod +x /root/cleanbackups.sh\ncrontab -e\n# Clean up old backups every day at midnight 0 0 * * * /root/cleanbackups.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # Backup MySQL database every 12 hours 0 */12 * * * bash -c \u0026#39;/usr/bin/mysqldump --single-transaction -u root -p{password} nextcloud \u0026gt; /home/backup/nextclouddb-backup_$(date +\u0026#34;\\%Y\\%m\\%d\\%H\\%M\\%S\u0026#34;).bak\u0026#39; # Rsync Nextcloud data directory every day at midnight 15 0 * * * /usr/bin/rsync -Aavx /var/www/nextcloud/ /home/databkup/ --delete-before mkdir /home/backup\nUpdate Mariadb: # systemctl stop mariadb.service dnf module switch-to mariadb:10.11 systemctl start mariadb.service mariadb-upgrade --user=root --password=\u0026#39;{ password }\u0026#39; mariadb --version Mimetype migration error: sudo -u apache /var/www/nextcloud/occ maintenance:repair --include-expensive\nIndices error: sudo -u apache /var/www/nextcloud/occ db:add-missing-indices\nRedis install for memcache # This setup uses Redis for File locking and APCu for memcache\ndnf -y install redis php-pecl-redis php-pecl-apcu\nsystemctl enable --now redis\nAdd to config.php:\n\u0026#39;memcache.locking\u0026#39; =\u0026gt; \u0026#39;\\OC\\Memcache\\Redis\u0026#39;, \u0026#39;memcache.local\u0026#39; =\u0026gt; \u0026#39;\\OC\\Memcache\\APCu\u0026#39;, \u0026#39;redis\u0026#39; =\u0026gt; [ \u0026#39;host\u0026#39; =\u0026gt; \u0026#39;/run/redis/redis-server.sock\u0026#39;, \u0026#39;port\u0026#39; =\u0026gt; 0, ], Update /etc/redis/redis.conf vim /etc/redis/redis.conf change port to port 0/\nuncomment the socket options under \u0026ldquo;Unix Socket\u0026rdquo; and change to:\nunixsocket /run/redis/redis-server.sock unixsocketperm 770 Update permissions for redis usermod -a -G redis apache\nUncomment the line in /etc/php.d/40-apcu.ini and change from 32M to 256M vim /etc/php.d/40-apcu.ini apc.shm_size=256M\nRestart apache and redis:\nsystemctl restart redis systemctl restart httpd Added logging and phone region to config.php: mkdir /var/log/nextcloud/\n\u0026#39;log_type\u0026#39; =\u0026gt; \u0026#39;file\u0026#39;, \u0026#39;logfile\u0026#39; =\u0026gt; \u0026#39;/var/log/nextcloud/nextcloud.log\u0026#39;, \u0026#39;logfilemode\u0026#39; =\u0026gt; 416, \u0026#39;default_phone_region\u0026#39; =\u0026gt; \u0026#39;US\u0026#39;, \u0026#39;logtimezone\u0026#39; =\u0026gt; \u0026#39;America/Phoenix\u0026#39;, \u0026#39;loglevel\u0026#39; =\u0026gt; \u0026#39;1\u0026#39;, \u0026#39;logdateformat\u0026#39; =\u0026gt; \u0026#39;F d, Y H:i:s\u0026#39;, OP-Cache error # Change opcache.interned_strings_buffer to 16 and uncomment:\nvim /etc/php.d/10-opcache.ini\nopcache.interned_strings_buffer=16 systemctl restart php-fpm httpd\nLast job execution ran 2 months ago. Something seems wrong. # Set up cron job for the Apache user: crontab -u apache -e\nAdd to file that shows up:\n*/5 * * * * php -f /var/www/nextcloud/cron.php Other Errors # The \u0026ldquo;files_reminders\u0026rdquo; app needs the notification app to work properly. You should either enable notifications or disable files_reminder. Disabled files_reminder app\nServer has no maintenance window start time configured. This means resource intensive daily background jobs will also be executed during your main usage time. We recommend to set it to a time of low usage, so users are less impacted by the load caused from these heavy tasks. For more details see the documentation ↗. Added 'maintenance_window_start' =\u0026gt; 1, to config.php\nSome headers are not set correctly on your instance - The Strict-Transport-Security HTTP header is not set (should be at least 15552000 seconds). For enhanced security, it is recommended to enable HSTS. For more details see the documentation ↗. Added after closing directory line in SSL config:\nvim nextcloud-le-ssl.conf Header always set Strict-Transport-Security \u0026#34;max-age=15552000; includeSubDomains; preload\u0026#34; And add to the bottom of /var/www/nextcloud/.htaccess: \u0026quot;Header always set Strict-Transport-Security \u0026quot;max-age=15552000; includeSubDomains\u0026quot;\nChange this line in config.php from localhost to server name:\n\u0026#39;overwrite.cli.url\u0026#39; =\u0026gt; \u0026#39;http://cloud.{ site-name }.com\u0026#39;, Integrity checker has been disabled. Integrity cannot be verified. Ignore this\nYour webserver does not serve .mjs files using the JavaScript MIME type. This will break some apps by preventing browsers from executing the JavaScript files. You should configure your webserver to serve .mjs files with either the text/javascript or application/javascript MIME type.\nsudo vim /etc/httpd/conf.d/nextcloud.conf ad AddType text/javascript .mjs inside the virtual host block. Restart apache. Your web server is not properly set up to resolve \u0026ldquo;/ocm-provider/\u0026rdquo;. This is most likely related to a web server configuration that was not updated to deliver this folder directly. Please compare your configuration against the shipped rewrite rules in \u0026ldquo;.htaccess\u0026rdquo; for Apache add to vim /var/www/nextcloud/.htaccess\n\u0026lt;IfModule mod_rewrite.c\u0026gt; RewriteEngine on RewriteRule ^ocm-provider/(.*)$ /index.php/apps/ocm/$1 [QSA,L] \u0026lt;/IfModule\u0026gt; Your web server is not properly set up to resolve .well-known URLs, failed on: /.well-known/caldav\nadded to vim /var/www/nextcloud/.htaccess\n# .well-known URLs for CalDAV/CardDAV and other services \u0026lt;IfModule mod_rewrite.c\u0026gt; RewriteEngine On RewriteRule ^\\.well-known/caldav$ /remote.php/dav/ [R=301,L] RewriteRule ^\\.well-known/carddav$ /remote.php/dav/ [R=301,L] RewriteRule ^\\.well-known/webfinger$ /index.php/.well-known/webfinger [R=301,L] RewriteRule ^\\.well-known/nodeinfo$ /index.php/.well-known/nodeinfo [R=301,L] RewriteRule ^\\.well-known/acme-challenge/.*$ - [L] \u0026lt;/IfModule\u0026gt; PHP configuration option \u0026ldquo;output_buffering\u0026rdquo; must be disabled vim /etc/php.ini\noutput_buffering = Off\n[root@oort31 nextcloud]# echo \u0026#34;output_buffering=off\u0026#34; \u0026gt; .user.ini [root@oort31 nextcloud]# chown apache:apache .user.ini chmod 644 .user.ini [root@oort31 nextcloud]# systemctl restart httpd Installed tmux dnf -y install tmux\nApps # Disabled File Reminders app\nAdd fail2ban # sudo dnf -y install fail2ban\nvim /etc/fail2ban/jail.local\n[DEFAULT] bantime = 24h ignoreip = 10.0.0.0/8 usedns = no [sshd] enabled = true maxretry = 3 findtime = 43200 bantime = 86400 systemctl enable \u0026ndash;now fail2ban fail2ban-client status sshd\n","externalUrl":null,"permalink":"/tech/tools/nextcloud-on-rhel-based-systems/","section":"Teches","summary":"\u003cp\u003eI\u0026rsquo;m going to show you how to set up your own, self-hosted Nextcloud server using Alma Linux 9 and Apache.\u003c/p\u003e\n\n\u003ch3 class=\"relative group\"\u003eWhat is Nextcloud?\n    \u003cdiv id=\"what-is-nextcloud\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#what-is-nextcloud\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eNextcloud is so many things. It offers so many features and options, it deserves a bulleted list:\u003c/p\u003e","title":"Nextcloud on RHEL Based Systems","type":"tech"},{"content":"Server hosting the storage:\n--- - name: Install Packages package: name: - nfs-utils state: present - name: Ensure directories to export exist file: # noqa 208 path: \u0026#34;{{ item }}\u0026#34; state: directory with_items: \u0026#34;{{ nfs_exports | map(\u0026#39;split\u0026#39;) | map(\u0026#39;first\u0026#39;) | unique }}\u0026#34; - name: Copy exports file template: src: exports.j2 dest: /etc/exports owner: root group: root mode: 0644 notify: reload nfs - name: Add firewall rule to enable NFS service ansible.posix.firewalld: immediate: true state: enabled permanent: true service: nfs notify: reload firewalld - name: Start and enable NFS service service: name: nfs-server state: started enabled: yes when: nfs_exports|length \u0026gt; 0 - name: Set SELinux boolean for NFS ansible.posix.seboolean: name: nfs_export_all_rw state: yes persistent: yes - name: install required package for sefcontext module yum: name: policycoreutils-python-utils state: present - name: Set proper SELinux context on export dir sefcontext: target: /{{ item }}(/.*)? setype: nfs_t state: present notify: run restorecon with_items: \u0026#34;{{ nfs_exports | map(\u0026#39;split\u0026#39;) | map(\u0026#39;first\u0026#39;) | unique }}\u0026#34; {% for host in nfs_hosts %} /data {{ host }} (rw,wdelay,root_squash,no_subtree_check,sec=sys,rw,root_squash,no_all_squash) {% endfor %} Variables: nfs_exports:\n/data server(rw,wdelay,root_squash,no_subtree_check,sec=sys,rw,root_squash,no_all_squash) Handlers\n--- - name: reload nfs command: \u0026#39;exportfs -ra\u0026#39; - name: reload firewalld command: firewall-cmd --reload - name: run restorecon command: restorecon -Rv /codata storage:\nname: Detect secondary disk name ignore_errors: yes set_fact: disk2name: vda when: ansible_facts[\u0026#39;devices\u0026#39;][\u0026#39;vda\u0026#39;] is defined - name: Search for second disk, continue only if it is found assert: that: - disk2name is defined fail_msg: second hard disk not found - name: Debug detected disk debug: msg: \u0026#34;{{ disk2name }} was found. Moving forward.\u0026#34; - name: Create LVM and partitions block: - name: Create LVM Partition on second disk parted: name: data label: gpt device: /dev/{{ disk2name }} number: 1 state: present flags: [ lvm ] - name: Create an LVM volume group lvg: vg: vgcodata pvs: /dev/{{ disk2name }}1 - name: Create lv lvol: lv: lvdata size: 100%FREE vg: vgdata - name: create filesystem filesystem: dev: /dev/vgdata/lvdata fstype: xfs when: ansible_facts[\u0026#39;devices\u0026#39;][\u0026#39;vda\u0026#39;][\u0026#39;partitions\u0026#39;] is not defined - name: Create data directory file: dest: /data mode: 777 state: directory - name: Mount the filesystem mount: src: /dev/vgdata/lvdata fstype: xfs state: present path: /data - name: Set permissions on mounted filesystem file: path: /data state: directory mode: \u0026#39;0777\u0026#39; ``` ","externalUrl":null,"permalink":"/tech/ansible/nfs-setup/","section":"Teches","summary":"\u003cp\u003eServer hosting the storage:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nn\"\u003e---\u003c/span\u003e\u003cspan class=\"w\"\u003e \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eInstall Packages\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003epackage\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003enfs-utils\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003epresent\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eEnsure directories to export exist\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003efile\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"c\"\u003e# noqa 208\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003epath\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;{{ item }}\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003edirectory\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003ewith_items\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;{{ nfs_exports | map(\u0026#39;split\u0026#39;) | map(\u0026#39;first\u0026#39;) | unique }}\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eCopy exports file\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003etemplate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003esrc\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eexports.j2\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003edest\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e/etc/exports\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eowner\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eroot\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003egroup\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eroot\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003emode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"m\"\u003e0644\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003enotify\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ereload nfs\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eAdd firewall rule to enable NFS service\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003eansible.posix.firewalld\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eimmediate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eenabled\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003epermanent\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eservice\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003enfs\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003enotify\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003ereload firewalld\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eStart and enable NFS service\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003eservice\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003enfs-server\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003estarted\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003eenabled\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003eyes\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e     \u003c/span\u003e\u003cspan class=\"nt\"\u003ewhen\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003enfs_exports|length \u0026gt; 0\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eSet SELinux boolean for NFS\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003eansible.posix.seboolean\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003enfs_export_all_rw\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003eyes\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003epersistent\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"kc\"\u003eyes\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003einstall required package for sefcontext module\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003eyum\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003epolicycoreutils-python-utils\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003epresent\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e- \u003cspan class=\"nt\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003eSet proper SELinux context on export dir\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003esefcontext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003etarget\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003e/{{ item }}(/.*)?\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003esetype\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003enfs_t\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e      \u003c/span\u003e\u003cspan class=\"nt\"\u003estate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003epresent\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003enotify\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"l\"\u003erun restorecon\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003ewith_items\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;{{ nfs_exports | map(\u0026#39;split\u0026#39;) | map(\u0026#39;first\u0026#39;) | unique }}\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-j2\" data-lang=\"j2\"\u003e{% for host in nfs_hosts %}\n/data {{ host }} (rw,wdelay,root_squash,no_subtree_check,sec=sys,rw,root_squash,no_all_squash)\n{% endfor %}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eVariables:\nnfs_exports:\u003c/p\u003e","title":"NFS Setup","type":"tech"},{"content":" Optimizing Ansible Processing # Parallel task execution\nmanages the number of hosts on which tasks are executed simultaneously. Serial task execution tasks are executed on a host or group of hosts before proceeding to the next host or group of hosts. Parallel Task Execution # Ansible can run tasks on all hosts at the same time, and in many cases that would not be a problem because processing is executed on the managed host anyway. If, however, network devices or other nodes that do not have their own Python stack are involved, processing needs to be done on the control host. To prevent the control host from being overloaded in that case, the maximum number of simultaneous connections by default is set to 5. You can manage this setting by using the forks parameter in ansible.cfg. Alternatively, you can use the -f option with the ansible and ansible-playbook commands. If only Linux hosts are managed, there is no reason to keep the maximum number of simultaneous tasks much lower than 100. Managing Serial Task Execution # While executing tasks, Ansible processes tasks in a playbook one by one. This means that, by default, the first task is executed on all managed hosts. Once that is done, the next task is processed, until all tasks have been executed. There is no specific order in the execution of tasks, so you may see that in one run ansible1 is processed before ansible2, while on another run they might be processed in the opposite order. In some cases, this is undesired behavior. If, for instance, a playbook is used to update a cluster of hosts this way, this would create a situation where the old software has been updated, but the new version has not been started yet and the entire cluster would be down. Use the serial keyword in the play header to configure serial: 3 all tasks are executed on three hosts, and after completely running all tasks on three hosts, the next group of three hosts is handled. Lab: Managing Parallelism # Add two more managed nodes with the names ansible3.example.com and ansible4.example.com. Open the inventory file with an editor and add the following lines: ansible3 ansible4 Open the ansible.cfg file and add the line forks = 4 to the [defaults] section. Write a playbook with the name exercise102-install that installs and enables the Apache web server and another playbook with the name exercise102-remove that disables and removes the Apache web server. Run ansible-playbook exercise102-remove.yaml to remove and disable the Apache web server on all hosts. This is just to make sure you start with a clean configuration. Run the playbook to install and run the web server, using time ansible-playbook exercise102-install.yaml, and notice the time it takes to run the playbook. Run ansible-playbook exercise102-remove.yaml again to get back to a clean state. Edit ansible.cfg and change the forks parameter to forks = 2. Run the time ansible-playbook exercise102-install.yaml command again to see how much time it takes now Edit the exercise102-install.yaml playbook and include the line serial: 2 in the play header. Run the ansible-playbook exercise102-remove.yaml command again to get back to a clean state. Run the ansible-playbook exercise102-install.yaml command again and observe that the entire play is executed on two hosts only before the next group of two hosts is taken care of. ","externalUrl":null,"permalink":"/tech/ansible/optimizing-ansible-processing/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eOptimizing Ansible Processing\n    \u003cdiv id=\"optimizing-ansible-processing\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#optimizing-ansible-processing\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eParallel task execution\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emanages the number of hosts on which tasks are executed simultaneously.\n\u003cstrong\u003eSerial task execution\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003etasks are executed on a host or group of hosts before proceeding to the next host or group of hosts.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 class=\"relative group\"\u003eParallel Task Execution\n    \u003cdiv id=\"parallel-task-execution\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#parallel-task-execution\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eAnsible can run tasks on all hosts at the same time, and in many cases that would not be a problem because processing is executed on the managed host anyway.\u003c/li\u003e\n\u003cli\u003eIf, however, network devices or other nodes that do not have their own Python stack are involved, processing needs to be done on the control host.\u003c/li\u003e\n\u003cli\u003eTo prevent the control host from being overloaded in that case, the maximum number of simultaneous connections by default is set to 5.\u003c/li\u003e\n\u003cli\u003eYou can manage this setting by using the \u003cstrong\u003eforks\u003c/strong\u003e parameter in ansible.cfg.\u003c/li\u003e\n\u003cli\u003eAlternatively, you can use the \u003ccode\u003e-f\u003c/code\u003e option with the \u003ccode\u003eansible\u003c/code\u003e and \u003ccode\u003eansible-playbook\u003c/code\u003e commands.\u003c/li\u003e\n\u003cli\u003eIf only Linux hosts are managed, there is no reason to keep the maximum number of simultaneous tasks much lower than 100.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 class=\"relative group\"\u003eManaging Serial Task Execution\n    \u003cdiv id=\"managing-serial-task-execution\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#managing-serial-task-execution\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eWhile executing tasks, Ansible processes tasks in a playbook one by one.\u003c/li\u003e\n\u003cli\u003eThis means that, by default, the first task is executed on all managed hosts. Once that is done, the next task is processed, until all tasks have been executed.\u003c/li\u003e\n\u003cli\u003eThere is no specific order in the execution of tasks, so you may see that in one run ansible1 is processed before ansible2, while on another run they might be processed in the opposite order.\u003c/li\u003e\n\u003cli\u003eIn some cases, this is undesired behavior.\u003c/li\u003e\n\u003cli\u003eIf, for instance, a playbook is used to update a cluster of hosts this way, this would create a situation where the old software has been updated, but the new version has not been started yet and the entire cluster would be down.\u003c/li\u003e\n\u003cli\u003eUse the \u003cstrong\u003eserial\u003c/strong\u003e keyword in the play header to configure\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eserial: 3\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eall tasks are executed on three hosts, and after completely running all tasks on three hosts, the next group of three hosts is handled.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eLab: Managing Parallelism\n    \u003cdiv id=\"lab-managing-parallelism\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#lab-managing-parallelism\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAdd two more managed nodes with the names ansible3.example.com and ansible4.example.com.\u003c/li\u003e\n\u003cli\u003eOpen the inventory file with an editor and add the following lines:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-pre1\" data-lang=\"pre1\"\u003eansible3\nansible4\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eOpen the ansible.cfg file and add the line \u003ccode\u003eforks = 4\u003c/code\u003e to the \u003cstrong\u003e[defaults]\u003c/strong\u003e section.\u003c/li\u003e\n\u003cli\u003eWrite a playbook with the name \u003cstrong\u003eexercise102-install\u003c/strong\u003e that installs and enables the Apache web server and another playbook with the name \u003cstrong\u003eexercise102-remove\u003c/strong\u003e that disables and removes the Apache web server.\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003eansible-playbook exercise102-remove.yaml\u003c/code\u003e to remove and disable the Apache web server on all hosts. This is just to make sure you start with a clean configuration.\u003c/li\u003e\n\u003cli\u003eRun the playbook to install and run the web server, using \u003ccode\u003etime ansible-playbook exercise102-install.yaml\u003c/code\u003e, and notice the time it takes to run the playbook.\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003eansible-playbook exercise102-remove.yaml\u003c/code\u003e again to get back to a clean state.\u003c/li\u003e\n\u003cli\u003eEdit \u003cstrong\u003eansible.cfg\u003c/strong\u003e and change the \u003cstrong\u003eforks\u003c/strong\u003e parameter to \u003ccode\u003eforks = 2\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eRun the \u003ccode\u003etime ansible-playbook exercise102-install.yaml\u003c/code\u003e command again to see how much time it takes now\u003c/li\u003e\n\u003cli\u003eEdit the \u003ccode\u003eexercise102-install.yaml\u003c/code\u003e playbook and include the line \u003ccode\u003eserial: 2\u003c/code\u003e in the play header.\u003c/li\u003e\n\u003cli\u003eRun the \u003ccode\u003eansible-playbook exercise102-remove.yaml\u003c/code\u003e command again to get back to a clean state.\u003c/li\u003e\n\u003cli\u003eRun the \u003ccode\u003eansible-playbook exercise102-install.yaml\u003c/code\u003e command again and observe that the entire play is executed on two hosts only before the next group of two hosts is taken care of.\u003c/li\u003e\n\u003c/ul\u003e","title":"Optimizing Ansible Processing","type":"tech"},{"content":"Thursday, August 26, 2021 1:37 PM\nmetric. If the network topology changeshave failed and pick a new currently best route. (This process is called convergence.)—for example, a link fails—react by advertising that some routes IP routing protocols fall into one of two major categories: interior gateway protocols (IGP) or exterior gateway protocols (EGP). IGP: A routing protocol that was designed and intended for use inside a single autonomous system (AS) EGP:A routing protocol that was designed and intended for use between different autonomous systems AnAS is a network under the administrative control of a single organization. routing protocols designed to exchange routes between routers in different autonomous systems are called EGPs. Today, Border Gateway Protocol (BGP) is the only EGP used. Interior and Exterior Routing Protocols\nIGP Routing Protocol Algorithms\nDistance vector (sometimes called Bellman-Ford after its creators)\nAdvanced distance vector (sometimes called “balanced hybrid”)\nLink-state (OSPF)\nRouting Information Protocol(RIP)was the first popularly used IP distance vector protocol, with\ntheCisco-proprietary Interior Gateway Routing Protocol (IGRP)being introduced a little later.\nproprietary routing protocol called Enhanced Interior Gateway Routing Protocol(EIGRP)\nit used more distance vector features than link-state, so it is more commonly classified as\nanadvanced distance vector protocol.\nRouting protocols choose the best route to reach a subnet by choosing the route with the metric lowest RIP usesa counter of the number of routers(hops) between a router and the destination subnet OSPF totals the cost on link bandwidth associated with each interface in the end-to-end route, with the cost based Metrics\nThe interface. It just tells IOS what speed to assume the interface is using.) bandwidth interface subcommanddoes not change the actual physical speed of the Other IGP Comparisons\nClassless routing protocols support variablesummarization (supernetting)by sending routing protocol messages that include the subnet -length subnet masks (VLSM) as well as manual route masks in the messageprotocols—do not.. The older RIPv1 and IGRP routing protocols—both classful routing If one company uses OSPF and the other uses EIGRP on at least one router, both OSPF and EIGRP Administrative Distance\nIf one company uses OSPF and the other uses EIGRP on at least one router, both OSPF and EIGRP must be used. Then thatrouter can take routes learned by OSPF and advertise them into EIGRP,\nand vice versa, through a process called route redistribution.\nWhen a single routing protocol learns multiple routes to the same subnet, the metric tells it which route is best. However,when two different routing protocols learn routes to the same\nsubnet, because each routing protocol’s metric is based on different information, IOS cannot\ncompare the metrics\nWhen IOSconcept called administrative distancemust choose between routes learned using different routing protocols, IOS uses a .Administrative distanceis a number that denotes how\nbelievable an entire routing protocol is on a single routermore believable, the routing protocol. For example, RIP has a default administrative distance of .The lower the number, the better,or\n120, OSPF uses a default of 110, and EIGRP defaults to 90. When using OSPF and EIGRP, the\nrouter will believe the EIGRP route instead of the OSPF route (at least by default).administrative distance values are configured on a single router and are not exchanged with The\nother routers\nThe show ip route command lists each route’s administrative distance as the first of the two\nnumbers inside the brackets. The second number in brackets is the metric\nIOSparticular route, or even a static route.can be configured to change the administrative distance of a particular routing protocol, a\nparticular route, or even a static route.\nFor example, the commandwith a default administrative distance of 1 ip route 10.1.3.0 255.255.255.0 10.1.130.253 , but the command ip route 10.1.3.0 255.255.255.0 defines a static route\n10.1.130.253 210 defines the same static route with an administrative distance of 210\nTopology Information and LSAs\nEach LSA is a data structure with some specific information about the network topology; the\nLSDB is simply the collection of all the LSAs known to a router.\nbefore sending an LSA to yet another neighbor, routers communicate, asking “Do you already have this LSA?,” and then sending the LSA to the next neighbor only if the neighbor has not yet\nlearned about the LSA.\nRouters reflood an LSA when some information changes(for example, when a link goes up or\ncomes down). They also minutes). reflood each LSA based on each LSA’s separate aging timer(default 30\nApplying Dijkstra SPF Math to Find the Best Routes\nBecoming neighbors:created so that the neighboring routers have a means to exchange their LSDBs.A relationship between two routers that connect to the same data link,\nExchanging databases: The process of sending LSAs to neighbors so that all routers learn the\nsame LSAs.\nAdding the best routes: The process of each routerindependently running SPF, on their local\ncopy of the LSDB, calculating the best routes, and adding those to the IPv4 routing table.\nThe Basics of OSPF Neighbors\nOSPF neighbors are routers that both use OSPF and both sit on the same data link.can become OSPF neighbors if connected to the same VLAN, or same serial link, or same Two routers\nEthernet WAN link.\nthey must send OSPF messages and agree to become OSPF neighbors.To do so, therouters send\nOSPF Hello messages, introducing themselves to the potential neighbor. Assuming the two potential neighbors have compatible OSPF parameters, the two form an OSPF neighbor\nrelationship, and would be displayed in the output of the show ip ospf neighbor command.\nallows new routers to be dynamically discovered. That means new routers can be added to a\nnetwork without requiring every router to be reconfiguredHello messages from new routers and react to those messages, attempting to become neighbors. Instead, OSPF routers listen for OSPF\nand exchange LSDBs.\nMeeting Neighbors and Learning Their Router ID\nprocess starts with messages called OSPF Hello messages. The Hellos in turn list each router’s\nrouter ID (RID), which serves as each router’s unique name or identifier for OSPF. does several checks of the information in the Hello messages to ensure that the two routers Finally, OSPF\nshould become neighbors.\nOSPF RIDs are 32 - bit numbers.\nOSPF RIDs are 32 - bit numbers.\nBy default, IOS chooses one of the router’s interface IPv4 addresses to use as its OSPF RID.However, the OSPF RID can be directly configured\nAs soonmeet its OSPF neighbors.as a router has chosen its OSPF RID and some interfaces come up, the router is ready to OSPF routers can become neighbors if they are connected to the same\nsubnet. each interface and hopes to receive OSPF Hello packets from other routers connected to those To discover other OSPF-speaking routers, a router sends multicast OSPF Hello packets to\ninterfaces.\nThey messages themselves have the following features:continue to send Hellos at a regular interval based on their Hello timer settings. The Hello\nTheHello messagefollows the IP packet header, withIP protocol type 89.\nHello packets are sent toOSPF-speaking routers. multicast IP address 224.0.0.5, a multicast IP address intended for all\nOSPF routers listen for packets sent to IP multicast address 224.0.0.5, in part hoping to receive Hello packets and learn about new neighbors.\nEach router keeps an OSPF state variable for how it views the neighbor.\nHello. This message tells R1 that R2 exists, and it allows R1 to move through the init state and\nquickly to a 2-way state.\nThe 2are true:-way state is a particularly important OSPF state. At that point, the following major facts\n(^2) it is ready to begin a 2-way means that the router is available to exchange its LSDB with the neighbor.-way exchange of the LSDB. In other words,\nFirst, they tell each other a list of LSAs in their respective databases—not all the details of the\nLSAs, just a list. (Think of these lists as checklists.) Then each router can check which LSAs it already has and then ask the other router for only the LSAs that are not known yet.\nthe OSPF messages that actually send the LSAs between neighbors are called Link(LSU) packets.That is, the LSU packet holds data structures called link-state advertisements -State Update\n(LSA). The LSAs are not packets, but rather data structures that sit inside the LSDB and describe the topology.\nMaintaining Neighbors and the LSDB\nFirst, the Hello Interval and the Dead Interval. routers monitor each neighbor relationship using Hello messages and two related timers:Routers send Hellos every Hello Interval to each\nneighbor.so if a neighbor is silent for the length of the Each router expects to receive a Hello from each neighbor based on the Hello Interval, Dead Interval (by default, four times as long as the\nHello Interval),the loss of Hellos means that the neighbor has failed.\nNext, routers must react when the topology changes as well,\nNext, routers must react when the topology changes as well, Each router’s LSDB now reflects the fact that the original router’s G0/0 interface failed, so each router will then use SPF to recalculate any routes affected by the failed interface. A third maintenance task done by neighbors is to reflood each LSA occasionally, even when the network is completely stable. By default, each router that creates an LSA also has the responsibility to reflood the LSA every 30 minutes (the default), even if no changes occurthateach LSA has a separate timer, based on when the LSA was created, so there is no single big. (Note event where the network is overloaded with flooding LSAs.) On Ethernet links, OSPF defaults to use a network type of broadcastone of the routers on the same subnet to act as the designated router (DR). , which causes OSPF to elect These five OSPF routers elect one router to act as the DR and one router to be a backup DR (BDR). The database exchange process on an Ethernet link does not happen between every pair of routers on the same VLAN/subnet. Instead, it happens between the DR and each of the other routers,with the DR making sure that all the other routers get a copy of each LSA. The BDR watches the status of the DR and takes over for the DR if it fails. (When the DR fails, the BDR takes over, and then a new BDR is elected.) TheDR can send a packet to all OSPF routers in the subnet by using multicast IP address 224.0.0. The each router.DR can send one set of messages to all the OSPF routers rather than sending one message to atake over for the DR) can send those messages to theny OSPF router needing to send a message to theDR and also to the BDR“All SPF DRs” multicast address 224.0.0.6.(so it remains ready to routers that are neither a DR nor a BDR—called DROthersby OSPF—never reach a full state because they do not exchange LSDBs directly with each other. full state(called fully adjacent neighbors 2 - way state(called neighbors) all OSPF routers on the same link that reach the 2-way state—that is, they send Hello messages and the parameters match—are called neighbors. The subset of neighbors for Using Designated Routers on Ethernet Links\nmessages and the parameters matchwhich the neighbor relationship continues on and reaches the full state are called adjacent —are called neighbors. The subset of neighbors for neighbors. OSPF Areas and LSAs\nA larger topology database requires more memory on each router.\nTheSPF algorithm requires processing power that grows exponentially compared to the size of\nthe topology database.\nA single interface status change anywhere in the internetwork (up to down, or down to up)\nforces every router to run SPF again!\nPut all interfaces connected to the same subnet inside the same area. An area should be contiguous. Some routers may be internal to an area, with all interfaces assigned to that single area. Some routers may be Area Border Routers (ABR) because some interfaces connect to the backbone area, and some connect to nonbackbone areas. All nonbackbone areas must have a path to reach the backbone area (area 0) by having at least one ABR connected to both the backbone area and the nonbackbone area. OSPF Areas\nHow Areas Reduce SPF Calculation Time\nRouters requirefewer CPU cycles to process the smaller per-area LSDB with the SPF algorithm,\nreducing CPU overhead and improving convergence time.\nThe smaller per-area LSDB requires less memory.\nChanges in the network (for example, links failing and recovering) require SPF calculations only\non routers in the area where the link changed state, reducing the number of routers that must rerun SPF.\nLess information must be advertised between areas, reducing the bandwidth required to send LSAs.\nLSAs.\n(OSPFv2) Link-State Advertisements\nOne router LSA for each router in the area\nOne network LSA for each network that has a DR plus one neighbor of the DR\nOne summary LSA for each subnet ID that exists in a different area\nRouter LSAs Build Most of the Intra-Area Topology\nRouter LSAs, also known as Type 1 LSAs, describe the router in detail. Each lists a router’s RID, its interfaces, its IPv4 addresses and masks, its interface state, and notes about what neighbors the\nrouter knows about via each of its interfaces.\nThis chapter covers the following exam topics: 3.0 IP Connectivity 3.2 Determine how a router makes a forwarding decision by default 3.2.b Administrative distance 3.2.c Routing protocol metric 3.4 Configure and verify single area OSPFv 3.4.a Neighbor adjacencies 3.4.b Point-to-point 3.4.c Broadcast (DR/BR selection) 3.4.d Router ID Implementing Single-Area OSPFv Step 1. Use the particular OSPF process. router ospf process-id global command to enter OSPF configuration modefor a Step 2. (Optional) Configure the OSPF router ID by doing the following: A. Use the router-id id-value router subcommand to define the router ID, or B. Use the command, to configure an IP address on a loopback interface ( interface loopback number global command, along with an chooses the ip address address mask highest IP address of all working loopbacks), or C. Rely on an interface IP address (chooses the highest IP address of all working nonloopbacks). Step 3. Use one or more network ip-address wildcard-mask area area-id router subcommands to enable OSPFv2 on any interfaces matched by the configured address and mask, enabling OSPF on the interface for the listed area. OSPF Single-Area Configuration for each matched interface, the router enables OSPF on those interfaces, discovers neighbors, creates neighbor relationships, and assigns the interface to the area listed in the network command. Verifying OSPF Operation The show ip ospf neighbor, show ip ospf database, and show ip route commands display ","externalUrl":null,"permalink":"/tech/networking/ospf-concepts/","section":"Teches","summary":"\u003cp\u003eThursday, August 26, 2021 1:37 PM\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emetric.\nIf the network topology changeshave failed and pick a new currently best route. (This process is called convergence.)—for example, a link fails—react by advertising that some routes\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eIP routing protocols fall into one of two major categories: interior gateway protocols (IGP) or exterior gateway protocols (EGP).\nIGP: A routing protocol that was designed and intended for use inside a single autonomous\nsystem (AS)\nEGP:A routing protocol that was designed and intended for use between different autonomous\nsystems\nAnAS is a network under the administrative control of a single organization.\nrouting protocols designed to exchange routes between routers in different autonomous systems are called EGPs. Today, Border Gateway Protocol (BGP) is the only EGP used.\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eInterior and Exterior Routing Protocols\u003c/p\u003e","title":"OSPF Concepts","type":"tech"},{"content":"Thursday, August 26, 2021 2:45 PM\nThe information to match each of these three steps, respectively. show ip ospf neighbor, show ip ospf database, and show ip route commands display FULL/ use a DR/BDR.-: The neighbor state is full, with the “-“ instead of letters meaning that the link does not FULL/DR: The neighbor state is full, and the neighbor is the DR. FULL/BDR: The neighbor state is full, and the neighbor is the backup DR (BDR). FULL/DROTHER: implies that the local router is a DR or BDR because the state is FULL.)The neighbor state is full, and the neighbor is neither the DR nor BDR. (It also 2WAY/DROTHER: The neighbor state is 2-way, and the neighbor is neither the DR nor BDR—that is, a DROther router. (It also implies that the local router is also a DROther router because otherwise the state would reach a full state.) Verifying OSPF Configuration\nIf you have configuration.enable mode access, use the show running-config command to examine the\nIf you have only configuration. user mode access,use the show ip protocols command to re-create the OSPF\nUse the show ip ospf interface [brief] command to determine whether the router enabled OSPF\non the correct interfaces or not based on the configuration.\nshow ip ospf interface brief showing all the interfaces on which OSPF has been enabledcommand shown here. It lists one line per interface, with the list\nthe show ip ospf interface command with the brief keywordat the end lists a single line of output per interface,displays about 20 lines of output per interfacebut the show ip ospf interfacecommand (without the brief keyword) Configuring the OSPF Router ID\nmost enterprise networks that use OSPF choose to configure each router’s OSPF router ID\nIf therouter-id rid OSPF subcommand is configured, this value is used as the RID.\nIf any loopback interfaces have an IP address configured, status of up, the router picks the highest numeric IP address among these loopback interfaces.and the interface has an interface\nThe router picks the code (first status code) is up. (In other words, an interface in up/down state will be included by highest numeric IP address from all other interfaces whose interface status\nOSPF when choosing its router ID.)\nstops and restarts the OSPF processup, and later the configuration changes in a way that would impact the OSPF RID, OSPF does not (with the clear ip ospf process command). So, if OSPF comes\nchange the RID immediately. Instead, IOS waits until the next time the OSPF process is restarted.\nOSPF Interface Configuration Example\nStep 1. Use the no network network-id area area-id subcommands in OSPF configuration mode\nto remove the network commands.\nStep 2. Add one ip ospf process-id area area-id command in interface configuration mode under\neach interface on which OSPF should operate,correct OSPF area number. with the correct OSPF process (process-id) and the\nThe some different details if you use interface configuration versus the network command show ip protocols command relists most of the routing protocol configuration,so it does list Verifying OSPF Interface Configuration\nAdditional OSPFv2 Features\nPassive interfaces\nDefault routes\nMetrics\nLoad balancing\nOSPF Passive Interfaces\nWhen no routers exist on a link\nOSPF continues to advertise about the subnet that is connected to the interface.\nOSPF no longer sends OSPF Hellos on the interface.\nOSPF no longer processes any received Hellos on the interface.\nOSPF still advertises about the connected subnet, but OSPF also does not form neighbor\nrelationships over the interface.\nFirst, configuration mode:you can add the following command to the configuration of the OSPF process, in router\npassive-interface type number\nAlternately, the configuration can change the default setting so that all interfaces are passive by default and then add a no passive-interface command for all interfaces that need to not be\npassive:\npassive-interface default\nno passive-interface type number\nThe passive interfaces. show ip ospf interface brief command lists all interfaces on which OSPF is enabled, including\nThe show ip ospf interface command lists a single line that mentions that the interface is passive.\nOSPF Default Routes\nAll routers learn specific (nondefault) routes for subnets inside the company; a default route is not needed when forwarding packets to these destinations.\nOne router connects to the Internet, and it has a default route that points toward the Internet.\nAll routers should dynamically learn a default route,that all packets destined to locations in the Internet go to the one router connected to the used for all traffic going to the Internet, so\nInternet.\ndefault using OSPF to the remote routers (B1 and B2). - information originate command (Step 2) makes the router advertise a default route\nThe default-information originate always router subcommand tells the router to always advertise the default route, no matter whether the router’s default route is working or not. OSPF Metrics (Cost)\nCisco routers allow three different ways to change the OSPF interface cost:\nDirectly, using the interface subcommand ip ospf cost x.\nUsing the default calculation per interface, and changing the interface bandwidth setting, which changes the calculated value.\nUsing the default calculation per interface, and changing the OSPF reference bandwidth setting, which changes the calculated value.\nThe output also shows a cost value of 1 for the other Gigabit interfaces, which is the default OSPF cost for any interface faster than 100 Mbps. interface bandwidth setting does not influence the actual transmission speed.Instead, the interface bandwidth acts as a configurable setting to represent the speed of the interface, with the option to configure the bandwidth to match the actual transmission speed...or not configuration of the interface bandwidth using bandwidth speed interfacesubcommand. Reference_bandwidth / Interface_bandwidth you should cost. avoid changing the interface bandwidth as a means to influence the default OSPF any interface with an interface bandwidth of 100 Mbps or faster ties with a calculated OSPF cost of 1 when using the default reference bandwidth. So, when relying on the default OSPF cost calculation, it helps to configure the reference bandwidth to another value. Setting the Cost Based on Interface and Reference Bandwidth\nYou can still use OSPF’s default cost calculation (and many do) just by changing the reference\nbandwidth with the auto-cost reference-bandwidth speed OSPF mode subcommand\nFor instance, routers to use autoin an enterprise whose fastest links are 10 Gbps (10,000 Mbps), you could set all -cost reference-bandwidth 10000, meaning 10,000 Mbps or 10 Gbps. In that\ncase, by default, a 10cost of 10, and a 100\u0026ndash;MBps link a cost of 100Gbps link would have an OSPF cost of 1, while a 1. -Gbps link would have a\nSet the cost explicitly, using the 65,535, inclusive. ip ospf cost x interface subcommand, to a value between 1 and\nAlthough it should be avoided, change the interface bandwidth with the bandwidth speed\ncommand,with speed being a number in kilobits per second (Kbps).\nChange the reference bandwidth, using router OSPF subcommandauto-cost reference-\nbandwidth ref-bw, with a unit of megabits per second (Mbps).\nOSPF Load Balancing\nwhen the metrics tie for multiple routes to the same subnet, the router can put multiple equalcost routes in the routing table (the default is four different routes) based on the setting of the -\nmaximum-paths number router subcommand\nthe default (and better) method, the load balancing could be on a per-destination IP address\nbasis.\nThis chapter covers the following exam topics: 3.0 IP Connectivity 3.4 Configure and verify single area OSPFv 3.4.a Neighbor adjacencies 3.4.b Point-to-point 3.4.c Broadcast (DR/BDR selection) 3.4.d Router ID OSPF Network Types TheOSPF Broadcast Network Type default to use network type broadcast Attempt to discover neighbors by sending OSPF Hellos to the 224.0.0.5 multicast address (address reserved for sending packets to all OSPF routers in the subnet) an Attempt to elect a DR and BDR on each subnet On the interface with no other routers on the subnet (G0/1), become the DR On the interface with three other routers on the subnet (G0/0), be either DR, BDR, or a DROther router all discovered routers on the link should become neighbors and at least reach the 2all neighbor relationships that include the DR and/or BDR, the neighbor relationship should -way state. For further reach the full state. refers to neighbors that reach this full state.That section defined the term fully adjacent as a special term that ","externalUrl":null,"permalink":"/tech/networking/ospfconfiguration/","section":"Teches","summary":"\u003cp\u003eThursday, August 26, 2021 2:45 PM\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eThe information to match each of these three steps, respectively. show ip ospf neighbor, show ip ospf database, and show ip route commands display\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eFULL/ use a DR/BDR.-: The neighbor state is full, with the “-“ instead of letters meaning that the link does not\nFULL/DR: The neighbor state is full, and the neighbor is the DR.\nFULL/BDR: The neighbor state is full, and the neighbor is the backup DR (BDR).\nFULL/DROTHER: implies that the local router is a DR or BDR because the state is FULL.)The neighbor state is full, and the neighbor is neither the DR nor BDR. (It also\n2WAY/DROTHER: The neighbor state is 2-way, and the neighbor is neither the DR nor BDR—that\nis, a DROther router. (It also implies that the local router is also a DROther router because otherwise the state would reach a full state.)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eVerifying OSPF Configuration\u003cbr\u003e\nIf you have configuration.enable mode access, use the \u003cstrong\u003eshow running-config\u003c/strong\u003e command to examine the\u003cbr\u003e\nIf you have only configuration. user mode access,use the \u003cstrong\u003eshow ip protocols\u003c/strong\u003e command to re-create the OSPF\u003cbr\u003e\nUse the \u003cstrong\u003eshow ip ospf interface [brief]\u003c/strong\u003e command to determine whether the router enabled OSPF\u003cbr\u003e\non the correct interfaces or not based on the configuration.\u003cbr\u003e\n\u003cstrong\u003eshow ip ospf interface brief\u003c/strong\u003e showing all the interfaces on which OSPF has been enabledcommand shown here. It lists one line per interface, with the list\u003c/p\u003e","title":"OSPF Configuration","type":"tech"},{"content":"Friday, August 27, 2021 9:44 AM\nTo see the setting, use the show ip ospf interface command,as shown in Example 21-4. The first highlighted item identifies the network type ip ospf network broadcast interface subcommand would configure the setting.\nConfiguring to Influence the DR/BDR Election\nIf the DR fails, the BDR becomes the DR, and a new BDR is elected.\nPreemption:If a new router is configured to be the DR, it will not become the DR until the\nOSPF process is reset.\nWhen a better router enters the subnet, no preemption of the existing DR or BDR occurs.- As a result of these rules, while you can configure a router to be the best (to become the DR in an election, doing so only increases that router’s statistical chances of being highest priority) router the DR at a given point in time. If the router fails, other routers will become DR and BDR, and the the DR at a given point in time. If the router fails, other routers will become DR and BDR, and best router will not be DR again until the current DR and BDR fail. the The highest OSPF interface priority: The highest value wins during an election, with values ranging from 0 to 255. The highest OSPF Router ID: If the priority ties, the election chooses the router with the highest OSPF RID. If an engineer preferred that R1 be the DR, the engineer could add the configuration in Example 21 - 5 to set R1’s interface priority to 99. (default of 1) show that the DR and BDR have not changed at all\nThe OSPF Point-to-Point Network Type\n(HDLC and PPP) do not support data-link broadcasts.\nto use OSPF network type pointsame configuration command on its matching interface. -to-point. R2, on the other end of the WAN link, would need the\nOSPF Neighbor Relationships\nThey must have compatible values for several settings as listed in the Hellos exchanged between the two routers\nFor items listing a “yes” in this column, if that item is configured incorrectly, the neighbor will not appear in lists of OSPF neighbors—for instance, with the show ip ospf neighbor command.\nthe last section (shaded) lists a couple of OSPF settings that give a different symptom when incorrect\nfor these two items, when incorrect, a router can list the other router as a neighbor, but the neighbor relationship does not work properly in that the routers do not exchange LSAs as they should 10/40 is the default hello/dead timer\nFinding Area Mismatches\nCheck the output of show running-config to look for\nip ospf process-id area area-number interface subcommands\nnetwork commands in OSPF configuration mode\nUse the show ip ospf interface [brief] command to list the area number\nboth routers automatically generate a log message for the duplicate OSPF RID problem between R1 and R3; show ip ospf commands on both R3 and R1 to easily list the RID on each router, use the router-id 3.3.3.3 OSPF subcommand and use the EXEC mode command clear ip ospf process or reload.). (OSPF will not begin using a new RID value until the process restarts, either via command Finding Duplicate OSPF Router IDs\nFinding OSPF Hello and Dead Timer Mismatches\nHello interval/timer: The per-interface timer that tells a router how often to send OSPF Hello\nmessages on an interface.\nDead interval/timer: The perreceived a Hello from a neighbor before believing that neighbor has failed. (Defaults to four times -interface timer that tells the router how long to wait without having\nthe Hello timer.)\nShutting Down the OSPF Process\nWhen a routing protocol process is shut down, IOS does the following:\nBrings down all neighbor relationships and clears the OSPF neighbor table\nClears the LSDB\nClears the IP routing table of any OSPF-learned routes\nIOS retains all OSPF configuration.\nIOS still lists all OSPF-enabled interfaces in the OSPF interface list( show ip ospf interface )\nbut in a DOWN state.\ndefining the largest network layer packet that the router will forward out each interface. default MTU size of 1500 bytes, The command sets the equivalent for IPv6 packets. ip mtu size interface subcommand defines the IPv4 MTU setting, and the ipv6 mtu size Symptom: they fail to exchange their LSDBs. Eventually, after trying and failing to exchange their LSDBs, the neighbor relationship also fails. show interfaces command (which lists the IP MTU). Mismatched MTU Settings\nMismatched OSPF Network Types\none router uses broadcast, and the other uses point-to-point, the following occurs:\nThe two routers become fully adjacent neighbors (that is, they reach a full state).\nThey exchange their LSDBs.\nThey do not add IP routes to the IP routing table.\nThis chapter covers the following exam topics:\n1.0 Network Fundamentals\n1.8 Configure and verify IPv6 addressing and prefix\nIntroduction to IPv6\nIPv6 increases the address to 128 bits in length.\nOlder OSPF Version 2 Upgraded to OSPF Version 3:\na newer version, OSPF version 3, was created to support IPv6. (Note: OSPFv3 was later\nupgraded to support advertising both IPv4 and IPv6 routes.)\nICMP Upgraded to ICMP Version 6:\nARP Replaced by Neighbor Discovery Protocol:\n32 hexadecimal digits (one hex digit per 4 bits)\nFigure 22-3 shows the required 40-byte part of the IPv6 header.\nIPv6 Routing\nTo be able to build and send IPv6 packets out an interface, endon that interface. -user devices need an IPv6 address\nEnd-user hosts need to know the IPv6 address of a default router, to which the host sends IPv6\n","externalUrl":null,"permalink":"/tech/networking/ospfnetworktypesandneighbors/","section":"Teches","summary":"\u003cp\u003eFriday, August 27, 2021 9:44 AM\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eTo see the setting, use the show ip ospf interface command,as shown in Example 21-4. The first\nhighlighted item identifies the network type\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eip ospf network broadcast\u003c/strong\u003e interface subcommand would configure the setting.\u003cbr\u003e\nConfiguring to Influence the DR/BDR Election\u003cbr\u003e\nIf the DR fails, the BDR becomes the DR, and a new BDR is elected.\u003cbr\u003e\nPreemption:If a new router is configured to be the DR, it will not become the DR until the\u003cbr\u003e\nOSPF process is reset.\u003c/p\u003e","title":"OSPF Network Types and Neighbors","type":"tech"},{"content":" Partition Information (MBR and GPT) # Partition information is stored on the disk in a small region. Read by the operating system at boot time. Master Boot Record (MBR) on the BIOS-based systems GUID Partition Table (GPT) on the UEFI-based systems. At system boot, the BIOS/UEFI: scans all storage devices, detects the presence of MBR/GPT areas, identifies the boot disks, loads the bootloader program in memory from the default boot disk, executes the boot code to read the partition table and identify the /boot partition, loads the kernel in memory, and passes control over to it. MBR and GPT store disk partition information and the boot code. Master Boot Record (MBR) # Resides on the first sector of the boot disk.\nwas the preferred choice for saving partition table information on x86-based computers.\nwith the arrival of bigger and larger hard drives, a new firmware specification (UEFI) was introduced.\nstill widely used, but its use is diminishing in favor of UEFI.\nallows the creation of three types of partition on a single disk.\nprimary, extended, and logical\nonly primary and logical can be used for data storage\nextended is a mere enclosure for holding the logical partitions and it is not meant for data storage.\nsupports the creation of up to four primary partitions numbered 1 through 4 at a time.\nIn case additional partitions are required, one of the primary partitions must be deleted and replaced with an extended partition to be able to add logical partitions (up to 11) within that extended partition.\nNumbering for logical partitions begins at 5.\nsupports a maximum of 14 usable partitions (3 primary and 11 logical) on a single disk.\nCannot address storage space beyond 2TB due to its 32-bit nature and its 512-byte disk sector size.\nnon-redundant; the record it contains is not replicated, resulting in an unbootable system in the event of corruption.\nIf your disk is smaller than 2TB and you don\u0026rsquo;t intend to build more than 14 usable partitions, you can use MBR without issues.\nGUID Partition Table (GPT) # ability to construct up to 128 partitions (no concept of extended or logical partitions) utilize disks larger than 2TB use 4KB sector size store a copy of the partition information before the end of the disk for redundancy allows a BIOS-based system to boot from a GPT disk using the bootloader program stored in a protective MBR at the first disk sector UEFI firmware also supports the secure boot feature, which only allows signed binaries to boot MBR Storage Management with parted # parted (partition editor)\ncan be used to partition disks run interactively or directly from the command prompt. understands and supports both MBR and GPT schemes can be used to create up to 128 partitions on a single GPT disk viewing, labeling, adding, naming, and deleting partitions. print Displays the partition table that includes disk geometry and partition number, start and end, size, type, file system type, and relevant flags.\nmklabel Applies a label to the disk. Common labels are gpt and msdos.\nmkpart Makes a new partition\nname Assigns a name to a partition\nrm Removes the specified partition\nuse the print subcommand to ensure you created what you wanted. /proc/partitions file is also updated to reflect the results of partition management operations. Lab: Create an MBR Partition (server2) # Assign partition type \u0026ldquo;msdos\u0026rdquo; to /dev/sdb for using it as an MBR disk create and confirm a 100MB primary partition on the disk. 1. Execute parted on /dev/sdb to view the current partition information:\n[root@server2 ~]# sudo parted /dev/sdb print Error: /dev/sdb: unrecognised disk label Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: unknown Disk Flags: There is an error on line 1 of the output, indicating an unrecognized label. disk must be labeled before it can be partitioned.\n2. Assign disk label \u0026ldquo;msdos\u0026rdquo; to the disk with mklabel. This operation is performed only once on a disk.\n[root@server2 ~]# sudo parted /dev/sdb mklabel msdos Information: You may need to update /etc/fstab. [root@server2 ~]# sudo parted /dev/sdb print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags To use the GPT partition table type, run \u0026ldquo;sudo parted /dev/sdb mklabel gpt\u0026rdquo; instead.\n3. Create a 100MB primary partition starting at 1MB (beginning of the disk) using mkpart:\n[root@server2 ~]# sudo parted /dev/sdb mkpart primary 1 101m Information: You may need to update /etc/fstab. 4. Verify the new partition with print:\n[root@server2 ~]# sudo parted /dev/sdb print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 101MB 99.6MB primary Partition numbering begins at 1 by default.\n5. Confirm the new partition with the lsblk command:\n[root@server2 ~]# lsblk /dev/sdb NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sdb 8:16 0 250M 0 disk └─sdb1 8:17 0 95M 0 part The device file for the first partition on the sdb disk is sdb1 as identified on the bottom line. The partition size is 95MB.\nDifferent tools will have variance in reporting partition sizes. ignore minor differences.\n6. Check the /proc/partitions file also:\n[root@server2 ~]# cat /proc/partitions | grep sdb 8 16 256000 sdb 8 17 97280 sdb1 Exercise 13-3: Delete an MBR Partition (server2) # delete the sdb1 partition that was created in Exercise 13-2 confirm the deletion.\n1. Execute parted on /dev/sdb with the rm subcommand to remove partition number 1:\n[root@server2 ~]# sudo parted /dev/sdb rm 1 Information: You may need to update /etc/fstab. 2. Confirm the partition deletion with print:\n[root@server2 ~]# sudo parted /dev/sdb print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 3. Check the /proc/partitions file:\n[root@server2 ~]# cat /proc/partitions | grep sdb 8 16 256000 sdb can also run the lsblk command for further verification. T\nEXAM TIP: Knowing either parted or gdisk for the exam is enough.\nGPT Storage Management with gdisk # gdisk (GPT disk) Command # partitions disks using the GPT format.\ntext-based, menu-driven program\nshow, add, verify, modify, and delete partitions\ncan create up to 128 partitions on a single disk on systems with UEFI firmware.\nMain interface of gdisk can be invoked by specifying a disk device name such as /dev/sdc with the command. Type help or ? (question mark) at the prompt to view available subcommands.\n[root@server2 ~]# sudo gdisk /dev/sdc GPT fdisk (gdisk) version 1.0.7 Partition table scan: MBR: not present BSD: not present APM: not present GPT: not present Creating new GPT entries in memory. Command (? for help): ? b\tback up GPT data to a file c\tchange a partition\u0026#39;s name d\tdelete a partition i\tshow detailed information on a partition l\tlist known partition types n\tadd a new partition o\tcreate a new empty GUID partition table (GPT) p\tprint the partition table q\tquit without saving changes r\trecovery and transformation options (experts only) s\tsort partitions t\tchange a partition\u0026#39;s type code v\tverify disk w\twrite table to disk and exit x\textra functionality (experts only) ?\tprint this menu Command (? for help): Exercise 13-4: Create a GPT Partition (server2) # Assign partition type \u0026ldquo;gpt\u0026rdquo; to /dev/sdc for using it as a GPT disk. create and confirm a 200MB partition on the disk. 1. Execute gdisk on /dev/sdc to view the current partition information:\n[root@server2 ~]# sudo gdisk /dev/sdc GPT fdisk (gdisk) version 1.0.7 Partition table scan: MBR: not present BSD: not present APM: not present GPT: not present Creating new GPT entries in memory. Command (? for help): The disk currently does not have any partition table on it.\n2. Assign \u0026ldquo;gpt\u0026rdquo; as the partition table type to the disk using the o subcommand. Enter \u0026ldquo;y\u0026rdquo; for confirmation to proceed. This operation is performed only once on a disk.\nCommand (? for help): o This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y 3. Run the p subcommand to view disk information and confirm the GUID partition table creation:\nCommand (? for help): p Disk /dev/sdc: 512000 sectors, 250.0 MiB Model: VBOX HARDDISK Sector size (logical/physical): 512/512 bytes Disk identifier (GUID): 9446222A-28AC-4F96-816F-518510F95019 Partition table holds up to 128 entries Main partition table begins at sector 2 and ends at sector 33 First usable sector is 34, last usable sector is 511966 Partitions will be aligned on 2048-sector boundaries Total free space is 511933 sectors (250.0 MiB) Number Start (sector) End (sector) Size Code Name The output returns the assigned GUID and states that the partition table can hold up to 128 partition entries.\n4. Create the first partition of size 200MB starting at the default sector with default type \u0026ldquo;Linux filesystem\u0026rdquo; using the n subcommand:\nCommand (? for help): n Partition number (1-128, default 1): First sector (34-511966, default = 2048) or {+-}size{KMGTP}: Last sector (2048-511966, default = 511966) or {+-}size{KMGTP}: +200M Current type is 8300 (Linux filesystem) Hex code or GUID (L to show codes, Enter = 8300): Changed type of partition to \u0026#39;Linux filesystem\u0026#39; 5. Verify the new partition with p:\nCommand (? for help): p Disk /dev/sdc: 512000 sectors, 250.0 MiB Model: VBOX HARDDISK Sector size (logical/physical): 512/512 bytes Disk identifier (GUID): 9446222A-28AC-4F96-816F-518510F95019 Partition table holds up to 128 entries Main partition table begins at sector 2 and ends at sector 33 First usable sector is 34, last usable sector is 511966 Partitions will be aligned on 2048-sector boundaries Total free space is 102333 sectors (50.0 MiB) Number Start (sector) End (sector) Size Code Name 1 2048 411647 200.0 MiB 8300 Linux filesystem 6. Run w to write the partition information to the partition table and exit out of the interface. Enter \u0026ldquo;y\u0026rdquo; to confirm when prompted.\nCommand (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): y OK; writing new GUID partition table (GPT) to /dev/sdc. The operation has completed successfully. You may need to run the partprobe command after exiting the gdisk utility to inform the kernel of partition table changes.\n7. Verify the new partition by issuing either of the following at the command prompt:\n[root@server2 ~]# grep sdc /proc/partitions 8 32 256000 sdc 8 33 204800 sdc1 [root@server2 ~]# lsblk /dev/sdc NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sdc 8:32 0 250M 0 disk └─sdc1 8:33 0 200M 0 part Exercise 13-5: Delete a GPT Partition(server2) # Delete the sdc1 partition that was created in Exercise 13-4 and confirm the removal. 1. Execute gdisk on /dev/sdc and run d1 at the utility\u0026rsquo;s prompt to delete partition number 1:\n[root@server2 ~]# gdisk /dev/sdc GPT fdisk (gdisk) version 1.0.7 Partition table scan: MBR: protective BSD: not present APM: not present GPT: present Found valid GPT with protective MBR; using GPT. Command (? for help): d1 Using 1 2. Confirm the partition deletion with p:\nCommand (? for help): p Disk /dev/sdc: 512000 sectors, 250.0 MiB Model: VBOX HARDDISK Sector size (logical/physical): 512/512 bytes Disk identifier (GUID): 9446222A-28AC-4F96-816F-518510F95019 Partition table holds up to 128 entries Main partition table begins at sector 2 and ends at sector 33 First usable sector is 34, last usable sector is 511966 Partitions will be aligned on 2048-sector boundaries Total free space is 511933 sectors (250.0 MiB) Number Start (sector) End (sector) Size Code Name 3. Write the updated partition information to the disk with w and quit gdisk:\nCommand (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): y OK; writing new GUID partition table (GPT) to /dev/sdc. The operation has completed successfully. 4. Verify the partition deletion by issuing either of the following at the command prompt:\n[root@server2 ~]# grep sdc /proc/partitions 8 32 256000 sdc [root@server2 ~]# lsblk /dev/sdc NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sdc 8:32 0 250M 0 disk Disk Partitions # Be careful when adding a new partition to elude data corruption with overlapping an extant partition or wasting storage by leaving unused space between adjacent partitions. Disk allocated at the time of installation is recognized as sda (s for SATA, SAS, or SCSI device) disk a, first partition identified as sda1 and the second partition as sda2. Any subsequent disks added to the system will be known as sdb, sdc, sdd, and so on, and will use 1, 2, 3, etc. for partition numbering. Use lsblk to list disk and partition information.\n[root@server1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 10G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 9G 0 part ├─rhel-root 253:0 0 8G 0 lvm / └─rhel-swap 253:1 0 1G 0 lvm [SWAP] sr0 11:0 1 9.8G 0 rom /mnt sr0 represents the ISO image mounted as an optical medium:\n[root@server1 ~]# sudo fdisk -l Disk /dev/sda: 10 GiB, 10737418240 bytes, 20971520 sectors Disk model: VBOX HARDDISK Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xfc8b3804 Device Boot Start End Sectors Size Id Type /dev/sda1 * 2048 2099199 2097152 1G 83 Linux /dev/sda2 2099200 20971519 18872320 9G 8e Linux LVM Disk /dev/mapper/rhel-root: 8 GiB, 8585740288 bytes, 16769024 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/rhel-swap: 1 GiB, 1073741824 bytes, 2097152 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes identifiers 83 and 8e are hexadecimal values for the partition types\nStorage Management Tools # parted, gdisk, and LVM Partitions created with a combination of most of these tools and toolsets can coexist on the same disk.\nparted understands both MBR and GPT formats.\ngdisk\nsupport the GPT format only may be used as a replacement of parted. LVM\nfeature-rich logical volume management solution that gives flexibility in storage management. ","externalUrl":null,"permalink":"/tech/linux/partitioning-mbr-and-gpt/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003ePartition Information (MBR and GPT)\n    \u003cdiv id=\"partition-information-mbr-and-gpt\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#partition-information-mbr-and-gpt\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePartition information is stored on the disk in a small region.\u003c/li\u003e\n\u003cli\u003eRead by the operating system at boot time.\u003c/li\u003e\n\u003cli\u003eMaster Boot Record (MBR) on the BIOS-based systems\u003c/li\u003e\n\u003cli\u003eGUID Partition Table (GPT) on the UEFI-based systems.\u003c/li\u003e\n\u003cli\u003eAt system boot, the BIOS/UEFI:\n\u003cul\u003e\n\u003cli\u003escans all storage devices,\u003c/li\u003e\n\u003cli\u003edetects the presence of MBR/GPT areas,\u003c/li\u003e\n\u003cli\u003eidentifies the boot disks,\u003c/li\u003e\n\u003cli\u003eloads the bootloader program in memory from the default boot disk,\u003c/li\u003e\n\u003cli\u003eexecutes the boot code to read the partition table and identify the /boot partition,\u003c/li\u003e\n\u003cli\u003eloads the kernel in memory, and passes control over to it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMBR and GPT store disk partition information and the boot code.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eMaster Boot Record (MBR)\n    \u003cdiv id=\"master-boot-record-mbr\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#master-boot-record-mbr\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eResides on the first sector of the boot disk.\u003c/p\u003e","title":"Partitioning, MBR, and GPT","type":"tech"},{"content":"podman desktop tutorial https://www.youtube.com/watch?v=YXfA5O5Mr18\u0026t=216s\nConnecting to remote machine\nIt\u0026#39;s still experimental though, and buried in the menus (Settings -\u0026gt; Preferences -\u0026gt; Podman Extension -\u0026gt; Remote (a toggle button). But the GUI seemed to throw js errors until after I had podman remote properly setup on the cli as follows, so only turn it on after the cli works! First, you will need to have your ssh keys already setup correctly to ssh in passwordlessly, and I have read that the ed25519 ssh key algorithm is preferred (YMMV). There are many generate and ssh-copy-id tutorials on the internet. But then you still have to add the remote connection at the command line, and not in the GUI. E.g. `podman --remote system connection add \u0026lt;\u0026lt;NICKNAMEYOURREMOTEMACHINEHERE\u0026gt;\u0026gt; --identity ~/.ssh/id_ed25519 ssh://\u0026lt;\u0026lt;YOURREMOTEUSERNAMEHERE\u0026gt;\u0026gt;@\u0026lt;\u0026lt;YOURIPHERE\u0026gt;\u0026gt;/run/user/\u0026lt;\u0026lt;YOURUIDHERE\u0026gt;\u0026gt;/podman/podman.sock` You should test it at the command-line with something like: podman -c REMOTEMACHINENICKNAME ps Once I sorted through the various connection errors (I initially had the wrong uid \u0026amp; had to upgrade my remote podman to version 4) and got it to work at the cli, I turned it on in the GUI and it worked. I can say the Podman Desktop GUI makes it easier to admin remote podman containers. But there are a few issues I can already see with it, such as it does not not yet visually distinguish between a remote and a local container. Again, it is very early stage... ctop to view metrics So, to work with podman there are some quick steps to set the socket enabled and be transparent with docker-ctop:\nInstall podman with dnf when up, enable both podman and the socket: systemctl enable podman systemctl enable --now podman.socket with both services enabled, now confirm where the socket path is set for podman: podman info --format \u0026#39;/run/podman/podman.sock\u0026#39; ## on rocky9 is /run/podman/podman.sock copy the prompted path, and add to your .bashrc (or shell in use) ## Set the DOCKER_HOST environment variable to your Podman socket location. ## Be sure to add the unix:// scheme to the path retrieved previously: ## export DOCKER_HOST=unix://\u0026lt;your_podman_socket_location\u0026gt; ## note to not remove the double slashes export DOCKER_HOST=unix:///run/podman/podman.sock and that should do it 👯‍♀\n","externalUrl":null,"permalink":"/tech/podman/podman-links/","section":"Teches","summary":"\u003cp\u003epodman desktop tutorial\n\u003ca\n  href=\"https://www.youtube.com/watch?v=YXfA5O5Mr18\u0026amp;t=216s\"\n    target=\"_blank\"\n  \u003ehttps://www.youtube.com/watch?v=YXfA5O5Mr18\u0026t=216s\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eConnecting to remote machine\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-Just\" data-lang=\"Just\"\u003e\nIt\u0026#39;s still experimental though, and buried in the menus (Settings -\u0026gt; Preferences -\u0026gt; Podman Extension -\u0026gt; Remote (a toggle button). But the GUI seemed to throw js errors until after I had podman remote properly setup on the cli as follows, so only turn it on after the cli works!\n\nFirst, you will need to have your ssh keys already setup correctly to ssh in passwordlessly, and I have read that the ed25519 ssh key algorithm is preferred (YMMV). There are many generate and ssh-copy-id tutorials on the internet.\n\nBut then you still have to add the remote connection at the command line, and not in the GUI. E.g.\n\n`podman --remote system connection add \u0026lt;\u0026lt;NICKNAMEYOURREMOTEMACHINEHERE\u0026gt;\u0026gt; --identity ~/.ssh/id_ed25519 ssh://\u0026lt;\u0026lt;YOURREMOTEUSERNAMEHERE\u0026gt;\u0026gt;@\u0026lt;\u0026lt;YOURIPHERE\u0026gt;\u0026gt;/run/user/\u0026lt;\u0026lt;YOURUIDHERE\u0026gt;\u0026gt;/podman/podman.sock`\n\nYou should test it at the command-line with something like:\n\npodman -c REMOTEMACHINENICKNAME ps\n\nOnce I sorted through the various connection errors (I initially had the wrong uid \u0026amp; had to upgrade my remote podman to version 4) and got it to work at the cli, I turned it on in the GUI and it worked.\n\nI can say the Podman Desktop GUI makes it easier to admin remote podman containers. But there are a few issues I can already see with it, such as it does not not yet visually distinguish between a remote and a local container. Again, it is very early stage...\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ectop to view metrics\nSo, to work with podman there are some quick steps to set the socket enabled and be transparent with docker-ctop:\u003c/p\u003e","title":"Podman Links","type":"tech"},{"content":"Rootless container quadlet files go under /etc/containers/systemd/users/${UID}/ and the user session will activate the quadlet.\nIf you put it under etc/containers/systemd/users/ then all user sessions will activate the quadlet\nWhen starting a container unit, systemd may time out due to the time it takes to pull a container. Use the TimeoutStartSec Service option to extend the default 90 second timeout or pre-pill the image.\n[Service] TimeoutStartSec=900 To start a container on boot, the generator manually applies the [Install] section of the container definition unit files during generation in the same way systemctl enable does.\n[Install] WantedBy=default.target Podman container gets the same name as the container unit file. So nextcloud.container unit file would create a container calle nextcloud.\nThe Image key is is required and defines the container image that runs.\nExample Quadlet file called /etc/containers/systemd/users/SSPR.container\n[Unit] Description=Self Service Password Container [Container] Image=docker.io/ltbproject/self-service-password:latest Volume=/sspr/config.inc.local.php:/var/www/conf/config.inc.local.php:Z PublishPort=80:80 [Install] WantedBy=multi-user.target default.target ","externalUrl":null,"permalink":"/tech/podman/podman-quadlets/","section":"Teches","summary":"\u003cp\u003eRootless container quadlet files go under /etc/containers/systemd/users/${UID}/ and the user session will activate the quadlet.\u003c/p\u003e\n\u003cp\u003eIf you put it under etc/containers/systemd/users/ then all user sessions will activate the quadlet\u003c/p\u003e\n\u003cp\u003eWhen starting a container unit, systemd may time out due to the time it takes to pull a container. Use the TimeoutStartSec Service option to extend the default 90 second timeout or pre-pill the image.\u003c/p\u003e","title":"Podman Quadlets","type":"tech"},{"content":" Processes and Priorities # Process\na unit for provisioning system resources. any program, application, or command that runs on the system. created in memory when a program, application, or command is initiated. organized in a hierarchical fashion. Each process has a parent process (a.k.a. a calling process) that spawns it. A single parent process may have one or many child processes passes many of its attributes to them at the time of their creation. Each process is assigned an exclusive identification number (Process IDentifier (PID)) is used by the kernel to manage and control the process through its lifecycle. When a process completes its lifespan or is terminated, this event is reported back to its parent process, and all the resources provisioned to it (cpu cycles, memory, etc.) are then freed and the PID is removed from the system. background system processes are called daemons which sit in the memory and wait for an event to trigger a request to use their services. /proc Where information for each running process is recorded and maintained. Referenced by ps and other commands Process States # Five basic process states: running being executed by the system CPU. sleeping waiting for input from a user or another process. waiting has received the input it was waiting for and is now ready to run as soon as its turn comes. stopped currently halted and will not run even when its turn comes unless a signal is sent to change its behavior. zombie Dead. Exists in the process table alongside other process entries takes up no resources. entry is retained until its parent process permits it to die also called a defunct process. ps command # Lists processes specific to the terminal where this command is issued. Shows: PID terminal (TTY) the process spawned in cumulative time (TIME) the system CPU has given to the process name of the command or program (CMD) being executed. may be customized to view only desired columns can use ps to list a process by it\u0026rsquo;s ownership or owning group. Output with -ef UID UID of process owner PID Process ID PPID Parent Process ID C CPU utilization STIME Start time TTY Controlling terminal ? daemon process console system console TIME Aggregated execution time CMD command or program name Flags -e every -f full format -F Extra full format -l long format -efl Detailed process report \u0026ndash;forest tree like hierarchy -x include daemon processes -o user-defined format Make sure there are no white spaces between comma separated values. -C command list list processes that match a specific command name. -U or -u List user supplied as argument. -G or -g List processes owned by a specific group top command # Display processes in real time q or ctrl+c to quit Hotkeys while in top o re-sequence the process list. f add or remove fields F select the field to sort on h help summary portion First 5 lines 1 system uptime, number of users logged in, and system load averages over the period of 1, 5, and 15 minutes. 2 task (or process) information total number of tasks running How many of the total are running, sleeping, stopped, and zombie 3 processor usage CPU time in percentage spent in running user and system processes, in idling and waiting, and so on. 4 memory utilization total, free, used, and allocated for buffering and caching 5 swap useage total, free, and in use avail Mem estimate of memory available for starting processes without using swap. tasks portion details for each process 12 columns 1 and 2 Process identifier (PID) and owner (USER) 3 and 4 Process priority (PR) and nice value (NI) 5 and 6 Depict amounts of virtual memory (VIRT) and non-swapped resident memory (RES) in use 7 Shows the amount of shareable memory available to the process (SHR) 8 Represents the process status (S) 9 and 10 Express the CPU (%CPU) and memory (%MEM) utilization 11 Exhibits the CPU time in hundredths of a second (TIME+) 12 Identifies the process name (COMMAND) Listing a Specific Process # pidof and pgrep command # List only the PID of a specific process pass a process name as an argument to view its PID identical if used without any options Listing Processes by User and Group Ownership # can use ps to list a process by it\u0026rsquo;s ownership or owning group. Process Niceness and Priority # A process is spawned at a certain priority, priority is established based on the nice value. Higher niceness lowers execution priority of a process Lower niceness increase priority. Child process inherits nice value of it\u0026rsquo;s calling process. Can choose a nicenes based on urgency, importance, or system load. Normal users can only increase niceness of their processes. Root can raise or lower niceness of any process. 40 nice values -20 highest and most favorable +19 lowest and least favorable 0 default Showing nice and priority with ps niceness of 0 corresponds to priority of 80 -20 corresponds to priority of 60 Showing nice and priority with top. niceness of 0 corresponds to priority of 20 -20 corresponds to priority of 0 nice command # Launch a program at a non-default priority. renice command # Alter the priority of a running program Controlling Processes with Signals # terminating the process gracefully killing it abruptly forcing it to re-read its configuration. Ordinary users can kill processes that they own, while the root user privilege is needed to kill any process on the system. Processes in a waiting state ignore the soft termination signal. kill command # Pass a signal to a process Requires one or more PIDs Flags\n-l view a list of signals Common signals - 1 SIGHUP (hangup) - causes a process to disconnect itself from a closed terminal that it was tied to - instruct a running daemon to re-read its configuration without a restart. - 2 SIGINT - ^c (Ctrl+c) signal issued on the controlling terminal to interrupt the execution of a process. - 9 SIGKILL - Terminates a process abruptly - 15 SIGTERM (default) - Soft termination signal to stop a process in an orderly fashion. - Default signal if none is specified with the command. - 18 SIGCONT - Same as using the bg command to resume - 19 SIGSTOP - Same as using Ctrl+z to suspend a job - 20 SIGTSTP - Same as using the fg command\npkill command # pass a signal to a process requires one or more process names to send a signal to. Job Scheduling # Run a command at a specified time. One time or periodic. One time command can be used to run a command at a time with low system usage. Periodic examples: creating a compressed archive trimming log files monitoring the system running a custom script removing unwanted files from the system. atd and crond manage jobs atd # Run one time jobs. atd daemon retries a missed job at the same time next day. Does not need a restart with changes crond # Run periodic scheduled jobs. Daemon reads the schedules in files located in the /var/spool/cron and /etc/cron.d directories. scans these files in short intervals updates the in-memory schedules to reflect any modifications. runs a job at its scheduled time only does not entertain any missed jobs. Does not need a restart with changes Controlling user access # all users can schedule jobs access to job scheduling can be edited must add users to allowed or deny file in /etc /etc/at.allow \u0026amp; /etc/cron.allow Does not exist by default. /etc/at.deny \u0026amp; /etc/cron.deny Exists by default list one username per line root user is always permitted Denial message appears if unauthorized user attempts to use at or cron. Only if there is an entry for the calling user in the deny files. at.allow / cron.allow at.deny / cron.deny Impact Exists, and contains user entries Existence does not matter All users listed in allow files are permitted Exists, but is empty Existence does not matter No users are permitted Does not exist Exists, and contains user entries All users, other than those listed in deny files, are permitted Does not exist Exists, but is empty All users are permitted Does not exist Does not exist No users are permitted Scheduler Log File # /var/log/cron - Logs for both atd and cron Shows - time of activity - hostname - process name and PID - owner - message for each invocation - service start time and delays - must have root privileges to view\nat command # schedule a one-time execution of a program in the future. Submitted jobs are spooled in the /var/spool/at/ and executed by the atd daemon at the specified time. file created containing the settings for establishing the user’s shell environment to ensure a successful execution. also includes the name of the command or program to be run. no need to restart the daemon after a job submission. assumes the current year and today’s date if the year and date are not mentioned. ways to express time: at 1:15am (executes the task at the next 1:15 a.m.) at noon (executes the task at 12:00 p.m.) at 23:45 (executes the task at 11:45 p.m.) at midnight (executes the task at 12:00 a.m.) at 17:05 tomorrow (executes the task at 5:05 p.m. on the next day) at now + 5 hours (executes the task 5 hours from now. We can specify minutes, days, or weeks in place of hours) at 3:00 10/15/20 (executes the task at 3:00 a.m. on October 15, 2020) Flags -f supply a filename Crontab # crontab command # other method for scheduling tasks for running in the future. Unlike atd, crond executes cron jobs on a regular basis as defined in the /etc/crontab file. Crontables (another name for crontab files) are located in the /var/spool/cron directory. Each authorized user with a scheduled job has a file matching their login name in this directory. such as /var/spool/cron/user1 /etc/crontab/ \u0026amp; /etc/cron.d/ Other locations for system crontables. Only root can create, modify, or delete them. crond daemon scans entries in all 3 directories. adds log entry to /var/log/cronfile no need to start after modifying cron jobs. flags -e edit crontables -l list crontables -r remove crontables. Do not run crontab -r if you do not wish to remove the crontab file. Instead, edit the file with crontab -e and just erase the entry. -u modify a different user’s crontable provided they are allowed to do so and the other user is listed in the cron.allow file. root user can use the -u flag to alter other users’ crontables even if the affected users are not listed in the allow file. Syntax of User Crontables # /etc/crontab Specifies the syntax that each user cron job must comply with in order for crond to interpret and execute it successfully. Each entry for a user crontable has 6 lines 1-5 schedule 6 login name of executing user rest for command or program to be executed. example crontable line 20 1,12 1-15 feb * ls\u0026gt; /tmp/ls.out Field Content Description 1 Minute of the hour Valid values are 0 (the exact hour) to 59. This field can have one specific value as in field 1, multiple comma-separated values as in field 2, a range of values as in field 3, a mix of fields 2 and 3 (1-5,6-19), or an * representing every minute of the hour as in field 5. 2 Hour of the day Valid values are 0 (midnight) to 23. Same usage applies as described for field 1. 3 Day of the month Valid values are 1 to 31. Same usage applies as described for field 1. 4 Month of the year Valid values are 1 to 12 or jan to dec. Same usage applies as described for field 1. 5 Day of the week Valid values are 0 to 7 or sun to sat, with 0 and 7 representing Sunday, 1 representing Monday, and so on. Same usage applies as described for field 1. 6 Command or program to execute Specifies the full path name of the command or program to be executed, along with any options or arguments that it requires. /etc/crontab contents: Step values may be used with * and ranges in the crontables using the forward slash character (/). Step values allow the number of skips for a given value. Example: /2 in the minute field every second minute /3 in the minute field every third minute, 0-59/4 in the minute field every 4th minute Make sure you understand and memorize the order of the fields defined in crontables.\nAnacron # service that runs after every system reboot checks for any cron and at jobs that were scheduled for execution during the time the system was down and were missed as a result. useful on laptop, desktop, and similar purpose systems with extended periods of frequent downtimes and are not intended for 24/7 operations. Scans the /etc/cron.hourly/0anacron file for three factors to learn whether to run missed jobs. May be run manually at the command line. Run anacron to run all jobs in /etc/anacrontab that were missed. /var/spool/anacron Where anacron stores job execution dates 3 factors must be true for anacron to execute scripts in /etc/cron.daily, /etc/cron.weekly, and /etc/cron.monthly Presence of the /var/spool/anacron/cron.daily file. Elapsed time of 24 hours since it was last run. System is plugged in to an AC source. settings defined in /etc/anacrontab 5 variables defined by default: SHELL and PATH Set the shell and path to be used for executing the programs. MAILTO Defines the login name or an email of the user who is to be sent any output and error messages. RANDOM_DELAY Expresses the maximum arbitrary delay in minutes added to the base delay of the jobs as defined in column 2 of the last three lines. START_HOURS_RANGE States the hour duration within which the missed jobs could be run. Bottom 3 lines define the schedule and the programs to be executed: Column 1: Period in days (or @daily, @weekly, @monthly, or @yearly) How often to run the specified job. Column 2: How many minutes to wait after system boot to execute the job. Column 3: Unique job identifier Columns 4 to 6: Command to be used to execute the scripts located under the /etc/cron.daily, /etc/cron.weekly, and /etc/cron.monthly directories. By default, the run-parts command is invoked for execution at the default niceness. For each job: Examines whether the job was already run during the specified period (column 1). Executes it after waiting for the number of minutes (column 2) plus the RANDOM_DELAY value if it wasn’t. When all missed jobs have been carried out and there is none pending, Anacron exits. Process and Task Scheduling Labs # Lab: ps # ps ps Check manual pages: man ps Run with \u0026ldquo;every\u0026rdquo; and \u0026ldquo;full format\u0026rdquo; flags: ps -ef Produce an output with the command name in column 1, PID in column 2, PPID in column 3, and owner name in column 4, run it as follows: ps -o comm,pid,ppid,user Check how many sshd processes are currently running on the system: ps -C sshd Lab: top # top top View manual page: man top Lab: List a specific process # list the PID of the rsyslogd daemon pidof rsyslogd or pgrep rsyslogd Lab: Listing Processes by User and Group Ownership # List processes owned by user1: ps -U user1 List processes owned by group root: ps -G root Lab: nice # View the default nice value: nice List priority and niceness for all processes: ps -efl Lab: Start Processes at Non-Default Priorities (2 terminals) # Run the top command at the default priority/niceness in Terminal 1: top Check the priority and niceness for the top command in Terminal 2 using the ps command: ps -efl | grep top Terminate the top session in Terminal 1 by pressing the letter q and relaunch it at a lower priority with a nice value of +2: nice -n 2 top \\Check the priority and niceness for the top command in Terminal 2 using the ps command: ps -efl | grep top Terminate the top session in Terminal 1 by pressing the letter q and relaunch it at a higher priority with a nice value of -10. Use sudo for root privileges. sudo nice -n -10 top Check the priority and niceness for the top command in Terminal 2 using the ps command: ps -efl | grep top Terminate the top session by pressing the letter q. Lab: Alter Process Priorities (2 terminals) # Run the top command at the default priority/niceness in Terminal 1: top Check the priority and niceness for the top command in Terminal 2 using the ps command: ps -efl | grep top While the top session is running in Terminal 1, increase its priority by renicing it to -5. Use the command substitution to get the PID of top. Prepend the renice command by sudo. The output indicates the old (0) and new (-5) priorities for the process. sudo renice -n -5 $(pidof top) Validate the above change with ps. Focus on columns 7 and 8. ps -efl | grep top Repeat the above but set the process to run at a lower priority by renicing it to 8: The output indicates the old (-5) and new (8) priorities for the process. sudo renice -n 8 $(pidof top) Validate the above change with ps. Focus on columns 7 and 8. ps -efl | grep top Lab: Controlling Processes with Signals # Pass the soft termination signal to the crond daemon, use either of the following: sudo pkill crond # or sudo kill $(pidof crond) Confirm: ps -ef | grep crond Forcefully kill crond: sudo pkill -9 crond # or sudo pkill -s SIGKILL crond # or sudo kill -9 $(pgrep crond) Kill all crond processes: sudo killall crond View manual pages: man kill man pkill man killall Lab: cron and atd # View log files for cron and atd sudo cat /var/log/cron Lab: at and crond # run /home/user1/.bash_profile file for user1 2 hours from now: at -f ~/.bash_profile now + 2 hours Consult crontab manual pages: man crontab Lab: Submit, View, List, and Erase an at Job # 1.Run the at command and specify the correct execution time and date for the job. Type the entire command at the first at\u0026gt; prompt and press Enter. Press Ctrl+d at the second at\u0026gt; prompt to complete the job submission and return to the shell prompt.\nat 1:30pm 3/31/20 date \u0026amp;\u0026gt; /tmp/date.out The system assigned job ID 5 to it, and the output also pinpoints the job’s execution time.\n2.List the job file created in the /var/spool/at directory:\nsudo ls -l /var/spool/at/ 3.List the spooled job with the at command. You may alternatively use atq to list it.\nat -l # or atq 4.Display the contents of this file with the at command and specify the job ID:\nat -c 5 5.Remove the spooled job with the at command by specifying its job ID. You may alternatively run atrm 5 to delete it.\nat -d 5 This should erase the job file from the /var/spool/at directory. You can\nconfirm the deletion by running atq or at -l. atq Lab: Add, List, and Erase a Cron Job # assume that all users are currently denied access to cron\nEdit the /etc/cron.allow file and add user1 to it: sudo vim /etc/cron.allow user1 Switch to user1 Open the crontable and append the following schedule to it. Save the file when done and exit out of the editor. crontab -e */5 10-11 5,20 * * echo \u0026#34;Hello, this is a cron test.\u0026#34; \u0026gt; /tmp/hello.out Check for the presence of a new file by the name user1 under the /var/spool/cron directory: sudo ls -l /var/spool/cron List the contents of the crontable: crontab -l Remove the crontable and confirm the deletion: crontab -r crontab -l Lab: Anacron # View the default content of /etc/anacrontab without commented or empty lines: cat /etc/anacrontab | grep -ve ^# -ve ^$ View anacron man pages: man anacron Lab 8-1: Nice and Renice a Process # As user1 with sudo on server1, open two terminal sessions. Run the top command in terminal 1. Run the pgrep or ps command in terminal 2 to determine the PID and the nice value of top. ps -efl | grep top Stop top on terminal 1 and relaunch at a lower priority (+8). nice -n 8 top Confirm the new nice value of the process in terminal 2. ps -efl | grep top Issue the renice command in terminal 2 and increase the priority of top to -10: renice -n -10 $(pidof top) Confirm: ps -efl | grep top Lab 8-2: Configure a User Crontab File # As user1 on server1, run the tty and date commands to determine the terminal file (assume /dev/pts/1) and current system time.\ntty date Create a cron entry to display “Hello World” on the terminal. Schedule echo “Hello World” \u0026gt; /dev/tty/1 to run 3 minutes from the current system time.\ncrontab -e */3 * * * * echo \u0026#34;Hello World\u0026#34; \u0026gt; /dev/pts/2 As root, ensure user1 can schedule cron jobs.\nsudo vim /etc/cron.allow user1 ","externalUrl":null,"permalink":"/tech/linux/process-and-task-scheduling/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eProcesses and Priorities\n    \u003cdiv id=\"processes-and-priorities\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#processes-and-priorities\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eProcess\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea unit for provisioning system resources.\u003c/li\u003e\n\u003cli\u003eany program, application, or command that runs on the system.\u003c/li\u003e\n\u003cli\u003ecreated in memory when a program, application, or command is initiated.\u003c/li\u003e\n\u003cli\u003eorganized in a hierarchical fashion.\u003c/li\u003e\n\u003cli\u003eEach process has a parent process (a.k.a. a calling process) that spawns it.\u003c/li\u003e\n\u003cli\u003eA single parent process may have one or many child processes\n\u003cul\u003e\n\u003cli\u003epasses many of its attributes to them at the time of their creation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEach process is assigned an exclusive identification number (Process IDentifier (PID))\n\u003cul\u003e\n\u003cli\u003eis used by the kernel to manage and control the process through its lifecycle.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWhen a process completes its lifespan or is terminated, this event is reported back to its parent process, and all the resources provisioned to it (cpu cycles, memory, etc.) are then freed and the PID is removed from the system.\u003c/li\u003e\n\u003cli\u003ebackground system processes are called daemons\n\u003cul\u003e\n\u003cli\u003ewhich sit in the memory and wait for an event to trigger a request to use their services.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cem\u003e/proc\u003c/em\u003e\n\u003cul\u003e\n\u003cli\u003eWhere information for each running process is recorded and maintained.\u003c/li\u003e\n\u003cli\u003eReferenced by \u003ccode\u003eps\u003c/code\u003e and other commands\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eProcess States\n    \u003cdiv id=\"process-states\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#process-states\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFive basic process states:\n\u003cul\u003e\n\u003cli\u003erunning\n\u003cul\u003e\n\u003cli\u003ebeing executed by the system CPU.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003esleeping\n\u003cul\u003e\n\u003cli\u003ewaiting for input from a user or another process.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ewaiting\n\u003cul\u003e\n\u003cli\u003ehas received the input it was waiting for and is now ready to run as soon as its turn comes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003estopped\n\u003cul\u003e\n\u003cli\u003ecurrently halted and will not run even when its turn comes unless a signal is sent to change its behavior.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ezombie\n\u003cul\u003e\n\u003cli\u003eDead.\u003c/li\u003e\n\u003cli\u003eExists in the process table alongside other process entries\u003c/li\u003e\n\u003cli\u003etakes up no resources.\u003c/li\u003e\n\u003cli\u003eentry is retained until its parent process permits it to die\u003c/li\u003e\n\u003cli\u003ealso called a defunct process.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"\"\n    src=\"../../../img/image-5YWJN0KN%201.jpg\"\n    \u003e\u003c/figure\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003eps\u003c/code\u003e command\n    \u003cdiv id=\"ps-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#ps-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLists processes specific to the terminal where this command is issued.\u003c/li\u003e\n\u003cli\u003eShows:\n\u003cul\u003e\n\u003cli\u003ePID\u003c/li\u003e\n\u003cli\u003eterminal (TTY) the process spawned in\u003c/li\u003e\n\u003cli\u003ecumulative time (TIME) the system CPU has given to the process\u003c/li\u003e\n\u003cli\u003ename of the command or program (CMD) being executed.\u003c/li\u003e\n\u003cli\u003emay be customized to view only desired columns\u003c/li\u003e\n\u003cli\u003ecan use ps to list a process by it\u0026rsquo;s ownership or owning group.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eOutput with -ef\n\u003cul\u003e\n\u003cli\u003eUID\n\u003cul\u003e\n\u003cli\u003eUID of process owner\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePID\n\u003cul\u003e\n\u003cli\u003eProcess ID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePPID\n\u003cul\u003e\n\u003cli\u003eParent Process ID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eC\n\u003cul\u003e\n\u003cli\u003eCPU utilization\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSTIME\n\u003cul\u003e\n\u003cli\u003eStart time\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTTY\n\u003cul\u003e\n\u003cli\u003eControlling terminal\u003c/li\u003e\n\u003cli\u003e?\n\u003cul\u003e\n\u003cli\u003edaemon process\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003econsole\n\u003cul\u003e\n\u003cli\u003esystem console\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTIME\n\u003cul\u003e\n\u003cli\u003eAggregated execution time\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCMD\n\u003cul\u003e\n\u003cli\u003ecommand or program name\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFlags\n\u003cul\u003e\n\u003cli\u003e-e\n\u003cul\u003e\n\u003cli\u003eevery\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-f\n\u003cul\u003e\n\u003cli\u003efull format\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-F\n\u003cul\u003e\n\u003cli\u003eExtra full format\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-l\n\u003cul\u003e\n\u003cli\u003elong format\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-efl\n\u003cul\u003e\n\u003cli\u003eDetailed process report\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u0026ndash;forest\n\u003cul\u003e\n\u003cli\u003etree like hierarchy\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-x\n\u003cul\u003e\n\u003cli\u003einclude daemon processes\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-o\n\u003cul\u003e\n\u003cli\u003euser-defined format\u003c/li\u003e\n\u003cli\u003eMake sure there are no white spaces between comma separated values.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-C\n\u003cul\u003e\n\u003cli\u003ecommand list\u003c/li\u003e\n\u003cli\u003elist processes that match a specific command name.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-U or -u\n\u003cul\u003e\n\u003cli\u003eList user supplied as argument.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e-G or -g\n\u003cul\u003e\n\u003cli\u003eList processes owned by a specific group\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003etop\u003c/code\u003e command\n    \u003cdiv id=\"top-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#top-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDisplay processes in real time\u003c/li\u003e\n\u003cli\u003eq or ctrl+c to quit\u003c/li\u003e\n\u003cli\u003eHotkeys while in top\n\u003cul\u003e\n\u003cli\u003eo\n\u003cul\u003e\n\u003cli\u003ere-sequence the process list.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ef\n\u003cul\u003e\n\u003cli\u003eadd or remove fields\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eF\n\u003cul\u003e\n\u003cli\u003eselect the field to sort on\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eh\n\u003cul\u003e\n\u003cli\u003ehelp\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003esummary portion\n\u003cul\u003e\n\u003cli\u003eFirst 5 lines\n\u003cul\u003e\n\u003cli\u003e1\n\u003cul\u003e\n\u003cli\u003esystem uptime, number of users logged in, and system load averages over the period of 1, 5, and 15 minutes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2\n\u003cul\u003e\n\u003cli\u003etask (or process) information\u003c/li\u003e\n\u003cli\u003etotal number of tasks running\u003c/li\u003e\n\u003cli\u003eHow many of the total are running, sleeping, stopped, and zombie\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e3\n\u003cul\u003e\n\u003cli\u003eprocessor usage\u003c/li\u003e\n\u003cli\u003eCPU time in percentage spent in running user and system processes, in idling and waiting, and so on.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e4\n\u003cul\u003e\n\u003cli\u003ememory utilization\n\u003cul\u003e\n\u003cli\u003etotal, free, used, and allocated for buffering and caching\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e5\n\u003cul\u003e\n\u003cli\u003eswap useage\n\u003cul\u003e\n\u003cli\u003etotal, free, and in use\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eavail Mem\n\u003cul\u003e\n\u003cli\u003eestimate of memory available for starting processes without using swap.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003etasks portion\n\u003cul\u003e\n\u003cli\u003edetails for each process\u003c/li\u003e\n\u003cli\u003e12 columns\n\u003cul\u003e\n\u003cli\u003e1 and 2\n\u003cul\u003e\n\u003cli\u003eProcess identifier (PID) and owner (USER)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e3 and 4\n\u003cul\u003e\n\u003cli\u003eProcess priority (PR) and nice value (NI)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e5 and 6\n\u003cul\u003e\n\u003cli\u003eDepict amounts of virtual memory (VIRT) and non-swapped resident memory (RES) in use\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e7\n\u003cul\u003e\n\u003cli\u003eShows the amount of shareable memory available to the process (SHR)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e8\n\u003cul\u003e\n\u003cli\u003eRepresents the process status (S)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e9 and 10\n\u003cul\u003e\n\u003cli\u003eExpress the CPU (%CPU) and memory (%MEM) utilization\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e11\n\u003cul\u003e\n\u003cli\u003eExhibits the CPU time in hundredths of a second (TIME+)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e12\n\u003cul\u003e\n\u003cli\u003eIdentifies the process name (COMMAND)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eListing a Specific Process\n    \u003cdiv id=\"listing-a-specific-process\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#listing-a-specific-process\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003epidof\u003c/code\u003e and \u003ccode\u003epgrep\u003c/code\u003e command\n    \u003cdiv id=\"pidof-and-pgrep-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#pidof-and-pgrep-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eList only the PID of a specific process\u003c/li\u003e\n\u003cli\u003epass a process name as an argument to view its PID\u003c/li\u003e\n\u003cli\u003eidentical if used without any options\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eListing Processes by User and Group Ownership\n    \u003cdiv id=\"listing-processes-by-user-and-group-ownership\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#listing-processes-by-user-and-group-ownership\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecan use \u003ccode\u003eps\u003c/code\u003e to list a process by it\u0026rsquo;s ownership or owning group.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eProcess Niceness and Priority\n    \u003cdiv id=\"process-niceness-and-priority\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#process-niceness-and-priority\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA process is spawned at a certain priority,\u003c/li\u003e\n\u003cli\u003epriority is established based on the nice value.\u003c/li\u003e\n\u003cli\u003eHigher niceness lowers execution priority of a process\u003c/li\u003e\n\u003cli\u003eLower niceness increase priority.\u003c/li\u003e\n\u003cli\u003eChild process inherits nice value of it\u0026rsquo;s calling process.\u003c/li\u003e\n\u003cli\u003eCan choose a nicenes based on urgency, importance, or system load.\u003c/li\u003e\n\u003cli\u003eNormal users can only increase niceness of their processes.\u003c/li\u003e\n\u003cli\u003eRoot can raise or lower niceness of any process.\u003c/li\u003e\n\u003cli\u003e40 nice values\n\u003cul\u003e\n\u003cli\u003e-20\n\u003cul\u003e\n\u003cli\u003ehighest and most favorable\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e+19\n\u003cul\u003e\n\u003cli\u003elowest and least favorable\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e0\n\u003cul\u003e\n\u003cli\u003edefault\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eShowing nice and priority with ps\n\u003cul\u003e\n\u003cli\u003eniceness of 0 corresponds to priority of 80\u003c/li\u003e\n\u003cli\u003e-20 corresponds to priority of 60\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eShowing nice and priority with top.\n\u003cul\u003e\n\u003cli\u003eniceness of 0 corresponds to priority of 20\u003c/li\u003e\n\u003cli\u003e-20 corresponds to priority of 0\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003enice\u003c/code\u003e command\n    \u003cdiv id=\"nice-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#nice-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLaunch a program at a non-default priority.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003erenice\u003c/code\u003e command\n    \u003cdiv id=\"renice-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#renice-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAlter the priority of a running program\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eControlling Processes with Signals\n    \u003cdiv id=\"controlling-processes-with-signals\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#controlling-processes-with-signals\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eterminating the process gracefully\u003c/li\u003e\n\u003cli\u003ekilling it abruptly\u003c/li\u003e\n\u003cli\u003eforcing it to re-read its configuration.\u003c/li\u003e\n\u003cli\u003eOrdinary users can kill processes that they own, while the root user privilege is needed to kill any process on the system.\u003c/li\u003e\n\u003cli\u003eProcesses in a waiting state ignore the soft termination signal.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003e\u003ccode\u003ekill\u003c/code\u003e command\n    \u003cdiv id=\"kill-command\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#kill-command\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePass a signal to a process\u003c/li\u003e\n\u003cli\u003eRequires one or more PIDs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlags\u003c/p\u003e","title":"Process and Task Scheduling","type":"tech"},{"content":" Remove a filesystem from a partition # To delete a filesystem, partition, raid and disk labels from the disk. Use wipefs -a /dev/sdb1 May also use wipefs -a /dev/sdb? to delete sub partitions? (I need to verify this)\nMake sure the filesystem is unmounted first.\n[root@server2 mapper]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part └─sda2 8:2 0 19G 0 part ├─rhel-root 253:0 0 17G 0 lvm / └─rhel-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 250M 0 disk ├─sdb1 8:17 0 95M 0 part ├─sdb2 8:18 0 95M 0 part └─sdb3 8:19 0 38M 0 part [SWAP] sdc 8:32 0 250M 0 disk sdd 8:48 0 250M 0 disk └─sdd1 8:49 0 163M 0 part ├─vgfs-ext4vol 253:2 0 128M 0 lvm └─vgfs-xfsvol 253:3 0 128M 0 lvm sde 8:64 0 250M 0 disk ├─vgfs-ext4vol 253:2 0 128M 0 lvm ├─vgfs-xfsvol 253:3 0 128M 0 lvm └─vgfs-swapvol 253:7 0 144M 0 lvm [SWAP] sdf 8:80 0 5G 0 disk └─vgvdo1-vpool0_vdata 253:4 0 5G 0 lvm └─vgvdo1-vpool0-vpool 253:5 0 20G 0 lvm └─vgvdo1-lvvdo 253:6 0 20G 0 lvm sr0 11:0 1 9.8G 0 rom [root@server2 mapper]# wipefs -a /dev/sdb1 /dev/sdb1: 2 bytes were erased at offset 0x00000438 (ext4): 53 ef [root@server2 mapper]# wipefs -a /dev/sdb2 /dev/sdb2: 8 bytes were erased at offset 0x00000036 (vfat): 46 41 54 31 36 20 20 20 /dev/sdb2: 1 byte was erased at offset 0x00000000 (vfat): eb /dev/sdb2: 2 bytes were erased at offset 0x000001fe (vfat): 55 aa [root@server2 mapper]# wipefs -a /dev/sdb3 wipefs: error: /dev/sdb3: probing initialization failed: Device or resource busy [root@server2 mapper]# wipefs -a /dev/sdb wipefs: error: /dev/sdb: probing initialization failed: Device or resource busy [root@server2 mapper]# swapoff /dev/sdb3 [root@server2 mapper]# wipefs -a /dev/sdb3 /dev/sdb3: 10 bytes were erased at offset 0x00000ff6 (swap): 53 57 41 50 53 50 41 43 45 32 [root@server2 mapper]# wipefs -a /dev/sdb /dev/sdb: 2 bytes were erased at offset 0x000001fe (dos): 55 aa /dev/sdb: calling ioctl to re-read partition table: Success [root@server2 mapper]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part └─sda2 8:2 0 19G 0 part ├─rhel-root 253:0 0 17G 0 lvm / └─rhel-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 250M 0 disk sdc 8:32 0 250M 0 disk sdd 8:48 0 250M 0 disk └─sdd1 8:49 0 163M 0 part ├─vgfs-ext4vol 253:2 0 128M 0 lvm └─vgfs-xfsvol 253:3 0 128M 0 lvm sde 8:64 0 250M 0 disk ├─vgfs-ext4vol 253:2 0 128M 0 lvm ├─vgfs-xfsvol 253:3 0 128M 0 lvm └─vgfs-swapvol 253:7 0 144M 0 lvm [SWAP] sdf 8:80 0 5G 0 disk └─vgvdo1-vpool0_vdata 253:4 0 5G 0 lvm └─vgvdo1-vpool0-vpool 253:5 0 20G 0 lvm └─vgvdo1-lvvdo 253:6 0 20G 0 lvm sr0 11:0 1 9.8G 0 rom I could not use this on a disk used in an LV. Remove the LVs: lvremove lvvdo vgfs\n[root@server2 mapper]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part └─sda2 8:2 0 19G 0 part ├─rhel-root 253:0 0 17G 0 lvm / └─rhel-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 250M 0 disk sdc 8:32 0 250M 0 disk sdd 8:48 0 250M 0 disk └─sdd1 8:49 0 163M 0 part sde 8:64 0 250M 0 disk └─vgfs-swapvol 253:7 0 144M 0 lvm [SWAP] sdf 8:80 0 5G 0 disk sr0 11:0 1 9.8G 0 rom Need to remove swapvol from swap:\n[root@server2 mapper]# swapoff /dev/mapper/vgfs-swapvol Remove the LV:\n[root@server2 mapper]# lvremove /dev/mapper/vgfs-swapvol Do you really want to remove active logical volume vgfs/swapvol? [y/n]: y Logical volume \u0026#34;swapvol\u0026#34; successfully removed. Wipe sdd:\n[root@server2 mapper]# wipefs -a /dev/sdd /dev/sdd: 2 bytes were erased at offset 0x000001fe (dos): 55 aa /dev/sdd: calling ioctl to re-read partition table: Success [root@server2 mapper]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part └─sda2 8:2 0 19G 0 part ├─rhel-root 253:0 0 17G 0 lvm / └─rhel-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 250M 0 disk sdc 8:32 0 250M 0 disk sdd 8:48 0 250M 0 disk sde 8:64 0 250M 0 disk sdf 8:80 0 5G 0 disk sr0 11:0 1 9.8G 0 rom ","externalUrl":null,"permalink":"/tech/linux/remove-a-filesystem-from-a-partition/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eRemove a filesystem from a partition\n    \u003cdiv id=\"remove-a-filesystem-from-a-partition\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#remove-a-filesystem-from-a-partition\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eTo delete a filesystem, partition, raid and disk labels from the disk. Use \u003ccode\u003ewipefs -a /dev/sdb1\u003c/code\u003e May also use \u003ccode\u003ewipefs -a /dev/sdb?\u003c/code\u003e to delete sub partitions? (I need to verify this)\u003c/p\u003e","title":"Remove a filesystem from a partition","type":"tech"},{"content":"There are a lot of great CCNA resources out there. This list does not include all of them. Only the ones that I personally used to pass the CCNA 200-301 exam.\nMaterials for CCNA are generally separated into 5 categories:\nBooks Video courses Labs Practice test Flashcards Books # Wendell Odom OCG Official cert guide library # To me, this is the king of CCNA study materials. Some people do not like reading but this will give you more depth than any other resource on this list. Link.\nTodd Lammle Books # Yes, I read both the OCG and Todd Lammle books cover to cover. No, I do not recommend doing this. Todd has a great way of adding humor into networking. If you need to build up your networking from the ground up. These books are great. Link.\nVideo Courses # CBT Nuggets # Jeremy Ciara makes learning networking so much fun. This was a great course but is not enough for you to pass the exam on it\u0026rsquo;s own. Also, a CBT nuggets monthly subscription will set you back $59 per month. Link.\nJeremy\u0026rsquo;s IT Lab # Jermey\u0026rsquo;s IT lab course was the most informative for me. Jeremy is really great at explaining the more complex topics. Jeremy\u0026rsquo;s course also includes Packet Tracer labs and and in depth Anki flashcard deck for free. Link.\nLabs # David Bombal\u0026rsquo;s Packet Tracer Labs # These labs will really make you think. Although they do steer off the exam objectives a bit. Link.\nJeremy\u0026rsquo;s IT labs # These were my favorite labs by far. Very easy to set up with clear instructions and video explanations. Link.\nPractice test # Boson Exsim # I can\u0026rsquo;t stress this enough. if there is one resource that you invest some money into. it\u0026rsquo;s the Boson practice exams. This is a test simulator that is very close to what the actual test will be like. Exsim comes with 3 exams.\nAfter taking one of these practice tests you will get a breakdown of your scores per category. You will also get to go through all of your questions and see detailed explantations for why each answer is right or wrong.\nThese practice exams were crucial for me to understand where my knowledge gaps were. Link.\nSubnettingpractice.com # You can learn subnetting pretty good. Then forget some of the steps a month later and have to learn all over again. It was very helpful to go over some of these subnetting questions once in a while. Link.\nFlashcards # Anki Deck # These are the only flashcards I used. It is very nice not to have to create your own flashcards. Having the Anki app on your phone is very convenient. You can study whenever you have a few minutes of downtime.\nAnki also used spaced-repetition. It will give you harder flashcards more often based on how you rate their difficulty.\nThis particular deck goes along with the OCG. You can filter by chapter and add more as you get through the book.\nI will be using Anki flashcards for every exam in the future. Link.\nMy Top 3 # Be careful not to use too many resources. You may get a bit overwhelmed. Especially if this is your first certification like it was for me. You will be building study habits and learning how to read questions correctly. So focus on quality over quantity.\nIf I had to study for the CCNA again, I would use these three resources:\nOCG Boson Exsim Anki Flashcards If you like these posts, please let me know so i can keep making more like them!\n","externalUrl":null,"permalink":"/tech/tools/resources-for-passing-ccna/","section":"Teches","summary":"\u003cp\u003eThere are a lot of great CCNA resources out there. This list does not include all of them. Only the ones that I personally used to pass the CCNA 200-301 exam.\u003c/p\u003e","title":"Resources for Passing CCNA","type":"tech"},{"content":"6.0 Automation and Programmability\n6.5 Describe characteristics of REST-based APIs (CRUD, HTTP verbs, and data encoding)\n6.7 Interpret JSON encoded data\nThe configuration of network devices or report facts about the state of the network.software analyzes data in the form of variables,makes decisions based on that analysis, and then may take action to change the\nIf communicate the variables used by a program: the variable names, their values, and the data structures of those variables.REST provides one standard method of how two automation programs should communicate over a network, JSON then defines how to\nREST-Based APIs\nApplications use application programming interfaces (APIs) to communicate.\none program can learn the variables and data structures used by another program, making logic choices based on those values, changing the values of those variables, creating new variables, and deleting variables\nsome applications create an API, with many other applications using (consuming) the API.\nREST-Based (RESTful) APIs\nREST APIs include the six attributes\n▪ Client/server architecture\n▪ Stateless operation\n▪ Clear statement of cacheable/uncacheable\n▪ Uniform interface\n▪ Layered\n▪ Code-on-demand\nClient/Server Architecture\nREST API call, which generates a message sent to the REST server.\n18 REST and JSON # Tuesday, November 2, 2021 2:46 PM\nthe use of HTTP is not a requirement for an API to be considered RESTful. Stateless Operation each API request and reply does not use any other past history considered when processing the request. For comparison, the TCP protocol uses a stateful approach, whereas UDP uses stateless operation Cacheable (or Not) improve performance by retrieving resources less often (cacheable) cacheable resources are marked with a timeframe so that the client knows when to ask for a new copy of the resource again. List and Dictionary Variables\na data structure defines a related set of variables and values\nPython uses list variables so that one variable name is assigned a value that is a list of values rather than a single value\nthe dictionary data structure lists paired items as well: keys (like terms) and values (like definitions) a single variable’s value could be a list, with each list element being a dictionary\nREST APIs and HTTP\nAPIs exist to allow two programs to exchange data\nMany APIs need to be available to programs that run on other computers, so the API must define the type of networking protocosupported by the API—and many REST-based APIs use the HTTP protocol. ls\nHTTP uses the same principles as REST: it operates with a client/server model; it uses a stateless operational model; and it headers that clearly mark objects as cacheable or not cacheable. It also includes verbs—words that dictate the desired action for a includes\npair HTTP Request and Reply—which matches how applications like to work.\nSoftware CRUD Actions and HTTP Verbs\nacronym—CRUD—for the four primary actions performed by an application. Those actions are:\nCreate:as kept at the serverAllows the client tocreate some new instances of variables and data structures at the server and initialize their values\nRead:structures, and values at the clientAllows the client to retrieve (read) the current value of variables that exist at the server, storing a copy of the variables,\nUpdate:Allows the client to change (update) the value of variables that exist at the server\nDelete:Allows the client todelete from the server different instances of data variables\nHTTP uses verbs that mirror CRUD actions.HTTP defines the concept of an HTTP request and reply Each request/reply lists an action verb in the HTTP request header, which defines the HTTP action. The HTTP messages also incURI, which identifies the resource being manipulated for this request. lude a the HTTP message is carried in IP and TCP, with headers and data, as represented Using URIs with HTTP to Specify the Resource\nREST uses URIs to identify what resource the HTTP request acts on. For REST APIs, the resource can be any one of the many resdefined by the API. Each resource contains a set of related variables, defined by the API and identified by a URI. ources\na URI specific to the Cisco DNA Center northbound REST API as an example of some of the components of the URI.\nHTTPS: The letters before the :// identify the protocol usedencryption). —in this case, HTTP Secure (which uses HTTP with SSL Hostname or IP Address: This value sits between the // and first /, and identifies the host; if using a hostname, the REST client must perform name resolution to learn the IP address of the REST server. Path (Resource): This value sits after the first / and finishes either at the end of the URI or before any additional fields (like a parameter query field). HTTP calls this field the path, but for use with REST, the field uniquely identifies the resource as defined by the API. Go to see more examples of the resources defined by the Cisco DNA Center API.https://developer.cisco.comand search for “Cisco DNA Center API documentation.” Continue to search for yourself to Many of the HTTP request messages need to pass information to the REST server beyond the API REST APIs use HTTP header fields to encode much of the authentication information for REST calls. Additionally, parameters related to a REST call can be passed as parameters as part of the URI itself. the major components of the URIs commonly used with a REST API, with the resource and parameter parts of the URI identifying specifically what the API should supply to the REST client. Example of REST API Call to DNA Center\nsample API calls using a software application called an API development environment tool.\nThe examples in this section use an app named Postman. Postman can be downloaded for free (shown in this section. Note that Cisco DevNet makes extensive use of Postman in its many labs and exampleswww.postman.co) and used as\nThe text follows a data modeling format called JavaScript Object Notation (JSON),\nData Serialization and JSON\nData serialization languages give us a way to represent variables with text rather than in the internal representation used bparticular programming language. y any\nEach data serialization language enables API servers to return data so that the API client can replicate the same variable nawell as data structures as found on the API server mes as\nJavaScript Object Notation (JSON).\nThe Need for a Data Model with APIs\nhow to move variables in a program on a server to a client program. First, Figure 18example as a way to identify some of the challenges with copying variable values from one device to another.-10 and surrounding text show aThen Figure 18nonworking -11 and\nits related text show how to use a data serialization language to solve the problems shown around Figure 18-10.\nThe process shown in Figure 18variables in the same ways. First, programs written in different languages use different conventions to store their -10 does not work (and is not attempted) because the REST client programs may not store variables internally because there is in the same language but with different versions of that language may not store all their variables with the same internal no standard for internal variable storage across languages.In fact, programs written conventions. applications need a standard method to represent variables for transmission and storage of those variables outside the prograData serialization languages provide that function. m. The API converts the internal representation to a data model representing those variables (with JSON shown in the figure). while data serialization languages like JSON enable applications to exchange variables over a network, applications can also data in JSON format. store Data Serialization Languages\nThe terms data serialization language and data modeling language should be considered equivalent\ndifferent data serialization languages\nJSON\nXML\nXML\nXML being a little more challenging to read for the average person\nXML uses beginning and ending tags for each variable,\nYAML\nYAML does not attempt to define markup details (while XML does)\nYAML focuses on the data model (structure) details.\nstrives to be clean and simple\nthe easiest to read for anyone new to data models.\nAnsible makes extensive use of YAML files\nYAML denotes variables in double curly brackets: {{ }}.)\nSummary of Data Serialization\nqbwn\nInterpreting JSON\nInterpreting JSON Key:Value Pairs\nrules about key:value pairs in JSON\nKey:Value Pair:Each and every colon identifies one key:value pair,with thekey before the colon and the value after the colon.\nKey:Text, inside double quotes, before the colon, used as the name that references a value.\nValue: The item after the colon that represents the value of the key, which can be\nText: Listed in double quotes.\nNumeric: Listed without quotes.\nArray:A special value (more details later).\nObject:A special value (more details later)\nMultiple Pairs:pair) When listing multiple key:value pairs, separate the pairs with a comma at the end of each pair (except the last\nThe first two key:value pairs end with a comma, meaning that another key:value pair should follow.begin and end the JSON data denote a single JSON object(one pair of curly brackets, so one object). The JSON files, and JSON data curly brackets that\nexchanged over an API, exist first as a JSON object, with an opening (left) and closing (right) curly bracket as shown.\nInterpreting JSON Objects and Arrays\nTo communicate data structures beyond a key:value pair with a simple value, JSON uses JSON objects and JSON arrays.be somewhat flexible, but in most uses, they act like a dictionary. Arrays list a series of values. Objects can\nFor general conversation, many people refer to the JSON structures as dictionaries and lists rather than as objects and arrays.\n{ } matching right curly bracket.-Object: A series of key:value pairs enclosed in a matched pair of curly brackets,with an opening left curly bracket and its\n[ ] and its matching right square bracket.-Array: A series of values (not key:value pairs) enclosed in a matched pair of square brackets, with an opening left square bracket\nKey:value pairs inside objects: All key:value pairs inside an object conform to the earlier rules for key:value pairs.\nValues inside arraysaround numbers). :All values conform to the earlier rules for formatting values (for example, double quotes around text, no quotes\nJSON arrays can be used as a value in any key:value pair.\nIt has a matched pair of curly brackets to begin and end the text, encapsulating one object.colons, so there are two key:value pairs inside the object. That object contains two JSON can nest objects and arrays; that is, JSON puts one object or array inside another.\nMinified and Beautified JSON\nWhen stored in a file or sent in a network, JSON does not use whitespace.\nYou might see the pretty version literally called pretty, or beautified, or spaced, while the version with no extra whitespaccalled minified or raw. e might be\n","externalUrl":null,"permalink":"/tech/networking/restandjson/","section":"Teches","summary":"\u003cp\u003e6.0 Automation and Programmability\u003cbr\u003e\n6.5 Describe characteristics of REST-based APIs (CRUD, HTTP verbs, and data encoding)\u003cbr\u003e\n6.7 Interpret JSON encoded data\u003c/p\u003e\n\u003cp\u003eThe configuration of network devices or report facts about the state of the network.software analyzes data in the form of variables,makes decisions based on that analysis, and then may take action to change the\u003c/p\u003e","title":"REST and JSON","type":"tech"},{"content":"Configure passwordless ssh access to all servers - write an adhoc script to copy keys to other servers\nStorage has secondary disk attached Configure ansible user \u0026ldquo;davidt\u0026rdquo; with password for all servers. With passwordless sudo Add this to the ad hoc script Add roles path to ansible.cfg as ansible/roles Disable privillege escalation by default? Set ansible to manage 10 hosts at a time. Set davidt as ansible user VMS\npodman-01 mariadb-01 apache-01 storage-01 Groups\nwebserver\ncontainership\nstorage\ndatabase\nMake a playbook motd.yaml that makes /etc/motd display \u0026ldquo;Welcome to {{ service }} server\u0026rdquo;\nMake a playbook sshd.yaml that runs on all hosts and sets the ssh banner to /etc/motd, X11 Forwarding is disabled, and MaxAuthTries is set to 3\nCreate Ansible vault, configure vault password file, make vault and password files utomatically used.\nAdd {{ database_pass }} {{ user_password }} and {{ ansible_user_pass }} variables to vault.\nAdd user list under /group_vars/all/user_list.yaml\n--- users: - username: alice uid: 1201 - username: vincent uid: 1202 - username: sandy uid: 2201 - username: patrick uid: 2202 Create playbook users.yaml\nUsers who\u0026rsquo;s ID begins with 1 is created on webservers group VMs Users Who\u0026rsquo;s id begins with 2 created on the database group user passwords should be used from the {{ user_password }} variable All users added to Wheel group Shell set to /bin/bash for all users Account passwords should use the SHA512 hash format. Each user should have an SSH key uploaded for passwordless ssh access to thier respective servers Create a playbook regular_tasks.yml that runs on servers in the proxy host group and does the following:\nA root crontab record is created that runs every hour. The cron job appends the file /var/log/time.log with the output from the date command. Create a playbook repository.yml that runs on servers in the database host group and does the following:\nA YUM repository file is created. The name of the repository is mysql80-community. The description of the repository is “MySQL 8.0 YUM Repo”. Repository baseurl is http://repo.mysql.com/yum/mysql-8.0-community/el/8/x86_64/. Repository GPG key is at http://repo.mysql.com/RPM-GPG-KEY-mysql. Repository GPG check is enabled. Repository is enabled. Create a role called sample-mysql and store it in /home/automation/plays/roles. The role should satisfy the following requirements:\nA primary partition number 1 of size 800MB on device /dev/sdb is created. An LVM volume group called vg_database is created that uses the primary partition created above. An LVM logical volume called lv_mysql is created of size 512MB in the volume group vg_database. An XFS filesystem on the logical volume lv_mysql is created. Logical volume lv_mysql is permanently mounted on /mnt/mysql_backups. mysql-community-server package is installed. Firewall is configured to allow all incoming traffic on MySQL port TCP 3306. MySQL root user password should be set from the variable database_password (see task #5). MySQL server should be started and enabled on boot. MySQL server configuration file is generated from the my.cnf.j2 Jinja2 template with the following content: [mysqld] bind_address = {{ ansible_default_ipv4.address }} skip_name_resolve datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock\nsymbolic-links=0 sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES\n[mysqld_safe] log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid\nCreate a playbook /home/automation/plays/mysql.yml that uses the role and runs on hosts in the database host group.\nTask 10: Create and Work with Roles (Some More) # Create a role called sample-apache and store it in /home/automation/plays/roles. The role should satisfy the following requirements:\nThe httpd, mod_ssl and php packages are installed. Apache service is running and enabled on boot. Firewall is configured to allow all incoming traffic on HTTP port TCP 80 and HTTPS port TCP 443. Apache service should be restarted every time the file /var/www/html/index.html is modified. A Jinja2 template file index.html.j2 is used to create the file /var/www/html/index.html with the following content: The address of the server is: IPV4ADDRESS\nIPV4ADDRESS is the IP address of the managed node.\nCreate a playbook /home/automation/plays/apache.yml that uses the role and runs on hosts in the webservers host group.\nTask 11: Download Roles From Ansible Galaxy and Use Them # Use Ansible Galaxy to download and install geerlingguy.haproxy role in /home/automation/plays/roles.\nCreate a playbook /home/automation/plays/haproxy.yml that runs on servers in the proxy host group and does the following:\nUse geerlingguy.haproxy role to load balance request between hosts in the webservers host group. Use roundrobin load balancing method. HAProxy backend servers should be configured for HTTP only (port 80). Firewall is configured to allow all incoming traffic on port TCP 80. If your playbook works, then doing “curl http://ansible2.hl.local/” should return output from the web server (see task #10). Running the command again should return output from the other web server.\nTask 12: Security # Create a playbook /home/automation/plays/selinux.yml that runs on hosts in the webservers host group and does the following:\nUses the selinux RHEL system role. Enables httpd_can_network_connect SELinux boolean. The change must survive system reboot. Task 13: Use Conditionals to Control Play Execution # Create a playbook /home/automation/plays/sysctl.yml that runs on all inventory hosts and does the following:\nIf a server has more than 2048MB of RAM, then parameter vm.swappiness is set to 10. If a server has less than 2048MB of RAM, then the following error message is displayed: Server memory less than 2048MB\nTask 14: Use Archiving # Create a playbook /home/automation/plays/archive.yml that runs on hosts in the database host group and does the following:\nA file /mnt/mysql_backups/database_list.txt is created that contains the following line: dev,test,qa,prod. A gzip archive of the file /mnt/mysql_backups/database_list.txt is created and stored in /mnt/mysql_backups/archive.gz. Task 15: Work with Ansible Facts # Create a playbook /home/automation/plays/facts.yml that runs on hosts in the database host group and does the following:\nA custom Ansible fact server_role=mysql is created that can be retrieved from ansible_local.custom.sample_exam when using Ansible setup module. Task 16: Software Packages # Create a playbook /home/automation/plays/packages.yml that runs on all inventory hosts and does the following:\nInstalls tcpdump and mailx packages on hosts in the proxy host groups. Installs lsof and mailx packages on hosts in the database host groups. Task 17: Services # Create a playbook /home/automation/plays/target.yml that runs on hosts in the webservers host group and does the following:\nSets the default boot target to multi-user. Task 18. Create and Use Templates to Create Customised Configuration Files # Create a playbook /home/automation/plays/server_list.yml that does the following:\nPlaybook uses a Jinja2 template server_list.j2 to create a file /etc/server_list.txt on hosts in the database host group. The file /etc/server_list.txt is owned by the automation user. File permissions are set to 0600. SELinux file label should be set to net_conf_t. The content of the file is a list of FQDNs of all inventory hosts. After running the playbook, the content of the file /etc/server_list.txt should be the following:\nansible2.hl.local ansible3.hl.local ansible4.hl.local ansible5.hl.local\nNote: if the FQDN of any inventory host changes, re-running the playbook should update the file with the new values\n","externalUrl":null,"permalink":"/tech/ansible/rhce-practice-exam-lab/","section":"Teches","summary":"\u003cp\u003eConfigure passwordless ssh access to all servers\n- write an adhoc script to copy keys to other servers\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStorage has secondary disk attached\u003c/li\u003e\n\u003cli\u003eConfigure ansible user \u0026ldquo;davidt\u0026rdquo; with password for all servers.\n\u003cul\u003e\n\u003cli\u003eWith passwordless sudo\u003c/li\u003e\n\u003cli\u003eAdd this to the ad hoc script\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAdd roles path to ansible.cfg as ansible/roles\u003c/li\u003e\n\u003cli\u003eDisable privillege escalation by default?\u003c/li\u003e\n\u003cli\u003eSet ansible to manage 10 hosts at a time.\u003c/li\u003e\n\u003cli\u003eSet davidt as ansible user\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eVMS\u003c/p\u003e","title":"RHCE Practice Exam Lab","type":"tech"},{"content":"We are going to use Vagrant to set up two RHEL 8 servers with some custom configuration options. I will include some helpful Vagrant commands at the end if you get stuck.\nIn this guide, I will be using Fedora 38 as my main operating system. I use Fedora because it is similar in features to Red Hat Linux Distributions. This will give me even more practice for the RHCSA exam as I use it in day-to-day operations.\nNote, if you are using Windows, you will need to install ssh. This can be done by installing Git. Which automatically installs ssh for you.\nYou will also need to have the latest version of Virtualbox installed.\nHere are the steps: # Download and install Vagrant Make a new directory for your vagrant lab to live in Add the vagrant box Install the Vagrant disk size plugin Initialize the Vagrant box and Edit the Vagrant file Bring up the Vagrant box 1. Download and install Vagrant. # In Fedora, this is very easy. Run the following command to download and install Vagrant:\nsudo dnf install vagrant\n2. Make a new directory for your vagrant lab to live in. # Make your vagrant directory and make it your current working directory:\ncd Vagrant Add the Vagrant box. vagrant box add generic/rhel8\nInstall the Vagrant disk size plugin. The disk size program will help us set up custom storage sizes. Since we will be re-partitioning storage, this is a useful feature.\nvagrant plugin install vagrant-disksize\nInitialize the Vagrant box and edit the Vagrant file. First, initialize the Vagrant box in the vagrant directory: vagrant init generic/rhel8\nAfter completion, there will now be a file called \u0026ldquo;Vagrantfile\u0026rdquo; in your current directory. Since Vim is on the RHCSA exam, it\u0026rsquo;s wise to practice with it whenever you can. So let\u0026rsquo;s open the file in Vim:\nvim Vagrantfile\nYou will see a bunch of lines commented out, and a few lines without comments. Go ahead and comment out everything and paste this at the end of the file:\nVagrant.configure(\u0026#34;2\u0026#34;) do |config config.vm.box = \u0026#34;generic/rhel8\u0026#34; config.vm.define \u0026#34;server1\u0026#34; do |server1| server1.vm.hostname = \u0026#34;server1.example.com\u0026#34; server1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.2.110\u0026#34; config.disksize.size = \u0026#39;10GB\u0026#39; end config.vm.define \u0026#34;server2\u0026#34; do |server2| server2.vm.hostname = \u0026#34;server2.example.com\u0026#34; server2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.2.120\u0026#34; config.disksize.size = \u0026#39;16GB\u0026#39; end config.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end end| The configuration file is fairly self-explanatory. Save Vagrantfile and exit Vim. Then, create /etc/vbox/networks.conf and add the following:\n* 10.0.0.0/8 192.168.0.0/1 * 2001::/646 This will allow you to be more flexible with what network addresses can be used in VirtualBox.\nBring up the Vagrant box. # Now, we bring up the Vagrant box. This will open two Virtual machines in Virtualbox named server1 and server2 in headless mode (there is no GUI).\nvagrant up\nGreat! Now we can use Vagrant to ssh into server1:\nvagrant ssh server 1\nFrom server1 ssh into server2 using its IP address:\n[vagrant@server1 ~]$ ssh 192.168.2.120\nNow you are in and ready to stir things up. The last thing you need is some commands to manage your Vagrant machines.\nHelpful Vagrant commands. # Shut down Vagrant machines:\nvagrant halt Suspend or resume a machine:\nvagrant suspend vagrand resume Restart a virtual machine:\nvagrant reload\nDestroy a Vagrant machine:\nvagrant destroy [machine-name]\nShow running VMs:\nvagrant status\nList other Vagrant options:\nvagrant\nIf you are going for RHCSA, there is no doubt that you will also use Vagrant sometime in the future. And as you can see, it\u0026rsquo;s pretty quick and simple to get started.\nFeel free to reach out with questions.\n","externalUrl":null,"permalink":"/tech/linux/rhcsa-vagrant-lab-setup/","section":"Teches","summary":"\u003cp\u003eWe are going to use Vagrant to set up two RHEL 8 servers with some custom configuration options. I will include some helpful Vagrant commands at the end if you get stuck.\u003c/p\u003e","title":"RHCSA Vagrant Lab Setup","type":"tech"},{"content":"The configuration of IP addresses differs in some ways, with switches using a VLAN interface and routers using an IP address configured on each working interface.\nswitches do not have auxiliary ports.\nLayer 2 switches support the show mac address-table command, while Cisco routers do not.\nrouters support the show ip route command, while Cisco Layer 2 switches do not.\nLayer 2 switches use the show interfaces status command to list one line of output per interface (and routers do not)\nshow protocols command. This command confirms the state of each of the three R1 interfaces in Figure 15-6 and the IP address and mask configured on those same interfaces.\n","externalUrl":null,"permalink":"/tech/networking/router_operation/","section":"Teches","summary":"\u003cp\u003eThe configuration of IP addresses differs in some ways, with switches using a VLAN interface and routers using an IP address configured on each working interface.\u003c/p\u003e\n\u003cp\u003eswitches do not have auxiliary ports.\u003c/p\u003e","title":"Router Operation","type":"tech"},{"content":"The configuration of IP addresses differs in some ways, with switches using a VLAN interface and routers using an IP address configured on each working interface.\nswitches do not have auxiliary ports.\nLayer 2 switches support the show mac address-table command, while Cisco routers do not.\nrouters support the show ip route command, while Cisco Layer 2 switches do not.\nLayer 2 switches use the show interfaces status command to list one line of output per interface (and\nrouters do not)\nshow protocols 15 - 6 and the IP address and mask configured on those same interfaces.command. This command confirms the state of each of the three R1 interfaces in Figure\n15 Router Operation # Friday, November 12, 2021 9:28 PM\n","externalUrl":null,"permalink":"/tech/networking/routeroperation/","section":"Teches","summary":"\u003cp\u003eThe configuration of IP addresses differs in some ways, with switches using a VLAN interface and routers using an IP address configured on each working interface.\u003c/p\u003e\n\u003cp\u003eswitches do not have auxiliary ports.\u003cbr\u003e\nLayer 2 switches support the \u003cstrong\u003eshow mac address-table\u003c/strong\u003e command, while Cisco routers do not.\u003cbr\u003e\nrouters support the \u003cstrong\u003eshow ip route\u003c/strong\u003e command, while Cisco Layer 2 switches do not.\u003cbr\u003e\nLayer 2 switches use the \u003cstrong\u003eshow interfaces status\u003c/strong\u003e command to list one line of output per interface (and\u003cbr\u003e\nrouters do not)\u003cbr\u003e\n\u003cstrong\u003eshow protocols\u003c/strong\u003e 15 - 6 and the IP address and mask configured on those same interfaces.command. This command confirms the state of each of the three R1 interfaces in Figure\u003c/p\u003e","title":"ROuter Operation","type":"tech"},{"content":"A. Use the sdm prefer lanbase-routing command (or similar) in global configuration mode to change the switch forwarding ASIC settings to make space for IPv4 routes at the next reload of the switch.\nB. Use the reload EXEC command in enable mode to reload (reboot) the switch to pick up the new sdm prefer command setting.\nC. Once reloaded, use the ip routing command in global configuration mode to enable the IPv4 routing function in IOS software and to enable key commands like show ip route.\nif you then enabled OSPF on the Layer 3 switch, the configuration and verification would work the same as it does on a router, as discussed in Chapter 20, “Implementing OSPF.” The routes that IOS adds to the Layer 3 switch’s IP routing table would list the VLAN interfaces as outgoing interfaces.\nTroubleshooting Routing with SVIs\nlook to those first few configuration commands listed in the configuration checklist found in the earlier section “Configuring Routing Using Switch SVIs.” Those commands are sdm prefer (followed by a reload) and then ip routing (after the reload).\nThe sdm prefer command changes how the switch forwarding chips allocate memory for different forwarding tables, and changes to those tables require a reload of the switch. By default, many access switches that support Layer 3 switching still have an SDM default that does not allocate space for an IP routing table. Once changed and reloaded, the ip routing command then enables IPv4 routing in IOS software. Both are necessary before some Cisco switches will act as a Layer 3 switch.\nScenario 1: The last access interface in VLAN 10 is shut down (F0/1), so IOS shuts down the VLAN 10 interface.\nScenario 2: VLAN 20 (not VLAN interface 20, but VLAN 20) is deleted, which results in IOS then bringing down (not shutting down) the VLAN 20 interface.\nScenario 3: VLAN 30 (not VLAN interface 30, but VLAN 30) is shut down, which results in IOS then bringing down (not shutting down) the VLAN 30 interface.\nVLAN Routing with Layer 3 Switch Routed Ports\nOn a routed port, the switch does not perform Layer 2 switching logic on that frame. Instead, frames arriving in a routed port trigger the Layer 3 routing logic, including\nStripping off the incoming frame’s Ethernet data-link header/trailer\nMaking a Layer 3 forwarding decision by comparing the destination IP address to the IP routing table\nAdding a new Ethernet data-link header/trailer to the packet\nForwarding the packet, encapsulated in a new frame\nThe exam topics do not mention routed interfaces specifically, but the exam topics do mention L3 EtherChannels, meaning Layer 3 EtherChannels.\nImplementing Routed Interfaces on Switches\nEnabling a switch interface to be a routed interface instead of a switched interface is simple: just use the no switchport subcommand on the physical interface.\nTo make the port stop acting like a switch port and instead act like a router port, use the no switchport command on the interface.\nOnce the port is acting as a routed port, think of it like a router interface\nthe routed interface will show up differently in command output in the switch. In particular, for an interface configured as a routed port with an IP address, like interface GigabitEthernet0/1 in the previous example:\nKey Topic.\nshow interfaces: Similar to the same command on a router, the output will display the IP address of the interface. (Conversely, for switch ports, this command does not list an IP address.)\nshow interfaces status: Under the “VLAN” heading, instead of listing the access VLAN or the word trunk, the output lists the word routed, meaning that it is a routed port.\nshow ip route: Lists the routed port as an outgoing interface in routes.\nshow interfaces type number switchport: If a routed port, the output is short and confirms that the port is not a switch port. (If the port is a Layer 2 port, this command lists many configuration and status details.)\nAll the ports that are links directly between the Layer 3 switches can be routed interfaces.\nImplementing Layer 3 EtherChannels\neach pair of switches has one routing protocol neighbor relationship with the neighbor, and not two. Each switch learns one route per destination per pair of links, and not two. IOS then balances the traffic, often with better balancing than the balancing that occurs with the use of multiple IP routes to the same subnet.\nStep 1. Configure the physical interfaces as follows, in interface configuration mode:\nA. Add the channel-group number mode on command to add it to the channel. Use the same number for all physical interfaces on the same switch, but the number used (the channel-group number) can differ on the two neighboring switches.\nB. Add the no switchport command to make each physical port a routed port.\nStep 2. Configure the PortChannel interface:\nA. Use the interface port-channel number command to move to port-channel configuration mode for the same channel number configured on the physical interfaces.\nB. Add the no switchport command to make sure that the port-channel interface acts as a routed port. (IOS may have already added this command.)\nC. Use the ip address address mask command to configure the address and mask.\nCisco uses the term EtherChannel in concepts discussed in this section and then uses the term PortChannel, with command keyword port-channel, when verifying and configuring EtherChannels.\nalthough the physical interfaces and PortChannel interface are all routed ports, the IP address should be placed on the PortChannel interface only. In fact, when the no switchport command is configured on an interface, IOS adds the no ip address command to the interface.\nTroubleshooting Layer 3 EtherChannels\nlook at the configuration of the channel-group command, which enables an interface for an EtherChannel. Second, you should check a list of settings that must match on the interfaces for a Layer 3 EtherChannel to work correctly.\nAs for the channel-group interface subcommand, this command can enable EtherChannel statically or dynamically. If dynamic, this command’s keywords imply either Port Aggregation Protocol (PAgP) or Link Aggregation Control Protocol (LACP) as the protocol to negotiate between the neighboring switches whether they put the link into the EtherChannel.\nno switchport: The PortChannel interface must be configured with the no switchport command, and so must the physical interfaces.\nSpeed: The physical ports in the channel must use the same speed.\nduplex: The physical ports in the channel must use the same duplex.\n","externalUrl":null,"permalink":"/tech/networking/lanrouting/","section":"Teches","summary":"\u003cp\u003eA. Use the sdm prefer lanbase-routing command (or similar) in global configuration mode to change the switch forwarding ASIC settings to make space for IPv4 routes at the next reload of the switch.\u003c/p\u003e","title":"Routing in the LAN","type":"tech"},{"content":"2.0 Network Access2.4 Configure and verify (Layer 2/Layer 3) EtherChannel (LACP) 2.5 Describe the need for and basic operations of Rapid PVST+ Spanning Tree Protocol and identify basic operations2.5.a Root port, root bridge (primary/secondary), and other port names 2.5.b Port states (forwarding/blocking)2.5.c PortFast benefits Most network engineers make the distribution layer switches be the root. STP Modes and Standards Three options to configure on the spanninguse -tree mode command, which tells the switch which type of STP to and STPtheswitches do not support STP or RSTP with the single tree (CST)-based PVST+, Cisco-proprietary and RSTP-based RPVST+, or the IEEE standard MSTP.. They can use either the Cisco-proprietary\nMSTP allows the definition of as many instances (multiple spanning tree instances, or MSTIs) as chosen by the network designer but does not require one per VLAN. STP and MSTP now exist as part of the 802.1Q standard, which defines VLANs and VLAN trunking. The revised rules divide the original priority field into two separate fields, as shown in Figure 10priority field and a 12-bit subfield called the system ID extension (which represents the VLAN ID-4: a 4-bit Cisco switches let you configure the BID, but only the priority part. The switch fills in its universal (burnedin the 12-bit system ID extension field; you cannot change that behavior either. -in) MAC address as the system ID. It also plugs in the VLAN ID of a VLAN The only part configurable by the network engineer is the 4-bit priority field. the configuration command (65,535. But not just any number in that range will suffice; it must be a multiple of 4096spanning-tree vlan vlan-id priority x) requires a decimal number between 0 and ,as emphasized in the help text shown in Example 10-2. Command Guide RSTP #spanning#spanning--tree vlan vlantree vlan x root primary-id priority x #spanning#show spanning-tree vlan x root secondary-tree vlan 9 EtherChannel (int)#channel#show etherchannel 1 summary-group 1 mode on #show etherchannel summary#show etherchannel 1 port-channel #show etherchannel load#channel-group 1 mode desirable/auto (PAgP)-balance #channel#port-channel load-group 1 mode active/passive (LACP)-balance src-dst-mac 10. RSTP and EtherChannelTuesday, July 27, 2021 12:04 PM # help text shown in Example 10-2. #spanning-tree vlan x root primary(on the switch that should be primary)\n#spanning-tree vlan x root secondary(on the switch that should be secondary)\nThese two commands cause the switch to make a choice of priority value but then store the chosen priority value in thespanning-tree vlan x priority valuecommand.The command with root primary or root secondary\ndoes not appear in the configurationcurrent root switch and chooses either (a) 24,576 or (b) 4096 less than the current root’s priority (if the current. When configuring root primary, the switch looks at the priority of the\nroot’s priority is 24,576 or less) to the configuration instead. When configuring, root secondary always results in that switch using a priority of 28,672, with the assumption that the value will be less than other switches that\nuse the default of 32,768, and higher than any switch configured as root primary.\nHow Switches Use the Priority and System ID Extension\nCisco Catalyst switches configure the priority value using a number that represents a 16system ID extension exists as the low-order 12 bits of that same number. -bit value; however, the\nWhen the switch builds its BID to use for RSTP in a VLAN, it must combine the configured priority with the VLAN ID of that VLAN.\nthe configured priority results in a 16-bit priority that always ends with 12 binary 0s.\nRoot Switch: 24,576 (priority) + 9 (VLAN ID) = 24585Local Switch: 32,768 (priority) + 9 (VLAN ID) = 32777 RSTP creates one treeVLAN. —the Common Spanning Tree (CST)—while RPVST+ creates one tree for each and every RSTP sends one set of RSTP messages (BPDUs) in the network, no matter the number of VLANs, while RPVST+ sends one set of messages per VLAN. RSTP and RPVST+ use different destination MAC addresses: RSTP with multicast address 0180.C200.0000 (an address defined in the IEEE standard), and RPVST+ with multicast address 0100.0CCC.CCCD (an address chosen by Cisco). When transmitting messages on VLAN trunks, RSTP sends the messages in the native VLAN with no VLAN header/tag. RPVST+ sends each VLAN’s messages inside that VLAN—for instance, BPDUs about VLAN 9 have an 802.1Q header that lists VLAN 9. RPVST+ adds an extra type(because it does not need to, as RSTP ignores VLANs.)-length value (TLV) to the BPDU that identifies the VLAN ID, while RSTP does not Both view the 160000.0000.0000, meaning “no VLAN,” while RPVST+ uses the VLAN ID.-bit priority as having a 12-bit System ID Extension, with RSTP setting the value to RSTP Methods to Support Multiple Spanning Trees\nOther RSTP Configuration Options\nSwitch Priority: The global commandthat VLAN. spanning-tree vlan x priority ylets an engineer set the switch’s priority in\nPrimary and Secondary Root Switches: The global command lets you set the priority, but the switch decides on a value to make that switch likely to be the primary root spanning-tree vlan x root primary | secondaryalso\nswitch (the root) or the secondary root switch (the switch that becomes root if the primary fails).\nPort Costs: The interface subcommand cost on that port, either for all VLANs or for a specific VLAN on that port. Changing those costs then changes the spanning-tree [vlan x] cost ylets an engineer set the switch’s STP/RSTP\nroot cost for some switches, which impacts the choice of root ports and designated ports.\nConfiguring Layer 2 Etherchannel\nConfiguring a Manual Layer 2 EtherChannel\nsimply add the correct channelall with the on keyword, and all with the same number. The on keyword tells the switches to place a -group configuration command to each physical interface, on each switch,\nphysical interface into an EtherChannel, and the number identifies the PortChannel interface number that the interface should be a part of.\nthree terms as synonyms: EtherChannel, PortChannel, and Channelgroup configuration command, but then to display its status, IOS uses the show etherchannel command. -group. Oddly, IOS uses the channel-\nThen the output of this show command refers to neither an “EtherChannel” nor a “Channelinstead using the term “PortChannel.” So, pay close attention to these three terms in the example.-group,”\nStep 1. Add theeach physical interface that should be in the channel to add it to the channel.channel-group number mode oncommand in interface configuration mode under\nStep 2. on the neighboring switch can differ.Use the same number for all commands on the same switch, but the channel-group number\nPo1, short for PortChannel1\nConfiguring Dynamic EtherChannels\nCisco switches also support two different configuration options that then use a dynamic protocol to negotiate whether a particular link becomes part of an EtherChannel or not. Basically, the configuration\nenables a protocol for a particular channelto send messages to/from the neighboring switch and discover whether their configuration settings pass -group number. At that point, the switch can use the protocol\nall checks. If a given physical link passes, the link is added to the EtherChannel and used; if not, it is placed in a down state, and not used, until the configuration inconsistency can be resolved.\nMost Cisco Catalyst switches support thestandard Link Aggregation Control Protocol (LACP),Cisco-proprietary Port Aggregation Protocol (PAgP) and the IEEE\nnegotiate so that only links that pass the configuration checks are actually used in an EtherChannel.\nLACP does support more links in a channelcan be active at one time, with the others waiting to be used should any of the other links fail.— 16 —as compared to PAgP’s maximum of 8. With LACP, only 8\nTo configure either protocol, a switch uses the channelbut with a keyword that either means “use this protocol and begin negotiations” or “use this protocol and -group configuration commands on each switch,\nwait for the other switch to begin negotiations.”\ndesirable and auto keywords enable PAgP\nactive and passive keywords enable LACP.\nwith PAgP, at least one of the two sides must use desirable, and with LACP, at least one of the two sides must use active.\nDo not use the on parameter on one end, and either auto or desirable (or for LACP, active or passive) on the neighboring switch. The on option uses neither PAgP nor LACP, so a configuration that uses on, with PAgP or LACP options on the other end, would prevent the EtherChannel from working. #show etherchannel 1 port-channel. Physical Interface Configuration and EtherChannels\nthe switch compares the new physical port’s configuration to the existing ports in the channel. That new physical interface’s settings must be the same as the existing ports’ settings; otherwise, the switch does\nnot add the new link to the list of approved and working interfaces in the channel. That is, the physical interface remains configured as part of the PortChannel, but it is not used as part of the channel,\nThe list of items the switch checks includes the following:\n▪▪SpeedDuplex\n▪▪Operational access or trunking state (all must be access, or all must be trunks)If an access port, the access VLAN\n▪▪If a trunk port, the allowed VLAN list (per the switchport trunk allowed command)If a trunk port, the native VLAN\n▪STP interface settings\nswitches check the settings on the neighboring switch. To do so, the switches either use PAgP or LACP (if already in use) or use Cisco Discovery Protocol (CDP) if using manual configuration. When checking\nneighbors, all settings except the STP settings must match.\nerrthe -disabled state.PortChannel and physical interfaces must be shutdown, and then no shutdown, to recover from the when a switch applies the shutdown and no shutdown commands to a PortChannel, it applies those same commands to the physical interfaces, as well; so, just do the shutdown/no shutdown on the PortChannel\ninterface.\nEtherChannel Load Distribution\nWhen using Layer 2 EtherChannels,PortChannel interfaces and not the underlying physical portsa switch’s MAC learning process associates MAC addresses with the. Later, when a switch makes a forwarding\ndecision to send a frame out a PortChannel interface, the switch must do more work: tspecific physical port to use to forward the frame. IOS documentation refers to those rules as o decide out which\nEtherChannel load distribution or load balancing.\nConfiguration options for EtherChannel Load Distribution\nEtherChannel load distribution makes the choice for each frame based on various numeric values found in the Layer 2, 3, and 4 headers. The process uses one configurable setting as input: the load distribution\nmethod as defined with theperforms some match against the fields identified by the configured method.port-channel load-balance methodglobal command. The process then\nsome switches may support only MACthe model and software version. -based methods, or only MAC-and IP-based methods, depending on\nvarious load distribution algorithms do share some common goals:\nTo being sent over different links.cause all messages in a single application flow to use the same link in the channel, rather than Doing so means that the switch will not inadvertently reorder the\nmessages sent in that application flow by sending one message over a busy link that has a queue of waiting messages, while immediately sending the next message out an unused link.\nTo integrate the load distribution algorithm work into the hardware forwarding ASIC so that load distribution works just as quickly as the work to forward any other frame.\nTo use all the active links in the EtherChannel, adjusting to the addition and removal of active links over time.\nWithin the constraints of the other goals, balance the traffic across those active links.\nthe algorithms first intend to avoid message reordering, make use of the switch forwarding ASICs, and use all the active links. However, the algorithm does not attempt to send the exact same number of bits over\neach link over time.other goals. The algorithm does try to balance the traffic, but always within the constraints of the\nTtypically differ the most in real networks, while the highhe algorithm focuses on the low-order bits in the fields in the headers because the low-order bits do not differ much.By focusing on the -order bits\nlower-order bits, the algorithm achieves better balancing of traffic over the links.\nThe Effects of the EtherChannel Load Distribution Algorithm\nshowing the use of theconsider some addresses or ports and answer the question: which link would you use when forwarding a test etherchannel load-balance EXEC command. That command asks the switch to\nmessage with those address/port values?\nAll three tests list the same outgoing physical interface because (1) the method uses only the source MAC address, and all three tests use the same MAC addresses. All three tests use a different destination MAC address, with different low-order bits, but that had no impact on the choice because the method—src-mac— does not consider the destination MAC address. chennel-group command cannot override the channel-protocol command\n","externalUrl":null,"permalink":"/tech/networking/rstpandetherchannel/","section":"Teches","summary":"\u003cp\u003e2.0 Network Access2.4 Configure and verify (Layer 2/Layer 3) EtherChannel (LACP)\n2.5 Describe the need for and basic operations of Rapid PVST+ Spanning Tree Protocol and identify basic operations2.5.a Root port, root bridge (primary/secondary), and other port names\n2.5.b Port states (forwarding/blocking)2.5.c PortFast benefits\nMost network engineers make the distribution layer switches be the root.\nSTP Modes and Standards\nThree options to configure on the spanninguse -tree mode command, which tells the switch which type of STP to\nand STPtheswitches do not support STP or RSTP with the single tree (CST)-based PVST+, Cisco-proprietary and RSTP-based RPVST+, or the IEEE standard MSTP.. They can use either the Cisco-proprietary\u003c/p\u003e","title":"RSTP and Etherchannel","type":"tech"},{"content":"Thursday, September 23, 2021 10:32 AM\n- MD5 # Type 8 SHA- 256 enable algorithm-type sha256 secret password Type 9 SHA- 256 enable algorithm-type scrypt secret password New enable secret commands with different algorithm types replace any existing enable secret command. - # Encoding the Passwords for Local Usernames\nUsername secret command Encoding\nusername name [algorithm-type md5] secret password\nusername username namename algorithmalgorithm\u0026ndash;type shatype scrypt secret -256 secret passwordpassword\nControlling Password Attacks with ACLs\n(line vty)# - Bond ACL 3 to a vty line access-class 3 in\nlooks at the destination IP address instead of the source filters based on the device to which the telnet or ssh command is trying to connect. (line vty)# access-class 3 out Traditional Firewalls\nmatch the source and destination IP addressesmatching their static well-known TCP and UDP ports additional TCP and UDP portsMatch the text in the URI of an HTTP request - storing information about each packetmake decisions about filtering future packets based on the historical state - - information (could be tracking the number of TCP connections per secondstateful inspection) - recording state information based on earlier packets state information (stateful firewall) would not have had the historical state information to realize that a DoS attack was occurring. stateless firewall or a router ACL Security Zones\nSecure\nZone Inside\nNot secure\nZone Outside\nDMZ\ndefining which hosts can initiate new connections.\nDMZ- Access by public\nIntrusion Prevention Systems (IPS\nIPS first IPS can log the event, discard packets, or even redirect the packets to another security application downloads a database of exploit signatures\nfor further examination. - # needs to download and keep updating its signature database.\nCisco Next-Generation Firewalls\nCisco firewall- Cisco Adaptive Security Appliance (ASA).\ncomparing fields in the IP, TCP, and UDP headers, and using security zones when defining\nfirewall rules stateful filtering- looks at the application layer data to identify the applicationdeep packet inspection can identify many applications based on the data sent application data structures far past the TCP and UDP headers).(application layer headers plus Application Visibility and Control (AVC) Duties of a NGFW Traditional firewall:stateful firewall filtering, NAT/PAT, and VPN termination. Application Visibility and Control (AVC) A networktransfers that would install malware, and saving copies of files for later analysis.-based antimalware function can run on the firewall itself, blocking file\nAdvanced Malware Protection: examines the URLs in each web request, categorizes the URLs, and either filters or rate limits the traffic based on rules. The Cisco Talos security group monitors and creates reputation scores for each domain known in the Internet, with URL filtering being able to use those scores in its decision to categorize, filter, or rate limit. URL Filtering- : This feature NGIPS # Cisco Next-Generation IPS\nexamines the context by gathering data from all the hosts and the users of those hosts. will know the OS, software revision levels, what apps are running, open ports, the transport\nprotocols and port numbers in use, and so on.\nArmed with that data, the NGIPSlog. can make much more intelligent choices about what events to\nNGIPS Duties\nusing exploit signatures to compare packet flows, creating a log of events, and possibly discarding and/or redirecting packets.\nTraditional IPS:\nApplication Visibility and Control (AVC):\ngather data from hosts—OS, software version/level, patches applied, applications running, open ports, applications currently sending data, and so on. Those facts inform the NGIPS as to the often more limited vulnerabilities in a portion of the network so that the NGIPS can focus on actual vulnerabilities while greatly reducing the number of logged events. Contextual Awareness: -\nReputationcan perform reputation-Based Filtering: -based filtering, taking the scores into account.\nprovides an assessment based on impact levels Event Impact Level:\n5.0 Security Fundamentals\n5.7 Configure Layer 2 security features (DHCP snooping, dynamic ARP inspection, and port security)\nPort Security Concepts and Configuration\nidentifies devices based on the source MAC address of Ethernet frames that the devices send.no restrictions on whether the frame came from a local device or was forwarded through other\nswitches - # examines frames received on the interface to determine if a violation has occurred. defines a maximum number of unique source MAC addresses allowed for all frames coming in the interface. keeps a list and counter of all unique source MAC addresses on the interface.monitors newly learned MAC addresses, considering those MAC addresses to cause a violation if\nthe newly learned MAC address would push the total number of MAC table entries for the\ninterface past the configured maximum allowed MAC addresses for that port. - # takes action to discard frames from the violating MAC addresses, plus other actions depending on the configured violation mode. - # Other port security options Define a maximum of three MAC addresses, defining all three specific MAC addresses. Define a maximum of three MAC addresses but allow those addresses to be dynamically learned, allowing the first three MAC addresses learned. Define a maximum of three MAC addresses, predefining one specific MAC address, and allowing two more to be dynamically learned.\nport security learns the MAC addresses off each port so that you do not have to preconfigure the values. It also adds the learned MAC addresses to the port security\nconfiguration (in the running-config file). sticky secure MAC addresses.- Configuring Port Security\nworks on both access ports and trunk portsrequires you to statically configure the port as a trunk or an access port. Use theUse the switchport mode accessswitchport port-security or the interface subcommand to enable port security on the switchport mode trunk interface subcommands\ninterface. - # switchport port□ override the default maximum number of allowed MAC addresses associated with the interface (1).-security maximum number (Optional) override thedefault action to take upon a security violation (shutdown). (Optional) - switchport port-security violation {protect | restrict | shutdown} ","externalUrl":null,"permalink":"/tech/networking/securing-network-devices/","section":"Teches","summary":"\u003cp\u003eThursday, September 23, 2021 10:32 AM\u003c/p\u003e\n\n\u003ch5 class=\"relative group\"\u003e- MD5\n    \u003cdiv id=\"--md5\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#--md5\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eType 8\u003c/li\u003e\n\u003cli\u003eSHA- 256\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eenable algorithm-type sha256 secret password\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eType 9 SHA- 256\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eenable algorithm-type scrypt secret password\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eNew enable secret commands with different algorithm types replace any existing enable secret\ncommand.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5 class=\"relative group\"\u003e-\n    \u003cdiv id=\"-\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#-\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h5\u003e\n\u003cp\u003eEncoding the Passwords for Local Usernames\u003cbr\u003e\nUsername secret command Encoding\u003cbr\u003e\n\u003cstrong\u003eusername\u003c/strong\u003e \u003cem\u003ename\u003c/em\u003e \u003cstrong\u003e[algorithm-type md5] secret\u003c/strong\u003e \u003cem\u003epassword\u003c/em\u003e\u003cbr\u003e\n\u003cstrong\u003eusername username\u003c/strong\u003e \u003cem\u003enamename\u003c/em\u003e \u003cstrong\u003ealgorithmalgorithm\u0026ndash;type shatype scrypt secret -256 secret\u003c/strong\u003e \u003cem\u003epasswordpassword\u003c/em\u003e\u003c/p\u003e","title":"Securing Network Devices","type":"tech"},{"content":"This chapter covers the following exam topics:\n5.0 Security Fundamentals\n5.1 Define key security concepts (threats, vulnerabilities, exploits, and mitigation techniques)\n5.2 Describe security program elements (user awareness, training, and physical access control)\n5.4 Describe security password policies elements, such as management, complexity, and password\nalternatives (multifactor authentication, certificates, and biometrics)\n5.8 Differentiate authentication, authorization, and accounting concepts\nAttacks That Spoof Addresses\nattacker sends packets with a spoofed source IP addressattacker sends spoofed MAC addresses DHCP requests with spoofed MAC addresses can be sent to a DHCP server○ filling its address lease table and leaving no free IP addresses for normal use. Denial-of-Service Attacks\n○ no ack replies cause the server to leave open the connection and fill up the table ○ Server is no longer able to do legitimate TCP connections server adds the TCP connection and replies to the fake address with a SYN-ACK. ICMP echo (ping) packets flood of UDP packets, and TCP connections, such as the TCP SYN flood distributed denial- attack is distributed across a large number of bots, all flooding or attacking the same target.-of-service (DDoS) Reflection and Amplification Attacks\nReflection attack\nAttacker sends packets to the reflector hosthost (reflector) reflects the exchange toward the spoofed address that is the target. The attacker might also send the spoofed packets to multiple reflectors, causing the target to receive multiple copies of the unexpected traffic. ▪ when reflector (Corporate server) responds, it sends packets to victim Attackers source packet is the IP of the victim attacker sends a small amount of traffic to a reflector to generate large volume of traffic to a target leverages a protocol or service to generate the traffic mechanisms of DNS and NTP have been exploited for this Amplification attack Tuesday, September 21, 2021 2:02 PM\nmechanisms of DNS and NTP have been exploited for this\nMan-in-the-Middle Attacks can exploit the ARP table to communicate with other hosts on the local networkattacker sends the last ARP reply so that any listening host will update its ARP table with the most recent information. attacker can repeat this process by poisoning the ARP entries on multiple hosts and then relaying traffic between them without easy detection. Address Spoofing Attack Summary\nExhaust a system service or resource Crash target system DoS/DDoS Reflection/ Amplification- Trick unwilling accomplice host to send traffic to target Eavesdrop on traffic Modify traffic passing through man-in-the-middle Reconnaissance Attacks\ndiscover details about the target and its systems prior to an actual attack.use common tools to uncover public details like who owns a domain and what IP address ranges are used there. nslookup The whois can reveal the owner of the domain and the IP address space registered to it. and dig commands are complementary tools that can query DNS information to reveal\ndetailed information about domain owners, contact information, mail servers, authoritative name servers, and so on. - # using ping sweeps to send pings to each IP address in the target range. Hosts that answer the ping sweep then become live targets.\nPort scanning tools can then sweep through a range of UDP and TCP ports to see if a target host\nanswers on any port numbers. Any replies indicate that a corresponding service is running on the target host. - # not a true attack because nothing is exploited as a result.\nBuffer Overflow Attacks\nincoming data might be stored in unexpected memory locations if a buffer is allowed to fill beyond\nits limit. - # sending data that is larger than expected. If a vulnerability exists, the target system might store that data, overflowing its buffer into another area of memory, eventually crashing a service or the entire system.\nattackers specially craft the large message by inserting malicious code in it. - If the target system stores that data as a result of a buffer overflow, then it can potentially run the malicious code without realizing. - # can spread from one computer to another only through user interaction Trojan horse Malware\ncan spread from one computer to another only through user interaction such as opening email attachments, downloading software from the Internet, and inserting\na USB drive into a computer. - # Packaged inside other software\ncan propagate between systems more readily. To spread, into another application, then rely on users to transport the infected application software to virus software must inject itself\nother victims. - # Self-injected into other software viruses Worms- -propagates automatically Human Vulnerabilities\ntargets a group of similar users who might work for the same company, shop at the same stores, and so on. - # all receive the same convincing email with a link to a malicious site.\nWhaling ▪▪ similar to phishing targets high-profile individuals in corporations, governments, and organizations. - # voice calls (vishing)SMS text messages (smishing). Spear phishing altered DNS service or local hosts file entry for a legitimate site. altered name resolution returns the address of a malicious site instead.\nPharming: frequently visited site is compromised and malware is deposited there. infects only the target users who visit the site Watering Hole: Securing Network Access with Cisco AAA\nAuthentication, Authorization, and Accounting\nAuthentication\n-Name and passwordChallenge and response Token cards\nAuthorization Takes place after authentication is validated the operation that specific user is allowed to performProvides needed resources specifically allowed to a certain user and permits\nAccounting accessed.Records what the user did on the network as well as the resources they Keeps track of how much time they spent using network resources.\nAuthentication Methods\nLeast secure to most secure methods: No username or password Username/password (static)Aging username/password OneToken cards/soft tokens-time passwords (OTP This chapter covers the following exam topics:\n1.0 Network Fundamentals\n1.1 Explain the Role of Network Components\n1.1.c Next-generation Firewalls and IPS\n4.0 IP Services\n4.8 Configure network devices for remote access using SSH\n5.0 Security Fundamentals\n5.3 Configure device access control using local passwords\nEncrypting Older IOS Passwords with service password-encryption\n▪▪ password username passwordname password (console or vty mode) password ▪ enable password password encrypts passwords that are normally held as clear text encoding type of “7” service password-encryption no service password - passwords remain encrypted until password is changed - encryption Hashing the enable secret\nnever stores the clearIOS computes the MD5 hash of the password in the enable secret command and stores the hash -text password\nof the password in the configuration. - # IOS hashes the clear-text password as typed by the user. IOS compares the two hashed values Can be used without having to enter the password no enable secret Improved Hashes for Cisco’s Enable Secret\nThe two newer alternative algorithm types - Both use an SHA-256 hash instead of MD5\nType 5 MD5 enable [algorithm-type md5] secret password ","externalUrl":null,"permalink":"/tech/networking/security-architectures/","section":"Teches","summary":"\u003cp\u003eThis chapter covers the following exam topics:\u003cbr\u003e\n5.0 Security Fundamentals\u003cbr\u003e\n5.1 Define key security concepts (threats, vulnerabilities, exploits, and mitigation techniques)\u003cbr\u003e\n5.2 Describe security program elements (user awareness, training, and physical access control)\u003cbr\u003e\n5.4 Describe security password policies elements, such as management, complexity, and password\u003cbr\u003e\nalternatives (multifactor authentication, certificates, and biometrics)\u003cbr\u003e\n5.8 Differentiate authentication, authorization, and accounting concepts\u003c/p\u003e","title":"Security Architectures","type":"tech"},{"content":" SELinux Terminology # Implementation of the Mandatory Access Control (MAC) architecture\ndeveloped by the U.S. National Security Agency (NSA) flexible, enriched, and granular security controls in Linux. integrated into the Linux kernel as a set of patches using the Linux Security Modules (LSM) framework allows the kernel to support various security implementations provides an added layer of protection above and beyond the standard Linux Discretionary Access Control (DAC) security architecture. DAC includes: traditional file and directory permissions extended attribute settings setuid/setgid bits su/sudo privileges etc. Limits the ability of a subject (Linux user or process) to access an object (file, directory, file system, device, network interface/connection, port, pipe, socket, etc.) To reduce or eliminate the potential damage the subject may be able to inflict on the system if compromised. MAC controls\nFine-grained Protect other services in the event one service is negotiated. Example: If the HTTP service process is compromised, the attacker can only damage the files the hacked process will have access to, and not the other processes running on the system, or the objects the other processes will have access to. To ensure this coarse-grained control, MAC uses a set of defined authorization rules called policy to examine security attributes associated with subjects and objects when a subject tries to access an object, and decides whether to permit the access attempt. These attributes are stored in contexts (a.k.a. labels), and are applied to both subjects and objects. SELinux decisions are stored in a special cache area called Access Vector Cache (AVC).\nThis cache area is checked for each access attempt by a process to determine whether the access attempt was previously allowed.\nWith this mechanism in place, SELinux does not have to check the policy ruleset repeatedly, thus improving performance.\nSELinux is enabled by default\nConfines processes to the bare minimum privileges that they need to function. Key Terms # Subject\nAny user or process that accesses an object. Examples: system_u for the SELinux system user unconfined_u for subjects that are not bound by the SELinux policy Stored in field 1 of the context. Object\nA resource, such as a file, directory, hardware device, network interface/connection, port, pipe, or socket, that a subject accesses. Examples: object_r for general objects system_r for system-owned objects unconfined_r for objects that are not bound by the SELinux policy. Access\nAn action performed by the subject on an object. Examples: creating, reading, or updating a file creating or navigating a directory accessing a network port or socket Policy\nA defined ruleset that is enforced system-wide Used to analyze security attributes assigned to subjects and objects. Referenced to decide whether to permit a subject\u0026rsquo;s access attempt to an object, or a subject\u0026rsquo;s attempt to interact with another subject. The default behavior of SELinux in the absence of a rule is to deny the access. Standard preconfigured policies: targeted (default) Any process that is targeted runs in a confined domain Any process that is not targeted runs in an unconfined domain. Example: SELinux runs logged-in users in the unconfined domain, and the httpd process in a confined domain by default. Any subject running unconfined is more vulnerable than the one running confined. mls Places tight security controls at deeper levels. minimum light version of the targeted policy designed to protect only selected processes. Context (label)\nA tag to store security attributes for subjects and objects. Every subject and object has a context assigned Consists of a SELinux user, role, type (or domain), and sensitivity level. Used by SELinux to make access control decisions. Labeling\nMapping of files with their stored contexts. SELinux User\nSeveral predefined SELinux user identities that are authorized for a particular set of roles. Linux user to SELinux user identity mapping to place SELinux user restrictions on Linux users. Controls what roles and levels a process (with a particular SELinux user identity) can enter. Example: A Linux user cannot run the su and sudo commands or the programs located in their home directories if they are mapped to the SELinux user user_u. Role\nAn attribute of the Role-Based Access Control (RBAC) security model that is part of SELinux. Classifies who (subject) is allowed to access what (domains or types). SELinux users are authorized for roles, and roles are authorized for domains and types. Each subject has an associated role to ensure that the system and user processes are separated. A subject can transition into a new role to gain access to other domains and types. Examples: user_r for ordinary users sysadm_r for administrators system_r for processes that initiate under the system_r role Stored in field 2 of the context. Type Enforcement (TE)\nIdentifies and limits a subject\u0026rsquo;s ability to access domains for processes, and types for files. References the contexts of the subjects and objects for this enforcement. Type\nAttribute of type enforcement. Group of objects based on uniformity in their security requirements. Objects such as files and directories with common security requirements, are grouped within a specific type. Examples: user_home_dir_t for objects located in user home directories usr_t for most objects stored in the /usr directory. Stored in field 3 of a file context. Domain\nDetermines the type of access that a process has. Processes with common security requirements are grouped within a specific domain type, and they run confined within that domain. Examples: init_t for the systemd process firewalld_t for the firewalld process unconfined_t for all processes that are not bound by SELinux policy. Stored in field 3 of a process context. Rules\nOutline how types can access each other, domains can access types, and domains can access each other. Level\nAn attribute of Multi-Level Security (MLS) and Multi-Category Security (MCS). Pair of sensitivity:category values that defines the level of security in the context. category may be defined as a single value or a range of values, such as c0.c4 to represent c0 through c4. In RHEL 9, the targeted policy is used as the default, which enforces MCS (MCS supports only one sensitivity level (s0) with 0 to 1023 different categories). SELinux Contexts # SELinux Contexts for Users # SELinux contexts define security attributes placed on subjects and objects. Each context contains a type and a security level with subject and object information. Use the id command with the -Z option to view the context set on Linux users:\n[root@server30 ~]# id -Z unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 Output:\nMapped to the SELinux unconfined_u user\nNo SELinux restrictions placed on this user.\nAll Linux users, including root, run unconfined by default, (full system access)\nSeven confined user identities with restricted access to objects.\nMapped to Linux users via SELinux policy. Helps safeguard the system from potential damage that Linux users might inflict on the system. Use the seinfo query command to list the SELinux users; however, the setools-console software package must be installed before doing so.\n[root@server30 ~]# seinfo -u Users: 8 guest_u root staff_u sysadm_u system_u unconfined_u user_u xguest_u Use the semanage command to view the mapping between Linux and SELinux users:\n[root@server30 ~]# semanage login -l Login Name SELinux User MLS/MCS Range Service __default__ unconfined_u s0-s0:c0.c1023 * root unconfined_u s0-s0:c0.c1023 * MLS/MCS Range\nAssociated security level and the context for the Linux user (the * represents all services). By default, all non-root Linux users are represented as __default__, which is mapped to the unconfined_u user in the policy. SELinux Contexts for Processes # Determine the context for processes using the ps command with the -Z flag:\n[root@server30 ~]# ps -eZ | head -2 LABEL PID TTY TIME CMD system_u:system_r:init_t:s0 1 ? 00:00:02 systemd Output:\nThe subject system_u is a SELinux username (mapped to Linux user root)\nObject is system_r\nDomain init_t reveals the type of protection applied to the process.\nLevel of security s0\nA process that is unprotected will run in the unconfined_t domain.\nSELinux Contexts for Files # ls -Z\nView context for files and directories. Show the four attributes set on the /etc/passwd file:\n[root@server30 ~]# ls -lZ /etc/passwd -rw-r--r--. 1 root root system_u:object_r:passwd_file_t:s0 2806 Jul 19 21:54 /etc/passwd Subject system_u Object object_r Type passwd_file_t Security level s0 for the passwd file. /etc/selinux/targeted/contexts/files/file_contexts /etc/selinux/targeted/contexts/files/file_contexts.local\nStores contexts for system-installed and user-created files. Policy files. Can be updated using the semanage command. Copying, Moving, and Archiving Files with SELinux Contexts # All files in RHEL are labeled with an SELINUX security context by default. New files inherit the parent directory\u0026rsquo;s context at the time of creation. Rules for copy move and archive:\nIf a file is copied to a different directory, the destination file will receive the destination directory\u0026rsquo;s context, unless the --preserve=context switch is specified with the cp command to retain the source file\u0026rsquo;s original context.\nIf a copy operation overwrites the destination file in the same or different directory, the file being copied will receive the context of the overwritten file, unless the --preserve=context switch is specified with the cp command to preserve the source file\u0026rsquo;s original context.\nIf a file is moved to the same or different directory, the SELinux context will remain intact, which may differ from the destination directory\u0026rsquo;s context.\nIf a file is archived with the tar command, use the --selinux option to preserve the context.\nSELinux Contexts for Ports # View attributes for network ports with the semanage command:\n[root@server30 ~]# semanage port -l | head -7 SELinux Port Type Proto Port Number afs3_callback_port_t tcp 7001 afs3_callback_port_t udp 7001 afs_bos_port_t udp 7007 afs_fs_port_t tcp 2040 afs_fs_port_t udp 7000, 7005 By default, SELinux allows services to listen on a restricted set of network ports only. Domain Transitioning # SELinux allows a process running in one domain to enter another domain to execute an application that is restricted to run in that domain only. A rule must exist in the policy to support such transition. entrypoint Permission setting Control processes that can transition into another domain. Example: What happens when a Linux user attempts to change their password using the /usr/bin/passwd command.\nThe passwd command is labeled with the passwd_exec_t type:\n[root@server30 ~]# ls -lZ /usr/bin/passwd -rwsr-xr-x. 1 root root system_u:object_r:passwd_exec_t:s0 32648 Aug 10 2021 /usr/bin/passwd The passwd command requires access to the /etc/shadow file in order to modify a user password. The shadow file has a different type set on it (shadow_t):\n**[root@server30 ~]# ls -lZ /etc/shadow ----------. 1 root root system_u:object_r:shadow_t:s0 2756 Jul 19 21:54 /etc/shadow The SELinux policy has rules that specifically allow processes running in domain passwd_t to read and modify the files with type shadow_t, and allow them entrypoint permission into domain passwd_exec_t. This rule enables the user\u0026rsquo;s shell process executing the passwd command to switch into the passwd_t domain and update the shadow file. Open two terminal windows. In window 1, issue the passwd command as user1 and wait at the prompt:\n[user1@server30 root]$ passwd Changing password for user user1. Current password: In window 2, run the ps command:\n[root@server30 ~]# ps -eZ | grep passwd unconfined_u:unconfined_r:passwd_t:s0-s0:c0.c1023 13001 pts/1 00:00:00 passwd The passwd command (process) transitioned into the passwd_t domain to change the user password. SELinux Booleans # on/off switches that SELinux uses to determine whether to permit an action. Activate or deactivate certain rule in the SELinux policy immediately and without the need to recompile or reload the policy. For instance, the ftpd_anon_write Boolean can be turned on to enable anonymous users to upload files. This privilege can be revoked by turning this Boolean off. Boolean values are stored in virtual files located in /sys/fs/selinux/booleans/. The filenames match the Boolean names. A sample listing of this directory is provided below:\n[root@server30 ~]# ls -l /sys/fs/selinux/booleans/ | head -7 total 0 -rw-r--r--. 1 root root 0 Jul 23 04:44 abrt_anon_write -rw-r--r--. 1 root root 0 Jul 23 04:44 abrt_handle_event -rw-r--r--. 1 root root 0 Jul 23 04:44 abrt_upload_watch_anon_write -rw-r--r--. 1 root root 0 Jul 23 04:44 antivirus_can_scan_system -rw-r--r--. 1 root root 0 Jul 23 04:44 antivirus_use_jit -rw-r--r--. 1 root root 0 Jul 23 04:44 auditadm_exec_content The manual pages of the Booleans are available through the selinux-policy-doc package.\nOnce installed, use the -K option with the man command to bring the pages up for a specific Boolean.\nFor instance, issue man -K abrt_anon_write to view the manual pages for the abrt_anon_write Boolean.\nCan be viewed, and flipped temporarily or for permanence.\nNew value takes effect right away.\nTemporary changes are stored as a \u0026ldquo;1\u0026rdquo; or \u0026ldquo;0\u0026rdquo; in the corresponding Boolean file in the /sys/fs/selinux/booleans/\nPermanent changes are saved in the policy database.\nSELinux Administration # Controlling the activation mode, checking operational status, setting security contexts on subjects and objects, and switching Boolean values. Utilities and the commands they provide\nlibselinux-utils getenforce getsebool policycoreutils sestatus setsebool restorecon policycoreutils-python-utils semanage setools-console seinfo sesearch SELinux Alert Browser\nGraphical tool for viewing alerts and debugging SELinux issues.\nPart of the setroubleshoot-server package.\nIn order to fully manage SELinux, you need to ensure that all these packages are installed on the system.\nManagement Commands # SELinux delivers a variety of commands for effective administration. Table 20-1 lists and describes the commands mentioned above plus a few more under various management categories.\nMode Management\ngetenforce\nDisplays the current mode of operation grubby\nUpdates and displays information about the configuration files for the grub2 boot loader sestatus\nShows SELinux runtime status and Boolean values setenforce\nSwitches the operating mode between enforcing and permissive temporarily Context Management\nchcon\nChanges context on files (changes do not survive file system relabeling) restorecon\nRestores default contexts on files by referencing the files in /etc/selinux/targeted/contexts/files/ semanage Changes context on files with the fcontext subcommand (changes survive file system relabeling) Policy Management\nseinfo Provides information on policy components semanage\nManages policy database sesearch\nSearches rules in the policy database Boolean Management\ngetsebool\nDisplays Booleans and their current settings. setsebool\nModifies Boolean values temporarily, or in the policy database. semanage\nModifies Boolean values in the policy database with the booleansubcommand. Troubleshooting\nsealert\nThe graphical troubleshooting tool Viewing and Controlling SELinux Operational State # /etc/selinux/config\nOne of the key configuration files that controls the SELinux operational state, and sets its default type The default content of the file is displayed below:\n[root@server30 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. # See also: # https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/using_selinux/changing-selinux-states-and-modes_using-selinux#changing-selinux-modes-at-boot-time_changing-selinux-states-and-modes # # NOTE: Up to RHEL 8 release included, SELINUX=disabled would also # fully disable SELinux during boot. If you need a system with SELinux # fully disabled instead of SELinux running with no policy loaded, you # need to pass selinux=0 to the kernel command line. You can use grubby # to persistently set the bootloader to boot with selinux=0: # # grubby --update-kernel ALL --args selinux=0 # # To revert back to SELinux enabled: # # grubby --update-kernel ALL --remove-args selinux # SELINUX=enforcing # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE=targeted Directives:\nSELINUX\nSets the activation mode for SELinux. Enforcing activates it and allows or denies actions based on the policy rules. Permissive activates SELinux, but permits all actions. It records all security violations. Useful for troubleshooting and developing or tuning the policy. The third option is to completely turn SELinux off. When running in enforcing mode SELINUXTYPE\nDictates the type of policy to be enforced. Default is targeted. Determine the current operating mode: getenforce\nChange the state to permissive and verify:\n[root@server30 ~]# setenforce permissive [root@server30 ~]# getenforce Permissive Can also \u0026ldquo;0\u0026rdquo; for permissive and a \u0026ldquo;1\u0026rdquo; for enforcing. Changes will be lost at the next system reboot. Edit /etc/selinux/config SELINUX directive to the desired mode for persistence. EXAM TIP: You may switch SELinux to permissive for troubleshooting a non-functioning service. Don\u0026rsquo;t forget to change it back to enforcing when the issue is resolved.\nDisable SELinux persistently: grubby --update-kernel ALL --args selinux=0\nAppends the selinux=0 setting to the end of the \u0026ldquo;options\u0026rdquo; line in the bootloader configuration file located in the /boot/loader/entries directory: cat /boot/loader/entries/dcb323fab47049e8b89dae2ae00d41e8-5.14.0-427.26.1.el9_4.x86_64.conf Revert the above: grubby --update-kernel ALL --remove-args selinux=0\nQuerying Status # sestatus Command\nView the current runtime status of SELinux Displays the location of principal directories, the policy in effect, and the activation mode. [root@server30 ~]# sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: permissive Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 33 -v Report on security contexts set on files and processes, as listed in /etc/sestatus.conf . Reports the contexts for the current process (Current context) and the init (systemd) process (Init context) under Process Contexts. Reveals the file contexts for the controlling terminal and associated files under File Contexts. [root@server30 ~]# cat /etc/sestatus.conf [files] /etc/passwd /etc/shadow /bin/bash /bin/login /bin/sh /sbin/agetty /sbin/init /sbin/mingetty /usr/sbin/sshd /lib/libc.so.6 /lib/ld-linux.so.2 /lib/ld.so.1 [process] /sbin/mingetty /sbin/agetty /usr/sbin/sshd [root@server30 ~]# sestatus -v SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: permissive Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 33 Process contexts: Current context: unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 Init context: system_u:system_r:init_t:s0 /sbin/agetty system_u:system_r:getty_t:s0-s0:c0.c1023 /usr/sbin/sshd system_u:system_r:sshd_t:s0-s0:c0.c1023 File contexts: Controlling terminal: unconfined_u:object_r:user_devpts_t:s0 /etc/passwd system_u:object_r:passwd_file_t:s0 /etc/shadow system_u:object_r:shadow_t:s0 /bin/bash system_u:object_r:shell_exec_t:s0 /bin/login system_u:object_r:login_exec_t:s0 /bin/sh system_u:object_r:bin_t:s0 -\u0026gt; system_u:object_r:shell_exec_t:s0 /sbin/agetty system_u:object_r:getty_exec_t:s0 /sbin/init system_u:object_r:bin_t:s0 -\u0026gt; system_u:object_r:init_exec_t:s0 /usr/sbin/sshd system_u:object_r:sshd_exec_t:s0 Lab: Modify SELinux File Context # Create a directory sedir1 under /tmp and a file sefile1 under sedir1. Check the context on the directory and file. Change the SELinux user and type to user_u and public_content_t on both and verify. 1. Create the hierarchy sedir1/sefile1 under /tmp:\n[root@server30 ~]# cd /tmp [root@server30 tmp]# mkdir sedir1 [root@server30 tmp]# touch sedir1/sefile1 2. Determine the context on the new directory and file:\n[root@server30 tmp]# ls -ldZ sedir1 drwxr-xr-x. 2 root root unconfined_u:object_r:user_tmp_t:s0 21 Jul 28 15:12 sedir1 [root@server30 tmp]# ls -ldZ sedir1/sefile1 -rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Jul 28 15:12 sedir1/sefile1 3. Modify the SELinux user (-u) on the directory to user_u and type (-t) to public_content_t recursively (-R) with the chcon command:\n[root@server30 tmp]# chcon -vu user_u -t public_content_t sedir1 -R changing security context of \u0026#39;sedir1/sefile1\u0026#39; changing security context of \u0026#39;sedir1\u0026#39; 4. Validate the new context:\n[root@server30 tmp]# ls -ldZ sedir1 drwxr-xr-x. 2 root root user_u:object_r:public_content_t:s0 21 Jul 28 15:12 sedir1 [root@server30 tmp]# ls -ldZ sedir1/sefile1 -rw-r--r--. 1 root root user_u:object_r:public_content_t:s0 0 Jul 28 15:12 sedir1/sefile1 Lab: Add and Apply File Context # Add the current context on sedir1 to the SELinux policy database to ensure a relabeling will not reset it to its previous value Change the context on the directory to some random values. Restore the default context from the policy database back to the directory recursively. Determine the current context: [root@server30 tmp]# ls -ldZ sedir1 drwxr-xr-x. 2 root root user_u:object_r:public_content_t:s0 21 Jul 28 15:12 sedir1 [root@server30 tmp]# ls -ldZ sedir1/sefile1 -rw-r--r--. 1 root root user_u:object_r:public_content_t:s0 0 Jul 28 15:12 sedir1/sefile1 Add (-a) the directory recursively to the policy database using the semanage command with the fcontext subcommand: [root@server30 tmp]# semanage fcontext -a -s user_u -t public_content_t \u0026#34;/tmp/sedir1(/.*)?\u0026#34; The regular expression (/.*)? instructs the command to include all files and subdirectories under /tmp/sedir1. Needed only if recursion is required. The above command added the context to the /etc/selinux/targeted/contexts/files/file_contexts.local file.\nValidate the addition by listing (-l) the recent changes (-C) in the policy database: [root@server30 tmp]# semanage fcontext -Cl | grep sedir /tmp/sedir1(/.*)? all files user_u:object_r:public_content_t:s0 Change the current context on sedir1 to something random (staff_u/etc_t) with the chcon command: root@server30 tmp]# chcon -vu staff_u -t etc_t sedir1 -R changing security context of \u0026#39;sedir1/sefile1\u0026#39; changing security context of \u0026#39;sedir1\u0026#39; The security context is changed successfully. Confirm with the ls command: [root@server30 tmp]# ls -ldZ sedir1 ; ls -lZ sedir1/sefile1 drwxr-xr-x. 2 root root staff_u:object_r:etc_t:s0 21 Jul 28 15:12 sedir1 -rw-r--r--. 1 root root staff_u:object_r:etc_t:s0 0 Jul 28 15:12 sedir1/sefile1 Reinstate the context on the sedir1 directory recursively (-R) as stored in the policy database using the restorecon command: (-F option will update all attributes, only does type by default. ) $ restorecon -R -v -F sedir1 Relabeled /tmp/sedir1 from unconfined_u:object_r:public_content_t:s0 to user_u:object_r:public_content_t:s0 Relabeled /tmp/sedir1/sefile1 from unconfined_u:object_r:public_content_t:s0 to user_u:object_r:public_content_t:s0 Lab: Add and Delete Network Ports # Add a non-standard network port 8010 to the SELinux policy database for the httpd service. Confirm the addition. Remove the port from the policy and verify the deletion. List (-l) the ports for the httpd service as defined in the SELinux policy database: [root@server10 ~]# semanage port -l | grep ^http_port http_port_t tcp 80, 81, 443, 488, 8008, 8009, 8443, 9000 The output reveals eight network ports the httpd process is currently allowed to listen on.\nAdd port 8010 with type http_port_t and protocol tcp to the policy: [root@server10 ~]# semanage port -at http_port_t -p tcp 8010 Confirm the addition: [root@server10 ~]# semanage port -l | grep ^http_port http_port_t tcp 8010, 80, 81, 443, 488, 8008, 8009, 8443, 9000 Delete port 8010 from the policy and confirm: [root@server10 ~]# semanage port -dp tcp 8010 [root@server10 ~]# semanage port -l | grep ^http_port http_port_t tcp 80, 81, 443, 488, 8008, 8009, 8443, 9000 EXAM TIP: Any non-standard port you want to use for any service, make certain to add it to the SELinux policy database with the correct type.\nLab: Copy Files with and without Context # Create a file called sefile2 under /tmp and display its context. Copy this file to the /etc/default directory, and observe the change in the context. Remove sefile2 from /etc/default, and copy it again to the same destination, ensuring that the target file receives the source file\u0026rsquo;s context. 1. Create file sefile2 under /tmp and show context:\n[root@server10 ~]# touch /tmp/sefile2 [root@server10 ~]# ls -lZ /tmp/sefile2 -rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Jul 29 08:44 /tmp/sefile2 2. Copy this file to the /etc/default directory, and check the context again:\n[root@server10 ~]# cp /tmp/sefile2 /etc/default/ [root@server10 ~]# ls -lZ /etc/default/sefile2 -rw-r--r--. 1 root root unconfined_u:object_r:etc_t:s0 0 Jul 29 08:45 /etc/default/sefile2 3. Erase the /etc/default/sefile2 file, and copy it again with the --preserve=context option:\n[root@server10 ~]# rm /etc/default/sefile2 [root@server10 ~]# cp --preserve=context /tmp/sefile2 /etc/default 4. List the file to view the context:\n[root@server10 ~]# ls -lZ /etc/default/sefile2 -rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Jul 29 08:49 /etc/default/sefile2 Exercise 20-5: View and Toggle SELinux Boolean Values # Display the current state of the Boolean nfs_export_all_rw. Toggle its value temporarily, and reboot the system. Flip its value persistently after the system has been back up. 1. Display the current setting of the Boolean nfs_export_all_rw using three different commands\u0026mdash;getsebool, sestatus, and semanage:\n[root@server10 ~]# getsebool -a | grep nfs_export_all_rw nfs_export_all_rw --\u0026gt; on [root@server10 ~]# sestatus -b | grep nfs_export_all_rw nfs_export_all_rw on [root@server10 ~]# semanage boolean -l | grep nfs_export_all_rw nfs_export_all_rw (on , on) Allow nfs to export all rw [root@server10 ~]# 2. Turn off the value of nfs_export_all_rw using the setsebool command by simply furnishing \u0026ldquo;off\u0026rdquo; or \u0026ldquo;0\u0026rdquo; with it and confirm:\n[root@server10 ~]# setsebool nfs_export_all_rw 0 [root@server10 ~]# getsebool -a | grep nfs_export_all_rw nfs_export_all_rw --\u0026gt; off 3. Reboot the system and rerun the getsebool command to check the Boolean state:\n[root@server10 ~]# getsebool -a | grep nfs_export_all_rw nfs_export_all_rw --\u0026gt; on 4. Set the value of the Boolean persistently (-P or -m as needed) using either of the following:\n[root@server10 ~]# setsebool -P nfs_export_all_rw off [root@server10 ~]# semanage boolean -m -0 nfs_export_all_rw 5. Validate the new value using the getsebool, sestatus, or semanage command:\n[root@server10 ~]# sestatus -b | grep nfs_export_all_rw nfs_export_all_rw off [root@server10 ~]# semanage boolean -l | grep nfs_export_all_rw nfs_export_all_rw (off , off) Allow nfs to export all rw [root@server10 ~]# semanage boolean -l | grep nfs_export_all_rw nfs_export_all_rw (off , off) Allow nfs to export all rw Monitoring and Analyzing SELinux Violations # SELinux generates alerts for system activities when it runs in enforcing or permissive mode.\nIt writes the alerts to /var/log/audit/audit.logif the auditd daemon is running, or to /var/log/messages via the rsyslog daemon in the absence of auditd.\nSELinux also logs the alerts that are generated due to denial of an action, and identifies them with a type tag AVC (Access Vector Cache) in the audit.log file.\nIt also writes the rejection in the messages file with a message ID, and how to view the message details.\nSELinux denial messages are analyzed, and the audit data is examined to identify the potential cause of the rejection.\nThe results of the analysis are recorded with recommendations on how to fix it.\nThese results can be reviewed to aid in troubleshooting, and recommended actions taken to address the issue.\nSELinux runs a service daemon called setroubleshootd that performs this analysis and examination in the background.\nThis service also has a client interface called SELinux Troubleshooter (the sealert command) that reads the data and displays it for assessment.\nThe client tool has both text and graphical interfaces.\nThe server and client components are part of the setroubleshoot-server software package that must be installed on the system prior to using this service.\nHow SELinux handles an incoming access request (from a subject) to a target object:\nSubject (eg: a process) makes an Action request (eg: read) \u0026gt; SELinux Security Server checks the SELinux Policy Database \u0026gt; if permission is not granted the AVC Denied Message is diaplayed. If Permission is granted, then access to object (eg: a file) is granted.\nsu to root from user1 and view the log:\n[root@server10 ~]# cat /var/log/audit/audit.log | tail -10 ... type=USER_START msg=audit(1722274070.748:90): pid=1394 uid=1000 auid=0 ses=1 subj=unconfined_u:unconfined_r:unconfined_t:s0- s0:c0.c1023 msg=\u0026#39;op=PAM:session_open grantors=pam_keyinit,pam_limits,pam_systemd,pam_unix, pam_umask,pam_xauth acct=\u0026#34;root\u0026#34; exe=\u0026#34;/usr/bin/su\u0026#34; hostname=? addr=? terminal=/dev/pts/0 res=success\u0026#39;UID=\u0026#34;user1\u0026#34; AUID=\u0026#34;root\u0026#34; WIll show avc denied if denied.\nLab Get an AVC deny message # Change the SELinux type on the shadow file to something random (etc_t). Issue the passwd command as user1 to modify the password. Restored the type on the shadow file with restorecon /etc/shadow. ' Re-try the password change. Change the SELinux type on /etc/shadow to something random (etc_t) [root@server10 ~]# chcon -vt etc_t /etc/shadow changing security context of \u0026#39;/etc/shadow\u0026#39; Issue the passwd command as user1 to modify the password: [root@server10 ~]# su user1 [user1@server10 root]$ passwd Changing password for user user1. Current password: roopasswd: Authentication token manipulation error The following is a sample denial record from the same file in raw format:\nAVC type Related to the passwd command (comm) Source context (scontext) unconfined_u:unconfined_r:passwd_t:s0-s0:c0.c1023 nshadow file (name) with file type (tclass) \u0026ldquo;file\u0026rdquo; Target context (tcontext) system_u:object_r:etc_t:s0 Indicates the SELinux operating mode, which is enforcing permissive=0. This message indicates that the /etc/shadow file does not have the correct context set on it, and that\u0026rsquo;s why SELinux prevented the passwd command from updating the user\u0026rsquo;s password. Use sealert to analyze (-a) all AVC records in the audit.log file. This command produces a formatted report with all relevant details:\nSELinux DIY Labs # Lab: Disable and Enable the SELinux Operating Mode # Check and make a note of the current SELinux operating mode. [root@server30 ~]# getenforce Enforcing Modify the configuration file and set the mode to disabled. [root@server30 ~]# vim /etc/selinux/config SELINUX=disabled Reboot the system to apply the change. [root@server30 ~]# reboot Run sudo getenforce to confirm the change when the system is up. [root@server30 ~]# getenforce Disabled Restore the directive\u0026rsquo;s value to enforcing in the configuration file, and reboot to apply the new mode. [root@server30 ~]# vim /etc/selinux/config SELINUX=enforcing [root@server30 ~]# reboot Run sudo getenforce to confirm the mode when the system is up. [root@server30 ~]# getenforce Enforcing Lab: Modify Context on Files # Create directory hierarchy /tmp/d1/d2. mkdir -p /tmp/d1/d2 Check the contexts on /tmp/d1 and /tmp/d1/d2. [root@server30 d1]# ls -ldZ /tmp/d1 drwxr-xr-x. 3 root root unconfined_u:object_r:user_tmp_t:s0 16 Jul 29 13:17 /tmp/d1 [root@server30 d1]# ls -ldZ /tmp/d1/d2 drwxr-xr-x. 2 root root unconfined_u:object_r:user_tmp_t:s0 6 Jul 29 13:17 /tmp/d1/d2 Change the SELinux type on /tmp/d1 to etc_t recursively with the chcon command and confirm. [root@server30 tmp]# chcon -Rv -t etc_t /tmp/d1 changing security context of \u0026#39;/tmp/d1/d2\u0026#39; changing security context of \u0026#39;/tmp/d1\u0026#39; [root@server30 tmp]# ls -ldZ /tmp/d1 drwxr-xr-x. 3 root root unconfined_u:object_r:etc_t:s0 16 Jul 29 13:17 /tmp/d1 [root@server30 tmp]# ls -ldZ /tmp/d1/d2 drwxr-xr-x. 2 root root unconfined_u:object_r:etc_t:s0 6 Jul 29 13:17 /tmp/d1/d2 Add /tmp/d1 to the policy database with the semanage command to ensure the new context is persistent on the directory hierarchy. [root@server30 tmp]# semanage fcontext -a -t etc_t /tmp/d1 [root@server30 tmp]# reboot [root@server30 ~]# ls -ldZ /tmp/d1 drwxr-xr-x. 3 root root unconfined_u:object_r:etc_t:s0 16 Jul 29 13:17 /tmp/d1 [root@server30 ~]# ls -ldZ /tmp/d1/d2 drwxr-xr-x. 2 root root unconfined_u:object_r:etc_t:s0 6 Jul 29 13:17 /tmp/d1/d2 Lab: Add Network Port to Policy Database # Add network port 9005 to the SELinux policy database for the secure HTTP service using the semanage command. [root@server30 ~]# semanage port -l | grep ^http_port http_port_t tcp 80, 81, 443, 488, 8008, 8009, 8443, 9000 [root@server30 ~]# semanage port -at http_port_t -p tcp 9005 Verify the addition. [root@server30 ~]# semanage port -l | grep ^http_port http_port_t tcp 9005, 80, 81, 443, 488, 8008, 8009, 8443, 9000 Lab: Copy Files with and without Context # Create file sef1 under /tmp. [root@server30 ~]# touch /tmp/sef1 Copy the file to the /usr/local directory. [root@server30 ~]# cp /tmp/sef1 /usr/local Check and compare the contexts on both source and destination files. [root@server30 ~]# ls -lZ /tmp/sef1 -rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Jul 29 13:33 /tmp/sef1 [root@server30 ~]# ls -lZ /usr/local/sef1 -rw-r--r--. 1 root root unconfined_u:object_r:usr_t:s0 0 Jul 29 13:33 /usr/local/sef1 Create another file sef2 under /tmp and copy it to the /var/local directory using the --preserve=context option with the cp command. [root@server30 ~]# touch /tmp/sef2 [root@server30 ~]# cp --preserve=context /tmp/sef2 /var/local/ Check and compare the contexts on both source and destination files. [root@server30 ~]# ls -lZ /tmp/sef2 /var/local/sef2 -rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Jul 29 13:35 /tmp/sef2 -rw-r--r--. 1 root root unconfined_u:object_r:user_tmp_t:s0 0 Jul 29 13:36 /var/local/sef2 Lab: Flip SELinux Booleans # Check the current value of Boolean ssh_use_tcpd using the getsebool and sestatus commands. [root@server30 ~]# getsebool -a | grep ssh_use_tcpd ssh_use_tcpd --\u0026gt; off Use the setsebool command and toggle the value of the directive. [root@server30 ~]# setsebool ssh_use_tcpd 1 Confirm the new value with the getsebool, semanage, or sestatus command. [root@server30 ~]# getsebool -a | grep ssh_use_tcpd ssh_use_tcpd --\u0026gt; on [root@server30 ~]# sestatus -b | grep ssh_use_tcpd ssh_use_tcpd on [root@server30 ~]# semanage boolean -l | grep ssh_use_tcpd ssh_use_tcpd (on , off) Allow ssh to use tcpd ","externalUrl":null,"permalink":"/tech/linux/security-enhanced-linux/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eSELinux Terminology\n    \u003cdiv id=\"selinux-terminology\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#selinux-terminology\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eImplementation of the Mandatory Access Control (MAC) architecture\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edeveloped by the U.S. National Security Agency (NSA)\u003c/li\u003e\n\u003cli\u003eflexible, enriched, and granular security controls in Linux.\u003c/li\u003e\n\u003cli\u003eintegrated into the Linux kernel as a set of patches using the Linux Security Modules (LSM) framework\n\u003cul\u003e\n\u003cli\u003eallows the kernel to support various security implementations\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eprovides an added layer of protection above and beyond the standard Linux Discretionary Access Control (DAC) security architecture.\u003c/li\u003e\n\u003cli\u003eDAC includes:\n\u003cul\u003e\n\u003cli\u003etraditional file and directory permissions\u003c/li\u003e\n\u003cli\u003eextended attribute settings\u003c/li\u003e\n\u003cli\u003esetuid/setgid bits\u003c/li\u003e\n\u003cli\u003esu/sudo privileges\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eLimits the ability of a \u003cstrong\u003esubject\u003c/strong\u003e (Linux user or process) to access an \u003cstrong\u003eobject\u003c/strong\u003e (file, directory, file system, device, network interface/connection, port, pipe, socket, etc.)\n\u003cul\u003e\n\u003cli\u003eTo reduce or eliminate the potential damage the subject may be able to inflict on the system if compromised.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMAC controls\u003c/strong\u003e\u003c/p\u003e","title":"Security Enhanced Linux","type":"tech"},{"content":"This is a step-by-step guide to setting up Nextcloud on a Debian server. You will need a server hosted by a VPS like Vultr. And a Domain hosted by a DNS provider such as Cloudflare\nWhat is Nextcloud? # Nextcloud is so many things. It offers so many features and options, it deserves a bulleted list:\nFree and open source Cloud storage and syncing Email client Custom browser dashboard with widgets Office suite RSS newsfeed Project organization (deck) Notebook Calender Task manager Connect to decentralized social media (like Mastodon) Replacement for all of google\u0026rsquo;s services Create web forms or surveys It is also free and open source. This mean the source code is available to all. And hosting yourself means you can guarantee that your data isn\u0026rsquo;t being shared.\nAs you can see. Nextcloud is feature packed and offers an all in one solution for many needs. The set up is fairly simple!\nInstall Dependencies # sudo apt update Sury Dependencies # sudo apt install software-properties-common ca-certificates lsb-release apt-transport-https Enable Sury Repository # sudo sh -c \u0026#39;echo \u0026#34;deb https://packages.sury.org/php/ $(lsb_release -sc) main\u0026#34; \u0026gt; /etc/apt/sources.list.d/php.list\u0026#39; Import the GPG key for the repository # wget -qO - https://packages.sury.org/php/apt.gpg | sudo apt-key add - Install PHP 8.2 # https://computingforgeeks.com/how-to-install-php-8-2-on-debian/?expand_article=1 (This is also part of the other dependencies install command below)\nsudo apt install php8.2 Install other dependencies: # apt install -y nginx python3-certbot-nginx mariadb-server php8.2 php8.2-{fpm,bcmath,bz2,intl,gd,mbstring,mysql,zip,xml,curl} Improving Nextcloud server performance # Adding more child processes for PHP to use:\nvim /etc/php/8.2/fpm/pool.d/www.conf # update the following parameters in the file pm = dynamic pm.max_children = 120 pm.start_servers = 12 pm.min_spare_servers = 6 pm.max_spare_servers = 18 Start your MariaDB server: # systemctl enable mariadb --now Set up a SQL Database # Nextcloud needs some tables setup in order to store information in a database. First set up a secure sql database:\nsudo mysql_secure_installation Say “Yes” to the prompts and enter root password:\nSwitch to unix_socket authentication [Y/n]: Y Change the root password? [Y/n]: Y\t# enter password. Remove anonymous users? [Y/n]: Y Disallow root login remotely? [Y/n]: Y Remove test database and access to it? [Y/n]: Y Reload privilege tables now? [Y/n]: Y Sign in to your SQL database with the password you just chose:\nmysql -u root -p Creating a database for NextCloud # While signed in with the mysql command, enter the commands below one at a time. Make sure to replace the username and password. But leave localhost as is:\nCREATE DATABASE nextcloud; GRANT ALL ON nextcloud.* TO \u0026#39;david\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;@Rfanext12!\u0026#39;; FLUSH PRIVILEGES; EXIT; Install SSL with Certbot # Obtain an SSL certificate. See my website setup post for information about Certbot and nginx setup.\ncertbot certonly --nginx -d nextcloud.example.com Create a CNAME record for DNS. # You will need to have a domain name set up for your server. I use Cloudflare to manage my DNS records. You will want to make a CNAME record for your nextcloud subdomain.\nJust add \u0026ldquo;nextcloud\u0026rdquo; as the name and \u0026ldquo;yourwebsite.com\u0026rdquo; as the content. This will make it so \u0026ldquo;nextcloud.yourwebsite.com\u0026rdquo;. Make sure to select \u0026ldquo;DNS Only\u0026rdquo; under proxy status.\nNginx Setup # Edit your sites-available config at /etc/nginx/sites-available/nextcloud. See comments in the following text box:\nvim /etc/nginx/sites-available/nextcloud # Add this to the file: # replace example.org with your domain name # use the following vim command to make this easier # :%s/example.org/perfectdarkmode.com/g # ^ this will replace all instances of example.org with perfectdarkmode.com. Replace with yur domain upstream php-handler { server unix:/var/run/php/php8.2-fpm.sock; server 127.0.0.1:9000; } map $arg_v $asset_immutable { \u0026#34;\u0026#34; \u0026#34;\u0026#34;; default \u0026#34;immutable\u0026#34;; } server { listen 80; listen [::]:80; server_name nextcloud.example.org ; return 301 https://$server_name$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name nextcloud.example.org ; root /var/www/nextcloud; ssl_certificate /etc/letsencrypt/live/nextcloud.example.org/fullchain.pem ; ssl_certificate_key /etc/letsencrypt/live/nextcloud.example.org/privkey.pem ; client_max_body_size 512M; client_body_timeout 300s; fastcgi_buffers 64 4K; gzip on; gzip_vary on; gzip_comp_level 4; gzip_min_length 256; gzip_proxied expired no-cache no-store private no_last_modified no_etag auth; gzip_types application/atom+xml application/javascript application/json application/ld+json application/manifest+json application/rss+xml application/vnd.geo+json application/vnd.ms-fontobject application/wasm application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/bmp image/svg+xml image/x-icon text/cache-manifest text/css text/plain text/vcard text/vnd.rim.location.xloc text/vtt text/x-component text/x-cross-domain-policy; client_body_buffer_size 512k; add_header Referrer-Policy \u0026#34;no-referrer\u0026#34; always; add_header X-Content-Type-Options \u0026#34;nosniff\u0026#34; always; add_header X-Download-Options \u0026#34;noopen\u0026#34; always; add_header X-Frame-Options \u0026#34;SAMEORIGIN\u0026#34; always; add_header X-Permitted-Cross-Domain-Policies \u0026#34;none\u0026#34; always; add_header X-Robots-Tag \u0026#34;none\u0026#34; always; add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34; always; fastcgi_hide_header X-Powered-By; index index.php index.html /index.php$request_uri; location = / { if ( $http_user_agent ~ ^DavClnt ) { return 302 /remote.php/webdav/$is_args$args; } } location = /robots.txt { allow all; log_not_found off; access_log off; } location ^~ /.well-known { location = /.well-known/carddav { return 301 /remote.php/dav/; } location = /.well-known/caldav { return 301 /remote.php/dav/; } location /.well-known/acme-challenge { try_files $uri $uri/ =404; } location /.well-known/pki-validation { try_files $uri $uri/ =404; } return 301 /index.php$request_uri; } location ~ ^/(?:build|tests|config|lib|3rdparty|templates|data)(?:$|/) { return 404; } location ~ ^/(?:\\.|autotest|occ|issue|indie|db_|console) { return 404; } location ~ \\.php(?:$|/) { # Required for legacy support rewrite ^/(?!index|remote|public|cron|core\\/ajax\\/update|status|ocs\\/v[12]|updater\\/.+|oc[ms]-provider\\/.+|.+\\/richdocumentscode\\/proxy) /index.php$request_uri; fastcgi_split_path_info ^(.+?\\.php)(/.*)$; set $path_info $fastcgi_path_info; try_files $fastcgi_script_name =404; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_param HTTPS on; fastcgi_param modHeadersAvailable true; fastcgi_param front_controller_active true; fastcgi_pass php-handler; fastcgi_intercept_errors on; fastcgi_request_buffering off; fastcgi_max_temp_file_size 0; } location ~ \\.(?:css|js|svg|gif|png|jpg|ico|wasm|tflite|map)$ { try_files $uri /index.php$request_uri; add_header Cache-Control \u0026#34;public, max-age=15778463, $asset_immutable\u0026#34;; access_log off; # Optional: Don\u0026#39;t log access to assets location ~ \\.wasm$ { default_type application/wasm; } } location ~ \\.woff2?$ { try_files $uri /index.php$request_uri; expires 7d; access_log off; } location /remote { return 301 /remote.php$request_uri; } location / { try_files $uri $uri/ /index.php$request_uri; } } Enable the site # Create a link between the file you just made and /etc/nginx/sites-enabled\nln -s /etc/nginx/sites-available/nextcloud /etc/nginx/sites-enabled/ Install Nextcloud # Download the latest Nextcloud version. Then extract into /var/www/. Also, update the file\u0026rsquo;s permissions to give nginx access:\nwget https://download.nextcloud.com/server/releases/latest.tar.bz2 tar -xjf latest.tar.bz2 -C /var/www chown -R www-data:www-data /var/www/nextcloud chmod -R 755 /var/www/nextcloud Start and enable php-fpm on startup # \u0026lt;systemctl enable php8.2fpm --now](\u0026gt;\u0026lt;--may not need this. # Do need this-\u0026gt; sudo systemctl enable php8.2-fpm.service --now Reload nginx # systemctl reload nginx Nextcloud occ tool # Here is a built in Nextcloud tool just in case things break. Here is a guide on troubleshooting with occ. The basic command is as follows:\nsudo -u www-data php /var/www/nextcloud/occ Add this as an alias in ~/.bashrc for ease of use.\nYou are ready to log in to Nextcloud! # Go to your nextcloud domain in a browser. In my case, I head to nextcloud.perfectdarkmode.com. Fill out the form to create your first Nextcloud user:\nChoose an admin username and secure password. Leave Data folder as the default value. For Database user, enter the user you set for the SQL database. For Database password, enter the password you chose for the new user in MariaDB. For Database name, enter: nextcloud Leave \u0026ldquo;localhost\u0026rdquo; as \u0026ldquo;localhost\u0026rdquo;. Click Finish. Now that you are signed in. Here are a few things you can do to start you off:\nDownload the desktop and mobile app and sync all of your data. (covered below) Look at different apps to consolodate your programs all in one place. Put the Nextcloud dashboard as your default browser homepage and customize themes. Set up email integration. NextCloud desktop synchronization # Install the desktop client (Fedora)\nSudo dnf install nextcloudclient Install on other distros: https://help.nextcloud.com/t/install-nextcloud-client-for-opensuse-arch-linux-fedora-ubuntu-based-android-ios/13657\nRun the nextcloud desktop app and sign in. Choose folders to sync. Folder will be ~/Nextcloud. Move everything into your nextcloud folder. This may break things with filepaths so beware. Now you are ready to use and explore nextcloud. Here is a video from TechHut to get you started down the NextCloud rabbit hole.\nChange max upload size (default is 500mg) # /var/www/nextcloud/.user.ini php_value upload_max_filesize = 16G php_value post_max_size = 16G\nRemove file locks # Put Nextcloud in maintenance mode: Edit config/config.php and change this line:\n'maintenance' =\u0026gt; true,\nEmpty table oc_file_locks: Use tools such as phpmyadmin or connect directly to your database and run (the default table prefix is oc_, this prefix can be different or even empty):\nDELETE FROM oc_file_locks WHERE 1\ndisable maintenance mode (undo first step). Make sure your cron-jobs run properly (you admin page tells you when cron ran the last time): https://docs.nextcloud.org/server/13/admin_manual/configuration_server/background_jobs_configuration.html 2.7k mysql -u root -p MariaDB [(none)]\u0026gt; use nextcloud; MariaDB [nextcloud]\u0026gt; DELETE FROM oc_file_locks WHERE 1; *figure out redis install if this happens regularly* [https://docs.nextcloud.org/server/13/admin_manual/configuration_server/caching_configuration.html#id4 9.1k](https://docs.nextcloud.org/server/13/admin_manual/configuration_server/caching_configuration.html#id4) ","externalUrl":null,"permalink":"/tech/tools/setting-up-a-self-hosted-nextcloud-server/","section":"Teches","summary":"\u003cp\u003eThis is a step-by-step guide to setting up Nextcloud on a Debian server. You will need a server hosted by a VPS like Vultr. And a Domain hosted by a DNS provider such as Cloudflare\u003c/p\u003e","title":"Self hosting a Nextcloud Server","type":"tech"},{"content":" Managing SELinux Properties # SELinux can be used on files to manage file context context can be set on ports SELinux properties can be managed using Booleans. Modules for Managing Changes on SELinux: file\nManages context on files but not in the SELinux Policy sefcontext Manages file context in the SELinux policy command Is required to run the restorecon command after using sefcontext selinux Manages current SELinux state seboolean Manages SELinux Booleans Managing SELinux File Context # The context type that is set on the file defines which processes can work with the files. The file context type can be set on a file directly, or it can be set on the SELinux policy. All SELinux properties should be set in the SELinux policy. sefcontext module.\nSetting a context type in the policy doesn\u0026rsquo;t automatically apply it to files though. You still need to run the Linux restorecon command to do this. Ansible does not offer a module to run this command; it needs to be invoked using the command module. file module\nCan set SELinux context. The context is set directly on the file, not in the SELinux policy. As a result, if at any time default context is applied from the policy to the file system, all context that has been set with the Ansible file module risks being overwritten. policycoreutils-python-utils RPM\nNot installed by default in all installation patterns. Needed to be able to work with the Ansible sefcontext module and the Linux restorecon command Lab Managing SELinux Context with sefcontext # --- - name: show selinux hosts: all tasks: - name: install required packages yum: name: policycoreutils-python-utils state: present - name: create testfile file: name: /tmp/selinux state: touch - name: set selinux context sefcontext: target: /tmp/selinux setype: httpd_sys_content_t state: present notify: - run restorecon handlers: - name: run restorecon command: restorecon -v /tmp/selinux You might just have to configure a service with a nondefault documentroot, which means that SELinux will deny access to the service. You should ask yourself if this task requires any changes at an SELinux level. Applying Generic SELinux Management Tasks # selinux module\nenables you to set the current state of SELinux to either permissive, enforcing, or disabled. seboolean module\nenables you to easily enable or disable functionality in SELinux using Booleans. Lab: Changing SELinux State and Booleans # --- - name: enabling SELinux and a boolean hosts: ansible1 vars: myboolean: httpd_read_user_content tasks: - name: enabling SELinux selinux: policy: targeted \u0026lt;--- must specify policy state: enforcing - name: checking current {{ myboolean }} Boolean status shell: getsebool -a | grep {{ myboolean }} register: bool_stat - name: showing boolean status debug: msg: the current {{ myboolean }} status is {{ bool_stat.stdout }} - name: enabling boolean seboolean: name: \u0026#34;{{ myboolean }}\u0026#34; state: yes persistent: yes Lab: Changing SELinux Context # Install, start, and configure a web server that has the DocumentRoot set to the /web directory. In this directory, create a file named index.html that shows the message \u0026ldquo;welcome to the webserver.\u0026rdquo; Ensure that SELinux is enabled and allows access to the web server document root. Also ensure that SELinux allows users to publish web pages from their home directory. 1. Start by creating a playbook outline. A good approach for doing this is to create the playbook play header and list all tasks that need to be accomplished by providing a name as well as the name of the task that you want to run. 2. Enable SELinux and set to the enforcing state. 3. Install the web server, start and enable it, create the /web directory, and create the index.html file in the /web directory. 4. Use the lineinfile module to change the httpd.conf contents. Two different lines need to be changed. 5. Configure the SELinux-specific settings. 6. Run the playbook and verify its output. 8. Verify that the web service is accessible by using curl http://ansible1. In this case, it should not work. Try to analyze why.\n--- - name: Managing web server SELinux properties hosts: ansible1 tasks: - name: ensure SELinux is enabled and enforcing selinux: policy: targeted state: enforcing - name: install the webserver yum: name: httpd state: latest - name: start and enable the webserver service: name: httpd state: started enabled: yes - name: open the firewall service firewalld: service: http state: enabled immediate: yes - name: create the /web directory file: name: /web state: directory - name: create the index.html file in /web copy: content: ’welcome to the exercise82 web server’ dest: /web/index.html - name: use lineinfile to change webserver configuration lineinfile: path: /etc/httpd/conf/httpd.conf regexp: ’^DocumentRoot \u0026#34;/var/www/html\u0026#34;’ line: DocumentRoot \u0026#34;/web\u0026#34; notify: restart httpd - name: use lineinfile to change webserver security lineinfile: path: /etc/httpd/conf/httpd.conf regexp: ’^\u0026lt;Directory \u0026#34;/var/www\u0026#34;\u0026gt;’ line: ’\u0026lt;Directory \u0026#34;/web\u0026#34;\u0026gt;’ - name: use sefcontext to set context on new documentroot sefcontext: target: ’/web(/.*)?’ setype: httpd_sys_content_t state: present - name: run the restorecon command command: restorecon -Rv /web - name: allow the web server to run user content seboolean: name: httpd_read_user_content state: yes persistent: yes handlers: - name: restart httpd service: name: httpd state: restarted ","externalUrl":null,"permalink":"/tech/ansible/selinux-file-properties/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eManaging SELinux Properties\n    \u003cdiv id=\"managing-selinux-properties\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#managing-selinux-properties\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSELinux can be used on files to manage file context\u003c/li\u003e\n\u003cli\u003econtext can be set on ports\u003c/li\u003e\n\u003cli\u003eSELinux properties can be managed using Booleans.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eModules for Managing Changes on SELinux:\n\u003cstrong\u003efile\u003c/strong\u003e\u003c/p\u003e","title":"SeLinux File Properties","type":"tech"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" Shell Scripts # A group of Linux commands along with control structures and optional comments stored in a text file.\nCan be executed directly at the Linux command prompt.\nDo not need to be compiled as they are interpreted by the shell line by line.\nManaging packages and users, administering partitions and file systems, monitoring file system utilization, trimming log files, archiving and compressing files, finding and removing unnecessary files, starting and stopping database services and applications, and producing reports.\nRun by the shell one at a time in the order in which they are listed.\nEach line is executed as if it is typed and run at the command prompt.\nControl structures are utilized for creating and managing conditional and looping constructs.\nComments are also generally included to add information about the script such as the author name, creation date, previous modification dates, purpose, and usage.\nIf the script encounters an error during execution, the error message is printed on the screen.\nCan use the nl command to enumerate the lines for troubleshooting.\nCan store your scripts in the /usr/local/bin directory, which is included in the PATH of all users by default.\nScript01: Displaying System Information # Create the first script called sys_info.sh in /usr/local/bin/ Use the vim editor with sudo to write the script. #!/bin/bash echo \u0026#34;Display Basic System Information\u0026#34; echo \u0026#34;==================================\u0026#34; echo echo \u0026#34;The hostname, hardware, and OS information is:\u0026#34; /usr/bin/hostnamectl echo echo \u0026#34;The Following users are currently logged in:\u0026#34; /usr/bin/who Within vim, press the ESC key and then type :set nu to view line numbers associated with each line entry. Must add execute bit to run the script Executing a Script # chmod +x /usr/local/bin/sys_info.sh ll /usr/local/bin/sys_info.sh -rwxr-xr-x. 1 root root 244 Jul 30 09:47 /usr/local/bin/sys_info.sh\u0026gt;) Any user on the system can now run this script using either its name or the full path. Let\u0026rsquo;s run the script and see what the output will look like:\n$ sys_info.sh Display Basic System Information ================================== The hostname, hardware, and OS information is: Static hostname: server30 Icon name: computer-vm Chassis: vm 🖴 Machine ID: eaa6174e108d4a27bd619754… Boot ID: 13d8b3c167b24757b3678e4f… Virtualization: oracle Operating System: Red Hat Enterprise Linux… CPE OS Name: cpe:/o:redhat:enterprise… Kernel: Linux 5.14.0-362.24.1.el… Architecture: x86-64 Hardware Vendor: innotek GmbH Hardware Model: VirtualBox Firmware Version: VirtualBox The Following users are currently logged in: root pts/0 2024-07-30 07:22 (172.16.7.95) Debugging a Script # Can either append the -x option to the \u0026ldquo;#!/bin/bash\u0026rdquo; at the beginning of the script to look like \u0026ldquo;#!/bin/bash -x\u0026rdquo;, or execute the script as follows:\n[root@server30 ~]# bash -x sys_info.sh + echo \u0026#39;Display Basic System Information\u0026#39; Display Basic System Information + echo ================================== ================================== + echo + echo \u0026#39;The hostname, hardware, and OS information is:\u0026#39; The hostname, hardware, and OS information is: + /usr/bin/hostnamectl Static hostname: server30 Icon name: computer-vm Chassis: vm 🖴 Machine ID: eaa6174e108d4a27bd6197548ce77270 Boot ID: 13d8b3c167b24757b3678e4fd3fe19ee Virtualization: oracle Operating System: Red Hat Enterprise Linux 9.3 (Plow) CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos Kernel: Linux 5.14.0-362.24.1.el9_3.x86_64 Architecture: x86-64 Hardware Vendor: innotek GmbH Hardware Model: VirtualBox Firmware Version: VirtualBox + echo + echo \u0026#39;The Following users are currently logged in:\u0026#39; The Following users are currently logged in: + /usr/bin/who root pts/0 2024-07-30 07:22 (172.16.7.95) Actual lines from the script prefixed by the + sign and followed by the command execution result. Shows the line number of the problem line in the output if there is any. This way you can identify any issues pertaining to the path, command name, use of special characters, etc., and address it quickly. Change one of the echo commands in the script to \u0026ldquo;iecho\u0026rdquo; and re-run the script in the debug mode to see the error:\n[root@server30 ~]# bash -x sys_info.sh + echo \u0026#39;Display Basic System Information\u0026#39; Display Basic System Information + echo ================================== ================================== + iecho /usr/local/bin/sys_info.sh: line 4: iecho: command not found + echo \u0026#39;The hostname, hardware, and OS information is:\u0026#39; The hostname, hardware, and OS information is: + /usr/bin/hostnamectl Static hostname: server30 Icon name: computer-vm Chassis: vm 🖴 Machine ID: eaa6174e108d4a27bd6197548ce77270 Boot ID: 13d8b3c167b24757b3678e4fd3fe19ee Virtualization: oracle Operating System: Red Hat Enterprise Linux 9.3 (Plow) CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos Kernel: Linux 5.14.0-362.24.1.el9_3.x86_64 Architecture: x86-64 Hardware Vendor: innotek GmbH Hardware Model: VirtualBox Firmware Version: VirtualBox + echo + echo \u0026#39;The Following users are currently logged in:\u0026#39; The Following users are currently logged in: + /usr/bin/who root pts/0 2024-07-30 07:22 (172.16.7.95) Script02: Using Local Variables # Create a script called use_var.sh Define a local variable and display its value on the screen. Re-check the value of the variable after the script execution has completed. [root@server30 ~]# vim /usr/local/bin/use_var.sh #!/bin/bash echo \u0026#34;Setting a Local Variable\u0026#34; echo \u0026#34;========================\u0026#34; SYSNAME=server30.example.com echo \u0026#34;The hostname of this system is $SYSNAME\u0026#34; [root@server30 ~]# chmod +x /usr/local/bin/use_var.sh [root@server30 ~]# use_var.sh Setting a Local Variable ======================== The hostname of this system is server30.example.com If you run the echo command to see what is stored in the SYSNAME variable, you will get nothing:\n[root@server30 ~]# echo $SYSNAME Script03: Using Pre-Defined Environment Variables # The following script called pre_env.sh will display the values of SHELL and LOGNAME environment variables:\n[root@server30 ~]# vim /usr/local/bin/pre_env.sh #!/bin/bash echo \u0026#34;The location of my shell command is:\u0026#34; echo $SHELL echo \u0026#34;I am logged in as $LOGNAME\u0026#34; [root@server30 ~]# chmod +x /usr/local/bin/pre_env.sh [root@server30 ~]# pre_env.sh The location of my shell command is: /bin/bash I am logged in as root Script04: Using Command Substitution # Can use the command substitution feature of the bash shell and store the output generated by the command into a variable.\nTwo different ways to use command substitution: Backtics or subshell\n#!/bin/bash SYSNAME=$(hostname) KERNVER=`uname -r` echo \u0026#34;The hostname is $SYSNAME\u0026#34; echo \u0026#34;The kernel version is $KERNVER\u0026#34; [root@server30 ~]# vim /usr/local/bin/cmd_out.sh [root@server30 ~]# chmod +x /usr/local/bin/cmd_out.sh [root@server30 ~]# cmd_out.sh The hostname is server30 The kernel version is 5.14.0-362.24.1.el9_3.x86_64 Shell Parameters # An entity that holds a value such as a name, special character, or number. The parameter that holds a name is referred to as a variable A parameter that holds a special character is referred to as a special parameter Represents the command or script itself ($0), count of supplied arguments ($* or $@), all arguments ($#), and PID of the process ($$) One or more digits, except for 0 is referred to as a positional parameter (a command line argument). ($1, $2, $3 . . .) is an argument supplied to a script at the time of its invocation Position is determined by the shell based on its location with respect to the calling script. Positional parameters beyond 9 are to be enclosed in curly brackets. Just like the variable and command substitutions, the shell uses the dollar ($) sign for special and positional parameter expansions as well. Script05: Using Special and Positional Parameters # Create com_line_arg.sh to show the supplied arguments, total count, value of the first argument, and PID of the script:\n[root@server30 ~]# vim /usr/local/bin/com_line_arg.sh #!/bin/bash echo \u0026#34;There are $# arguments specified at the command line\u0026#34; echo \u0026#34;The arguments supplied are: $*\u0026#34; echo \u0026#34;The first argument is: $1\u0026#34; echo \u0026#34;The Process ID of the script is: $$\u0026#34; [root@server30 ~]# chmod +x /usr/local/bin/com_line_arg.sh [root@server30 ~]# com_line_arg.sh There are 0 arguments specified at the command line The arguments supplied are: The first argument is: The Process ID of the script is: 1935 [root@server30 ~]# com_line_arg.sh the dog jumped over the frog There are 6 arguments specified at the command line The arguments supplied are: the dog jumped over the frog The first argument is: the The Process ID of the script is: 1936 Script06: Shifting Command Line Arguments # shift command\nUsed to move arguments one position to the left. During this move, the value of the first argument is lost. [root@server30 ~]# vim /usr/local/bin/com_line_arg_shift.sh #!/bin/bash echo \u0026#34;There are $# arguments specified at the command line\u0026#34; echo \u0026#34;The arguments supplied are: $*\u0026#34; echo \u0026#34;The first argument is: $1\u0026#34; echo \u0026#34;The Process ID of the script is: $$\u0026#34; shift echo \u0026#34;The new first argument after the first shift is: $1\u0026#34; shift echo \u0026#34;The new first argument after the second shift is: $1\u0026#34; [root@server30 ~]# chmod +x /usr/local/bin/com_line_arg_shift.sh [root@server30 ~]# com_line_arg_shift.sh There are 0 arguments specified at the command line The arguments supplied are: The first argument is: The Process ID of the script is: 1941 The new first argument after the first shift is: The new first argument after the second shift is: [root@server30 ~]# com_line_arg_shift.sh the dog jumped over the frog There are 6 arguments specified at the command line The arguments supplied are: the dog jumped over the frog The first argument is: the The Process ID of the script is: 1942 The new first argument after the first shift is: dog The new first argument after the second shift is: jumped Multiple shifts in a single attempt may be performed by furnishing a count of desired shifts to the shift command as an argument. For example, \u0026ldquo;shift 2\u0026rdquo; will carry out two shifts, \u0026ldquo;shift 3\u0026rdquo; will make three shifts, and so on. Logical Constructs # Use test conditions, which decides what to do next based on the true or false status of the condition. The shell offers two logical constructs: if-then-fi case\nExit Codes (exit values)\nRefer to the value returned by a command when it finishes execution. Value is based on the outcome of the command. If the command runs successfully, you typically get a zero exit code(return code), otherwise you get a non-zero value. Return code is stored in a special shell parameter called ? (question mark). Let\u0026rsquo;s look at the following two examples to understand their usage:\n[root@server30 ~]# pwd /root [root@server30 ~]# echo $? 0 [root@server30 ~]# man What manual page do you want? For example, try \u0026#39;man man\u0026#39;. [root@server30 ~]# echo $? 1 Exit code was returned and stored in the ?. Non-zero exit code was stored in ? with an error. Can define exit codes within a script at different locations to help debug the script by knowing where exactly it terminated. Test Conditions\nUsed in logical constructs to decide what to do next. Can be set on integer values, string values, or files using the test command or by enclosing them within the square brackets []. man test Operation on Integer Value\ninteger1 -eq (-ne) integer2\nInteger1 is equal (not equal) to integer2 integer1 -lt (-gt) integer2 Integer1 is less (greater) than integer2 integer1 -le (-ge) integer2\nInteger1 is less (greater) than or equal to integer2 Operation on String Value\nstring1=(!=)string2\nTests whether the two strings are identical (not identical) -l string or -z string\nTests whether the string length is zero string or -n string\nTests whether the string length is non-zero Operation on File\n-b (-c) file\nTests whether the file is a block (character) device file -d (-f) file\nTests whether the file is a directory (normal file) -e (-s) file\nTests whether the file exists (non-empty) -L file\nTests whether the file is a symlink -r (-w) (-x) file\nTests whether the file is readable (writable) (executable) -u (-g) (-k) file\nTests whether the file has the setuid (setgid) (sticky) bit file1 -nt (-ot) file2\nTests whether file1 is newer (older) than file2 Logical Operators\n!\nThe logical NOT operator -a or \u0026amp;\u0026amp; (two ampersand characters)\nThe logical AND operator. Both operands must be true for the condition to be true. Syntax: [ -b file1 \u0026amp;\u0026amp; -r file1 ] -o or || (two pipe characters)\nThe logical OR operator. Either of the two or both operands must be true for the condition to be true. Syntax: [ (x == 1 -o y == 2) ] if-then-fi Construct # Evaluates the condition for true or false. Executes the specified action if the condition is true. Otherwise, it exits the construct. Begins with an if and ends with a fi Can execute an action only if the specified condition is true. It quits the statement if the condition is untrue. The general syntax of this statement is as follows:\nif condition \u0026gt; then \u0026gt; action \u0026gt; fi\nScript07: The if-then-fi Construct # Create if_then_fi.sh to determine the number of arguments and print an error message if there are none provided:\n[root@server30 ~]# vim /usr/local/bin/if_then_fi.sh #!/bin/bash if [ $# -ne 2 ] # Ensure there is a space after [ and before ] then echo \u0026#34;Error: Invalid number of arguments supplied\u0026#34; echo \u0026#34;Usage: $0 source_file destination_file\u0026#34; exit 2 fi echo \u0026#34;Script terminated\u0026#34; [root@server30 ~]# chmod +x /usr/local/bin/if_then_fi.sh [root@server30 ~]# if_then_fi.sh Error: Invalid number of arguments supplied Usage: /usr/local/bin/if_then_fi.sh source_file destination_file This script will display the following messages on the screen if it is executed without exactly two arguments specified at the command line:\n[root@server30 ~]# if_then_fi.sh Error: Invalid number of arguments supplied Usage: /usr/local/bin/if_then_fi.sh source_file destination_file Return code value reflects the exit code in the script .\n[root@server30 ~]# echo $? 2 Return code will be 0 if you supply a pair of arguments:\n[root@server30 ~]# if_then_fi.sh a b Script terminated [root@server30 ~]# echo $? 0 if-then-else-fi Construct # Can execute an action if the condition is true and another action if the condition is false. The general syntax of this statement is as follows:\nif condition \u0026gt; then \u0026gt; action1 \u0026gt; else \u0026gt; action2 \u0026gt; fi\nScript08: The if-then-else-fi Construct # Create a script called if_then_else_fi.sh that will accept an integer value as an argument and tell if the value is positive or negative:\nvim /usr/local/bin/if_then_else_fi.sh #!/bin/bash if [ $1 -gt 0 ] then echo \u0026#34;$1 is a positive integer value\u0026#34; else echo \u0026#34;$1 is a negative integer value\u0026#34; fi [root@server30 ~]# chmod +x /usr/local/bin/if_then_else_fi.sh [root@server30 ~]# if_then_else_fi.sh /usr/local/bin/if_then_else_fi.sh: line 2: [: -gt: unary operator expected is a negative integer value [root@server30 ~]# if_then_else_fi.sh 3 3 is a positive integer value [root@server30 ~]# if_then_else_fi.sh -3 -3 is a negative integer value [root@server30 ~]# if_then_else_fi.sh a /usr/local/bin/if_then_else_fi.sh: line 2: [: a: integer expression expected a is a negative integer value [root@server30 ~]# echo $? 0 The if-then-elif-fi Construct # Can define multiple conditions and associate an action with each one of them. The action corresponding to the true condition is performed. The general syntax of this statement is as follows:\nif condition1 \u0026gt; then action1 \u0026gt; elif condition2 \u0026gt; then action2 \u0026gt; elif condition3 \u0026gt; then action3 \u0026gt; else \u0026gt; action(n) \u0026gt; fi\nScript09: The if-then-elif-fi Construct (Example 1) # Create if_then_elif_fi.sh script to accept an integer value as an argument and tell if the integer is positive, negative, or zero. If a non-integer value or no argument is supplied, the script will complain. Employ the exit command after each action to help you identify where it exited.\n[root@server30 ~]# vim /usr/local/bin/if_then_elif_fi.sh #!/bin/bash if [ $1 -gt 0 ] then echo \u0026#34;$1 is a positive integer value\u0026#34; exit 1 elif [ $1 -eq 0 ] then echo \u0026#34;$1 is a zero integer value\u0026#34; exit 2 elif [ $1 -lt 0 ] then echo \u0026#34;$1 is a negative integer value\u0026#34; exit 3 else echo \u0026#34;$1 is not an integer value. Please supply an i nteger.\u0026#34; exit 4 fi [root@server30 ~]# if_then_elif_fi.sh -0 -0 is a zero integer value [root@server30 ~]# echo $? 2 [root@server30 ~]# if_then_elif_fi.sh -1 -1 is a negative integer value [root@server30 ~]# echo $? 3 [root@server30 ~]# if_then_elif_fi.sh 10 10 is a positive integer value [root@server30 ~]# echo $? 1 [root@server30 ~]# if_then_elif_fi.sh abd /usr/local/bin/if_then_elif_fi.sh: line 2: [: abd: integer expression expected /usr/local/bin/if_then_elif_fi.sh: line 6: [: abd: integer expression expected /usr/local/bin/if_then_elif_fi.sh: line 10: [: abd: integer expression expected abd is not an integer value. Please supply an i nteger.\u0026gt;) [root@server30 ~]# echo $? 4 Script10: The if-then-elif-fi Construct (Example 2) # Create ex200_ex294.sh to display the name of the Red Hat exam RHCSA or RHCE in the output based on the input argument (ex200 or ex294). If a random or no argument is provided, it will print \u0026ldquo;Usage: Acceptable values are ex200 and ex294\u0026rdquo;. Add white spaces in the conditions.\n[root@server30 ~]# vim /usr/local/bin/ex200_ex294.sh #!/bin/bash if [ \u0026#34;$1\u0026#34; = ex200 ] then echo \u0026#34;RHCSA\u0026#34; elif [ \u0026#34;$1\u0026#34; = ex294 ] then echo \u0026#34;RHCE\u0026#34; else echo \u0026#34;Usage: Acceptable values are ex200 and ex294\u0026#34; fi [root@server30 ~]# chmod +x /usr/local/bin/ex200_ex294.sh [root@server30 ~]# ex200_ex294.sh ex200 RHCSA [root@server30 ~]# ex200_ex294.sh ex294 RHCE [root@server30 ~]# ex200_ex294.sh frog Usage: Acceptable values are ex200 and ex294 Looping Constructs # Perform certain task on a number of given elements. Or repeatedly until a specified condition becomes true or false. Examples: if plenty of disks need to be initialized for use in LVM, you can either run the pvcreate command on each disk one at a time manually or employ a loop to do it for you. Based on a condition, you may want a program to continue to run until that condition becomes true or false. Three looping constructs: for-do-done\nfor loop is also referred to as the foreach loop. iterates on a list of given values until the list is exhausted. while-do-done while loop runs repeatedly until the specified condition becomes false. until-do-done until loop does just the opposite of the while loop Performs an operation repeatedly until the specified condition becomes true. Test Conditions # let command\nUsed in looping constructs to evaluate a condition at each iteration. Compares the value stored in a variable against a pre-defined value Each time the loop does an iteration, the variable value is altered. Can enclose the condition for arithmetic evaluation within a pair of parentheses (( )) or quotation marks (\u0026quot; \u0026ldquo;) instead of using the let command explicitly. Operators used in test conditions\n!\nNegation + / \u0026ndash; / * / /\nAddition / subtraction /multiplication / division %\nRemainder \u0026lt; / \u0026lt;=\nLess than / less than or equal to \u0026gt; / \u0026gt;=\nGreater than / greater than or equal to =\nAssignment == / !=\nComparison for equality /non-equality The for Loop # Executed on an array of elements until all the elements in the array are consumed. Each element is assigned to a variable one after the other for processing. The general syntax of this construct is as follows:\nfor VAR in list \u0026gt; do \u0026gt; action \u0026gt; done\nScript11: Print Alphabets Using for Loop # Create script for_do_done.sh script that initializes the variable COUNT to 0. The for loop will read each letter sequentially from the range placed within curly brackets (no spaces before the letter A and after the letter Z), assign it to another variable LETTER, and display the value on the screen. The expr command is an arithmetic processor, and it is used here to increment the COUNT by 1 at each loop iteration.\n[root@server10 ~]# vim /usr/local/bin/for_do_done.sh #!/bin/bash COUNT=0 for LETTER in {A..Z} do COUNT=`/usr/bin/expr $COUNT + 1` echo \u0026#34;Letter $COUNT is [$LETTER]\u0026#34; done [root@server10 ~]# chmod +x /usr/local/bin/for_do_done.sh [root@server10 ~]# for_do_done.sh Letter 1 is [A] Letter 2 is [B] Letter 3 is [C] Letter 4 is [D] Letter 5 is [E] Letter 6 is [F] Letter 7 is [G] Letter 8 is [H] Letter 9 is [I] Letter 10 is [J] Letter 11 is [K] Letter 12 is [L] Letter 13 is [M] Letter 14 is [N] Letter 15 is [O] Letter 16 is [P] Letter 17 is [Q] Letter 18 is [R] Letter 19 is [S] Letter 20 is [T] Letter 21 is [U] Letter 22 is [V] Letter 23 is [W] Letter 24 is [X] Letter 25 is [Y] Letter 26 is [Z] Script12: Create Users Using for Loop # Create script create_user.sh script to create several Linux user accounts. As each account is created, the value of the variable ? is checked. If the value is 0, a message saying the account is created successfully will be displayed, otherwise the script will terminate. In case of a successful account creation, the passwd command will be invoked to assign the user the same password as their username.\n[root@server10 ~]# vim /usr/local/bin/create_user.sh #!/bin/bash for USER in user{10..12} do echo \u0026#34;Create account for user $USER\u0026#34; /usr/sbin/useradd $USER if [ $? = 0 ] then echo $USER | /usr/bin/passwd --stdin $USER echo \u0026#34;$USER is created successfully\u0026#34; else echo \u0026#34;Failed to create account $USER\u0026#34; exit fi done [root@server10 ~]# chmod +x /usr/local/bin/create_user.sh [root@server10 ~]# create_user.sh Create account for user user10 Changing password for user user10. passwd: all authentication tokens updated successfully. user10 is created successfully Create account for user user11 Changing password for user user11. passwd: all authentication tokens updated successfully. user11 is created successfully Create account for user user12 Changing password for user user12. passwd: all authentication tokens updated successfully. user12 is created successfully Script fails if ran again:\n[root@server10 ~]# create_user.sh Create account for user user10 useradd: user \u0026#39;user10\u0026#39; already exists Failed to create account user10 Shell Scripting DIY Labs # Lab: Write Script to Create Logical Volumes # Present 2x1GB virtual disks to server40 in VirtualBox Manager. As user1 with sudo on server40, write a single bash script to create 2x400MB partitions on each disk using parted and then bring both partitions into LVM control with the pvcreate command. vim /usr/local/bin/lvscript.sh Create a volume group called vgscript and add both physical volumes to it. Create three logical volumes each of size 200MB and name them lvscript1, lvscript2, and lvscript3. #!/bin/bash for DEVICE in \u0026#34;/dev/sd\u0026#34;{b..c} do echo \u0026#34;Creating partition 1 with the size of 400MB on $DEVICE\u0026#34; parted $DEVICE mklabel msdos parted $DEVICE mkpart primary 1 401 pvcreate $DEVICE[1] echo \u0026#34;Creating partition 2 with the size of 400MB on $DEVICE\u0026#34; parted $DEVICE mkpart primary 402 802 pvcreate $DEVICE[2] vgcreate vgscript $DEVICE[1] $DEVICE[2] done for LV in \u0026#34;lvscript\u0026#34;{1..3} do echo \u0026#34;Creating logical volume $LV in volume group vgscript with the size of 200MB\u0026#34; lvcreate vgscript -L 200MB -n $LV done Lab: Write Script to Create File Systems # Write another bash script to create xfs, ext4, and vfat file system structures in the logical volumes, respectively. Create mount points /mnt/xfs, /mnt/ext4, and /mnt/vfat, and mount the file systems. Include the df command with -h in the script to list the mounted file systems. vim /usr/local/bin/fsscript.sh [root@server40 ~]# chmod +x /usr/local/bin/fsscript.sh #!/bin/bash for DEVICE in lvscript{1..3} do if [ \u0026#34;$DEVICE\u0026#34; = lvscript1 ] then echo \u0026#34;Creating xfs filesystem on logical volume lvscript1\u0026#34; echo mkfs.xfs /dev/vgscript/lvscript1 echo \u0026#34;Creating /mnt/xfs\u0026#34; mkdir /mnt/xfs echo \u0026#34;Mounting filesystem\u0026#34; mount /dev/vgscript/lvscript1 /mnt/xfs elif [ \u0026#34;$DEVICE\u0026#34; = lvscript2 ] then echo \u0026#34;Creating ext4 filesystem on logical volume lvscript2\u0026#34; echo mkfs.ext4 /dev/vgscript/lvscript2 echo \u0026#34;Creating /mnt/ext4\u0026#34; mkdir /mnt/ext4 echo \u0026#34;Mounting filesystem\u0026#34; mount /dev/vgscript/lvscript2 /mnt/ext4 elif [ \u0026#34;$DEVICE\u0026#34; = lvscript3 ] then echo \u0026#34;Creating vfat filesystem on logical volume lvscript3\u0026#34; echo mkfs.vfat /dev/vgscript/lvscript3 echo \u0026#34;Creating /mnt/vfat\u0026#34; mkdir /mnt/vfat echo \u0026#34;Mounting filesystem\u0026#34; mount /dev/vgscript/lvscript3 /mnt/vfat echo echo echo \u0026#34;Done!\u0026#34; df -h else echo fi done [root@server40 ~]# fsscript.sh Creating xfs filesystem on logical volume lvscript1 Filesystem should be larger than 300MB. Log size should be at least 64MB. Support for filesystems like this one is deprecated and they will not be supported in future releases. meta-data=/dev/vgscript/lvscript1 isize=512 agcount=4, agsize=12800 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=1 inobtcount=1 nrext64=0 data = bsize=4096 blocks=51200, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=1368, version=2 = sectsz=512 sunit=0 blks, lazy- count=1 realtime =none extsz=4096 blocks=0, rtextents=0 Creating /mnt/xfs Mounting filesystem Creating ext4 filesystem on logical volume lvscript2 mke2fs 1.46.5 (30-Dec-2021) Creating filesystem with 204800 1k blocks and 51200 inodes Filesystem UUID: b16383bf-7b65-4a00-bb6d-c297733f60b3 Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729 Allocating group tables: done Writing inode tables: done Creating journal (4096 blocks): done Writing superblocks and filesystem accounting information: done Creating /mnt/ext4 Mounting filesystem Creating vfat filesystem on logical volume lvscript3 mkfs.fat 4.2 (2021-01-31) Creating /mnt/vfat Mounting filesystem Done! Lab 21-3: Write Script to Configure New Network Connection Profile # Present a new network interface to server40 in VirtualBox Manager. As user1 with sudo on server40, write a single bash script to run the nmcli command to configure custom IP assignments (choose your own settings) on the new network device. Make a copy of the /etc/hosts file as part of this script. Choose a hostname of your choice and add a mapping to the /etc/hosts file without overwriting existing file content. [root@server40 ~]# vim /usr/local/bin/network.sh #!/bin/bash cp /etc/hosts /etc/hosts.bak \u0026amp;\u0026amp; nmcli c a type Ethernet con-name enp0s9 ifname enp0s9 ip4 10.32.32.2/24 gw4 10.32.32.1 echo \u0026#34;10.32.33.14 frog.example.com frog\u0026#34; \u0026gt;\u0026gt; /etc/hosts [root@server40 ~]# chmod +x /usr/local/bin/network.sh [root@server40 ~]# network.sh Connection \u0026#39;enp0s9\u0026#39; (5a342243-e77b-452e-88e2-8838d3ecea6d) successfully added. [root@server40 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.32.33.14 frog.example.com frog [root@server40 ~]# ip a enp0s9: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:1d:f4:c1 brd ff:ff:ff:ff:ff:ff inet 10.32.32.2/24 brd 10.32.32.255 scope global noprefixroute enp0s9 valid_lft forever preferred_lft forever inet6 fe80::2c5d:31cc:1d79:6b43/64 scope link noprefixroute valid_lft forever preferred_lft forever [root@server40 ~]# nmcli d s DEVICE TYPE STATE CONNECTION enp0s3 ethernet connected enp0s3 enp0s8 ethernet connected enp0s8 enp0s9 ethernet connected enp0s9 lo loopback connected (externally) lo ","externalUrl":null,"permalink":"/tech/linux/shell-scripting/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eShell Scripts\n    \u003cdiv id=\"shell-scripts\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#shell-scripts\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eA group of Linux commands along with control structures and optional comments stored in a text file.\u003c/p\u003e","title":"Shell Scripting","type":"tech"},{"content":"Automated setup https://universal-blue.org/\nYou get all the benefits of using containers Separates system level packages from applications.\nSystem Level\nDesktop, kernel? Layering Apps at system level because containers aren\u0026rsquo;t as developed yet Locks to the fedora version you are on Layered package examples # - gnome shell extensions - distrobox Uses rpm-ostree? https://coreos.github.io/rpm-ostree/administrator-handbook/\nFlatpacks\nRemove fedora flatpack stuff and use flathub repos instead https://flatpak.org/setup/Fedora\nSystemd unit for automatic flatpack updates\nUpdate every 4 hours to mirror ubuntu\nflatseal adjust permissions of flatpacks\ncheck out apps.gnome.org\nRebase into Universal Blue # Rebase onto the \u0026ldquo;unsigned\u0026rdquo; image then reboot: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/silverblue-main:39 and\nThen the signed image and reboot: rpm-ostree rebase ostree-image-signed:docker://ghcr.io/ublue-os/silverblue-main:39\nThen do we you do after install, open the app store and install stuff via GUI or we\njust # have a .justfile to install all flatpak/ homebrew packages https://universal-blue.discourse.group/t/introduction-to-just/42 https://just.systems/man/en/chapter_1.html\nMy justfile\nimport \u0026#34;/usr/share/ublue-os/justfile\u0026#34; # You can add your own commands here! For documentation, see: [https://ublue.it/guide/just/](https://ublue.it/guide/just/) first-install: flatpak install \\ flathub com.bitwarden.desktop \\ flathub com.brave.Browser \\ flathub com.discordapp.Discord \\ flathub net.cozic.joplin_desktop \\ flathub org.gimp.GIMP \\ flathub org.gnome.Snapshot \\ flathub org.libreoffice.LibreOffice \\ flathub org.remmina.Remmina \\ flathub com.termius.Termius \\ flathub net.devolutions.RDM \\ flathub com.slack.Slack \\ flathub org.keepassxc.KeePassXC \\ flathub md.obsidian.Obsidian \\ flathub com.calibre_ebook.calibre \\ flathub com.logseq.Logseq \\ flathub org.mozilla.Thunderbird \\ flathub us.zoom.Zoom \\ flathub org.wireshark.Wireshark \\ flathub com.nextcloud.desktopclient.nextcloud \\ flathub com.google.Chrome brew install \\ ansible \\ chezmoi \\ neovim \\ onedrive \\ wireguard-tools Set up github dotfiles repo\nInstall chezmoi and initialize: chezmoi init\nSync with Chezmoi: https://www.chezmoi.io/quick-start/\nAdd dotfiles chezmoi add ~/.bashrc\nEdit a dotfile chezmoi edit ~/.bashrc\nSee changes chezmoi diff\nApply changes chezmoi -v apply\nto sync chezmoi with git:\nchezmoi cd git remote add origin https://github.com/$GITHUB_USERNAME/dotfiles.git $ git push -u origin main $ exit For subsequent git pushes:\ngit commit -a -m \u0026#34;commit\u0026#34; \u0026amp;\u0026amp; git push From a second machine: # Install all dotfiles with a single command: chezmoi init --apply https://github.com/$GITHUB_USERNAME/dotfiles.git\nIf you use GitHub and your dotfiles repo is called dotfiles then this can be shortened to: $ chezmoi init --apply $GITHUB_USERNAME\nSee a list of full commands: chezmoi help\nOr you can initialize and choose what you want: chezmoi init https://github.com/$GITHUB_USERNAME/dotfiles.git\nSee what changes are awaiting: chezmoi diff\nApply changes: chezmoi apply -v\ncan also edit a file before applying: chezmoi edit $FILE\nOr merge the current file with new file: chezmoi merge $FILE\nFrom any machine, you can pull and apply changes from your repo: chezmoi update -v\nAdd the justfile: chezmoi add .justfile\nInstall Connect Tunnel # Download from website Install java rpm-ostree install java\nRunn the connect tunnel install script\nCommands located in /var/usrlocal/Aventail must be ran as root `sudo ./startctui.sh\n","externalUrl":null,"permalink":"/tech/tools/silverblue/","section":"Teches","summary":"\u003cp\u003eAutomated setup \u003ca\n  href=\"https://universal-blue.org/\"\n    target=\"_blank\"\n  \u003ehttps://universal-blue.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYou get all the benefits of using containers\nSeparates system level packages from applications.\u003c/p\u003e\n\u003cp\u003eSystem Level\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDesktop, kernel?\u003c/li\u003e\n\u003cli\u003eLayering\n\u003cul\u003e\n\u003cli\u003eApps at system level because containers aren\u0026rsquo;t as developed yet\u003c/li\u003e\n\u003cli\u003eLocks to the fedora version you are on\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eLayered package examples\n    \u003cdiv id=\"layered-package-examples\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#layered-package-examples\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e- gnome shell extensions\n- distrobox\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUses rpm-ostree? \u003ca\n  href=\"https://coreos.github.io/rpm-ostree/administrator-handbook/\"\n    target=\"_blank\"\n  \u003ehttps://coreos.github.io/rpm-ostree/administrator-handbook/\u003c/a\u003e\u003c/p\u003e","title":"Silverblue","type":"tech"},{"content":"1.0 Network Fundamentals\n1.1 Explain the role and function of network components\n1.1.e Controllers (Cisco DNA Center and WLC)\n6.0 Automation and Programmability\n6.1 Explain how automation impacts network management\n6.2 Compare traditional networks with controller-based networking\n6.3 Describe controller-based and software defined architectures (overlay, underlay, and fabric)\n6.3.a Separation of control plane and data plane\n6.3.b Northbound and southbound APIs\n6.4 Compare traditional campus device management with Cisco DNA Center enabled device\nmanagement\nThe southbound side of the controller contains the fabric, underlay, and overlay\nOverlay: transport traffic from one fabric endpoint to another over the fabric.Themechanisms to create VXLAN tunnels between SDA switches, which are then used to\nTraffic sent by the endpoint devices flows through VXLAN tunnels in the overlay Monday, November 1, 2021 9:36 AM\nUnderlay: The network of devices and connections (cables and wireless) to provide IP connectivity to all\nnodes in the fabric, with a goal to support the dynamic discovery of all SDA devices and endpoints as a part of the process to create overlay VXLAN tunnels.\nthe underlay exists asmultilayer switches and their links, with IP connectivity\nFabric: across the network with the desired features and attributes.The combination of overlay and underlay, which together provide all features to deliver data\nThe overlay drawing at the top of the figure shows only two switches—called fabric edge nodes,\nbecause they happen to betwo. Both concepts (underlay and overlay) together create the SDA fabric.at the edges of the SDA fabric—with a tunnel labeled VXLAN connecting the\nThe SDA Underlay\nalso includes the configuration and operation of the underlay so it can support the work of the\noverlay network\nUsing Existing Gear for the SDA Underlay\nBecause of the possibility of harming the existing production configuration, should not be used to configure the underlay if the devices are currently used in production.DNA Center\n(DNA Center will be used to configure the underlay with deployments that use all new\nhardware.)\n○ # ○ Thesupported depending on their different SDA roles (see a link at existing hardware must be from the SDA compatibility list, with different models http://www.cisco.com/go/sda). ○ Thethat same compatibility list.device software levels must meet the requirements, based on their roles, as detailed in roles include: Fabric edge node:A switch that connects to endpoint devices (similar to Fabric edge node:traditional access switches)A switch that connects to endpoint devices (similar to\nFabric border node: A switch that connects to devices outside SDA’s control, for\nexample, switches that connect to the WAN routers or to an ACI data center\nFabric control node:A switch that performs special control plane functions for\nthe underlay (LISP), requiring more CPU and memory\nUsing New Gear for the SDA Underlay\nbuying new hardware for the SDA fabric—that is, agreenfield design\nthink about these traditional LAN design points:\n▪ The number of ports needed in switches in each wiring closet\n○ ▪ The port speeds required\n▪ The benefit of a switch stack in each wiring closet ○ # ▪ The cable length and types of cabling already installed ○ # ▪ The need for power (PoE/PoE+) ○ # ▪ The power available in each new switch versus the PoE power requirements ○ # ○ ▪ Link capacity (speed and number of links) for links between switches In comparison, a greenfield SDA fabric uses a routed access layer design DNA Center will configure the devices’ underlay configuration to use a routed access layer routed access layer design has these features: routed access layer design has these features: All switches act as Layer 3 switches. The switches use theIS-ISrouting protocol. All links between switches (single links, EtherChannels) are routed Layer 3 links (not Layer 2 links). As a resultwhich links to use based on the IP routing tables., STP/RSTP is not needed,with the routing protocol instead choosing The equivalent of a traditional access layer switchthe default gateway for the endpoint devices, rather than distribution switches.—anSDA edge node—acts as As a result, HSRP (or any FHRP) is no longer needed. The SDA Overlay\nThe first SDA node to receive the frame encapsulates the frame in a new message—using a\ntunneling specification called VXLAN\nandin VXLAN, the other SDA nodes forward the frame based on the VXLAN tunnel details. The last SDA forwards the frame into the fabric. Once the ingress node has encapsulated the original frame\nnode removes the VXLAN details, leaving the original frame, and forwards the original frame on\ntoward the destination endpoint.\nthere isno performance penalty for the switches to perform the extra work. (happens in ASIC)\nSDA uses LISP for endpoint discovery and location needed to create the VXLAN tunnels.\nVXLAN Tunnels in the Overlay (Data Plane)\ngoals in mind:\nThe VXLAN tunneling (the encapsulation and de-encapsulation) must be performed by the ASIC on each switch so that there is no performance penalty. (for the SDA hardware compatibility list: the switches must have ASICs that can That is one reason perform the work.) The VXLAN encapsulation must supply header fields that SDA needs for its features, so the tunneling protocol should be flexible and extensible, while still being supported by the switch ASICs. The tunneling encapsulation needs to encapsulate the entire data link frame instead of encapsulating the IP packet. That allows SDA to support Layer 2 forwarding features as well as Layer 3 forwarding features. Virtual Extensible LAN (VXLAN) protocol to create the tunnels used by SDA.\nthe egress edge nodeingress edge node encapsulates the frame and sends it across a VXLAN tunnel to the\nTo support the VXLAN encapsulation, the underlay uses a separate IP address space as compared with the rest of the enterprise\nThe overlay tunnels use addresses from the enterprise address space. For instance, imagine\nan enterprise used these address spaces:\n10.0.0.0/8: Entire enterprise\n172.16.0.0/16: SDA underlay\nthe underlay would be built using the 172.16.0.0/16 IPv4 address space, with all links using\naddresses from that address space\neach with one underlay IP address\nLISP for Overlay Discovery and Location (Control Plane)\nTraditional Layer 2 switches learn possible destinations by examining the source MAC\naddresses of incoming frames, storing those MAC addresses as possible future destinations in the switch’s MAC address table. When new frames arrive, the Layer 2 switch data plane\nthen attempts to match the Ethernet frame’s destination MAC address to an entry in its\nMAC address table.\nTraditional Layer 3 routers learn destination IP subnets using routing protocols, storing routes to reach each subnet in their routing tables. When new packets arrive, the Layer 3\ndata plane attempts to match the IP packet’s destination IP address to some entry in the IP routing table.\nNodes in the SDA network do not do these same control plane actions to support endpoint traffic\nFabric edge nodes—SDA nodes that connect to the edge of the SDA fabric—learn the\nlocation of possible endpoints using traditional means, based on their MAC address, individual IP address, and by subnet, identifying each endpoint with an endpoint identifier\n(EID).\nThe fabric edge nodes register the fact that the node can reach a given endpoint (EID) into a\ndatabase called the LISP map server.\nThelocators (RLOCs) (which identify the fabric edge node that can reach the EID).LISP map server keeps the list of endpoint identifiers (EIDs) and matching routing\nIn the future, when the fabric data plane needs to forward a message, it will look for and find the destination in the LISP map server’s database.\nstep 1 in the figure, switch SW3 sent a message to the LISP map server, registering the\nstep 1 in the figure, switch SW3 sent a message to the LISP map server, registering the information about subnet 10.1.3.0/24 (an EID), with its RLOC setting to identify itself as the\nnode that can reach that subnet.\nStep 2 shows an equivalent registration process, this time for SW4, with EID 10.1.4.0/24, and with R4’s RLOC of 172.16.4.4.\ningress tunnel router (ITR)-where the frame arrives\n3 An Ethernet frame to a new destination arrives at ingress edge node SW1 (upper left), and\nthe switch does not know where to forward the frame.\n4 The ingress node sends a message to the LISP map server asking if the LISP server knows how to reach IP address 10.1.3.1.\n5 The LISP map server looks in its database and finds the entry it built back at step 1 in the previous figure, listing SW3’s RLOC of 172.16.3.3.\n6 The LISP map server contacts SW3—the node listed as the RLOC—to confirm that the\nentry is correct.\n7 SW3 completes the process of informing the ingress node (SW1) that 10.1.3.1 can be reached through SW3.\nIt adds the IP, UDP, and VXLAN headers shown so it can deliver the message over the SDA network that outer IP header listing a destination IP address of the RLOC IP address as all the VXLAN processing happens in an ASIC.\nDNA Center and SDA Operation\ntwo notable roles:\nAs the controller in a network that uses Cisco SDA\nAs expectation that one day DNA Center may become Cisco’s primary enterprise network a network management platform for traditional (non-SDA) network devices, with an\nmanagement platform\nCisco DNA Center\nCisco DNA Center\nCisco DNA Center includes a robust northbound REST API along with a series of southbound APIs supports several southbound APIs two categories: Protocols to support traditional networking devicesSSH, SNMP /software versions: Telnet, Protocols to support more recent networking devices/software versions: NETCONF, RESTCONF Cisco DNA Center and Scalable Groups SDA creates many interesting new and powerful features beyond how traditional campus networks work. Cisco DNA Center not only enables an easier way to configure and operate those features, but it also completely changes the operational model one feature as an example:scalable groups. Issues with Traditional IP-Based Security ACL management suffers with these kinds of issues: ACEs (access control entries) cannot be removed from ACLs because of the risk of causing failures to the logic for some other past requirement. New changes become more and more challenging due to the length of the ACLs. Troubleshooting ACLs as a systembe delivered from end-to-end—becomes an even greater challenge.—determining whether a packet would SDA Security Based on User Groups\nThe model in Figure 17-14 helps demonstrate the concept of intent-based\nnetworking (IBN)the network—in this case, a set of security policies. The controller. The engineer configures the intent or outcome desired from\ncommunicates with the devices in the network, with the devices determining exactly what configuration and behavior are necessary to achieve those\nintended policies.\nThe engineer can consider each new security requirement separately,\nwithout analysis of an existing (possibly lengthy) ACL.\nEach new requirement can be considered without searching for all the ACLs in the likely paths between endpoints and analyzing each and every\nACL.\nDNA Center (and related software) keeps the policies separate, with space\nto keep notes about the reason for the policy.\nEach policy can be removed without fear of impacting the logic of the other policies.\nachieved bygroup assigned a tying security to groups of usersscalable group tag (SGT). , called scalable groups, with each\nsecurity tools in the network, like Cisco’s Identity Services Engine (ISE) DNA Center as a Network Management Platform\nCisco Prime Infrastructure (PI)(www.cisco.com/go/primeinfrastructure) is an example ofa\ntraditional enterprise network management product\nincludes the following features:\nSinglefeatures-pane-of-glass: Provides one GUI from which to launch all PI functions and\nDiscovery, inventory, and topology:and arranges them in a topology mapDiscovers network devices, builds an inventory,\nEntire enterprise: Provides support for traditional enterprise LAN, WAN, and data\ncenter management functions\nMethods and protocols: Uses SNMP, SSH, and Telnet, as well as CDP and LLDP, to\ndiscover and learn information about the devices in the network\nLifecycle management: configure it to be working in production (day 1), and perform ongoing monitoring and Supports different tasks to install a new device (day 0),\nmake changes (day n)\nmake changes (day n)\nApplication visibility:Simplifies QoS configuration deployment to each device\nConverged wired and wireless: Enables you to manage both the wired and wireless LAN from the same management platform\nSoftware Image Management (SWIM): Manages software images on network devices and automates updates\nPlug-and-Play: Performs initial installation tasks for new network devices after you\nphysically install the new device, connect a network cable, and power on\nPI itself runs as an application on a server platform with GUI access via a web browser\nserver can be purchased from Cisco as a software package to be installed and run on your\nservers, or as a physical appliance.\nDNA Center Similarities to Traditional Management\nboth can discover network devices and create a network topology map\nI encourage you to take some time to use and watch some videos about Cisco DNA Center. The\n“Chapter Review” section for this chapter on the companion website lists some links for good videos. Also, start at https://developer.cisco.comand look for Cisco DNA Center sandbox labs to\nfind a place to experiment with Cisco DNA Center.\nDNA Center Differences with Traditional Management\nCisco DNA Center supports SDA, whereas other management apps do not\nSo think of PI as comprehensive to traditional device management, with Cisco DNA Center having\nmany of those features, while focusing on future features like SDA support.\nCisco DNA Center features help make initial installation easier, simplify the work to implement\nfeatures that traditionally have challenging configuration, and use tools to help you notice issues more quickly. Some of the features unique to Cisco DNA Center include\nEasyQoS: Deploys QoS, one of the most complicated features to configure manually, with\njust a few simple choices from Cisco DNA Center\nEncrypted traffic analysis: Enables Cisco DNA to use algorithms to recognize security threats\neven in encrypted traffic\nDevice 360 and Client 360: Gives a comprehensive (360device -degree) view of the health of the\nNetwork time travel: Shows past client performance in a timeline for comparison to current behavior\nPath trace: Discovers the actual path packets would take from source to destination based\non current forwarding tables\n","externalUrl":null,"permalink":"/tech/networking/softwaredefinedaccess/","section":"Teches","summary":"\u003cp\u003e1.0 Network Fundamentals\u003cbr\u003e\n1.1 Explain the role and function of network components\u003cbr\u003e\n1.1.e Controllers (Cisco DNA Center and WLC)\u003cbr\u003e\n6.0 Automation and Programmability\u003cbr\u003e\n6.1 Explain how automation impacts network management\u003cbr\u003e\n6.2 Compare traditional networks with controller-based networking\u003cbr\u003e\n6.3 Describe controller-based and software defined architectures (overlay, underlay, and fabric)\u003cbr\u003e\n6.3.a Separation of control plane and data plane\u003cbr\u003e\n6.3.b Northbound and southbound APIs\u003cbr\u003e\n6.4 Compare traditional campus device management with Cisco DNA Center enabled device\u003cbr\u003e\nmanagement\u003c/p\u003e","title":"Software-Defined Access (SDA)","type":"tech"},{"content":"2.0 Network Access 2.4 Configure and verify (Layer 2/Layer 3) EtherChannel (LACP)2.5 Describe the need for and basic operations of Rapid PVST+ Spanning Tree Protocol and identify basic operations 2.5.a Root port, root bridge (primary/secondary), and other port names2.5.b Port states (forwarding/blocking) 2.5.c PortFast benefits RSTP is most common nowCisco defaults to RSTP\nMAC table instability The switches arrive on different ports.MAC address tables keep changing because frames with the same source MAC\nBroadcast storms forwarding of a frame repeatedly on the same links\nMultiple frame transmission side effect of looping framesMultiple copies are delivered to a host, confusing the host.\nWhat Spanning Tree Does interfaces does not process any frames except STP/RSTP messages and some other overhead messages blocking state STP Convergence switches collectively realize that something has changed in the LAN topology determine whether they need to change which ports block and which port forward\nHow Spanning Tree works\nthree criteria to choose whether to put and interface in forwarding state: STP puts all working interfaces on the root switch in forwarding state elect a root switch. select the port with the least administrative cost (root port)\u0026ndash; cost between itself and the root switch (root cost) (root cost path)root port (RP) gets put in a forwarding state That switch is the designated switch, and that switch\u0026rsquo;s interface, attached to that The switch with the lowest root cost, as compared with the other switches attached to the same link, is placed in forwarding state. nonroot switches Spanning Tree Protocol Concepts\nThat switch is the designated switch, and that switch\u0026rsquo;s interface, attached to that segment, is called the designated port (DP) STP States\nforwarding root switch is always the designated switch All the root switches ports forwarding port with the least cost to the root switch (lowest root cost) All nonroot switch\u0026#39;s root ports forwardingswitch forwarding the Hello on to the segment with the lowest root cost is the designated\nswitch for the segment Each LAN\u0026#39;s designated port blockingNot used for forwarding frames frames received on these interfaces to not forward All other working ports The STP Bridge ID and Hello BPDU\n82 - -byte value unique to each switch. byte priority field 6 - byte system ID- based on a universal (burned-in) MAC address bridge ID (BID) configuration BPDUs, which switches used to exchange information with each other (switches) The most common BPDU, called a switches can tell which switch sent which Hello BPDU sending switch\u0026rsquo;s BID BID the sender currently believes to be the root switch Root Bridge ID Sender\u0026rsquo;s root cost- STP cost between this switch and current root Timer values on the root switch- Hello timer, MaxAge timer, and forward delay timer hello BPDU, bridge protocol data units (BPDU) Electing the Root Switch\nbased on the BIDs in the BPDUs.root switch is the switch with the lowest numeric value for the BID. Because the two- essentially the switch with the lowest priority becomes the root.-part BID starts with the priority value,\nIf a tie occurs based on the priority portion of the BID, - the switch with the lowest MAC address portion of the BID is the root.\nMac addresses are the second part of the BID\nMac addresses are the second part of the BID\nall switches claim to be the root by sending Hello BPDUs listing their own BID as the root BID.\nthat switch stops advertising itself as root and starts forwarding the superior Hello. - The Hello sent by the better switch lists the better switch’s BID as the root.\nIf a switch hears a Hello that lists a better (lower) BID-\nsuperior hello (better hello)- the listed root’s BID is better (numerically lower),\nInferior hello (worse Hello), meaning that - the listed root’s BID is not as good (numerically higher)\neach nonroot switch chooses its one and only root port.which it has the least STP/RSTP cost to reach the root switch (least root cost).A switch’s RP is its interface through\nChoosing Each Switch\u0026rsquo;s Root Port\nits interface through which it has the least STP/RSTP cost to reach the root switch (least root cost).\neach nonroot switch chooses its one and only root port.\nThe STP/RSTP port cost is simply an integer value assigned to each interface, per VLAN\nThe switches also look at their neighbor’s root cost, as announced in Hello BPDUs received from each neighbor.\nSW3 calculates its cost to reach the root over the two possible paths by adding the advertised cost (in Hello messages) to the interface costs listed in the figure.\nThe root switch sends Hellos, with a listed root cost of 0. The idea is that the root’s cost to reach itself is 0.\nEach switch places its root port into a forwarding state.\ntiebreaker to use in case the best root cost ties for two or more paths.\nChoose based on the lowest neighbor bridge ID. Choose based on the lowest neighbor port priority.Choose based on the lowest neighbor internal port number. Choosing the Designated Port on each LAN Segment\nfinal step to choose the STP/RSTP topology is to choose the designated port on each LAN segment.\nThe designated port (DP) on each LAN segment is the switch port that advertises the lowestHello onto a LAN segment. -cost\nWhen a nonroot switch forwards a Hello, the nonroot switch sets the root cost field in the Hello\nto that switch’s cost to reach the root. In effect, the switch with the lower cost to reach the root, among all switches connected to a segment, becomes the DP on that segment.\nAll DPs are placed into a forwarding state\nthese would be unlikely today. In that case, the one switch hears its own BPDUs. the lowest interface STP/RSTP priority and, if that ties, the lowest internal interface number. So, if a switch ties with itself, two additional tiebreakers are used: A single switch can connect two or more interfaces to the same collision domain by connecting to a hub. Two additional tiebreakers are needed in some cases switch ports connected to endpoint devices should become DPs and settle into a forwarding state. Configuring to influence the STP Topology\nconfigure the bridge ID and change STP/RSTP port costs. set the priority used by the switch, while\ngiving a switch the lowest priority value among all switches will cause that\nswitch to win the root election. continues to use the universal MAC address as the final 48 bits of the BID. - change the BID, the engineer can to favor one link, give the ports on that link a lower cost, or to avoid a link, give the ports a higher cost. Link Costs\n10 Mbps\u0026ndash; 2,000,000100 (old)\n200,00019 (old) 100Mbps 20,0004 (old) 1gbps 2000 2 (old) 10Gbps 200 N/A (old) 100Gbps (^20) N/A (old) the cost defaults based on the operating speed of the link, not the maximum speed\n(config) # spanning - Cisco Catalyst switches can be configured to use the long values as defaults - tree pathcost method long\n1Tbps Details specific to STP\nSTP Activity When the Network Remains Stable\nAn Each nonroot switch forwards the Hello on all DPs, but only after changing items listed in STP root switch sends a new Hello BPDU every 2 seconds by default.\nthe Hello.\n(As a result, the Hello flows once over every working link in the LAN.)\nWhen forwarding the Hello BPDU, each switch sets the root cost to that local switch’s\ncalculated root cost. The switch also sets the “sender’s bridge ID” field to its own bridge ID. (The root’s bridge ID field is not changed.)\nStep 1. interfaces (those in a forwarding state).The root creates and sends a Hello BPDU, with a root cost of 0, out all its working Step 2. The nonroot switches receive the Hello on their root ports. After changing the Hello to list their own BID as the sender’s BID and listing that switch’s root cost, the switch forwards the Hello out all designated ports. Step 3. Steps 1 and 2 repeat until something changes. When a switch ceases to receive the Hellos, or receives a Hello that lists different details, something has failed, so the switch reacts and starts the process of changing the spanning- tree topology. STP convergence process requires the use of three timers\nAll switches use the timers as dictated by the root switch, which the root lists in its\nperiodic Hello BPDU messages. STP Timers That Manage STP Convergence periodic Hello BPDU messages. 2 seconds by default Period between hellos created by the root hello 10 times the hello (20 seconds by default hello)How long the switch will go without receiving any hellos before it attempts to\nchange the stp topology MaxAge Forward Delay-- 15 secondshow long the port stays in listening and learning state After MaxAge expires, the switch essentially makes all its STP choices again, based on any Hellos it receives from other switches. Changing Interface States with STP\nRoles, like root port and designated port, relate to how STP analyzes the LAN topology.\nStates, like forwarding and blocking, tell a switch whether to send or receive frames.\nWhen STP converges, a switch chooses new port roles, and the port roles determine the state (forwarding or blocking).\nSwitches using STP can simply move immediately from forwarding to blocking state, but they must take extra time to transition from blocking state to forwarding state.\nwhen a port that formerly blocked needs to transition to forwarding, the switch first puts\nthe port through two intermediate interface states.\ninterface does not forward frames. switch removes old stale (unused) MAC table entries for which no frames are\nreceived from each MAC address during this period.\nThese stale MAC table entries could be the cause of the temporary loops.\nTransitory\nListening: do not forward framesswitch begins to learn the MAC addresses of frames received on the interface. Transitory Learning: does not forward frames does not learn mac addressesstable Blocking learns mac addresses forwards framesstable Forwarding does not forward or learn mac addressesstable Disabled blocking \u0026gt; listening, \u0026gt; learning \u0026gt; forwarding.\nSTP leaves the interface in each interim state for a time equal to the forward delay timer, which defaults to 15 seconds.\na convergence event that causes an interface to change from blocking to forwarding\nrequires 30 seconds to transition from blocking to forwarding.\na switch might have to wait MaxAge seconds (default 20 seconds) before even choosing to\nmove an interface from blocking to forwarding state.\nRapid STP Concepts\n802.1wSits in 802.1q standards document Comparing STP and RSTP elect the root switch using the same rules and tiebreakers. switches select their root ports with the same rules.elect designated ports on each LAN segment with the same rules and tiebreakers. - (RSTP calls the blocking state the discarding state.) place each port in either forwarding or blocking state similarities they can both be used in the same network. RSTP improves network convergence when topology changes occur, usually converging within a few seconds (or in slow conditions, in about 10 seconds). RSTP defines more cases in which the switch can avoid waiting for a timer to expire, such as the following: a switch can replace its root port, without any waiting to reach a forwarding state (in some conditions). replace a designated port, without any waiting to reach a forwarding state (in some conditions). lowers waiting times for cases in which RSTP must wait for a timer.MaxAge is only 3 times the hello uses the term alternate port to refer to a switch’s other ports that could be used as the root port if the root port ever fails.\nThe backup port concept provides a backup port on the local switch for a designated port.- backup ports apply only to designs that use hubs, so they are unlikely to be useful today.) RSTP Port roles Nonroot switch\u0026rsquo;s port that has the best path to the root\nRoot Port\nNonroot switch\u0026rsquo;s port that has the best path to the root\nAlternate port- Replaces the root port when the root port fails\nDesignated port- port designated to forward onto a collision domain\nReplaces designated port when designated port fails\nBackup Port Disabled ports- administratively disabled each switch independently generates its own Hellos. (rather than waiting on timers to expire to learn new information) allows for queries between neighbors RSTP and the Alternate (Root) Port Role\nboth the RP and the alternate port must receive Hellos that identify the same root switch. alternate port the role from root port to a disabled port, and the state from forwarding to discarding - its role changes to be the root port, with a forwarding state. without waiting on any timers, the switch changes roles and state for the alternate port: the switch changes the former root port’s role and state: the new root port also does not need to spend time in other states, such as learning state, instead moving immediately to forwarding state. Step 1. The link between SW1 and SW3 fails, so SW3’s current root port (Gi0/1) fails.\nStep 2. SW3 and SW2 exchange RSTP messages to confirm that SW3 will now transition its former alternate port (Gi0/2) to be the root port. This action causes SW2 to flush the\nrequired MAC table entries.Step 3. SW3 transitions Gi0/1 to the disabled role and Gi0/2 to the root port role.\nStep 4. SW3 transitions Gi0/2 to a forwarding state immediately, without using learning state, because this is one case in which RSTP knows the transition will not create a loop. RSTP States and Processes\nRSTP keeps both the learning and forwarding states as compared with STP, for the same purposes\nRSTP does not even define a listening state,\nRSTP renames the blocking state to the discarding state and redefines its use slightly.\nRSTP uses the discarding state for what STP defines as two states: disabled state and blocking state.\nRSTP switches tell each other (using messages) that the topology has changed.\nThose messages also direct neighboring switches to flush the contents of their MAC tables in a way that removes all the potentially loop-causing entries, without a wait.\nAs a result, RSTP creates more scenarios in which a formerly discarding port can immediately transition to a forwarding state, without waiting, and without using the\nlearning state, as shown in the example in Figure 9-9.\nRSTP backup port role creates a way for RSTP to quickly replace a switch’s designated port on some LAN. RSTP Port Types\nseveral links between two switches. RSTP considers these links to be point-to-point links\nand the ports connected to them to be point-to-point ports\nPorts that instead connect to a single endpoint device at the edge of the network, like a PC or server, are called point-to-point edge ports, or simply edge ports.\n\u0026ldquo;shared\u0026rdquo; to describe ports connected to a hub.\nhubs also force the attached switch port to use halfall half-duplex ports may be connected to hubs, treating ports that use half duplex -duplex logic. RSTP assumes that\nas shared ports.\nRSTP converges more slowly on shared ports as compared to all point-to-point ports.\nOptional STP Features\nEtherchannel\nThe switches treat the EtherChannel as a single interface with regard to STP.\nLayer 2 EtherChannels combine links that switches use as switch ports, with the\nswitches using Layer 2 switching logic to forward and receive Ethernet frames over the EtherChannels. Layer 3 EtherChannels also combine links, but the switches use\nLayer 3 routing logic to forward packets over the EtherChannels.\nPortFast\nallows a switch to immediately transition from blocking to forwarding, bypassing listening and learning states.\nPortFast only ports on which you can safely enable PortFast are ports on which you know that no bridges, switches, or other STP-speaking devices are connected. Cisco switches enable RSTP point-to-point edge ports by enabling PortFast on the\nport.\nBPDU Guard An attacker could connect a switch to one of these ports, one with a low STP/RSTP An attacker could connect a switch to one of these ports, one with a low STP/RSTP priority value, and become the root switch. The new STP/RSTP topology could have worse performance than the desired topology. Tactually forward much of the traffic in the LAN. Without the networking staff he attacker could plug into multiple ports, into multiple switches, become root, and realizing it, the attacker could use a LAN analyzer to copy large numbers of data frames sent through the LAN. Users could innocently harm the LAN when they buy and connect an inexpensive consumer LAN switch (one that does not use STP/RSTP). Such a switch, without any STP/RSTP function, would not choose to block any ports and could cause a loop. Cisco BPDU Guard feature helps defeat these kinds of problems by disabling a port if any BPDUs are received on the port.that should be used only as an access port and never connected to another switch.So, this feature is particularly useful on ports In addition, the BPDU Guard feature helps prevent problems with PortFast. PortFast should be enabled only on access ports that connect to user devices, not to other LAN switches. Using BPDU Guard on these same ports makes sense because if another switch connects to such a port, the local switch can disable the port before a loop is created. ","externalUrl":null,"permalink":"/tech/networking/spanningtreeprotocol/","section":"Teches","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e2.0 Network Access\n2.4 Configure and verify (Layer 2/Layer 3) EtherChannel (LACP)2.5 Describe the need for and basic operations of Rapid PVST+ Spanning Tree Protocol and identify\nbasic operations\n2.5.a Root port, root bridge (primary/secondary), and other port names2.5.b Port states (forwarding/blocking)\n2.5.c PortFast benefits\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eRSTP is most common nowCisco defaults to RSTP\u003cbr\u003e\nMAC table instability\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe switches arrive on different ports.MAC address tables keep changing because frames with the same source MAC\u003cbr\u003e\nBroadcast storms\u003c/li\u003e\n\u003cli\u003eforwarding of a frame repeatedly on the same links\u003cbr\u003e\nMultiple frame transmission\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eside effect of looping framesMultiple copies are delivered to a host, confusing the host.\u003cbr\u003e\nWhat Spanning Tree Does\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003einterfaces does not process any frames\u003c/li\u003e\n\u003cli\u003eexcept STP/RSTP messages and some other overhead messages\u003c/li\u003e\n\u003cli\u003eblocking state\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSTP Convergence\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eswitches collectively realize that something has changed in the LAN topology\u003c/li\u003e\n\u003cli\u003edetermine whether they need to change which ports block and which port forward\u003cbr\u003e\nHow Spanning Tree works\u003cbr\u003e\nthree criteria to choose whether to put and interface in forwarding state:\u003c/li\u003e\n\u003cli\u003eSTP puts all working interfaces on the root switch in forwarding state\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eelect a root switch.\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eselect the port with the least administrative cost (root port)\u0026ndash; cost between itself and the root switch (root cost) (root cost path)root port (RP) gets put in a forwarding state\n\u003cul\u003e\n\u003cli\u003eThat switch is the designated switch, and that switch\u0026rsquo;s interface, attached to that\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe switch with the lowest root cost, as compared with the other switches attached to the same link, is placed in forwarding state.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enonroot switches\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSpanning Tree Protocol Concepts\u003c/p\u003e","title":"Spanning Tree Protocol","type":"tech"},{"content":"Routers first learn connected routes, which are routes for subnets attached to a router interface. Routers can also use static routes, which are routes created through a configuration command (ip route) that tells the router what route to put in the IPv4 routing table. And routers can use a routing protocol, in which routers tell each other about all their known routes, so that all routers can learn and build routes to all networks and subnets.\nStep 1. If the destination is local, send directly:\nA. \u0026lsquo;\u0026lsquo;Find the destination host’s MAC address. Use the already-known Address Resolution Protocol (ARP) table entry, or use ARP messages to learn the information.\nB. Encapsulate the IP packet in a data-link frame, with the destination data-link address of the destination host.\nFor each received data-link frame, choose whether or not to process the frame. Process it if\nThe frame has no errors (per the data-link trailer Frame Check Sequence [FCS] field).\nThe frame’s destination data-link address is the router’s address (or an appropriate multicast or broadcast address).\nIf choosing to process the frame at Step 1, de-encapsulate the packet from inside the data-link frame.\nMake a routing decision. To do so, compare the packet’s destination IP address to the routing table and find the route that matches the destination address. This route identifies the outgoing interface of the router and possibly the next-hop router.\nEncapsulate the packet into a data-link frame appropriate for the outgoing interface. When forwarding out LAN interfaces, use ARP as needed to find the next device’s MAC address.\nTransmit the frame out the outgoing interface, as listed in the matched IP route.\nThe interface is in a working state. In other words, the interface status in the show interfaces command lists a line status of up and a protocol status of up. The interface has an IP address assigned through the ip address interface subcommand. Routing Protocol Code: The legend at the top of the show ip route output (about nine lines) lists all the routing protocol codes (exam topic 3.1.a). This book references the codes for connected routes (C), local (L), static (S), and OSPF (O).\nPrefix: The word prefix (exam topic 3.1.b) is just another name for subnet ID.\nMask: Each route lists a prefix (subnet ID) and network mask (exam topic 3.1.c) in prefix format, for example, /24.\nThe ARP Table on a Cisco Router\nDynamically learned ARP table entries have an upward counter, like the 35-minute value for the ARP table entry for IP address 172.16.1.9. By default, IOS will time out (remove) an ARP table entry after 240 minutes in which the entry is not used.\nclear ip arp [ip-address] EXEC command.\nConfiguring Static Routes\nThe static route is considered a network route when the destination listed in the ip route command defines a subnet, or an entire Class A, B, or C network. In contrast, a default route matches all destination IP addresses, while a host route matches a single IP address (that is, an address of one host.)\nHowever, the route that used the outgoing interface configuration is also noted as a connected route; this is just a quirk of the output of the show ip route command.\nalso lists a few statistics about all IPv4 routes. For example, the example shows two lines, for the two static routes configured in Example 16-4, but statistics state that this router has routes for eight subnets.\nIOS adds and removes these static routes dynamically over time, based on whether the outgoing interface is working or not. For example, in this case, if R1’s S0/0/0 interface fails, R1 removes the static route to 172.16.2.0/24 from the IPv4 routing table. Later, when the interface comes up again, IOS adds the route back to the routing table.\nStatic Host Routes\nTo configure such a static route, the ip route command uses an IP address plus a mask of 255.255.255.255 so that the matching logic matches just that one address.\nAn engineer might use host routes to direct packets sent to one host over one path, with all other traffic to that host’s subnet over some other path. For instance, you could define these two static routes for subnet 10.1.1.0/24 and host 10.1.1.9, with two different next-hop addresses, as follows:\nrouters use the most specific route (that is, the route with the longest prefix length)\nFloating Static Routes\nthe router must first decide which routing source has the better administrative distance, with lower being better, and then use the route learned from the better source\nBy default, IOS considers static routes better than OSPF-learned routes. By default, IOS gives static routes an administrative distance of 1 and OSPF routes an administrative distance of 110\nTo implement a floating static route, you need to use a parameter on the ip route command that sets the administrative distance for just that route, making the value larger than the default administrative distance of the routing protocol. For example, the ip route 172.16.2.0 255.255.255.0 172.16.5.3 130\nwhile the show ip route command lists the administrative distance of most routes, as the first of two numbers inside two brackets, the show ip route subnet command plainly lists the administrative distance.\nStatic Default Routes\nip route 0.0.0.0 0.0.0.0 S0/0/1 creates a static default route on Router B1—a route that matches all IP packets—and sends those packets out interface S0/0/1.\nshow ip route\na *, meaning it is a candidate default route. A router can learn about more than one default route, and the router then has to choose which one to use; the * means that it is at least a candidate to become the default route.\nTroubleshooting Static Routes\nThe route is in the routing table but is incorrect.\nIs there a subnetting math error in the subnet ID and mask?\nIs the next-hop IP address correct and referencing an IP address on a neighboring router?\nDoes the next-hop IP address identify the correct router?\nIs the outgoing interface correct, and referencing an interface on the local router (that is, the same router where the static route is configured)?\nThe route is not in the routing table.\ntable. IOS also considers the following before adding the route to its routing table:\nFor ip route commands that list an outgoing interface, that interface must be in an up/up state.\nFor ip route commands that list a next-hop IP address, the local router must have a route to reach that next-hop address.\nYou can configure a static route so that IOS ignores these basic checks, always putting the IP route in the routing table. To do so, just use the permanent keyword on the ip route command. For example, by adding the permanent keyword to the end of the two commands as demonstrated in Example 16-7, R1 would now add these routes, regardless of whether the two WAN links were up.\nThe route is in the routing table and is correct, but the packets do not arrive at the destination host.\nMany legitimate router features can cause these multiple routes to appear in a router’s routing table, including\nStatic routes Route autosummarization Manual route summarization When a particular destination IP address matches more than one route in a router’s IPv4 routing table, the router uses the most specific route—in other words, the route with the longest prefix length mask.\nA second way to identify the route a router will use, one that does not require any subnetting math, is the show ip route address command. The last parameter on this command is the IP address of an assumed IP packet. The router replies by listing the route it would use to route a packet sent to that address.\n","externalUrl":null,"permalink":"/tech/networking/static-routing/","section":"Teches","summary":"\u003cp\u003eRouters first learn connected routes, which are routes for subnets attached to a router interface. Routers can also use static routes, which are routes created through a configuration command (ip route) that tells the router what route to put in the IPv4 routing table. And routers can use a routing protocol, in which routers tell each other about all their known routes, so that all routers can learn and build routes to all networks and subnets.\u003c/p\u003e","title":"Static Routing","type":"tech"},{"content":"1.0 Network Fundamentals1.1 Explain the role and function of network components 1.1.b L2 and L3 switches1.4 Describe switching concepts Configuring Speed, Duplex, and Description\n- autonegotiate○○What speed to useenabled by default **duplex {auto | full | half} and speed {auto | 10 | 100 | 1000}** ○configure the speed and duplex settings **(config** - add a text description to the interface **- int) # description** _text_ **show interfaces status** - lists port #, Name, status, vlan, duplex, speed, and type a-full and aa-means that the listed speed and duplex values were autonegotiated.- 100 IEEE autonegotiation (IEEE standard 802.3u)\n- each node states what it can do, and - then each node picks the best options that both nodes support: -the fastest speed and the best duplex setting, with full duplex being better than half duplex. - disable autonegotiation-Configure both the speed and duplex on a switch interface - when a node tries to use autonegotiation but hears nothing from the device. Speed: Use your slowest supported speed (often 10 Mbps).Duplex: If your speed = 10 or 100, use half duplex; otherwise, use full duplex. Cisco switches can actually sense the speed used by other nodes, even without IEEE autonegotiation. -Cisco switches use this slightly different logic to choose the speed when autonegotiation fails: - Speed: Sense the speed (without using autonegotiation), but if that fails, use the IEEE default (slowest supported speed, often 10 Mbps). - Duplex: Use the IEEE defaults: If speed = 10 or 100, use half duplex; otherwise, use full duplex. - -Ethernet interfaces using speeds faster than 1 Gbps always use full duplex.hubs do not react to autonegotiation messages Autonegotiation\nshow interfaces and show interfaces description\n- -Line status = administratively downProtocol status = down - Interface status = disabled Shutdown command is configured\n- -Line status = downProtocol status = down - Interface status = notconnect Cable, speed mismatch, neighbor device is off , shutdown, or err-disabled\n- -Line status = upProtocol status = down - Interface status = notconnect Not expected on LAN switch physical interfaces\n- -Line status = downProtocol status = down (err-disabled) - Interface status = err-disabled Port security has disabled the interface\n- -Line status = upProtocol status = up - Interface status = connected the interface is working\nshow interfaces fa0/13 - -lists the speed and duplex for interface Fast Ethernet 0/13with nothing implying that the values were learned through autonegotiation.(without the status option) speed manually set 10 Mbps on one switch and 100 Mbps on the other\nQuick Commands Speed/ duplex #duplex auto/full/half#speed auto/10/100/1000 Description #description (text) Interface #show interfaces status#show interfaces fa0/3\n###### 7. Switch InterfacesWednesday, June 30, 2021 8:01 AM speed manually set 10 Mbps on one switch and 100 Mbps on the other-both switches would list the port in a down/down or notconnect state if the duplex settings do not match-the switch interface will still be in a connected (up/up) or connected state. How to identify duplex mismatch problems, \u0026ndash;check the duplex setting on each end of the link to see if the values mismatch.watch for incrementing collision and late collision counters\nCommon Layer 1 problems - -receiving device might receive a frame whose bits have changed values.These frames do not pass the error detection logic as implemented in the FCS field in the Ethernet trailer, - -The receiving device discards the frame and counts it as some kind of input error.Cisco switches list this error as a CRC error - -Frames that did not meet the minimum frame size requirement Can be caused by collisions.-(64 bytes, including the 18-byte destination MAC, source MAC, type, and FCS). Runts: - Frames that exceed the maximum frame size requirement -(1518 bytes, including the 18-byte destination MAC, source MAC, type, and FCS) Giants: Input Errors: -A total of many counters, including runts, giants, no buffer, CRC, frame, overrun, and ignored counts. CRC: --Received frames that did not pass the FCS mathcan be caused by collisions - -Received frames that have an illegal formatcan be caused by collisions.-(like ending with a partial byte) Frame: Packets Output: -Total number of packets (frames) forwarded out the interface. Output Errors: -Total number of packets (frames) that the switch port tried to transmit, but for which some problem occurred. Collisions: -Counter of all collisions that occur when the interface is transmitting a frame\n- -The subset of all collisions that happen after the 64th byteOften point to a duplex mismatch-(In a properly working Ethernet LAN, collisions should occur within the first 64 bytes Late Collisions: Collisions occur as a normal part of the halfa switch interface with an increasing collisions counter might not even have a problem. -duplex logic imposed by CSMA/CD\n- if the CRC errors grow, but the collisions counters do not, the problem might simply be interference on the cable. ","externalUrl":null,"permalink":"/tech/networking/switchinterfaces/","section":"Teches","summary":"\u003cp\u003e1.0 Network Fundamentals1.1 Explain the role and function of network components\n1.1.b L2 and L3 switches1.4 Describe switching concepts\nConfiguring Speed, Duplex, and Description\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\n- autonegotiate○○What speed to useenabled by default  \n    **duplex {auto | full | half} and speed {auto | 10 | 100 | 1000}** ○configure the speed and duplex settings  \n    **(config** - add a text description to the interface **- int) # description** _text_  \n    **show interfaces status** - lists port #, Name, status, vlan, duplex, speed, and type  \n    a-full and aa-means that the listed speed and duplex values were autonegotiated.- 100\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIEEE autonegotiation (IEEE standard 802.3u)\u003c/p\u003e","title":"Switch Interfaces","type":"tech"},{"content":"1.0 Network Fundamentals1.1 Explain the role and function of network components\n1.1.b L2 and L3 Switches1.13 Describe switching concepts\n1.13.a MAC learning and aging1.13.b Frame switching\n1.13.c Frame flooding1.13.d MAC address table\n2.0 Network Access2.5 Describe the need for and basic operations of Rapid PVST+ Spanning Tree Protocol and identify basic operations\nOverview of Switching Logic\n-Forward or filter frame based on destination MAC AddressExamine source MAC address of each frame received\nLearning MAC Addresses If a frame enters the switch and the source MAC address is not in the MAC address table, the switch creates an entry in the table when there is no matching entry in the table, switches forward the frame out all interfaces (except the incoming interface) (Flooding)\nSTP either a blocking state or a forwarding state. Blocking ○interface cannot forward or receive data frames forwarding○interface can send and receive data frames.\nLAN Switching Summary\nForward Frames based on destination MAC address If the destination MAC address is a broadcast, multicast, or unknown destination unicast (a unicast not listed in the MAC table), the switch floods the frame. If the destination MAC address is a known unicast address (a unicast address found in the MAC table): the switch forwards the frame out the outgoing interface.If the outgoing interface is the same as the interface in which the frame was received, the switch filters the frame\nlearning MAC address table entries: examine the source MAC address on each frame received and note the interface from which the frame was received. If it is not already in the table, add the MAC address and interface it was learned on.\nVerifying and Analyzing Ethernet Switching -interfaces are enabled by default10/100 and 10/100/1000 interfaces use autonegotiation by default.\nDemonstrating MAC Learning\n#show mac address - View mac address table - table\n#show mac address - Show only dynamically learned MAC addresses - table dynamic\n#show interfaces status - status of all interfaces (connected or disconnected)\n#show interfaces f0/1 status - interface status of f0/\n#show interfaces f0/1 - Displays detailed set of messages about the interface\n#show interfaces f0/1 counters - Lists the number of unicast, multicast, and broadcast frames (inbound and outbound), and total byte count for those frames\n#show mac address - Shows MAC entry for a single MAC address - table dynamic address 0200.1111.\n#show mac address - Shows MAC entries for a single interface - table dynamic interface fastEthernet 0/ Quick Commands MAC Addresses #show mac address#show mac address--tabletable dynamic #show interfaces status#show interfaces f0/1 status #show interfaces f0/1show interfaces f0/1 counters #show mac address#show mac address--table dynamic address 0200.1111.1111table dynamic interface fastEthernet 0/ #show mac address-table dynamic vlan 1 Managing MAC tables #show mac address#show mac address--table agingtable count-time #(en)clear mac address#(en)clear mac address--table dynamictable dynamic vlan 1 #(en)clear mac address#(en)clear mac address--table dynamic interface fa0/1table dynamic address 3d44.2aab.12c 5. SwitchingFriday, June 18, 2021 7:07 AM # #show mac address - Shows mac entries for vlan 1 - table dynamic vlan 1\nManaging theMAC Address Table (Aging, Clearing)\n-agetable filling using a command The switch will remove (time out) the entries due to: aging out MAC table entries,-default of 300 seconds if an entry already exists-inactivity timer goes back to 0 for that entry globallyper-VLAN using the mac address-table aging-time time-in-seconds [vlan vlan-number ] global configuration command. aging time -can be configured to a different time #show mac address - Shows age time settings for mac entries - table aging-time #show mac address - Shows how many mac addresses in the table and how much address space is available - table count if the table fills.-Oldest entries are removed content--a physical memory that has great table lookup capabilities.Used for MAC Address tables-addressable memory (CAM), #(en)clear mac address - remove dynamic entries from the mac address table - table dynamic #(en)clear mac address-table dynamic vlan 1 #(en)clear mac address-table dynamic interface fa0/1 #(en)clear mac address-table dynamic address 3d44.2aab.12c3 ","externalUrl":null,"permalink":"/tech/networking/switching/","section":"Teches","summary":"\u003cp\u003e1.0 Network Fundamentals1.1 Explain the role and function of network components\u003cbr\u003e\n1.1.b L2 and L3 Switches1.13 Describe switching concepts\u003cbr\u003e\n1.13.a MAC learning and aging1.13.b Frame switching\u003cbr\u003e\n1.13.c Frame flooding1.13.d MAC address table\u003cbr\u003e\n2.0 Network Access2.5 Describe the need for and basic operations of Rapid PVST+ Spanning Tree Protocol and identify basic operations\u003cbr\u003e\nOverview of Switching Logic\u003c/p\u003e","title":"Switching","type":"tech"},{"content":"Tuesday, September 28, 2021 2:44 PM\nswitchport port - predefine any allowed source MAC addresses for this interface. - security mac-address mac-address (Optional) tell the switch to “sticky learn” dynamically learned MAC addresses. (Optional)- switchport port-security mac-address sticky switchport port - enables port security, with all defaults - security defines a specific source MAC address. With the default maximum source address setting of 1 switchport port - - security mac-address 0200.1111.1111 default violation action disable the interface. Port security does not save the configuration of the sticky addresses - use the copy running-config startup-config command if desired. make sure to configure the maximum MAC address to at least two (one for the phone, or for a PC connected to the phone) voice ports- the port security configuration should be placed on the portthan the individual physical interfaces in the channel. -channel interface, rather EtherChannels voice ports and EtherChannels Verifying Port Security\nprovides the most insight to how port security operates lists the configuration settings for port security on an interface Port Security: EnabledPort Status: Secure-shutdown Violation mode: shutdownMaximum MAC Addresses : 1 Last source Address:VLAN: 0013:197b:5004 1 includes information about any security violations Show port-security interface Port Security: EnabledPort Status: Secure-shutdown Violation Mode: ShutdownMaximum MAC Addresses: 1 Sticky MAC Addresses: 1 Last Source Address: Vlan 0013:197b:5004 1 Show port-security interface fastEthernet 0/2 secure- the interface has been disabled because of port security-shutdown state Port Security MAC Addresses\nNo longer listed as dynamic entries- Do not show up in the results from show mac address-table dynamic EXEC command. - # you need to use one of these options to see the MAC table entries associated with ports using port security: - # Port security ports show mac address - Lists MAC addresses associated with ports that use port security - table secure : Lists MAC addresses associated with ports that use port security, as well as any other statically defined MAC addresses show mac address - - table static: show mac address-table secure interface F0/2 (All three options cause the switch to discard the offending frame) Discard frame Protect Discard frameSend log and snmp messages increment violation counter Restrict Discard frame Send log and snmp messagesincrement violation counter Puts interface into err-disabled state, discarding all traffic Shutdown Port Security Violation Modes\nShutdown Mode\ninterface must be shut down with the shutdown command and then enabled with the no shutdown command. recover from an err-disabled state automatically recover from the err-disabled state: automatic recovery for interfaces in an err-disabled state caused by port security errdisable recovery cause psecure-violation errdisable recovery interval - set the time to wait before recovering the interface seconds lists the current port-security status (secure-shutdown) as well as the configured mode (shutdown) - # last line of output lists the number of violations that caused the interface to fail to an err- disabled state - # show port-security interface disabled statesecond-to-last line identifies the MAC address and VLAN of the device that caused the violation. - # notes the number of times the interface has been moved to the errshutdown) state. -disabled (secure- while errafter shutdown/no shutdown, another violation that causes the interface to fail to an err-disabled, many frames can arrive, but the counter remains at 1. -\ndisabled state will cause the counter to increment to 2. - # violations counter Protect and Restrict Modes\nstill discard offending traffic, but the interface remains in a connected (up/up) state and in a port\nsecurity state of secure-up.\n- # port continues to forward good traffic but discards offending traffic. discard the frame.does not change the port to an err-disabled state does not generate messages does not even increment the violations counter protect ","externalUrl":null,"permalink":"/tech/networking/switchport-security-config/","section":"Teches","summary":"\u003cp\u003eTuesday, September 28, 2021 2:44 PM\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eswitchport port\u003c/strong\u003e - predefine any allowed source MAC addresses for this interface. \u003cstrong\u003e- security mac-address\u003c/strong\u003e \u003cem\u003emac-address\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e(Optional)\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003etell the switch to “sticky learn” dynamically learned MAC addresses.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e(Optional)- switchport port-security mac-address sticky\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eswitchport port - enables port security, with all defaults - security\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edefines a specific source MAC address. With the default maximum source address setting of\n1\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eswitchport port - - security mac-address 0200.1111.1111\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edefault violation action disable the interface.\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003ePort security does not save the configuration of the sticky addresses - use the copy running-config startup-config command if desired.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emake sure to configure the maximum MAC address to at least two (one for the phone,\nor for a PC connected to the phone)\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003evoice ports-\n\u003cul\u003e\n\u003cli\u003ethe port security configuration should be placed on the portthan the individual physical interfaces in the channel. -channel interface, rather\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEtherChannels\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003evoice ports and EtherChannels\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eVerifying Port Security\u003c/p\u003e","title":"Switchport Security Configuration","type":"tech"},{"content":" System Initialization and Service Management # systemd (system daemon)\nSystem initialization and service management mechanism.\nUnits and targets for initialization, service administration, and state changes\nHas fast-tracked system initialization and state transitioning by introducing:\nParallel processing of startup scripts Improved handling of service dependencies On-demand activation of services Supports snapshotting of system states.\nUsed to handle operational states of services\nBoots the system into one of several predefined targets\nTracks processes using control groups\nAutomatically maintains mount points.\nFirst process with PID 1 that spawns at boot\nLast process that terminates at shutdown.\nSpawns several processes during a service startup.\nPlaces the processes in a private hierarchy composed of control groups (or cgroups for short) to organize processes for the purposes of monitoring and controlling system resources such as:\nprocessor memory network bandwidth disk I/O Limit, isolate, and prioritize process usage of resources.\nResources distributed among users, databases, and applications based on need and priority\nInitiates distinct services concurrently, taking advantage of multiple CPU cores and other compute resources.\nCreates sockets for all enabled services that support socket-based activation at the very beginning of the initialization process.\nIt passes them on to service daemon processes as they attempt to start in parallel.\nThis lets systemd handle inter-service order dependencies\nAllows services to start without any delays.\nSystemd creates sockets first, starts daemons next, and caches any client requests to daemons that have not yet started in the socket buffer.\nFiles the pending client requests when the daemons they were awaiting come online.\nSocket\nCommunication method that allows a single process to talk to another process on the same or remote system. During the operational state, systemd:\nmaintains the sockets and uses them to reconnect other daemons and services that were interacting with an old instance of a daemon before that daemon was terminated or restarted. services that use activation based on D-Bus (Desktop Bus) are started when a client application attempts to communicate with them for the first time. Additional methods used by systemd for activation are device-based starting the service when a specific hardware type such as USB is plugged in path-based starting the service when a particular file or directory alters its state. D-Bus\nAllows multiple services running in parallel on a system or remote systems to talk to one another on-demand activation\nsystemd defers the startup of services\u0026mdash;Bluetooth and printing\u0026mdash;until they are actually needed. parallelization and on-demand activation\nsave time and compute resources. contribute to expediting the boot process considerably. benefit of parallelism witnessed at system boot is\nthe file systems are checked that may result in unnecessary delays. With autofs, the file systems are temporarily mounted on their normal mount points as soon as the checks on the file systems are finished, systemd remounts them using their standard devices. Parallelism in file system mounts does not affect the root and virtual file systems. Units # Units\nsystemd objects used for organizing boot and maintenance tasks, such as:\nhardware initialization socket creation file system mounts service startups. Unit configuration is stored in their respective configuration files\nConfig files are:\nAuto-generated from other configurations Created dynamically from the system state Produced at runtime User-developed. Units operational states:\nactive inactive in the process of being activated deactivated failed. Units can be enabled or disabled\nenabled unit can be started to an active state disabled unit cannot be started. Units have a name and a type, and they are\nencoded in files with names in the form unitname.type. Some examples: tmp.mount sshd.service syslog.socket umount.target. There are two types of unit configuration files:\nSystem unit files distributed with installed packages and located in the /usr/lib/systemd/system/ User unit files user-defined and stored in the /etc/systemd/user/ View unit config file directories: ls -l /usr/lib/systemd/system ls -l /etc/systemd/user\npkg-config command:\nView systemd unit config directory information: pkg-config systemd --variable=systemdsystemunitdir pkg-config systemd --variable=systemduserconfdir\nadditional system units that are created at runtime and destroyed when they are no longer needed.\nlocated in /run/systemd/system/ runtime unit files take precedence over the system unit files\nuser unit files take priority over the runtime files.\nUnit configuration files\ndirect replacement of the initialization scripts found in /etc/rc.d/init.d/ in older RHEL releases. 11 unit types\nUnit Type Description Automount automount capabilities for on-demand mounting of file systems Device Exposes kernel devices in systemd and may be used to implement device-based activation Mount Controls when and how to mount or unmount file systems Path Activates a service when monitored files or directories are accessed Scope Manages foreign processes instead of starting them Service Starts, stops, restarts, or reloads service daemons and the processes they are made up of Slice May be used to group units, which manage system processes in a tree-like structure for resource management Socket Encapsulates local inter-process communication or network sockets for use by matching service units Swap Encapsulates swap partitions Target Defines logical grouping of units Timer Useful for triggering activation of other units based on timers Unit files contain common and specific configuration elements. Common elements\nfall under the [Unit] and [Install] sections description documentation location dependency information conflict information other options independent of the type of unit unit-specific configuration data located under the unit type section: [Service] for the service unit type [Socket] for the socket unit type so forth Sample unit file for sshd.service from the /usr/lib/systemd/system/:\ndavid@fedora:~$ cat /usr/lib/systemd/system/sshd.service [Unit] Description=OpenSSH server daemon Documentation=man:sshd(8) man:sshd_config(5) After=network.target sshd-keygen.target Wants=sshd-keygen.target # Migration for Fedora 38 change to remove group ownership for standard host keys # See https://fedoraproject.org/wiki/Changes/SSHKeySignSuidBit Wants=ssh-host-keys-migration.service [Service] Type=notify EnvironmentFile=-/etc/sysconfig/sshd ExecStart=/usr/sbin/sshd -D $OPTIONS ExecReload=/bin/kill -HUP $MAINPID KillMode=process Restart=on-failure RestartSec=42s [Install] WantedBy=multi-user.target Units can have dependencies based on a sequence (ordering) or a requirement. sequence outlines one or more actions that need to be taken before or after the activation of a unit (the Before and After directives). requirement specifies what must already be running (the Requires directive) or not running (the Conflicts directive) in order for the successful launch of a unit. Example:\nThe graphical.target unit file tells us that the system must already be operating in the multi-user mode and not in rescue mode in order for it to boot successfully into the graphical mode. Wants\nMay be used instead of Requires in the \\[Unit\\] or \\[Install\\] section so that the unit is not forced to fail activation if a required unit fails to start. Run man systemd.unit for details on systemd unit files.\nThere are also other types of dependencies systemd generally sets and maintains inter-service dependencies automatically This can be done manually if needed. Targets\nlogical collections of units special systemd unit type with the .target file extension. share the directory locations with other unit configuration files. used to execute a series of units. true for booting the system to a desired operational run level with all the required services up and running. Some targets inherit services from other targets and add their own to them. systemd includes several predefined targets Target Description halt Shuts down and halts the system poweroff Shuts down and powers off the system shutdown Shuts down the system rescue Single-user target for running administrative and recovery functions. All local file systems are mounted. Some essential services are started, but networking remains disabled. emergency Runs an emergency shell. The root file system is mounted in read-only mode; other file systems are not mounted. Networking and other services remain disabled. multi-user Multi-user target with full network support, but without GUI graphical Multi-user target with full network support and GUI reboot Shuts down and reboots the system default A special soft link that points to the default system boot target (multi-user.target or graphical.target) hibernate Puts the system into hibernation by saving the running state of the system on the hard disk and powering it off. When powered up, the system restores from its saved state rather than booting up. Systemd Targets # Target unit files\ncontain all information under the \\[Unit\\] section description documentation location dependency and conflict information. Show the graphical target file (/usr/lib/systemd/system/graphical.target):\nroot@localhost ~]# cat /usr/lib/systemd/system/graphical.target [Unit] Description=Graphical Interface Documentation=man:systemd.special(7) Requires=multi-user.target Wants=display-manager.service Conflicts=rescue.service rescue.target After=multi-user.target rescue.service rescue.target display-manager.service AllowIsolate=yes Requires, Wants, Conflicts, and After suggests that the system must have already accomplished the rescue.service, rescue.target, multi-user.target, and display-manager.service levels in order to be declared running in the graphical target.\nRun man systemd.targetfor details\nsystemctl Command # Performs administrative functions and supports plentiful subcommands and flags. Subcommand Description daemon-reload Re-reads and reloads all unit configuration files and recreates the entire user dependency tree. enable (disable) Activates (deactivates) a unit for autostart at system boot get-default (set-default) Shows (sets) the default boot target get-property (set-property) Returns (sets) the value of a property is-active Checks whether a unit is running is-enabled Displays whether a unit is set to autostart at system boot is-failed Checks whether a unit is in the failed state isolate Changes the running state of a system kill Terminates all processes for a unit list-dependencies Lists dependency tree for a unit list-sockets Lists units of type socket list-unit-files Lists installed unit files list-units Lists known units. This is the default behavior when systemctl is executed without any arguments. mask (unmask) Prohibits (permits) auto and manual activation of a unit to avoid potential conflict reload Forces a running unit to re-read its configuration file. This action does not change the PID of the running unit. restart Stops a running unit and restarts it show Shows unit properties start (stop) Starts (stops) a unit status Presents the unit status information Listing and Viewing Units # List all units that are currently loaded in memory along with their status and description: systemctl\nOutput: UNIT column\nshows the name of the unit and its location in the tree LOAD column\nreflects whether the unit configuration file was properly loaded (loaded, not found, bad setting, error, and masked) ACTIVE column\nreturns the high-level activation state ( active, reloading, inactive, failed, activating, and deactivating) SUB column\ndepicts the low-level unit activation state (reports unit-specific information) DESCRIPTION column\nillustrates the unit\u0026rsquo;s content and functionality.\nsystemctl only lists active units by default\n--all\ninclude the inactive units: List all active and inactive units of type socket:\nsystemctl -t socket --all List all units of type socket currently loaded in memory and the service they activate, sorted by the listening address:\nsystemctl list-sockets List all unit files (column 1) installed on the system and their current state (column 2):\nsystemctl list-unit-files List all units that failed to start at the last system boot:\nsystemctl --failed List the hierarchy of all dependencies (required and wanted units) for the current default target:\nsystemctl list-dependencies List the hierarchy of all dependencies (required and wanted units) for a specific unit such as atd.service:\nsystemctl list-dependencies atd.service Managing Service Units # systemctl subcommands to manage service units, including\nstarting stopping restarting checking status Check the current operational status and other details for the atd service:\nsystemctl status atd Output: service description\nread from /usr/lib/systemd/system/atd.service load status, which reveals the current load status of the unit configuration file in memory. Other possibilities for \u0026ldquo;Loaded\u0026rdquo; include \u0026ldquo;error\u0026rdquo; (if there was a problem loading the file) \u0026quot;not-found\u0026quot; (if no file associated with this unit was found) \u0026quot;bad-setting\u0026quot; (if a key setting was missing) \u0026quot;masked\u0026quot; (if the unit configuration file is masked) (enable or disable) for autostart at system boot. Active current activation status time the service was started Possible states: Active (running): The service is running with one or more processes Active (exited): Completed a one-time configuration Active (waiting): Running but waiting for an event Inactive: Not running Activating: In the process of being activated Deactivating: In the process of being deactivated Failed: If the service crashed or could not be started Also includes Main PID of the service process and more. Disable the atd service from autostarting at the next system reboot:\nsudo systemctl disable atd Re-enable atd to autostart at the next system reboot:\nsystemctl enable atd Check whether atd is set to autostart at the next system reboot:\nsystemctl is-enabled atd Check whether the atd service is running:\nsystemctl is-active atd Stop and restart atd, run either of the following:\nsystemctl stop atd ; systemctl start atd systemctl restart atd Show the details of the atd service:\nsystemctl show atd Prohibit atd from being enabled or disabled:\nsystemctl mask atd Try disabling or enabling atd and observe the effect of the previous command:\nsystemctl disable atd Reverse the effect of the mask subcommand and try disable and enable operations:\nsystemctl unmask atd \u0026amp;\u0026amp; systemctl disable atd \u0026amp;\u0026amp; systemctl enable atd Managing Target Units # systemctl can also manage target units.\nview or change the default boot target switch from one running target into another View what units of type target are currently loaded and active:\nsystemctl -t target output:\ntarget unit\u0026rsquo;s name load state high-level and low-level activation states short description. \u0026ndash;all option to the above\nsee all loaded targets in either active or inactive state. Viewing and Setting Default Boot Target\nview the current default boot target and to set it. get-default and set-default subcommands Check the current default boot target:\nYou may have to modify the default boot target persistently for the exam. Change the current default boot target from graphical.target to multi-user.target:\nsystemctl set-default multi-user removes the existing symlink (default.target) pointing to the old boot target and replaces it with the new target file path. revert the default boot target to graphical:\nsystemctl set-default graphical Switching into Specific Targets # Use systemctl to transition the running system from one target state into another. graphical, multi-user, reboot, shutdown\u0026mdash;are the most common rescue and emergency targets are for troubleshooting and system recovery purposes, poweroff and halt are similar to shutdown hibernate is suitable for mobile devices. Switch into multi-user using the isolate subcommand:\nsystemctl isolate multi-user This will stop the graphical service on the system and display the text-based console login screen. Type in a username such as user1 and enter the password to log in:\nLog in and return to the graphical target:\nsystemctl isolate graphical Shut down the system and power it off, use the following or simply run the poweroff command:\nsystemctl poweroff poweroff Shut down and reboot the system:\nsystemctl reboot reboot halt, poweroff, and reboot are symbolic links to the systemctl command:\n[root@localhost ~]# ls -l /usr/sbin/halt /usr/sbin/poweroff /usr/sbin/reboot lrwxrwxrwx. 1 root root 16 Aug 22 2023 /usr/sbin/halt -\u0026gt; ../bin/systemctl lrwxrwxrwx. 1 root root 16 Aug 22 2023 /usr/sbin/poweroff -\u0026gt; ../bin/systemctl lrwxrwxrwx. 1 root root 16 Aug 22 2023 /usr/sbin/reboot -\u0026gt; ../bin/systemctl shutdown command options: -H now\nHalt -P now poweroff -r now reboot broadcasts a warning message to all logged-in users blocks new user login attempts waits for the specified amount of time for users to log off stops the services shut the system down to the specified target state. System Logging # Log files need to be rotated periodically to prevent the file system space from filling up. Configuration files that define the default and custom locations to direct the log messages to and to configure rotation settings. system log file records custom messages sent to it. systemd includes a service for viewing and managing system logs in addition to the traditional logging service. This service maintains a log of runtime activities for faster retrieval and can be configured to store the information permanently. System logging (syslog for short)\ncapture messages generated by: kernel daemons commands user activities applications other events Forwards messages to various log files For security auditing, service malfunctioning, system troubleshooting, or informational purposes. rsyslogd daemon (rocket-fast system for log processing)\nResponsible for system logging Multi-threaded support for: enhanced filtering encryption-protected message relaying variety of configuration options. Reads its configuration file /etc/rsyslog.conf and the configuration files located in /etc/rsyslog.d/ at startup. /var/log Default depository for most system log files Other services such as audit, Apache, etc. have subdirectories here as well. rsyslog service\nmodular allows the modules listed in its configuration file to be dynamically loaded in the kernel when/as needed. Each module brings a new functionality to the system upon loading. rsyslogd daemon\ncan be stopped manually using systemctl stop rsyslog\nstart, restart, reload, and status options are also available\nA PID is assigned to the daemon at startup\nrsyslogd.pid file is created in the /run directory to save the PID.\nPID is stored to prevent multiple instances of this daemon.\nTheSyslog Configuration File # /etc/rsyslog.conf\nprimary syslog configuration file View /etc/rsyslog.conf: cat /etc/rsyslog.conf\nOutput: Three sections:\nModules, Global Directives, and Rules.\nModules section default defines two modules imuxsock and imjournal loaded on demand. imuxsock module furnishes support for local system logging via the logger command imjournal module\nallows access to the systemd journal.\nGlobal Directives section\ncontains three active directives. Definitions in this section influence the overall functionality of the rsyslog service. first directive Sets the location for the storage of auxiliary files (/var/lib/rsyslog). second directive instructs the rsyslog service to save captured messages using traditional file formatting third directive directs the service to load additional configuration from files located in the /etc/rsyslogd.d/ directory. Rules section\nRight field is referred to as action. selector field left field of the rules section divided into two period-separated sub-fields called facility (left) representing one or more system process categories that generate messages priority (right) identifying the severity associated with the messages. semicolon (;) is used as a distinction mark if multiple facility.priority groups are present. action field determines the destination to send the messages to. numerous supported facilities: auth authpriv cron daemon kern lpr mail news syslog user uucp local0 throughv local7 asterisk (*) character represents all of them. supported priorities in the descending criticality order: emerg alert crit error warning notice info debug none If a lower priority is selected, the daemon logs all messages of the service at that and higher levels.\nAfter modifying the syslog configuration file, Inspect it and set the verbosity: rsyslogd -N 1 (-N inspect, 1 level 1)\nRestart or reload the rsyslog service in order for the changes to take effect. Rotating Log Files # Log location is defined in the rsyslog configuration file.\nView the /var/log/ directory: ls -l /var/log\nsystemd unit file called logrotate.timer under the /usr/lib/systemd/system directory invokes the logrotate service (/usr/lib/systemd/system/logrotate.service) on a daily basis. Here is what this file contains:\n[root@localhost cron.daily]# systemctl cat logrotate.timer # /usr/lib/systemd/system/logrotate.timer [Unit] Description=Daily rotation of log files Documentation=man:logrotate(8) man:logrotate.conf(5) [Timer] OnCalendar=daily AccuracySec=1h Persistent=true [Install] WantedBy=timers.target The logrotate service runs rotations as per the schedule and other parameters defined in the /etc/logrotate.conf and additional log configuration files located in the /etc/logrotate.d directory.\n/etc/cron.daily/logrotate script # invokes the logrotate command on a daily basis. runs a rotation as per the schedule defined in /etc/logrotate.conf and the configuration files for various services are located in /etc/logrotate.d/ The configuration files may be modified to alter the schedule or include additional tasks on log files such as: removing compressing emailing grep -v ^$ /etc/logrotate.conf # see \u0026#34;man logrotate\u0026#34; for details # global options do not affect preceding include directives # rotate log files weekly weekly # keep 4 weeks worth of backlogs rotate 4 # create new (empty) log files after rotating old ones create # use date as a suffix of the rotated file dateext # uncomment this if you want your log files compressed #compress # packages drop log rotation information into this directory include /etc/logrotate.d # system-specific logs may be also be configured here. content:\ndefault log rotation frequency (weekly).\nperiod of time (4 weeks) to retain the rotated logs before deleting them.\nEach time a log file is rotated:\nEmpty replacement file is created with the date as a suffix to its name rsyslog service is restarted script presents the option of compressing the rotated files using the gzip utility.\nlogrotate command checks for the presence of additional log configuration files in /etc/logrotate.d/ and includes them as necessary. directives defined in /etc/logrotate.conf file have a global effect on all log files can define custom settings for a specific log file in /etc/logrotate.conf/ or create a separate file in /etc/logrotate.d/ settings defined in user-defined files overrides the global settings. The /etc/logrotate.d/ directory includes additional configuration files for other service logs:\nls -l /etc/logrotate.d/ Show the file content for btmp (records of failed user login attempts) that is used to control the rotation behavior for /var/log/btmp:\ncat /etc/logrotate.d/btmp ``` - rotation is once a month. - replacement file created will get read/write permission bits for the owner (*root*) - owning group will be set to *utmp* - rsyslog service will maintain one rotated copy of the *btmp* log file. ### The Boot Log File Logs generated during the system startup: - Display the service startup sequence. - Status showing whether the service was started successfully. - May help in any post-boot troubleshooting if required. - /var/log/boot.log View /var/log/boot.log: sudo head /var/log/boot.log\noutput: - OK or FAILED - indicates if the service was started successfully or not. ### The System Log File /var/log/messages - default location for storing most system activities, as defined in the *rsyslog.conf* file - saves log information in plain text format - may be viewed with any file display utility (*cat*, *more*, *pg*, *less*, *head*, or *tail*.) - may be observed in real time using the *tail* command with the -f switch. The *messages* file - captures: - the date and time of the activity, - hostname of the system, - name and PID of the service - short description of the event being logged. View /var/log messages: ```bash tail /var/log/messages Logging Custom Messages # The Modules section in the rsyslog.conf file\nProvides the support via the imuxsock module to record custom messages to the messages file using the logger command. logger command # Add a note indicating the calling user has rebooted the system:\nlogger -i \u0026#34;System rebooted by $USER\u0026#34; observe the message recorded along with the timestamp, hostname, and PID:\ntail -l /var/log/messages -p option\nspecify a priority level either as a numerical value or in the facility.priority format. default priority user.notice. View logger man pages: man logger\nThe systemd Journal # Systemd-based logging service for the collection and storage of logging data.\nImplemented via the systemd-journald daemon.\nGather, store, and display logging events from a variety of sources such as:\nthe kernel rsyslog and other services initial RAM disk alerts generated during the early boot stage. journals stored in the binary format files\nlocated in /run/log/journal/ (remember run is not a persistent directory)\nstructured and indexed for faster and easier searches\nMay be viewed and managed using the journalctl command.\nCan enable persistent storage for the logs if desired.\nRHEL runs both rsyslogd and systemd-journald concurrently.\ndata gathered by systemd-journald may be forwarded to rsyslogd for further processing and persistent storage in text format.\n/etc/systemd/journald.conf\nmain config file for journald contains numerous default settings that affect the overall functionality of the service. Retrieving and Viewing Messages # journalctl command # retrieve messages from the journal for viewing in a variety of ways using different options. run journalctl without any options to see all the messages generated since the last system reboot: journalctl\nformat of the messages is similar to that of the events logged to /var/log/messages Each line begins with a timestamp followed by the system hostname, process name with or without a PID, and the actual message. Display verbose output for each entry:\njournalctl -o verbose View all events since the last system reboot:\njournalctl -b -0 (default, since the last system reboot), -1 (the previous system reboot), -2 (two reboots before) 1 \u0026amp; 2 only work if there are logs persistently stored.\nView only kernel-generated alerts since the last system reboot:\njournalctl -kb0 Limit the output to view 3 entries only:\njournalctl -n3 To show all alerts generated by a particular service, such as crond:\njournalctl /usr/sbin/crond Retrieve all messages logged for a certain process, such as the PID associated with the chronyd service:\njournalctl _PID=$(pgrep chronyd) Reveal all messages for a particular system unit, such as sshd.service:\njournalctl _SYSTEMD_UNIT=sshd.service View all error messages logged between a date range, such as October 10, 2019 and October 16, 2019:\njournalctl --since 2019-10-16 --until 2019-10-16 -p err Get all warning messages that have appeared today and display them in reverse chronological order:\njournalctl --since today -p warning -r Can specify the time range in hh:mm:ss format, or yesterday, today, or tomorrow as well. follow option\njournalctl -f man journalctl man systemd-journald Preserving Journal Information # enable a separate storage location for the journal to save all its messages there persistently. default is under /var/log/journal/ The systemd-journald service supports four options with the Storage directive to control how the logging data is handled.\nOption Description volatile Stores data in memory only persistent Stores data permanently under /var/log/journal and falls back to memory-only option if this directory does not exist or has a permission or other issue. The service creates /var/log/journal in case of its non-existence. auto Similar to \u0026ldquo;persistent\u0026rdquo; but does not create /var/log/journal if it does not exist. This is the default option. none Disables both volatile and persistent storage options. Not recommended. Journal Data Storage Options # create the /var/log/journal/ manually and use preferred \u0026ldquo;auto\u0026rdquo; option.\nfaster query responses from in-memory storage access to historical log data from on-disk storage. Lab: Configure Persistent Storage for Journal Information # Run the necessary steps to enable and confirm persistent storage for the journals.\nCreate a subdirectory called journal under the /var/log/ directory and confirm: sudo mkdir /var/log/journal Restart the systemd-journald service and confirm: systemctl restart systemd-journald \u0026amp;\u0026amp; systemctl status systemd- journald List the new directory and observe a subdirectory matching the machine ID of the system as defined in the /etc/machine-id file is created: ll /var/log/journal \u0026amp;\u0026amp; cat /etc/machine-id This log file is rotated automatically once a month based on the settings in the journald.conf file. Check the manual pages of journal.conf\nman journald.conf System Tuning # System tuning service\nMonitor connected devices Tweak their parameters to improve performance or conserve power. Recommended tuning profile may be identified and activated for optimal performance and power saving. tuned system tuning service monitor storage, networking, processor, audio, video, and a variety of other connected devices Adjusts their parameters for better performance or power saving based on a chosen profile. Several predefined tuning profiles may be activated either statically or dynamically. tuned service\nstatic behavior (default)\nactivates a selected profile at service startup and continues to use it until it is switched to a different profile. dynamic\nadjusts the system settings based on the live activity data received from monitored system components tuned tuning Profiles # Nine profiles to support a variety of use cases. Can create custom profiles from nothing or by using one of the existing profiles as a template. Must to store the custom profile in /etc/tuned/ Three groups: (1) Performance (2) Power consumption (3) Balanced\nProfile Description Performance Desktop Based on the balanced profile for desktop systems. Offers improved throughput for interactive applications. Latency-performance For low-latency requirements Network-latency Based on the latency-performance for faster network throughput Network-throughput Based on the throughput-performance profile for maximum network throughput Virtual-guest Optimized for virtual machines Virtual-host Optimized for virtualized hosts Power Saving Powersave Saves maximum power at the cost of performance Balanced/Max Profiles Balanced Preferred choice for systems that require a balance between performance and power saving Throughput-performance Provides maximum performance and consumes maximum power Tuning Profiles # Predefined profiles are located in /usr/lib/tuned/ in subdirectories matching their names.\nView predefined profiles:\nls -l /usr/lib/tuned The default active profile set on server1 and server2 is the virtual-guest profile, as the two systems are hosted in a VirtualBox virtualized environment.\nThe tuned-adm Command # single profile management command that comes with tuned can list active and available profiles, query current settings, switch between profiles, and turn the tuning off. Can recommend the best profile for the system based on many system attributes. View the man pages:\nman tuned-adm Lab 12-2: Manage Tuning Profiles # install the tuned service start it now enable it for auto-restart upon future system reboots. display all available profiles and the current active profile. switch to one of the available profiles and confirm. determine the recommended profile for the system and switch to it. deactivate tuning and reactivate it. confirm the activation Install the tuned package if it is not already installed: dnf install tuned Start the tuned service and set it to autostart at reboots: systemctl --now enable tuned Confirm the startup: systemctl status tuned Display the list of available and active tuning profiles: tuned-adm list List only the current active profile: tuned-adm active Switch to the powersave profile and confirm: tuned-adm profile powersave tuned-adm active Determine the recommended profile for server1 and switch to it: [root@localhost ~]# tuned-adm recommend virtual-guest [root@localhost ~]# tuned-adm profile virtual-guest [root@localhost ~]# tuned-adm active Current active profile: virtual-guest Turn off tuning: [root@localhost ~]# tuned-adm off [root@localhost ~]# tuned-adm active No current active profile. Reactivate tuning and confirm: [root@localhost ~]# tuned-adm profile virtual-guest [root@localhost ~]# tuned-adm active Current active profile: virtual-guest Sysinit, Logging, and Tuning Labs # Lab: Modify Default Boot Target # Modify the default boot target from graphical to multi-user, and reboot the system to test it. systemctl set-default multi-user Run the systemctl and who commands after the reboot for validation. Restore the default boot target back to graphical and reboot to verify. Lab: Record Custom Alerts # Write the message \u0026ldquo;This is $LOGNAME adding this marker on $(date)\u0026rdquo; to /var/log/messages. logger -i \u0026#34;This is $LOGNAME adding this marker on $(date)\u0026#34; Ensure that variable and command expansions work. Verify the entry in the file. tail -l /var/log/messages Lab: Apply Tuning Profile # identify the current system tuning profile with the tuned-adm command. tuned-adm active List all available profiles. tuned-adm list List the recommended profile for server1. tuned-adm recommend Apply the \u0026ldquo;balanced\u0026rdquo; profile and verify with tuned-adm. tuned-adm profile balanced tuned-adm active ","externalUrl":null,"permalink":"/tech/linux/system-initialization-message-logging-and-system-tuning/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eSystem Initialization and Service Management\n    \u003cdiv id=\"system-initialization-and-service-management\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#system-initialization-and-service-management\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003esystemd\u003c/strong\u003e (system daemon)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eSystem initialization and service management mechanism.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUnits and targets for initialization, service administration, and state changes\u003c/p\u003e","title":"System Initialization, Message Logging, and System Tuning","type":"tech"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"ApplicationTransport NetworkData Link Physical\nApplication Layer\nservices for applications http provides interface between software and the network\n○ HTTP Header GET home.html\n○ HTTP Header OK/ Data Same layer interaction on different computers\nCommunicate with the same layer on another computer.\nAdjacent layer interaction on the same computer One lower layer provides a service to the layer just above. The higher layer makes the next lower layer perform a function *Wireless protocols are Layer 2\nEncapsulation\nTCP/ Data (segment)IP/ Data (Packet) LH/ Data/ LT (Frame) (Link header \u0026amp; Link Trailer)\nProtocol Data Units L7H/ Data (L7PDU)L6H/ Data (L6PDU) L5H/ Data (L5PDU) L4H/ Data (L4PDU) L4H/ Data (L4PDU)L3H/ Data (L3PDU) L2H/ Data/ L2T (L2PDU) ","externalUrl":null,"permalink":"/tech/networking/tcpipbasic/","section":"Teches","summary":"\u003cp\u003eApplicationTransport\nNetworkData Link\nPhysical\u003cbr\u003e\nApplication Layer\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eservices for applications\u003c/li\u003e\n\u003cli\u003ehttp provides interface between software and the network\u003cbr\u003e\n○ HTTP Header GET home.html\u003cbr\u003e\n○ HTTP Header OK/ Data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSame layer interaction on different computers\u003c/p\u003e","title":"TCP/IP Basics","type":"tech"},{"content":" firewalld Zones # firewalld # The host-based firewall solution employed in RHEL uses a kernel module called netfilter together with a filtering and packet classification framework called nftables for policing the traffic movement. It also supports other advanced features such as Network Address Translation (NAT) and port forwarding. This firewall solution inspects, modifies, drops, or routes incoming, outgoing, and forwarded network packets based on defined rulesets. Default host-based firewall management service in RHEL Ability to add, modify, or delete firewall rules immediately without disrupting current network connections or restarting the service process. Also allows to save rules persistently so that they are activated automatically at system reboots. Lets you perform management operations at the command line using the firewall-cmd command, graphically using the web console, or manually by editing rules files. Stores the default rules in files located in the /usr/lib/firewalld directory, and those that contain custom rules in the /etc/firewalld directory. The default rules files may be copied to the custom rules directory and modified. firewalld Zones # Easier and transparent traffic management. Define policies based on the trust level of network connections and source IP addresses. A network connection can be part of only one zone at a time; A zone can have multiple network connections assigned to it. Zone configuration may include services, ports, and protocols that may be open or closed. May include rules for advanced configuration items such as masquerading, port forwarding, NAT\u0026rsquo;ing, ICMP filters, and rich language. Rules for each zone are defined and manipulated independent of other zones. Match source ip to zone that matches address \u0026gt; match based on zone the interface is in \u0026gt; matches default zone\nfirewalld inspects each incoming packet to determine the source IP address and applies the rules of the zone that has a match for the address.\nIn the event no zone configuration matches the address, it associates the packet with the zone that has the network connection defined, and applies the rules of that zone.\nIf neither works, firewalld associates the packet with the default zone, and enforces the rules of the default zone on the packet.\nSeveral predefined zone files that may be selected or customized.\nThese files include templates for traffic that must be blocked or dropped, and for traffic that is:\npublic-facing internal external home public trusted work-related. public zone is the default zone, and it is activated by default when the firewalld service is started.\nPredefined zones sorted based on the trust level from trusted to untrusted:\ntrusted\nAllow all incoming traffic internal\nReject all incoming traffic except for what is allowed. Intended for use on internal networks. home\nReject all incoming traffic except for what is allowed. Intended for use in homes. work\nReject all incoming traffic except for what is allowed. Intended for use at workplaces. dmz\nReject all incoming traffic except for what is allowed. Intended for use in publicly accessible demilitarized zones. external\nReject all incoming traffic except for what is allowed. Outgoing IPv4 traffic forwarded through this zone is masqueraded to look like it originated from the IPv4 address of an outgoing network interface. Intended for use on external networks with masquerading enabled. public\nReject all incoming traffic except for what is allowed. Default zone for any newly added network interfaces. Intended for us in public places. block\nReject all incoming traffic with icmp-host-prohibited message returned. Intended for use in secure places. drop\nDrop all incoming traffic without responding with ICMP errors.\nIntended for use in highly secure places.\nFor all the predefined zones, outgoing traffic is allowed by default.\nZone Configuration Files # firewalld stores zone rules in XML format at two locations\nsystem-defined rules in the /usr/lib/firewalld/zones directory can be used as templates for adding new rules, or applied instantly to any available network connection automatically copied to the /etc/firewalld/zones directory if it is modified with a management tool user-defined rules in the /etc/firewalld/zones directory can copy the required zone file to the /etc/firewalld/zones directory manually, and make the necessary changes.\nThe firewalld service reads the files saved in this location, and applies the rules defined in them.\nView the system Zones:\n[root@server30 ~]# ll /usr/lib/firewalld/zones total 40 -rw-r--r--. 1 root root 312 Nov 6 2023 block.xml -rw-r--r--. 1 root root 306 Nov 6 2023 dmz.xml -rw-r--r--. 1 root root 304 Nov 6 2023 drop.xml -rw-r--r--. 1 root root 317 Nov 6 2023 external.xml -rw-r--r--. 1 root root 410 Nov 6 2023 home.xml -rw-r--r--. 1 root root 425 Nov 6 2023 internal.xml -rw-r--r--. 1 root root 729 Feb 21 23:44 nm-shared.xml -rw-r--r--. 1 root root 356 Nov 6 2023 public.xml -rw-r--r--. 1 root root 175 Nov 6 2023 trusted.xml -rw-r--r--. 1 root root 352 Nov 6 2023 work.xml View the public zone:\n[root@server30 ~]# cat /usr/lib/firewalld/zones/public.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Public\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.\u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;dhcpv6-client\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;cockpit\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; See the manual pages for firewalld.zone for details on zone configuration files. firewalld Services # For easier activation and deactivation of specific rules. Preconfigured firewall rules delineated for various services and stored in different files. The rules consist of necessary settings, such as the port number, protocol, and possibly helper modules, to support the loading of the service. Can be added to a zone. By default, firewalld blocks all traffic unless a service or port is explicitly opened. Service Configuration Files # firewalld stores service rules in XML format at two locations: system-defined rules in the /usr/lib/firewalld/services directory Can be used as templates for adding new service rules, or activated instantly. A system service configuration file is automatically copied to the /etc/firewalld/services directory if it is modified with a management tool. user-defined rules in the /etc/firewalld/services directory. You can copy the required service file to the /etc/firewalld/services directory manually, and make the necessary changes. Service reads the files saved in this location, and applies the rules defined in them. A listing of the system service files is presented below:\nroot@server30 ~]# ll /usr/lib/firewalld/services total 884 -rw-r--r--. 1 root root 352 Nov 6 2023 afp.xml -rw-r--r--. 1 root root 399 Nov 6 2023 amanda-client.xml -rw-r--r--. 1 root root 427 Nov 6 2023 amanda-k5-client.xml -rw-r--r--. 1 root root 283 Nov 6 2023 amqps.xml -rw-r--r--. 1 root root 273 Nov 6 2023 amqp.xml -rw-r--r--. 1 root root 285 Nov 6 2023 apcupsd.xml -rw-r--r--. 1 root root 301 Nov 6 2023 audit.xml -rw-r--r--. 1 root root 436 Nov 6 2023 ausweisapp2.xml -rw-r--r--. 1 root root 320 Nov 6 2023 bacula-client.xml -rw-r--r--. 1 root root 346 Nov 6 2023 bacula.xml -rw-r--r--. 1 root root 390 Nov 6 2023 bareos-director.xml ... ... Shows the content of the ssh service file:\n[root@server30 ~]# cat /usr/lib/firewalld/services/ssh.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;service\u0026gt; \u0026lt;short\u0026gt;SSH\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Secure Shell (SSH) is a protocol for logging into and executing commands on remote machines. It provides secure encrypted communications. If you plan on accessing your machine remotely via SSH over a firewalled interface, enable this option. You need the openssh-server package installed for this option to be useful.\u0026lt;/description\u0026gt; \u0026lt;port protocol=\u0026#34;tcp\u0026#34; port=\u0026#34;22\u0026#34;/\u0026gt; \u0026lt;/service\u0026gt; Has a name and description Defines the port and protocol for the service. See the manual pages for firewalld.service for details on service configuration files. firewalld Management # Listing, querying, adding, changing, and removing zones, services, ports, IP sources, and network connections. Three methods: firewall-cmd Web interface for graphical administration. Edit Zone and service templates manually firewall-cmd Command # Add or remove rules from the runtime configuration, or save any modifications to service configuration for persistence. Supports numerous options for the management of zones, services, ports, connections, and so on Common options # General # --state\nDisplays the running status of firewalld --reload\nReloads firewall rules from zone files. All runtime changes are lost. --permanent\nStores a change persistently. The change only becomes active after a service reload or restart. Zones # --get-default-zone\nShows the name of the default/active zone --set-default-zone\nChanges the default zone for both runtime and permanent configuration --get-zones\nPrints a list of available zones \u0026ndash;get-active-zones\nDisplays the active zone and the assigned interfaces --list-all\nLists all settings for a zone --list-all-zones\nLists the settings for all available zones \u0026ndash;zone\nSpecifies the name of the zone to work on. Without this option, the default zone is used. Services # --get-services\nPrints predefined services --list-services\nLists services for a zone --add-service\nAdds a service to a zone --remove-service\nRemoves a service from a zone --query-service\nQueries for the presence of a service Ports # --list-ports\nLists network ports --add-port\nAdds a port or a range of ports to a zone --remove-port\nRemoves a port from a zone --query-port\nQueries for the presence of a port Network Connections # --list-interfaces\nLists network connections assigned to a zone --add-interface\nBinds a network connection to a zone --change-interface\nChanges the binding of a network connection to a different zone --remove-interface\nUnbinds a network connection from a zone IP Sources # --list-sources\nLists IP sources assigned to a zone --add-source\nAdds an IP source to a zone --change-source\nChanges an IP source --remove-source\nRemoves an IP source from a zone --add and --remove options\n--permanent switch may be specified to ensure the rule is stored in the zone configuration file under the /etc/firewalld/zones directory for persistence. Querying the Operational Status of firewalld # Check the running status of the firewalld service using either the systemctl or the firewall-cmd command.\n[root@server20 ~]# firewall-cmd --state running [root@server20 ~]# systemctl status firewalld -l --no-pager ● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; preset: enabled) Active: active (running) since Thu 2024-07-25 13:25:21 MST; 44min ago Docs: man:firewalld(1) Main PID: 829 (firewalld) Tasks: 2 (limit: 11108) Memory: 43.9M CPU: 599ms CGroup: /system.slice/firewalld.service └─829 /usr/bin/python3 -s /usr/sbin/firewalld --nofork --nopid Jul 25 13:25:21 server20 systemd[1]: Starting firewalld - dynamic firewall daemon... Jul 25 13:25:21 server20 systemd[1]: Started firewalld - dynamic firewall daemon. Lab: Add Services and Ports, and Manage Zones # Determine the current active zone. Add and activate a permanent rule to allow HTTP traffic on port 80 Add a runtime rule for traffic intended for TCP port 443 (the HTTPS service). Add a permanent rule to the internal zone for TCP port range 5901 to 5910. Confirm the changes and display the contents of the affected zone files. Switch the default zone to the internal zone and activate it. 1. Determine the name of the current default zone:\n[root@server20 ~]# firewall-cmd --get-default-zone public 2. Add a permanent rule to allow HTTP traffic on its default port:\n[root@server20 ~]# firewall-cmd --permanent --add-service http success The command made a copy of the public.xml file from /usr/lib/firewalld/zones directory into the /etc/firewalld/zones directory, and added the rule for the HTTP service.\n3. Activate the new rule:\n[root@server20 zones]# firewall-cmd --reload success 4. Confirm the activation of the new rule:\n[root@server20 zones]# firewall-cmd --list-services cockpit dhcpv6-client http nfs ssh 5. Display the content of the default zone file to confirm the addition of the permanent rule:\n[root@server20 zones]# cat /etc/firewalld/zones/public.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Public\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. \u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;dhcpv6-client\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;cockpit\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;nfs\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; 6. Add a runtime rule to allow traffic on TCP port 443 and verify:\n[root@server20 zones]# firewall-cmd --add-port 443/tcp success [root@server20 zones]# firewall-cmd --list-ports 443/tcp 7. Add a permanent rule to the internal zone for TCP port range 5901 to 5910:\n[root@server20 zones]# firewall-cmd --add-port 5901-5910/tcp --permanent --zone internal success 8. Display the content of the internal zone file to confirm the addition of the permanent rule:\n[root@server20 zones]# cat /etc/firewalld/zones/internal.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Internal\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;For use on internal networks. You mostly trust the other computers on the networks to not harm your computer. Only selected incoming connections are accepted. \u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;mdns\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;samba-client\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;dhcpv6-client\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;cockpit\u0026#34;/\u0026gt; \u0026lt;port port=\u0026#34;5901-5910\u0026#34; protocol=\u0026#34;tcp\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; The firewall-cmd command makes a backup of the affected zone file with a .old extension whenever an update is made to a zone. 9. Switch the default zone to internal and confirm:\n[root@server20 zones]# firewall-cmd --set-default-zone internal success [root@server20 zones]# firewall-cmd --get-default-zone internal 10. Activate the rules defined in the internal zone and list the port range added earlier:\n[root@server20 zones]# firewall-cmd --list-ports 5901-5910/tcp Lab: Remove Services and Ports, and Manage Zones # Remove the two permanent rules that were added in the last lab. Switch the public zone back as the default zone, and confirm the changes. 1. Remove the permanent rule for HTTP from the public zone:\n[root@server20 zones]# firewall-cmd --remove-service=http --zone public --permanent success Must specify public zone as it is not the current default. 2. Remove the permanent rule for ports 5901 to 5910 from the internal zone:\n[root@server20 zones]# firewall-cmd --remove-port 5901- 5910/tcp --permanent success 3. Switch the default zone to public and validate:\n[root@server20 zones]# firewall-cmd --set-default- zone=public success [root@server20 zones]# firewall-cmd --get-default-zone public 4. Activate the public zone rules, and list the current services:\n[root@server20 zones]# firewall-cmd --reload success [root@server20 zones]# firewall-cmd --list-services cockpit dhcpv6-client nfs ssh Lab: Test the Effect of Firewall Rule # Remove the sshd service rule from the runtime configuration on server20 Try to access the server from server10 using the ssh command. 1. Remove the rule for the sshd service on server20:\n[root@server20 zones]# firewall-cmd --remove-service ssh success 2. Issue the ssh command on server10 to access server20:\n[root@server10 ~]# ssh 192.168.0.37 ssh: connect to host 192.168.0.37 port 22: No route to host 3. Add the rule back for sshd on server20:\n[root@server20 zones]# firewall-cmd --add-service ssh success 4. Issue the ssh command on server10 to access server20. Enter \u0026ldquo;yes\u0026rdquo; if prompted and the password for user1.\n[root@server10 ~]# ssh 192.168.0.37 The authenticity of host \u0026#39;192.168.0.37 (192.168.0.37)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:Z8nFu0Jj1ASZeXByiy3aAWHpUhGhUmDCr+Omu/iWTjs. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;192.168.0.37\u0026#39; (ED25519) to the list of known hosts. root@192.168.0.37\u0026#39;s password: Web console: https://server20:9090/ or https://192.168.0.37:9090/ Register this system with Red Hat Insights: insights- client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Thu Jul 25 13:37:47 2024 from 192.168.0.21/ The Linux Firewall DIY Labs # Lab: Add Service to Firewall # Add and activate a permanent rule for HTTPs traffic to the default zone. [root@server20 ~]# firewall-cmd --add-service https --permanent success [root@server20 ~]# firewall-cmd --reload success Confirm the change by viewing the zone\u0026rsquo;s XML file and running the firewall-cmd command. [root@server20 ~]# cat /etc/firewalld/zones/public.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Public\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.\u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;dhcpv6-client\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;cockpit\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;nfs\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;https\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; [root@server20 ~]# firewall-cmd --list-services cockpit dhcpv6-client https nfs ssh Lab: Add Port Range to Firewall # Add and activate a permanent rule for the UDP port range 8000 to 8005 to the trusted zone. [root@server20 ~]# firewall-cmd --add-port 8000- 8005/udp --zone trusted --permanent success [root@server20 ~]# firewall-cmd --reload success Confirm the change by viewing the zone\u0026rsquo;s XML file and running the firewall-cmd command. [root@server20 ~]# firewall-cmd --list-ports -- zone trusted 8000-8005/udp [root@server20 ~]# cat /etc/firewalld/zones/trusted.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone target=\u0026#34;ACCEPT\u0026#34;\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;All network connections are accepted.\u0026lt;/description\u0026gt; \u0026lt;port port=\u0026#34;8000-8005\u0026#34; protocol=\u0026#34;udp\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; ","externalUrl":null,"permalink":"/tech/linux/the-linux-firewall/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003e\u003ccode\u003efirewalld\u003c/code\u003e Zones\n    \u003cdiv id=\"firewalld-zones\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#firewalld-zones\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\n\u003ch2 class=\"relative group\"\u003e\u003ccode\u003efirewalld\u003c/code\u003e\n    \u003cdiv id=\"firewalld\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#firewalld\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThe host-based firewall solution employed in RHEL uses a kernel module called \u003cstrong\u003enetfilter\u003c/strong\u003e together with a filtering and packet classification framework called \u003cstrong\u003enftables\u003c/strong\u003e for policing the traffic movement.\u003c/li\u003e\n\u003cli\u003eIt also supports other advanced features such as \u003cstrong\u003eNetwork Address Translation (NAT)\u003c/strong\u003e and \u003cstrong\u003eport forwarding\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eThis firewall solution inspects, modifies, drops, or routes incoming, outgoing, and forwarded network packets based on defined rulesets.\u003c/li\u003e\n\u003cli\u003eDefault host-based firewall management service in RHEL\u003c/li\u003e\n\u003cli\u003eAbility to add, modify, or delete firewall rules immediately without disrupting current network connections or restarting the service process.\u003c/li\u003e\n\u003cli\u003eAlso allows to save rules persistently so that they are activated automatically at system reboots.\u003c/li\u003e\n\u003cli\u003eLets you perform management operations at the command line using the \u003ccode\u003efirewall-cmd\u003c/code\u003e command, graphically using the web console, or manually by editing rules files.\u003c/li\u003e\n\u003cli\u003eStores the default rules in files located in the /usr/lib/firewalld directory, and those that contain custom rules in the /etc/firewalld directory.\u003c/li\u003e\n\u003cli\u003eThe default rules files may be copied to the custom rules directory and modified.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003efirewalld Zones\n    \u003cdiv id=\"firewalld-zones-1\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#firewalld-zones-1\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEasier and transparent traffic management.\u003c/li\u003e\n\u003cli\u003eDefine policies based on the trust level of network connections and source IP addresses.\u003c/li\u003e\n\u003cli\u003eA network connection can be part of only one zone at a time;\u003c/li\u003e\n\u003cli\u003eA zone can have multiple network connections assigned to it.\u003c/li\u003e\n\u003cli\u003eZone configuration may include services, ports, and protocols that may be open or closed.\u003c/li\u003e\n\u003cli\u003eMay include rules for advanced configuration items such as masquerading, port forwarding, NAT\u0026rsquo;ing, ICMP filters, and rich language.\u003c/li\u003e\n\u003cli\u003eRules for each zone are defined and manipulated independent of other zones.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMatch source ip to zone that matches address \u0026gt; match based on zone the interface is in \u0026gt; matches default zone\u003c/p\u003e","title":"The Linux Firewall","type":"tech"},{"content":" The OpenSSH Service # Secure Shell (SSH)\nDelivers a secure mechanism for data transmission between source and destination systems over IP networks. Designed to replace the old remote login programs that transmitted user passwords in clear text and data unencrypted. Employs digital signatures for user authentication with encryption to secure a communication channel. this makes it extremely hard for unauthorized people to gain access to passwords or the data in transit. Monitors the data being transferred throughout a session to ensure integrity. Includes a set of utilities ssh and sftp for remote users to log in, transfer files, and execute commands securely. Common Encryption Techniques # Two common techniques: symmetric and asymetric Symmetric Technique\nSecret key encryption. Uses a single key called a secret key that is generated as a result of a negotiation process between two entities at the time of their initial contact. Both sides use the same secret key during subsequent communication for data encryption and decryption. Asymmetric Technique\nPublic key encryption Combination of private and public keys Randomly generated and mathematically related strings of alphanumeric characters attached to messages being exchanged. The client transmutes the information with a public key and the server decrypts it with the paired private key. Private key must be kept secure since it is private to a single sender the public key is disseminated to clients. used for channel encryption and user authentication. Authentication Methods # Encrypted channel is established between the client and server Then additional negotiations take place between the two to authenticate the user trying to access the server. Methods listed in the order in which they are attempted during the authentication process: GSSAPI-based ( Generic Security Service Application Program Interface) authentication Host-based authentication Public key-based authentication Challenge-response authentication Password-based authentication GSSAPI-Based Authentication\nProvides a standard interface that allows security mechanisms, such as Kerberos, to be plugged in. OpenSSH uses this interface and the underlying Kerberos for authentication. Exchange of tokens takes place between the client and server to validate user identity. Host-Based Authentication\nAllows a single user, a group of users, or all users on the client to be authenticated on the server. A user may be configured to log in with a matching username on the server or as a different user that already exists there. For each user that requires an automatic entry on the server, a ~/.shosts file is set up containing the client name or IP address, and, optionally, a different username. The same rule applies to a group of users or all users on the client that require access to the server. In that case, the setup is done in the /etc/ssh/shosts.equiv file on the server. Private/Public Key-Based Authentication\nUses a private/public key combination for user authentication. User on the client has a private key and the server stores the corresponding public key. At the login attempt, the server prompts the user to enter the passphrase associated with the key and logs the user in if the passphrase and key are validated. Challenge-Response Authentication\nBased on the response(s) to one or more arbitrary challenge questions that the user has to answer correctly in order to be allowed to log in to the server. Password-Based Authentication\nLast fall back option. Server prompts the user to enter their password. Checks the password against the stored entry in the shadow file and allows the user in if the password is confirmed. OpenSSH Protocol Version and Algorithms # V2 Supports various algorithms for data encryption and user authentication (digital signatures) such as: RSA (Rivest-Shamir-Adleman)\nMore prevalent than the rest Supports both encryption and authentication. DSA and ECDSA (Digital Signature Algorithm and Elliptic Curve Digital Signature Algorithm)\nAuthentication only. Used to generate public and private key pairs for the asymmetric technique. OpenSSH Packages # Installed during OS installation openssh\nprovides the ssh-keygen command and some library routines openssh-clients\nincludes commands, such as sftp, ssh, and ssh-copy-id, and a client configuration file /etc/ssh/ssh_config openssh-server\ncontains the sshd service daemon, server configuration file /etc/ssh/sshd_config, and library routines. OpenSSH Server Daemon and Client Commands # OpenSSH server program is sshd sshd\nPreconfigured and operational on new RHEL installations\nAllows remote users to log in to the system using an ssh client program such as PuTTY or the ssh command.\nDaemon listens on TCP port 22\nDocumented in the /etc/ssh/sshd_config file with the Port directive. Use sftp instead of scp do to scp security flaws.\nsftp\nSecure remote file transfer program ssh\nSecure remote login command ssh-copy-id\nCopies public key to remote systems ssh-keygen\nGenerates and manages private and public key pairs Server Configuration File # /etc/ssh/sshd_config\n/var/log/secure\nlog file is used to capture authentication messages. View directives listed in /etc/ssh/sshd_config:\n[root@server30 tmp]# cat /etc/ssh/sshd_config Port\nPort number to listen on. Default is 22. Protocol\nDefault protocol version to use. ListenAddress\nSets the local addresses the sshd service should listen on. Default is to listen on all local addresses. SyslogFacility\nDefines the facility code to be used when logging messages to the /var/log/secure file. This is based on the configuration in the /etc/rsyslog.conf file. Default is AUTHPRIV. LogLevel Identifies the level of criticality for the messages to be logged. Default is INFO.\nPermitRootLogin Allows or disallows the root user to log in directly to the system. Default is yes.\nPubKeyAuthentication Enables or disables public key-based authentication. Default is yes.\nAuthorizedKeysFile Sets the name and location of the file containing a user\u0026rsquo;s authorized keys. Default is ~/.ssh/authorized_keys.\nPasswordAuthentication Enables or disables local password authentication. Default is yes.\nPermitEmptyPasswords Allows or disallows the use of null passwords. Default is no.\nChallengeResponseAuthentication Enables or disables challenge-response authentication mechanism. Default is yes.\nUsePAM Enables or disables user authentication via PAM. If enabled, only root will be able to run the sshd daemon. Default is yes.\nX11Forwarding Allows or disallows remote access to graphical applications. Default is yes.\nClient Configuration File # /etc/ssh/ssh_config\nLocal configuration file that directs how the client should behave. This file, , is located in the /etc/ssh directory. Directives preset in this file that affect all outbound ssh communication. View the default directive settings: [root@server30 tmp]# cat /etc/ssh/sshd_config\nHost\nContainer that declares directives applicable to one host, a group of hosts, or all hosts. Ends when another occurrence of Host or Match is encountered. Default is *, (all hosts) ForwardX11\nEnables or disables automatic redirection of X11 traffic over SSH connections. Default is no. PasswordAuthentication\nAllows or disallows password authentication. Default is yes. StrictHostKeyChecking\nWhether to add host keys (host fingerprints) to ~/.ssh/known_hosts when accessing a host for the first time\nWhat to do when the keys of a previously accessed host mismatch with what is stored in ~/.ssh/known_hosts.\nno:\nAdds new host keys and ignores changes to existing keys. yes:\nAdds new host keys and disallows connections to hosts with non-matching keys. accept-new:\nAdds new host keys and disallows connections to hosts with non-matching keys. ask (default):\nPrompts whether to add new host keys and disallows connections to hosts with non-matching keys. IdentityFile\nDefines the name and location of a file that stores a user\u0026rsquo;s private key for their identity validation. Defaults are: id_rsa, id_dsa, and id_ecdsa based on the type of algorithm used. Corresponding public key files with .pub extension are also stored at the same directory location. Port Sets the port number to listen on. Default is 22.\nProtocol Specifies the default protocol version to use\n~/.ssh/\ndoes not exist by default created when: a user executes the ssh-keygen command for the first time to generate a key pair A user connects to a remote ssh server and accepts its host key for the first time. The client stores the server\u0026rsquo;s host key locally in a file called known_hosts along with its hostname or IP address. On subsequent access attempts, the client will use this information to verify the server\u0026rsquo;s authenticity. System Access and File Transfer # Lab: Access RHEL System from Another RHEL System # issue the ssh command as user1 on server10 to log in to server20. Run appropriate commands on server20 for validation. Log off and return to the originating system. 1. Issue the ssh command as user1 on server10:\n[user1@server30 tmp]$ ssh server20 2. Issue the basic Linux commands whoami, hostname, and pwd to confirm that you are logged in as user1 on server20 and placed in the correct home directory:\n[user1@server40 ~]$ whoami user1 [user1@server40 ~]$ hostname server40 [user1@server40 ~]$ pwd /home/user1 3. Run the logout or the exit command or simply press the key combination Ctrl+d to log off server20 and return to server10:\n[user1@server40 ~]$ exit logout Connection to server40 closed. If you wish to log on as a different user such as user2 (assuming user2 exists on the target server server20), you may run the ssh command in either of the following ways:\n[user1@server30 tmp]$ ssh -l user2 server40\n[user1@server30 tmp]$ ssh user2@server40\nLab: Generate, Distribute, and Use SSH Keys # Generate a passwordless ssh key pair using RSA algorithm for user1 on server10. display the private and public file contents. Distribute the public key to server20 and attempt to log on to server20 from server10. Show the log file message for the login attempt. 1. Log on to server10 as user1.\n2. Generate RSA keys without a password (-N) and without detailed output (-q). Press Enter when prompted to provide the filename to store the private key.\n[user1@server30 tmp]$ ssh-keygen -N \u0026#34;\u0026#34; -q Enter file in which to save the key (/home/user1/.ssh/id_rsa): View the private key: [user1@server30 tmp]$ cat ~/.ssh/id_rsa\nView the public key: [user1@server30 tmp]$ cat ~/.ssh/id_rsa.pub\n3. Copy the public key file to server20 under /home/user1/.ssh directory.\nuser1@server30 tmp]$ ssh-copy-id server40 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \u0026#34;/home/user1/.ssh/id_rsa.pub\u0026#34; /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys user1@server40\u0026#39;s password: Number of key(s) added: 1 Now try logging into the machine, with: \u0026#34;ssh \u0026#39;server40\u0026#39;\u0026#34; and check to make sure that only the key(s) you wanted were added. This command also creates or updates the known_hosts file on server10 and stores the fingerprints for server20 in it. [user1@server30 tmp]$ cat ~/.ssh/known_hosts\n4. On server10, run the ssh command as user1 to connect to server20. You will not be prompted for a password because there was none assigned to the ssh keys.\n[user1@server30 tmp]$ ssh server40 Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Sun Jul 21 01:20:17 2024 from 192.168.0.30 View this login attempt in the /var/log/secure file on server20: [user1@server40 ~]$ sudo tail /var/log/secure\nExecuting Commands Remotely Using ssh # Can use ssh command to run programs without remoting in: Execute the hostname command on server20:\n[user1@server30 tmp]$ ssh server40 hostname server40 Run the nmcli command on server20 to show (s) active network connections(c):\n[user1@server30 tmp]$ ssh server40 nmcli c s NAME UUID TYPE DEVICE enp0s3 1c391bb6-20a3-4eb4-b717-1e458877dbe4 ethernet enp0s3 lo 175f8a4c-1907-4006-b838-eb43438d847b loopback lo sftp` command # Interactive file transfer tool. On server10, to connect to server20:\n[user1@server30 tmp]$ sftp server40 Connected to server40. sftp\u0026gt; Type ? at the prompt to list available commands along with a short description:\n[user1@server30 tmp]$ sftp server40 Connected to server40. sftp\u0026gt; ? Available commands: bye Quit sftp cd path Change remote directory to \u0026#39;path\u0026#39; chgrp [-h] grp path Change group of file \u0026#39;path\u0026#39; to \u0026#39;grp\u0026#39; chmod [-h] mode path Change permissions of file \u0026#39;path\u0026#39; to \u0026#39;mode\u0026#39; chown [-h] own path Change owner of file \u0026#39;path\u0026#39; to \u0026#39;own\u0026#39; df [-hi] [path] Display statistics for current directory or filesystem containing \u0026#39;path\u0026#39; exit Quit sftp get [-afpR] remote [local] Download file help Display this help text lcd path Change local directory to \u0026#39;path\u0026#39; lls [ls-options [path]] Display local directory listing lmkdir path Create local directory ln [-s] oldpath newpath Link remote file (-s for symlink) lpwd Print local working directory ls [-1afhlnrSt] [path] Display remote directory listing lumask umask Set local umask to \u0026#39;umask\u0026#39; mkdir path Create remote directory progress Toggle display of progress meter put [-afpR] local [remote] Upload file pwd Display remote working directory quit Quit sftp reget [-fpR] remote [local] Resume download file rename oldpath newpath Rename remote file reput [-fpR] local [remote] Resume upload file rm path Delete remote file rmdir path Remove remote directory symlink oldpath newpath Symlink remote file version Show SFTP version !command Execute \u0026#39;command\u0026#39; in local shell ! Escape to local shell ? Synonym for help Example:\nsftp\u0026gt; ls sftp\u0026gt; mkdir /tmp/dir10-20 sftp\u0026gt; cd /tmp/dir10-20 sftp\u0026gt; pwd Remote working directory: /tmp/dir10-20 sftp\u0026gt; put /etc/group Uploading /etc/group to /tmp/dir10-20/group group 100% 1118 1.0MB/s 00:00 sftp\u0026gt; ls -l -rw-r--r-- 1 user1 user1 1118 Jul 21 01:41 group sftp\u0026gt; cd .. sftp\u0026gt; pwd Remote working directory: /tmp sftp\u0026gt; cd /home/user1 sftp\u0026gt; get /usr/bin/gzip Fetching /usr/bin/gzip to gzip gzip 100% 90KB 23.0MB/s 00:00 sftp\u0026gt; lcd, lls, lpwd, and lmkdir are run on the source server. Other commands are also available. (See man pages) Type quit at the sftp\u0026gt; prompt to exit the program when you\u0026rsquo;re done:\nsftp\u0026gt; quit [user1@server30 tmp]$ Secure Shell Service DIY Labs # Lab: Establish Key-Based Authentication # Create user account user20 on both systems and assign a password. [root@server40 ~]# adduser user20 [root@server40 ~]# passwd user20 Changing password for user user20. New password: BAD PASSWORD: The password is shorter than 8 characters Retype new password: passwd: all authentication tokens updated successfully. As user20 on server40, generate a private/public key pair without a passphrase using the ssh-keygen command. [user20@server40 ~]# ssh-keygen -N \u0026#34;\u0026#34; -q Enter file in which to save the key (/root/.ssh/id_rsa): Distribute the public key to server30 with the ssh-copy-id command. [user20@server40 ~]# ssh-copy-id server30 Log on to server30 as user20 and accept the fingerprints for the server if presented. [user20@server40 ~]# ssh server30 Activate the web console with: systemctl enable --now cockpit.socket Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Fri Jul 19 14:09:22 2024 [user20@server30 ~]# On subsequent log in attempts from server40 to server30, user20 should not be prompted for their password. Lab: Test the Effect of PermitRootLogin Directive # As user1 with sudo on server30, edit the /etc/ssh/sshd_config file and change the value of the directive PermitRootLogin to \u0026ldquo;no\u0026rdquo;. [user1@server30 ~]$ sudo vim /etc/ssh/sshd_config\nUse the systemctl command to activate the change.\n[user1@server30 ~]$ systemctl restart sshd ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ==== Authentication is required to restart \u0026#39;sshd.service\u0026#39;. Authenticating as: root Password: ==== AUTHENTICATION COMPLETE ==== As root on server40, run ssh server40 (or use its IP). You\u0026rsquo;ll get permission denied message. (this didn\u0026rsquo;t work, I think it\u0026rsquo;s because I configured passwordless authentication on here)\nReverse the change on server40 and retry ssh server40. You should be able to log in. ","externalUrl":null,"permalink":"/tech/linux/the-secure-shell-service/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eThe OpenSSH Service\n    \u003cdiv id=\"the-openssh-service\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#the-openssh-service\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSecure Shell (SSH)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDelivers a secure mechanism for data transmission between source and destination systems over IP networks.\u003c/li\u003e\n\u003cli\u003eDesigned to replace the old remote login programs that transmitted user passwords in clear text and data unencrypted.\u003c/li\u003e\n\u003cli\u003eEmploys digital signatures for user authentication with encryption to secure a communication channel.\n\u003cul\u003e\n\u003cli\u003ethis makes it extremely hard for unauthorized people to gain access to passwords or the data in transit.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMonitors the data being transferred throughout a session to ensure integrity.\u003c/li\u003e\n\u003cli\u003eIncludes a set of utilities \u003ccode\u003essh\u003c/code\u003e and \u003ccode\u003esftp\u003c/code\u003e for remote users to log in, transfer files, and execute commands securely.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eCommon Encryption Techniques\n    \u003cdiv id=\"common-encryption-techniques\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#common-encryption-techniques\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTwo common techniques: symmetric and asymetric\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSymmetric Technique\u003c/strong\u003e\u003c/p\u003e","title":"The Secure Shell Service","type":"tech"},{"content":" Thin Provisioning # Allows for an economical allocation and utilization of storage space by moving arbitrary data blocks to contiguous locations, which results in empty block elimination. Can create a thin pool of storage space and assign volumes much larger storage space than the physical capacity of the pool. Workloads begin consuming the actual allocated space for data writing. When a preset custom threshold (80%, for instance) on the actual consumption of the physical storage in the pool is reached, expand the pool dynamically by adding more physical storage to it. The volumes will automatically start exploiting the new space right away. helps prevent spending more money upfront. Logical Volume Manager (LVM) # Used for managing block storage in Linux. Provides an abstraction layer between the physical storage and the file system Enables the file system to be resized, span across multiple disks, use arbitrary disk space, etc. Accumulates spaces taken from partitions or entire disks (called Physical Volumes) to form a logical container (called Volume Group) which is then divided into logical partitions (called Logical Volumes). online resizing of volume groups and logical volumes, online data migration between logical volumes and between physical volumes user-defined naming for volume groups and logical volumes mirroring and striping across multiple disks snapshotting of logical volumes. Made up of three key objects called physical volume, volume group, and logical volume. These objects are further virtually broken down into Physical Extents (PEs) and Logical Extents (LEs). Physical Volume(PV)\ncreated when a block storage device such as a partition or an entire disk is initialized and brought under LVM control. This process constructs LVM data structures on the device, including a label on the second sector and metadata shortly thereafter. The label includes the UUID, size, and pointers to the locations of data and metadata areas. Given the criticality of metadata, LVM stores a copy of it at the end of the physical volume as well. The rest of the device space is available for use. You can use an LVM command called pvs (physical volume scan or summary) to scan and list available physical volumes on server2:\n[root@server2 ~]# sudo pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 (a for allocatable under Attr) Try running this command again with the -v flag to view more information about the physical volume.\nVolume Group\nCreated when at least one physical volume is added to it. The space from all physical volumes in a volume group is aggregated to form one large pool of storage, which is then used to build logical volumes. Physical volumes added to a volume group may be of varying sizes. LVM writes volume group metadata on each physical volume that is added to it. The volume group metadata contains its name,date, and time of creation, how it was created, the extent size used, a list of physical and logical volumes, a mapping of physical and logical extents, etc. Can have a custom name assigned to it at the time of its creation. A copy of the volume group metadata is stored and maintained at two distinct locations on each physical volume within the volume group. Use vgs (volume group scan or summary) to scan and list available volume groups on server2:\n[root@server2 ~]# sudo vgs VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 Status of the volume group under the Attr column (w for writeable, z for resizable, and n for normal), Try running this command again with the -v flag to view more information about the volume group.\nPhysical Extent # A physical volume is divided into several smaller logical pieces when it is added to a volume group. These logical pieces are known as Physical Extents (PE). An extent is the smallest allocatable unit of space in LVM. At the time of volume group creation, you can either define the size of the PE or leave it to the default value of 4MB. This implies that a 20GB physical volume would have approximately 5,000 PEs. Any physical volumes added to this volume group thereafter will use the same PE size. Use vgdisplay (volume group display) on server2 and grep for \u0026lsquo;PE Size\u0026rsquo; to view the PE size used in the rhel volume group:\n[root@server2 ~]# sudo vgdisplay rhel | grep \u0026#39;PE Size\u0026#39; PE Size 4.00 MiB Logical Volume # A volume group consists of a pool of storage taken from one or more physical volumes. This volume group space is used to create one or more Logical Volumes (LVs). A logical volume can be created or weeded out online, expanded or shrunk online, and can use space taken from one or multiple physical volumes inside the volume group. The default naming convention used for logical volumes is lvol0, lvol1, lvol2, and so on you may assign custom names to them.\nUse lvs (logical volume scan or summary) to scan and list available logical volumes on server2:\n[root@server2 ~]# sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g Attr column (w for writeable, i for inherited allocation policy, a for active, and o for open) and their sizes. Try running this command again with the -v flag to view more information about the logical volumes.\nLogical Extent # A logical volume is made up of Logical Extents (LE). Logical extents point to physical extents, and they may be random or contiguous. The larger a logical volume is, the more logical extents it will have. Logical extents are a set of physical extents allocated to a logical volume. The LE size is always the same as the PE size in a volume group. The default LE size is 4MB, which corresponds to the default PE size of 4MB. Use lvdisplay (logical volume display) on server2 to view information about the root logical volume in the rhel volume group.\n[root@server30 ~]# lvdisplay /dev/rhel/root --- Logical volume --- LV Path /dev/rhel/root LV Name root VG Name rhel LV UUID DhHyeI-VgwM-w75t-vRcC-5irj-AuHC-neryQf LV Write Access read/write LV Creation host, time localhost.localdomain, 2024-07-08 17:32:18 -0700 LV Status available # open 1 LV Size \u0026lt;17.00 GiB Current LE 4351 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:0 The output does not disclose the LE size; however, you can convert the LV size in MBs (17,000) and then divide the result by the Current LE count (4,351) to get the LE size (which comes close to 4MB). LVM Operations and Commands # Creating and removing a physical volume, volume group, and logical volume Extending and reducing a volume group and logical volume Renaming a volume group and logical volume listing and displaying physical volume, volume group, and logical volume information. Create and Remove Operations # pvcreate/pvremove\nInitializes/uninitializes a disk or partition for LVM use vgcreate/vgremove\nCreates/removes a volume group lvcreate/lvremove\nCreates/removes a logical volume Extend and Reduce Operations # vgextend/vgreduce\nAdds/removes a physical volume to/from a volume group lvextend/lvreduce\nExtends/reduces the size of a logical volume lvresize\nResizes a logical volume. With the -r option, this command calls the fsadm command to resize the underlying file system as well. Rename Operations # vgrename\nRename a volume group lvrename\nRename a logical volume List and Display Operations # pvs/pvdisplay\nLists/displays physical volume information vgs/vgdisplay lvs/lvdisplay\nLists/displays volume group information Lists/displays logical volume information\nAll the tools accept the -v switch to support verbosity.\nExercise 13-6: Create Physical Volume and Volume Group (server2) # initialize one partition sdd1 (90MB) and one disk sde (250MB) for use in LVM. create a volume group called vgbook and add both physical volumes to it use the PE size of 16MB list and display the volume group and the physical volumes. 1. Create a partition of size 90MB on sdd using the parted command and confirm. You need to label the disk first, as it is a new disk.\n[root@server2 ~]# sudo parted /dev/sdd mklabel msdos Information: You may need to update /etc/fstab. [root@server2 ~]# sudo parted /dev/sdd mkpart primary 1 91m Information: You may need to update /etc/fstab. [root@server2 ~]# sudo parted /dev/sdd print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdd: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 91.2MB 90.2MB primary 2. Initialize the sdd1 partition and the sde disk using the pvcreate command. Note that there is no need to apply a disk label on sde with parted as LVM does not require it.\n[root@server2 ~]# sudo pvcreate /dev/sdd1 /dev/sde -v Wiping signatures on new PV /dev/sdd1. Wiping signatures on new PV /dev/sde. Set up physical volume for \u0026#34;/dev/sdd1\u0026#34; with 176128 available sectors. Zeroing start of device /dev/sdd1. Writing physical volume data to disk \u0026#34;/dev/sdd1\u0026#34;. Physical volume \u0026#34;/dev/sdd1\u0026#34; successfully created. Set up physical volume for \u0026#34;/dev/sde\u0026#34; with 512000 available sectors. Zeroing start of device /dev/sde. Writing physical volume data to disk \u0026#34;/dev/sde\u0026#34;. Physical volume \u0026#34;/dev/sde\u0026#34; successfully created. 3. Create vgbook volume group using the vgcreate command and add the two physical volumes to it. Use the -s option to specify the PE size in MBs.\n[root@server2 ~]# sudo vgcreate -vs 16 vgbook /dev/sdd1 /dev/sde Wiping signatures on new PV /dev/sdd1. Wiping signatures on new PV /dev/sde. Adding physical volume \u0026#39;/dev/sdd1\u0026#39; to volume group \u0026#39;vgbook\u0026#39; Adding physical volume \u0026#39;/dev/sde\u0026#39; to volume group \u0026#39;vgbook\u0026#39; Creating volume group backup \u0026#34;/etc/lvm/backup/vgbook\u0026#34; (seqno 1). Volume group \u0026#34;vgbook\u0026#34; successfully created 4. List the volume group information:\n[root@server2 ~]# sudo vgs vgbook VG #PV #LV #SN Attr VSize VFree vgbook 2 0 0 wz--n- 320.00m 320.00m 5. Display detailed information about the volume group and the physical volumes it contains:\n[root@server2 ~]# sudo vgdisplay -v vgbook --- Volume group --- VG Name vgbook System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 320.00 MiB PE Size 16.00 MiB Total PE 20 Alloc PE / Size 0 / 0 Free PE / Size 20 / 320.00 MiB VG UUID zRu1d2-ZgDL-bnzV-I9U1-0IFo-uM4x-w4bX0Q --- Physical volumes --- PV Name /dev/sdd1 PV UUID 8x8IgZ-3z5T-ODA8-dofQ-xk5s-QN7I-KwpQ1e PV Status allocatable Total PE / Free PE 5 / 5 PV Name /dev/sde PV UUID xJU0Hh-W5k9-FyKO-d6Ha-1ofW-ajvh-hJSo8R PV Status allocatable Total PE / Free PE 15 / 15 6. List the physical volume information:\n[root@server2 ~]# sudo pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 /dev/sdd1 vgbook lvm2 a-- 80.00m 80.00m /dev/sde vgbook lvm2 a-- 240.00m 240.00m 7. Display detailed information about the physical volumes:\n[root@server2 ~]# sudo pvdisplay /dev/sdd1 --- Physical volume --- PV Name /dev/sdd1 VG Name vgbook PV Size 86.00 MiB / not usable 6.00 MiB Allocatable yes PE Size 16.00 MiB Total PE 5 Free PE 5 Allocated PE 0 PV UUID 8x8IgZ-3z5T-ODA8-dofQ-xk5s-QN7I-KwpQ1e Once a partition or disk is initialized and added to a volume group, they are treated identically within the volume group. LVM does not prefer one over the other. Exercise 13-7: Create Logical Volumes(server2) # Create two logical volumes, lvol0 and lvbook1, in the vgbook volume group. Use 120MB for lvol0 and 192MB for lvbook1 from the available pool of space. Display the details of the volume group and the logical volumes. 1. Create a logical volume with the default name lvol0 using the lvcreate command. Use the -L option to specify the logical volume size, 120MB. You may use the -v, -vv, or -vvv option with the command for verbosity.\nroot@server2 ~]# sudo lvcreate -vL 120 vgbook Rounding up size to full physical extent 128.00 MiB Creating logical volume lvol0 Archiving volume group \u0026#34;vgbook\u0026#34; metadata (seqno 1). Activating logical volume vgbook/lvol0. activation/volume_list configuration setting not defined: Checking only host tags for vgbook/lvol0. Creating vgbook-lvol0 Loading table for vgbook-lvol0 (253:2). Resuming vgbook-lvol0 (253:2). Wiping known signatures on logical volume vgbook/lvol0. Initializing 4.00 KiB of logical volume vgbook/lvol0 with value 0. Logical volume \u0026#34;lvol0\u0026#34; created. Creating volume group backup \u0026#34;/etc/lvm/backup/vgbook\u0026#34; (seqno 2). Size for the logical volume may be specified in units such as MBs, GBs, TBs, or as a count of LEs\nMB is the default if no unit is specified\nThe size of a logical volume is always in multiples of the PE size. For instance, logical volumes created in vgbook with the PE size set at 16MB can be 16MB, 32MB, 48MB, 64MB, and so on.\n2. Create lvbook1 of size 192MB (16x12) using the lvcreate command. Use the -l switch to specify the size in logical extents and -n for the custom name.\n[root@server2 ~]# sudo lvcreate -l 12 -n lvbook1 vgbook Logical volume \u0026#34;lvbook1\u0026#34; created. 3. List the logical volume information:\n[root@server2 ~]# sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g lvbook1 vgbook -wi-a----- 192.00m lvol0 vgbook -wi-a----- 128.00m 4. Display detailed information about the volume group including the logical volumes and the physical volumes:\n[root@server2 ~]# sudo vgdisplay -v vgbook --- Volume group --- VG Name vgbook System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 320.00 MiB PE Size 16.00 MiB Total PE 20 Alloc PE / Size 20 / 320.00 MiB Free PE / Size 0 / 0 VG UUID zRu1d2-ZgDL-bnzV-I9U1-0IFo-uM4x-w4bX0Q --- Logical volume --- LV Path /dev/vgbook/lvol0 LV Name lvol0 VG Name vgbook LV UUID 9M9ahf-1L3y-c0yk-3Z2O-UzjH-0Amt-QLi4p5 LV Write Access read/write LV Creation host, time server2, 2024-06-12 02:42:51 -0700 LV Status available open 0 LV Size 128.00 MiB Current LE 8 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:2 --- Logical volume --- LV Path /dev/vgbook/lvbook1 LV Name lvbook1 VG Name vgbook LV UUID pgd8qR-YXXK-3Idv-qmpW-w8Az-WGLR-g2d8Yn LV Write Access read/write LV Creation host, time server2, 2024-06-12 02:45:31 -0700 LV Status available # open 0 LV Size 192.00 MiB Current LE 12 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:3 --- Physical volumes --- PV Name /dev/sdd1 PV UUID 8x8IgZ-3z5T-ODA8-dofQ-xk5s-QN7I-KwpQ1e PV Status allocatable Total PE / Free PE 5 / 0 PV Name /dev/sde PV UUID xJU0Hh-W5k9-FyKO-d6Ha-1ofW-ajvh-hJSo8R PV Status allocatable Total PE / Free PE 15 / 0 Alternatively, you can run the following to view only the logical volume details:\n[root@server2 ~]# sudo lvdisplay /dev/vgbook/lvol0 --- Logical volume --- LV Path /dev/vgbook/lvol0 LV Name lvol0 VG Name vgbook LV UUID 9M9ahf-1L3y-c0yk-3Z2O-UzjH-0Amt-QLi4p5 LV Write Access read/write LV Creation host, time server2, 2024-06-12 02:42:51 -0700 LV Status available # open 0 LV Size 128.00 MiB Current LE 8 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:2 [root@server2 ~]# sudo lvdisplay /dev/vgbook/lvbook1 --- Logical volume --- LV Path /dev/vgbook/lvbook1 LV Name lvbook1 VG Name vgbook LV UUID pgd8qR-YXXK-3Idv-qmpW-w8Az-WGLR-g2d8Yn LV Write Access read/write LV Creation host, time server2, 2024-06-12 02:45:31 -0700 LV Status available # open 0 LV Size 192.00 MiB Current LE 12 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:3 Exercise 13-8: Extend a Volume Group and a Logical Volume(server2) # Add another partition sdd2 of size 158MB to vgbook to increase the pool of allocatable space. Initialize the new partition prior to adding it to the volume group. Increase the size of lvbook1 to 336MB. Display basic information for the physical volumes, volume group, and logical volume. 1. Create a partition of size 158MB on sdd using the parted command. Display the new partition to confirm the partition number and size.\n[root@server20 ~]# parted /dev/sdd mkpart primary 91 250 [root@server2 ~]# sudo parted /dev/sdd print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdd: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 91.2MB 90.2MB primary 2 92.3MB 250MB 157MB primary lvm 2. Initialize sdd2 using the pvcreate command:\n[root@server2 ~]# sudo pvcreate /dev/sdd2 Physical volume \u0026#34;/dev/sdd2\u0026#34; successfully created. 3. Extend vgbook by adding the new physical volume to it:\n[root@server2 ~]# sudo vgextend vgbook /dev/sdd2 Volume group \u0026#34;vgbook\u0026#34; successfully extended 4. List the volume group:\n[root@server2 ~]# sudo vgs VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 vgbook 3 2 0 wz--n- 464.00m 144.00m 5. Extend the size of lvbook1 to 340MB by adding 144MB using the lvextend command:\n[root@server2 ~]# sudo lvextend -L +144 /dev/vgbook/lvbook1 Size of logical volume vgbook/lvbook1 changed from 192.00 MiB (12 extents) to 336.00 MiB (21 extents). Logical volume vgbook/lvbook1 successfully resized. EXAM TIP: Make sure the expansion of a logical volume does not affect the file system and the data it contains.\n6. Issue vgdisplay on vgbook with the -v switch for the updated details:\n[root@server2 ~]# sudo vgdisplay -v vgbook --- Volume group --- VG Name vgbook System ID Format lvm2 Metadata Areas 3 Metadata Sequence No 5 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 3 Act PV 3 VG Size 464.00 MiB PE Size 16.00 MiB Total PE 29 Alloc PE / Size 29 / 464.00 MiB Free PE / Size 0 / 0 VG UUID zRu1d2-ZgDL-bnzV-I9U1-0IFo-uM4x-w4bX0Q --- Logical volume --- LV Path /dev/vgbook/lvol0 LV Name lvol0 VG Name vgbook LV UUID 9M9ahf-1L3y-c0yk-3Z2O-UzjH-0Amt-QLi4p5 LV Write Access read/write LV Creation host, time server2, 2024-06-12 02:42:51 -0700 LV Status available open 0 LV Size 128.00 MiB Current LE 8 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:2 --- Logical volume --- LV Path /dev/vgbook/lvbook1 LV Name lvbook1 VG Name vgbook LV UUID pgd8qR-YXXK-3Idv-qmpW-w8Az-WGLR-g2d8Yn LV Write Access read/write LV Creation host, time server2, 2024-06-12 02:45:31 -0700 LV Status available # open 0 LV Size 336.00 MiB Current LE 21 Segments 3 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:3 --- Physical volumes --- PV Name /dev/sdd1 PV UUID 8x8IgZ-3z5T-ODA8-dofQ-xk5s-QN7I-KwpQ1e PV Status allocatable Total PE / Free PE 5 / 0 PV Name /dev/sde PV UUID xJU0Hh-W5k9-FyKO-d6Ha-1ofW-ajvh-hJSo8R PV Status allocatable Total PE / Free PE 15 / 0 PV Name /dev/sdd2 PV UUID 1olOnk-o8FH-uJRD-2pJf-8GCy-3K0M-gcf3pF PV Status allocatable Total PE / Free PE 9 / 0 7. View a summary of the physical volumes:\nroot@server2 ~]# sudo pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 /dev/sdd1 vgbook lvm2 a-- 80.00m 0 /dev/sdd2 vgbook lvm2 a-- 144.00m 0 /dev/sde vgbook lvm2 a-- 240.00m 0 8. View a summary of the logical volumes:\n[root@server2 ~]# sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g lvbook1 vgbook -wi-a----- 336.00m lvol0 vgbook -wi-a----- 128.00m Exercise 13-9: Rename, Reduce, Extend, and Remove Logical Volumes(server2) # Rename lvol0 to lvbook2. Decrease the size of lvbook2 to 50MB using the lvreduce command Add 32MB with the lvresize command. remove both logical volumes. display the summary for the volume groups, logical volumes, and physical volumes. 1. Rename lvol0 to lvbook2 using the lvrename command and confirm with lvs:\n[root@server2 ~]# sudo lvrename vgbook lvol0 lvbook2 Renamed \u0026#34;lvol0\u0026#34; to \u0026#34;lvbook2\u0026#34; in volume group \u0026#34;vgbook\u0026#34; 2. Reduce the size of lvbook2 to 50MB with the lvreduce command. Specify the absolute desired size for the logical volume. Answer \u0026ldquo;Do you really want to reduce vgbook/lvbook2?\u0026rdquo; in the affirmative.\n[root@server2 ~]# sudo lvreduce -L 50 /dev/vgbook/lvbook2 Rounding size to boundary between physical extents: 64.00 MiB. No file system found on /dev/vgbook/lvbook2. Size of logical volume vgbook/lvbook2 changed from 128.00 MiB (8 extents) to 64.00 MiB (4 extents). Logical volume vgbook/lvbook2 successfully resized. 3. Add 32MB to lvbook2 with the lvresize command:\n[root@server2 ~]# sudo lvresize -L +32 /dev/vgbook/lvbook2 Size of logical volume vgbook/lvbook2 changed from 64.00 MiB (4 extents) to 96.00 MiB (6 extents). Logical volume vgbook/lvbook2 successfully resized. 4. Use the pvs, lvs, vgs, and vgdisplay commands to view the updated allocation.\n[root@server2 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 /dev/sdd1 vgbook lvm2 a-- 80.00m 0 /dev/sdd2 vgbook lvm2 a-- 144.00m 0 /dev/sde vgbook lvm2 a-- 240.00m 32.00m [root@server2 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g lvbook1 vgbook -wi-a----- 336.00m lvbook2 vgbook -wi-a----- 96.00m [root@server2 ~]# vgs VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 vgbook 3 2 0 wz--n- 464.00m 32.00m [root@server2 ~]# vgdisplay --- Volume group --- VG Name vgbook System ID Format lvm2 Metadata Areas 3 Metadata Sequence No 8 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 3 Act PV 3 VG Size 464.00 MiB PE Size 16.00 MiB Total PE 29 Alloc PE / Size 27 / 432.00 MiB Free PE / Size 2 / 32.00 MiB VG UUID zRu1d2-ZgDL-bnzV-I9U1-0IFo-uM4x-w4bX0Q --- Volume group --- VG Name rhel System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size \u0026lt;19.00 GiB PE Size 4.00 MiB Total PE 4863 Alloc PE / Size 4863 / \u0026lt;19.00 GiB Free PE / Size 0 / 0 VG UUID UiK3fy-FGOc-2fnP-C1Y6-JS0l-irEe-Sq3c4h 5. Remove both lvbook1 and lvbook2 logical volumes using the lvremove command. Use the -f option to suppress the \u0026ldquo;Do you really want to remove active logical volume\u0026rdquo; message.\n[root@server2 ~]# sudo lvremove /dev/vgbook/lvbook1 -f Logical volume \u0026#34;lvbook1\u0026#34; successfully removed. [root@server2 ~]# sudo lvremove /dev/vgbook/lvbook2 -f Logical volume \u0026#34;lvbook2\u0026#34; successfully removed. Removing an LV is destructive Backup any data in the target LV before deleting it. You will need to unmount the file system or disable swap in the logical volume. 6. Execute the vgdisplay command and grep for \u0026ldquo;Cur LV\u0026rdquo; to see the number of logical volumes currently available in vgbook. It should show 0, as you have removed both logical volumes. [root@server2 ~]# sudo vgdisplay vgbook | grep \u0026#39;Cur LV\u0026#39; Cur LV 0 Exercise 13-10: Reduce and Remove a Volume Group(server2) # \\\nReduce vgbook by removing the sdd1 and sde physical volumes from it Remove the volume group. Confirm the deletion of the volume group and the logical volumes at the end. 1. Remove sdd1 and sde physical volumes from vgbook by issuing the vgreduce command:\n[root@server2 ~]# sudo vgreduce vgbook /dev/sdd1 /dev/sde Removed \u0026#34;/dev/sdd1\u0026#34; from volume group \u0026#34;vgbook\u0026#34; Removed \u0026#34;/dev/sde\u0026#34; from volume group \u0026#34;vgbook\u0026#34; 2. Remove the volume group using the vgremove command. This will also remove the last physical volume, sdd2, from it.\n[root@server2 ~]# sudo vgremove vgbook Volume group \u0026#34;vgbook\u0026#34; successfully removed Use the -f option with the vgremove command to force the volume group removal even if it contains any number of logical and physical volumes in it. 3. Execute the vgs and lvs commands for confirmation:\n[root@server2 ~]# sudo vgs VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 [root@server2 ~]# sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g Exercise 13-11: Uninitialize Physical Volumes (Server2)\\ # Uninitialize all three physical volumes\u0026mdash;sdd1, sdd2, and sde\u0026mdash;by deleting the LVM structural information from them. Use the pvs command for confirmation. Remove the partitions from the sdd disk and Verify that all disks used in Exercises 13-6 to 13-10 are now in their original raw state. 1. Remove the LVM structures from sdd1, sdd2, and sde using the pvremove command:\n[root@server2 ~]# sudo pvremove /dev/sdd1 /dev/sdd2 /dev/sde Labels on physical volume \u0026#34;/dev/sdd1\u0026#34; successfully wiped. Labels on physical volume \u0026#34;/dev/sdd2\u0026#34; successfully wiped. Labels on physical volume \u0026#34;/dev/sde\u0026#34; successfully wiped. 2. Confirm the removal using the pvs command:\n[root@server2 ~]# sudo pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 The partitions and the disk are now back to their raw state and can be repurposed.\n3. Remove the partitions from sdd using the parted command:\n[root@server2 ~]# sudo parted /dev/sdd rm 1 ; sudo parted /dev/sdd rm 2 Information: You may need to update /etc/fstab. Information: You may need to update /etc/fstab. 4. Verify that all disks used in previous exercises have returned to their original raw state using the lsblk command:\n[root@server2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─rhel-root 253:0 0 17G 0 lvm / └─rhel-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 250M 0 disk sdc 8:32 0 250M 0 disk sdd 8:48 0 250M 0 disk sde 8:64 0 250M 0 disk sdf 8:80 0 5G 0 disk sr0 11:0 1 9.8G 0 rom ","externalUrl":null,"permalink":"/tech/linux/thin-provisioning/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003eThin Provisioning\n    \u003cdiv id=\"thin-provisioning\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#thin-provisioning\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eAllows for an economical allocation and utilization of storage space by moving arbitrary data blocks to contiguous locations, which results in empty block elimination.\u003c/li\u003e\n\u003cli\u003eCan create a thin pool of storage space and assign volumes much larger storage space than the physical capacity of the pool.\u003c/li\u003e\n\u003cli\u003eWorkloads begin consuming the actual allocated space for data writing.\u003c/li\u003e\n\u003cli\u003eWhen a preset custom threshold (80%, for instance) on the actual consumption of the physical storage in the pool is reached, expand the pool dynamically by adding more physical storage to it.\u003c/li\u003e\n\u003cli\u003eThe volumes will automatically start exploiting the new space right away.\u003c/li\u003e\n\u003cli\u003ehelps prevent spending more money upfront.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eLogical Volume Manager (LVM)\n    \u003cdiv id=\"logical-volume-manager-lvm\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#logical-volume-manager-lvm\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUsed for managing block storage in Linux.\u003c/li\u003e\n\u003cli\u003eProvides an abstraction layer between the physical storage and the file system\u003c/li\u003e\n\u003cli\u003eEnables the file system to be resized, span across multiple disks, use arbitrary disk space, etc.\u003c/li\u003e\n\u003cli\u003eAccumulates spaces taken from partitions or entire disks (called Physical Volumes) to form a logical container (called Volume Group) which is then divided into logical partitions (called Logical Volumes).\u003c/li\u003e\n\u003cli\u003eonline resizing of volume groups and logical volumes,\u003c/li\u003e\n\u003cli\u003eonline data migration between logical volumes and between physical volumes\u003c/li\u003e\n\u003cli\u003euser-defined naming for volume groups and logical volumes\u003c/li\u003e\n\u003cli\u003emirroring and striping across multiple disks\u003c/li\u003e\n\u003cli\u003esnapshotting of logical volumes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"\"\n    src=\"/images/image-0A8W8M1U.jpg\"\n    \u003e\u003c/figure\u003e\n\u003cul\u003e\n\u003cli\u003eMade up of three key objects called physical volume, volume group, and logical volume.\u003c/li\u003e\n\u003cli\u003eThese objects are further virtually broken down into Physical Extents (PEs) and Logical Extents (LEs).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePhysical Volume(PV)\u003c/strong\u003e\u003c/p\u003e","title":"Thin Provisioning and LVM","type":"tech"},{"content":" Network Time Protocol (NTP) # Networking protocol for synchronizing the system clock with remote time servers for accuracy and reliability. Having steady and exact time on networked systems allows time-sensitive applications, such as authentication and email applications, backup and scheduling tools, financial and billing systems, logging and monitoring software, and file and storage sharing protocols, to function with precision. Sends a stream of messages to configured time servers and binds itself to the one with least amount of delay in its responses, the most accurate, and may or may not be the closest distance-wise. Client system maintains a drift in time in a file and references this file for gradual drop in inaccuracy. Chrony # RHEL 9 implementation of NTP Uses the UDP port 123. If enabled, it starts at system boot and continuously operates to keep the system clock in sync with a more accurate source of time. Performs well on computers that are occasionally connected to the network, attached to busy networks, do not run all the time, or have variations in temperature. Chrony is the RHEL implementation of NTP. And it operates on UDP port 123. If you enable it, it starts at system boot and continuously monitors system time and keeps in in sync.\nTime Sources # A time source is any reference device that acts as a provider of time to other devices. The most precise sources of time are the atomic clocks. They use Universal Time Coordinated (UTC) for time accuracy. They produce radio signals that radio clocks use for time propagation to computer servers and other devices that require correctness in time. When choosing a time source for a network, preference should be given to the one that takes the least amount of time to respond. This server may or may not be closest physically. The common sources of time employed on computer networks are:\nThe local system clock Internet-based public time server Radio clock. local system clock\nCan be used as a provider of time. This requires the maintenance of correct time on the server either manually or automatically via cron. Keep in mind that this server has no way of synchronizing itself with a more reliable and precise external time source. Using the local system clock as a time server is the least recommended option. Public time server\nSeveral public time servers are available over the Internet for general use (visit www.ntp.org for a list). These servers are typically operated by government agencies, research and scientific organizations, large software vendors, and universities around the world. One of the systems on the local network is identified and configured to receive time from one or more public time servers. Preferred over the use of the local system clock. The official ntp.org site also provides a common pool called pool.ntp.org for vendors and organizations to register their own NTP servers voluntarily for public use. Examples:\nrhel.pool.ntp.org and ubuntu.pool.ntp.org for distribution-specific pools, ca.pool.ntp.org and oceania.pool.ntp.org for country and continent/region-specific pools. Under these sub-pools, the owners maintain multiple time servers with enumerated hostnames such as 0.rhel.pool.ntp.org, 1.rhel.pool.ntp.org, 2.rhel.pool.ntp.org, and so on.\nRadio clock\nRegarded as the perfect provider of time Receives time updates straight from an atomic clock. Global Positioning System (GPS), WWVB, and DCF77 are some popular radio clock methods. A direct use of signals from these sources requires connectivity of some hardware to the computer identified to act as an organizational or site-wide time server. NTP Roles # A system can be configured to operate as a primary server, secondary server, peer, or client. Primary server\nGets time from a time source and provides time to secondary servers or directly to clients. secondary server\nReceives time from a primary server and can be configured to furnish time to a set of clients to offload the primary or for redundancy. The presence of a secondary server on the network is optional but highly recommended. peer\nReciprocates time with an NTP server. All peers work at the same stratum level, and all of them are considered equally reliable. client\nReceives time from a primary or a secondary server and adjusts its clock accordingly. Stratum Levels # Time sources are categorized hierarchically into several levels that are referred to as stratum levels based on their distance from the reference clocks (atomic, radio, and GPS).\nThe reference clocks operate at stratum level 0 and are the most accurate provider of time with little to no delay.\nBesides stratum 0, there are fifteen additional levels that range from 1 to 15.\nOf these, servers operating at stratum 1 are considered perfect, as they get time updates directly from a stratum 0 device.\nA stratum 0 device cannot be used on the network directly. It is attached to a computer, which is then configured to operate at stratum 1.\nServers functioning at stratum 1 are called time servers and they can be set up to deliver time to stratum 2 servers.\nSimilarly, a stratum 3 server can be configured to synchronize its time with a stratum 2 server and deliver time to the next lower-level servers, and so on.\nServers sharing the same stratum can be configured as peers to exchange time updates with one another.\nThere are numerous public NTP servers available for free that synchronize time. They normally operate at higher stratum levels such as 2 and 3.\nChrony Configuration File # /etc/chrony.conf\nkey configuration file for the Chrony service Referenced by the Chrony daemon at startup to determine the sources to synchronize the clock, the log file location, and other details. Can be modified by hand to set or alter directives as required. Common directives used in this file along with real or mock values: driftfile\n/var/lib/chrony/drift Indicates the location and name of the drift file to be used to record the rate at which the system clockgains or losses time. This data is used by Chrony to maintain local system clock accuracy. logdir\n/var/log/chrony Sets the directory location to store the log files in pool\n0.rhel.pool.ntp.org iburst Defines the hostname that represents a pool of time servers. Chrony binds itself with one of the servers to get updates. In case of a failure of that server, it automatically switches the binding to another server within the pool. The iburst option dictates the Chrony service to send the first four update requests to the time server every 2 seconds. This allows the daemon to quickly bring the local clock closer to the time server at startup. server\nserver20s8.example.com iburst Defines the hostname or IP address of a single time server. server\n127.127.1.0 The IP 127.127.1.0 is a special address that epitomizes the local system clock. peer\nprodntp1.abc.net Identifies the hostname or IP address of a time server running at the same stratum level. A peer provides time to a server as well as receives time from the same server man chrony.conf for details.\nChrony Daemon and Command # Chrony service runs as a daemon program called chronyd that handles time synchronization in the background. Uses /etc/chrony.conf file at startup and sets its behavior accordingly. If the local clock requires a time adjustment, Chrony takes multiple small steps toward minimizing the gap rather than doing it abruptly in a single step. The Chrony service has a command line program called chronyc.\nchronyc command # monitor the performance of the service and control its runtime behavior. Subcommands: sources\nList current sources of time tracking\nview performance statistics Lab: Configure NTP Client (server10) # Install the Chrony software package and activate the service without making any changes to the default configuration. Validate the binding and operation. 1. Install the Chrony package using the dnf command:\n[root@server10 ~]# sudo dnf -y install chrony 2. Ensure that preconfigured public time server entries are present in the /etc/chrony.conf file:\n[root@server1 ~]# grep -E \u0026#39;pool|server\u0026#39; /etc/chrony.conf | grep -v ^# pool 2.rhel.pool.ntp.org iburst There is a single pool entry set in the file by default. This pool name is backed by multiple NTP servers behind the scenes.\n3. Start the Chrony service and set it to autostart at reboots: sudo systemctl --now enable chronyd\n4. Examine the operational status of Chrony: sudo systemctl status chronyd --no-pager -l\n5. Inspect the binding status using the sources subcommand with chronyc:\n[root@server1 ~]# chronyc sources MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^+ ntp7-2.mattnordhoffdns.n\u0026gt; 2 8 377 324 -3641us[-3641us] +/- 53ms ^* 2600:1700:4e60:b983::123 1 8 377 430 +581us[ +84us] +/- 36ms ^- 2600:1700:5a0f:ee00::314\u0026gt; 2 8 377 58 -1226us[-1226us] +/- 50ms ^- 2603:c020:6:b900:ed2f:b4\u0026gt; 2 9 377 320 +142us[ +142us] +/- 73ms ^ means the source is a server * implies current association with the source.\nPoll\npolling rate (6 means 64 seconds), Reach reachability register (377 indicates a valid response was received), Last sample how long ago the last sample was received, and the offset between the local clock and the source at the last measurement 6. Display the clock performance using the tracking subcommand with chronyc:\n[root@server1 ~]# chronyc tracking Reference ID : 2EA39303 (2600:1700:4e60:b983::123) Stratum : 2 Ref time (UTC) : Sun Jun 16 12:05:45 2024 System time : 286930.187500000 seconds slow of NTP time Last offset : -0.000297195 seconds RMS offset : 2486.306152344 seconds Frequency : 3.435 ppm slow Residual freq : -0.034 ppm Skew : 0.998 ppm Root delay : 0.064471066 seconds Root dispersion : 0.003769779 seconds Update interval : 517.9 seconds Leap status : Normal EXAM TIP: You will not have access to the outside network during the exam. You will need to point your system to an NTP server available on the exam network. Simply comment the default server/pool directive(s) and add a single directive \u0026ldquo;server \u0026lt;hostname\u0026gt;\u0026rdquo; to the file. Replace \u0026lt;hostname\u0026gt; with the NTP server name or its IP address as provided.\ntimedatectl command. # Modify the date, time, and time zone. Outputs the local time, Universal time, RTC time (real-time clock, a battery-backed hardware clock located on the system board), time zone, and the status of the NTP service by default: [root@server10 ~]# timedatectl Local time: Mon 2024-07-22 10:55:11 MST Universal time: Mon 2024-07-22 17:55:11 UTC RTC time: Mon 2024-07-22 17:55:10 Time zone: America/Phoenix (MST, -0700) System clock synchronized: yes NTP service: active RTC in local TZ: no Requires that the NTP/Chrony service is deactivated in order to make time adjustments. Turn off NTP and verify:\n[root@server10 ~]# timedatectl set-ntp false [root@server10 ~]# timedatectl | grep NTP NTP service: inactive Modify the current date and confirm:\n[root@server10 ~]# timedatectl set-time 2024-07-22 [root@server10 ~]# timedatectl Local time: Mon 2024-07-22 00:00:30 MST Universal time: Mon 2024-07-22 07:00:30 UTC RTC time: Mon 2024-07-22 07:00:30 Time zone: America/Phoenix (MST, -0700) System clock synchronized: no NTP service: inactive RTC in local TZ: no Change both date and time in one go:\n[root@server10 ~]# timedatectl set-time \u0026#34;2024-07-22 11:00\u0026#34; [root@server10 ~]# timedatectl Local time: Mon 2024-07-22 11:00:06 MST Universal time: Mon 2024-07-22 18:00:06 UTC RTC time: Mon 2024-07-22 18:00:06 Time zone: America/Phoenix (MST, -0700) System clock synchronized: no NTP service: inactive RTC in local TZ: no Reactivate NTP:\n[root@server10 ~]# timedatectl set-ntp true [root@server10 ~]# timedatectl | grep NTP NTP service: active date command # view or modify the system date and time. View current date and time:\n[root@server10 ~]# date Mon Jul 22 11:03:00 AM MST 2024 Change the date and time:\n[root@server10 ~]# date --set \u0026#34;2024-07-22 11:05\u0026#34; Mon Jul 22 11:05:00 AM MST 2024 Return the system to the current date and time:\n[root@server10 ~]# timedatectl set-ntp false [root@server10 ~]# timedatectl set-ntp true DNS and Time Sync DIY Labs # Lab: Configure Chrony # Install Chrony and mark the service for autostart on reboots. systemctl enable --now chronyd\nEdit the Chrony configuration file and comment all line entries that begin with \u0026ldquo;pool\u0026rdquo; or \u0026ldquo;server\u0026rdquo;.\n[root@server10 ~]# vim /etc/chrony.conf Go to the end of the file, and add a new line \u0026ldquo;server 127.127.1.0\u0026rdquo;.\nStart the Chrony service and run chronyc sources to confirm the binding.\n[root@server10 ~]# systemctl restart chronyd [root@server10 ~]# chronyc sources MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^? 127.127.1.0 0 6 0 - +0ns[ +0ns] +/- 0ns Lab: Modify System Date and Time # Execute the date and timedatectl commands to check the current system date and time. [root@server10 ~]# date Mon Jul 22 11:37:54 AM MST 2024 [root@server10 ~]# timedatectl Local time: Mon 2024-07-22 11:37:59 MST Universal time: Mon 2024-07-22 18:37:59 UTC RTC time: Mon 2024-07-22 18:37:59 Time zone: America/Phoenix (MST, -0700) System clock synchronized: no NTP service: active RTC in local TZ: no Identify the distinctions between the two outputs.\nUse timedatectl and change the system date to a future date.\n[root@server10 ~]# timedatectl set-time 2024-07-23 Failed to set time: Automatic time synchronization is enabled [root@server10 ~]# timedatectl set-ntp false [root@server10 ~]# timedatectl set-time \u0026#34;2024-07-23\u0026#34; Issue the date command and change the system time to one hour ahead of the current time. [root@server10 ~]# date -s \u0026#34;2024-07-22 12:41\u0026#34; Mon Jul 22 12:41:00 PM MST 2024 Observe the new date and time with both commands. [root@server10 ~]# date -s \u0026#34;2024-07-22 12:41\u0026#34; Mon Jul 22 12:41:00 PM MST 2024 [root@server10 ~]# date Mon Jul 22 12:41:39 PM MST 2024 [root@server10 ~]# timedatectl Local time: Mon 2024-07-22 12:41:41 MST Universal time: Mon 2024-07-22 19:41:41 UTC RTC time: Tue 2024-07-23 07:01:41 Time zone: America/Phoenix (MST, -0700) System clock synchronized: no NTP service: inactive RTC in local TZ: no Reset the date and time to the current actual time by disabling and re-enabling the NTP service using the timedatectl command. [root@server10 ~]# timedatectl set-ntp true ","externalUrl":null,"permalink":"/tech/linux/time-synchronization/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eNetwork Time Protocol (NTP)\n    \u003cdiv id=\"network-time-protocol-ntp\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#network-time-protocol-ntp\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNetworking protocol for synchronizing the system clock with remote time servers for accuracy and reliability.\u003c/li\u003e\n\u003cli\u003eHaving steady and exact time on networked systems allows time-sensitive applications, such as authentication and email applications, backup and scheduling tools, financial and billing systems, logging and monitoring software, and file and storage sharing protocols, to function with precision.\u003c/li\u003e\n\u003cli\u003eSends a stream of messages to configured time servers and binds itself to the one with least amount of delay in its responses, the most accurate, and may or may not be the closest distance-wise.\u003c/li\u003e\n\u003cli\u003eClient system maintains a drift in time in a file and references this file for gradual drop in inaccuracy.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eChrony\n    \u003cdiv id=\"chrony\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#chrony\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRHEL 9 implementation of NTP\u003c/li\u003e\n\u003cli\u003eUses the UDP port 123.\u003c/li\u003e\n\u003cli\u003eIf enabled, it starts at system boot and continuously operates to keep the system clock in sync with a more accurate source of time.\u003c/li\u003e\n\u003cli\u003ePerforms well on computers that are occasionally connected to the network, attached to busy networks, do not run all the time, or have variations in temperature.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eChrony is the RHEL implementation of NTP. And it operates on UDP port 123. If you enable it, it starts at system boot and continuously monitors system time and keeps in in sync.\u003c/p\u003e","title":"Time Synchronization","type":"tech"},{"content":"Configure set poe interface ge-0/0/0 disable commit rollback 1 commit ","externalUrl":null,"permalink":"/tech/tools/toggle_poe/","section":"Teches","summary":"\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eConfigure\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eset\u003c/span\u003e poe interface ge-0/0/0 disable\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ecommit\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003erollback \u003cspan class=\"m\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ecommit\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Toggle PoE on a Juniper Switch","type":"tech"},{"content":"Problem Isolation Using the ping Command\nfunctions as part of Layer 3, as a control protocol to assist IP by helping manage the IP network functions.\nPing options\nThe name or IP address of the destination,\nhow many times the command should send an echo request,\nhow long the command should wait (timeout) for an echo reply,\nhow big to make the packets, and many other options.\nExtended ping allows R1’s ping command to use R1’s LAN IP address from within subnet 172.16.1.0/24.\nThis same command could have been issued from the command line as ping 172.16.2.101 source 172.16.1.1.\nR1# ping\nProtocol [ip]:\nTarget IP address: 172.16.2.101\nRepeat count [5]:\nDatagram size [100]:\nTimeout in seconds [2]:\nExtended commands [n]: y\nSource address or interface: 172.16.1.1\nType of service [0]:\nSet DF bit in IP header? [no]:\nValidate reply data? [no]:\nData pattern [0xABCD]:\nLoose, Strict, Record, Timestamp, Verbose[none]:\nSweep range of sizes [n]:\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 172.16.2.101, timeout is 2 seconds:\nPacket sent with a source address of 172.16.1.1\n!!!!!\nSuccess rate is 100 percent (5/5), round-trip min/avg/max = 1/2/4 ms\nProblems to test. ( ping from router or from host?)\nIP ACLs that discard packets based on host A’s IP address but allow packets that match the router’s IP address LAN switch port security that filters A’s frames (based on A’s MAC address) IP routes on routers that happen to match host A’s 172.16.1.51 address, with different routes that match R1’s 172.16.1.1 address Problems with host A’s default router setting If the ping works, it confirms the following, which rules out some potential issues:\nThe host with address 172.16.1.51 replied.\nThe LAN can pass unicast frames from R1 to host 172.16.1.51 and vice versa.\nYou can reasonably assume that the switches learned the MAC addresses of the router and the host, adding those to the MAC address tables.\nHost A and Router R1 completed the ARP process and list each other in their respective Address Resolution Protocol (ARP) tables.\nIP addressing problem: Host A could be statically configured with the wrong IP address. DHCP problems: If you are using Dynamic Host Configuration Protocol (DHCP), many problems could exist. VLAN trunking problems: The router could be configured for 802.1Q trunking, when the switch is not (or vice versa). LAN problems: A wide variety of issues could exist with the Layer 2 switches, preventing any frames from flowing between host A and the router. Testing LAN Neighbors with Extended Ping\nan extended ping can test the host’s default router setting. Both tests can be useful, especially for problem isolation, because\nIf a standard ping of a local LAN host works… But an extended ping of the same LAN host fails… The problem likely relates somehow to the host’s default router setting. Testing WAN Neighbors with Standard Ping\nA successful ping of the IP address on the other end of an Ethernet WAN link that sits between two routers confirms several specific facts, such as the following:\nBoth routers’ WAN interfaces are in an up/up state.\nThe Layer 1 and 2 features of the link work.\nThe routers believe that the neighboring router’s IP address is in the same subnet.\nInbound ACLs on both routers do not filter the incoming packets, respectively.\nThe remote router is configured with the expected IP address (172.16.4.2 in this case).\nUsing Ping with Names and with IP Addresses\nProblem Isolation Using the traceroute Command\nping vs Tracert\nBoth send messages in the network to test connectivity. Both rely on other devices to send back a reply. Both have wide support on many different operating systems. Both can use a hostname or an IP address to identify the destination. On routers, both have a standard and extended version, allowing better testing of the reverse route. Standard and Extended traceroute\na standard traceroute command chooses an IP address based on the outgoing interface for the packet sent by the command. So, in this example, the packets sent by R1 come from source IP address 172.16.4.1, R1’s G0/0/0 IP address.\nHost OS traceroute commands usually create ICMP echo requests. The Cisco IOS traceroute command instead creates IP packets with a UDP header. This bit of information may seem trivial at this point. However, note that an ACL may actually filter the traffic from a host’s traceroute messages but not the router traceroute command, or vice versa.\nTelnet and SSH\n","externalUrl":null,"permalink":"/tech/networking/troubleshootingrouting/","section":"Teches","summary":"\u003cp\u003eProblem Isolation Using the ping Command\u003c/p\u003e\n\u003cp\u003efunctions as part of Layer 3, as a control protocol to assist IP by helping manage the IP network functions.\u003c/p\u003e\n\u003cp\u003ePing options\u003c/p\u003e\n\u003cp\u003eThe name or IP address of the destination,\u003c/p\u003e","title":"Troubleshooting Routing Issues","type":"tech"},{"content":" Using Ansible Tags # https://docs.ansible.com/projects/ansible/latest/playbook_guide/playbooks_tags.html\nA tag is a label that is applied to a task or another item like a block or a play. Use ansible-playbook --tags or ansible-playbook --skip-tags to specify which tags need to be executed.\nUsing tags in a Playbook\n--- - name: using tags example hosts: all vars: service: - vsftpd - httpd tasks: - yum: name: - httpd - vsftpd state: present tags: - install - service: name: \u0026#34;{{ item }}\u0026#34; state: started enabled: yes loop: \u0026#34;{{ services }}\u0026#34; tags: - services ansible-playbook --tags \u0026quot;install\u0026quot; listing1115.yaml Would run only the tasks that are tagged with \u0026ldquo;install\u0026rdquo;\nTags can be applied to many structures, such as imported plays, tasks, and roles.\nTags cannot be applied on items that are dynamically included (instead of imported), using include_roles or include_tasks.\nYou may apply the same tag multiple times.\nAllows you to define groups of tasks, where multiple tasks are configured with the same tag. Lets you run a specific part of the requested configuration. Get an overview of tags used: ansible-playbook --list-tasks --list-tags\nWhen working with tags, you can use some special tags.\nSpecial Tags: always - Make sure task always runs unless specifically skipped with \u0026ndash;skip-tags always never - Never runs a task, unless it is specifically requested tagged - Runs all tagged tasks untagged - Runs all untagged tasks all - Runs all tasks\nSet a debug tag to easily identify tasks that should be run only if you specifically want to run debug tasks as well. If combined with the never tag, the task that is tagged with the debug,never tasks runs only if the debug tag is specifically requested. Then run debug tasks with ansible-playbook --tags all,debug command.\n--- - name: using assert to check if volume group vgdata exists hosts: all tasks: - name: check if vgdata exists command: vgs vgdata register: vg_result ignore_errors: true - name: show vg_result variable debug: var: vg_result tags: [ never, debug ] - name: print a message assert: that: - vg_result.rc == 0 fail_msg: volume group not found success_msg: volume group was found Apply tags to an entire play.\n- hosts: webservers tags: deploy roles: - role: tomcat tags: [\u0026#39;tomcat\u0026#39;, \u0026#39;app\u0026#39;] tasks: - name: Notify on completion local_action: module: osx_say msg: \u0026#34;{{inventory_hostname}} is finished!\u0026#34; voice: Zarvox tags: - notifications - say - include: foo.yaml tags: foo Assuming we save the above playbook as tags.yml, you could run the command below to only run the tomcat role and the Notify on completion task:\nansible-playbook tags.yml --tags \u0026amp;\u0026quot;tomcat,say\u0026quot;\nIf you want to exclude anything tagged with notifications, you can use --skip-tags. `ansible-playbook tags.yml \u0026ndash;skip-tags \u0026ldquo;notifications\u0026rdquo;\nThis is incredibly handy if you have a decent tagging structure; when you want to only run a particular portion of a playbook, or one play in a series (or, alternatively, if you want to exclude a play or included tasks), then it\u0026rsquo;s easy to do using --tags or --skip-tags.\nThere is one caveat when adding one or multiple tags using the tags option in a playbook: you can use the shorthand tags: tagname when adding just one tag, but if adding more than one tag, you have to use YAML\u0026rsquo;s list syntax, for example:\n# Shorthand list tags: [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;] # Explicit list tags: - one - two - three # Not valid tags: one, two, three Use tags for larger playbooks, especially with individual roles and plays. Avoid adding tags to individual tasks or includes (reduces visual clutter) unless debugging a set of tasks. Find a tagging style that suits your needs and lets you run (or not run) the specific parts of your playbooks you desire.\n","externalUrl":null,"permalink":"/tech/ansible/using-ansible-tags/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eUsing Ansible Tags\n    \u003cdiv id=\"using-ansible-tags\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-ansible-tags\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003e\u003ca\n  href=\"https://docs.ansible.com/projects/ansible/latest/playbook_guide/playbooks_tags.html\"\n    target=\"_blank\"\n  \u003ehttps://docs.ansible.com/projects/ansible/latest/playbook_guide/playbooks_tags.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA tag is a label that is applied to a task or another item like a block or a play.\nUse \u003ccode\u003eansible-playbook --tags\u003c/code\u003e or \u003ccode\u003eansible-playbook --skip-tags\u003c/code\u003e to specify which tags need to be executed.\u003c/p\u003e","title":"Using Ansible Tags","type":"tech"},{"content":" Using Loops and Items # Some modules enable you to provide a list that needs to be processed. Many modules don\u0026rsquo;t, and in these cases, it makes sense to use a loop mechanism to iterate over a list of items. Take, for instance, the yum module. While specifying the names of packages, you can use a list of packages. If, however, you want to do something similar for the service module, you find out that this is not possible. That is where loops come in. Working with Loops # Install software packages using the yum module and then ensures that services installed from these packages are started using the service module:\n--- - name: install and start services hosts: ansible1 tasks: - name: install packages yum: name: - vsftpd - httpd - samba state: latest - name: start the services service: name: \u0026#34;{{ item }}\u0026#34; state: started enabled: yes loop: - vsftpd - httpd - smb A loop is defined at the same level as the service module.\nThe loop has a list of services in a list (array) statement\nItems in the loop can be accessed by using the system internal variable item.\nAt no place in the playbook is there a definition of the variable item; the loop takes care of this.\nWhen considering whether to use a loop, you should first investigate whether a module offers support for providing lists as values to the keys that are used.\nIf this is the case, just provide a list, as all items in the list can be considered in one run of the module.\nIf not, define the list using loop and provide \u0026quot;{{ item }}\u0026quot; as the value to the key.\nWhen using loop, the module is activated again on each iteration.\nUsing Loops on Variables # Although it\u0026rsquo;s possible to define a loop within a task, it\u0026rsquo;s not the most elegant way. To create a flexible environment where static code is separated from dynamic site-specific parameters, it\u0026rsquo;s a much better idea to define loops outside the static code, in variables. When you define loops within a variable, all the normal rules for working with variables apply: The variables can be defined in the play header, using an include file, or as host/hostgroup variables. Include the loop from a variable:\n--- - name: install and start services hosts: ansible1 vars: services: - vsftpd - httpd - smb tasks: - name: install packages yum: name: - vsftpd - httpd - samba state: latest - name: start the services service: name: \u0026#34;{{ item }}\u0026#34; state: started enabled: yes loop: \u0026#34;{{ services }}\u0026#34; Using Loops on Multivalued Variables # An item can be a simple list, but it can also be presented as a multivalued variable, as long as the multivalued variable is presented as a list.\nUse variables that are imported from the file vars/users-list:\nusers: - username: linda homedir: /home/linda shell: /bin/bash groups: wheel - username: lisa homedir: /home/lisa shell: /bin/bash groups: users - username: anna homedir: /home/anna shell: /bin/bash groups: users Use the list in a playbook:\n--- - name: create users using a loop from a list hosts: ansible1 vars_files: vars/users-list tasks: - name: create users user: name: \u0026#34;{{ item.username }}\u0026#34; state: present groups: \u0026#34;{{ item.groups }}\u0026#34; shell: \u0026#34;{{ item.shell }}\u0026#34; loop: \u0026#34;{{ users }}\u0026#34; Working with multivalued variables is possible, but the variables in that case must be presented as a list; using dictionaries is not supported. The only way to loop over dictionaries is to use the dict2items filter. Use of filters is not included in the RHCE topics and for that reason is not explained further here. You can look up \u0026ldquo;Iterating over a dictionary\u0026rdquo; in the Ansible documentation for more information. Understanding with_items # Since Ansible 2.5, using loop has been the command way to iterate over the values in a list. In earlier versions of Ansible, the with_keyword statement was used instead. In this approach, the keyword is replaced with the name of an Ansible look-up plug-in, but the rest of the syntax is very common. Will be deprecated in a future version of Ansible. With_keyword Options Overview with_items\nUsed like loop to iterate over values in a list with_file Used to iterate over a list of filenames on the control node with_sequence Used to generate a list of values based on a numeric sequence Loop over a list using with_keyword:\n--- - name: install and start services hosts: ansible1 vars: services: - vsftpd - httpd - smb tasks: - name: install packages yum: name: - vsftpd - httpd - samba state: latest - name: start the services service: name: \u0026#34;{{ item }}\u0026#34; state: started enabled: yes with_items: \u0026#34;{{ services }}\u0026#34; Lab: Working with loop # 1. Use your editor to define a variables file with the name vars/packages and the following contents:\npackages: - name: httpd state: absent - name: vsftpd state: installed - name: mysql-server state: latest 2. Use your editor to define a playbook with the name exercise71.yaml and create the play header:\n- name: manage packages using a loop from a list hosts: ansible1 vars_files: vars/packages tasks: 3. Continue the playbook by adding the yum task that will manage the packages, using the packages variable as defined in the vars/packages variable include file:\n- name: manage packages using a loop from a list hosts: ansible1 vars_files: vars/packages tasks: - name: install packages yum: name: \u0026#34;{{ item.name }}\u0026#34; state: \u0026#34;{{ item.state }}\u0026#34; loop: \u0026#34;{{ packages }}\u0026#34; 4. Run the playbook using ansible-playbook exercise71.yaml, and observe the results. In the results you should see which packages it is trying to manage and in which state it is trying to get the packages.\n","externalUrl":null,"permalink":"/tech/ansible/using-loops-and-items/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eUsing Loops and Items\n    \u003cdiv id=\"using-loops-and-items\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-loops-and-items\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSome modules enable you to provide a list that needs to be processed.\u003c/li\u003e\n\u003cli\u003eMany modules don\u0026rsquo;t, and in these cases, it makes sense to use a loop mechanism to iterate over a list of items.\u003c/li\u003e\n\u003cli\u003eTake, for instance, the yum module. While specifying the names of packages, you can use a list of packages.\u003c/li\u003e\n\u003cli\u003eIf, however, you want to do something similar for the service module, you find out that this is not possible.\u003c/li\u003e\n\u003cli\u003eThat is where loops come in.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 class=\"relative group\"\u003eWorking with Loops\n    \u003cdiv id=\"working-with-loops\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#working-with-loops\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cp\u003eInstall software packages using the yum module and then ensures that services installed from these packages are started using the service module:\u003c/p\u003e","title":"Using Loops and Items","type":"tech"},{"content":" Using Modules for Troubleshooting and Testing # Modules used for troubleshooting\ndebug - Write debug information. Useful for checking variable values uri - Test answer coming from any URL. Useful for API calls fail - Uses when conditional to determine when a module should be considered failing script - Allows you to execute a shell script on the managed host stat - Gathers status information about files assert - tests whether the expected result is present and otherwise fails\nUsing the Debug Module # The debug module is useful to visualize what is happening at a certain point in a playbook. It works with two arguments: the msg argument can be used to print a message, and the var argument can be used to print the value of a variable. Notice that when you use the var argument, the variable does not have to be referred to using the usual {{ varname }} structure, just use varname instead. If variables are used in the msg argument, they must be referred to the normal way, using the {{ varname }} syntax.\nBecause you have already seen the debug module in action in numerous examples in Chapters 6, 7, and 8 of this book, no new examples are included here.\ndebug: Prints statements during execution. Used for debugging variables or expressions without stopping a playbook.\nPrint out the value of the ansible_facts variable:\ndebug: var: ansible_facts Using the uri Module # uri: Interacts with basic http and https web services. (Verify connectivity to a web server +9)\nTest httpd accessibility:\nuri: url: http://ansible1 Show result of the command while running the playbook:\nuri: url: http://ansible1 return_content: yes Show the status code that signifies the success of the request:\nuri: url: http://ansible1 status_code: 200 The best way to learn how to work with these modules is to look at some examples. Listing 11-7 shows an example where the uri module is used.\nListing 11-7 Using the uri Module\n::: pre_1 \u0026mdash; - name: test webserver access hosts: localhost become: no tasks: - name: connect to the web server uri: url: http://ansible2.example.com return_content: yes register: this failed_when: \u0026ldquo;’welcome’ not in this.content\u0026rdquo; - debug: var: this.content :::\nThe playbook in Listing 11-7 uses the uri module to connect to a web server. The return_content argument captures the web server content, which is stored in a variable using register. Next, the failed_when statement makes this module fail if the text \u0026ldquo;welcome\u0026rdquo; is not in the registered variable. For debugging purposes, the debug module is used to show the contents of the variable.\nIn Listing 11-8 you can see the partial result of running this playbook. Notice that the playbook does not generate a failure because the default web page that is shown by the Apache web server contains the text \u0026ldquo;welcome.\u0026rdquo;\nListing 11-8 ansible-playbook listing117.yaml Command Result\n[ansible@control rhce8-book]$ ansible-playbook listing117.yaml PLAY [test webserver access] *************************************************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [connect to the web server] *********************************************** ok: [localhost] TASK [debug] ******************************************************************* ok: [localhost] =\u0026gt; { \u0026#34;this.content\u0026#34;: \u0026#34; PLAY RECAP ********************************************************************* localhost : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Using the uri module can be useful to perform a simple test to check whether a web server is available, but you can also use it to check accessibility or returned information from an API endpoint.\nUsing the stat Module # You can use the stat module to check on the status of files. Although this module can be useful for checking on the status of just a few files, it\u0026rsquo;s not a file system integrity checker that was developed to check file status on a large scale. If you need large-scale file system integrity checking, you should use Linux utilities such as aide.\nThe stat module is useful in combination with register. In this use, the stat module is used to register the status of a specific file, and in a when statement, a check can be done to see whether the file status is not as expected. In combination with the fail module, you can use this module to generate a failure and error message if the file does not meet the expected status. Listing 11-9 shows an example, and Listing 11-10 shows the resulting output, where you can see that the fail module fails the playbook because the file owner is not root.\nListing 11-9 Using stat to Check Expected File Status\n::: pre_1 \u0026mdash; - name: create a file hosts: all tasks: - file: path: /tmp/statfile state: touch owner: ansible\n- name: check file status hosts: all tasks: - stat: path: /tmp/statfile register: stat_out - fail: msg: \u0026quot;/tmp/statfile file owner not as expected\u0026quot; when: stat_out.stat.pw_name != ’root’ :::\nListing 11-10 ansible-playbook listing119.yaml Command Result\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing119.yaml\nPLAY [create a file] *********************************************************** TASK [Gathering Facts] ********************************************************* ok: [ansible2] ok: [ansible1] ok: [ansible3] ok: [ansible4] fatal: [ansible6]: UNREACHABLE! =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;Failed to connect to the host via ssh: ansible@ansible6: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\u0026quot;, \u0026quot;unreachable\u0026quot;: true} fatal: [ansible5]: UNREACHABLE! =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;Failed to connect to the host via ssh: ssh: connect to host ansible5 port 22: No route to host\u0026quot;, \u0026quot;unreachable\u0026quot;: true} TASK [file] ******************************************************************** changed: [ansible2] changed: [ansible1] changed: [ansible3] changed: [ansible4] PLAY [check file status] ******************************************************* TASK [Gathering Facts] ********************************************************* ok: [ansible1] ok: [ansible2] ok: [ansible3] ok: [ansible4] TASK [stat] ******************************************************************** ok: [ansible2] ok: [ansible1] ok: [ansible3] ok: [ansible4] TASK [fail] ******************************************************************** fatal: [ansible2]: FAILED! =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;/tmp/statfile file owner not as expected\u0026quot;} fatal: [ansible1]: FAILED! =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;/tmp/statfile file owner not as expected\u0026quot;} fatal: [ansible3]: FAILED! =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;/tmp/statfile file owner not as expected\u0026quot;} fatal: [ansible4]: FAILED! =\u0026gt; {\u0026quot;changed\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;/tmp/statfile file owner not as expected\u0026quot;} PLAY RECAP ********************************************************************* ansible1 : ok=4 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 ansible2 : ok=4 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 ansible3 : ok=4 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 ansible4 : ok=4 changed=1 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 ansible5 : ok=0 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 ansible6 : ok=0 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 :::\nUsing the assert Module # The assert module is a bit like the fail module. You can use it to perform a specific conditional action. The assert module works with a that option that defines a list of conditionals. If any one of these conditionals is false, the task fails, and if all the conditionals are true, the task is successful. Based on the success or failure of a task, the module uses the success_msg or fail_msg options to print a message. Listing 11-11 shows an example that uses the assert module.\nListing 11-11 Using the assert Module\n::: pre_1 \u0026mdash; - hosts: localhost vars_prompt: - name: filesize prompt: \u0026ldquo;specify a file size in megabytes\u0026rdquo; tasks: - name: check if file size is valid assert: that: - \u0026ldquo;{{ (filesize | int) \u0026lt;= 100 }}\u0026rdquo; - \u0026ldquo;{{ (filesize | int) \u0026gt;= 1 }}\u0026rdquo; fail_msg: \u0026ldquo;file size must be between 0 and 100\u0026rdquo; success_msg: \u0026ldquo;file size is good, let\\’s continue\u0026rdquo; - name: create a file command: dd if=/dev/zero of=/bigfile bs=1 count={{ filesize }} :::\nThe example in Listing 11-11 contains a few new items. As you can see, the play header starts with a vars_prompt. This defines a variable named filesize, which is based on the input provided by the user. This filesize variable is next used by the assert module. The that statement contains a list in which two conditions are stated. If specified like this, all conditions stated in the that condition must be true. So you are looking for filesize to be equal to or bigger than 1, and smaller than or equal to 100.\nBefore this can be done, one little problem needs to be managed: when vars_prompt is used, the variable type is set to be a string by default. This means that a statement like\n**filesize left caret= 100** would fail with a type mismatch. That is why a Jinja2 filter is used to convert the variable type from string to integer.\nFilters are a powerful feature provided by the Jinja2 templating language and can be used in Ansible to modify variables before processing. For more information about filters, see https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html. The int filter can be used to convert the value of a string variable to an integer. To do this, you need to rewrite the entire variable as a Jinja2 operation, which is done using \u0026quot;{{ (filesize | int) left caret= 100 }}\u0026quot;.\nIn this line, the entire string is written as a variable. The variable is further treated in a Jinja2 context. In this context, the part (filesize | int) ensures that the string is converted to an integer, which makes it possible to check if the value is smaller than 100.\nWhen you run the code in Listing 11-11, the result shown in Listing 11-12 is produced.\nListing 11-12 ansible-playbook listing1111.yaml Output\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing1111.yaml\nPLAY [localhost] ***************************************************************** TASK [Gathering Facts] *********************************************************** ok: [localhost] TASK [check if file size is valid] *********************************************** fatal: [localhost]: FAILED! =\u0026gt; { \u0026quot;assertion\u0026quot;: \u0026quot;filesize left caret= 100\u0026quot;, \u0026quot;changed\u0026quot;: false, \u0026quot;evaluated_to\u0026quot;: false, \u0026quot;msg\u0026quot;: \u0026quot;file size must be between 0 and 100\u0026quot; } PLAY RECAP *********************************************************************** localhost : ok=1 changed=0 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 :::\nAs you can see, the task that is defined with the assert module fails because the variable has a value that is not between the minimum and maximum sizes that are defined.\nUnderstanding the need for using the filter to convert the variable type might not be easy. So, let\u0026rsquo;s also look at Listing 11-13, which shows an example of a playbook that will fail. You can see its behavior in Listing 11-14, where the playbook is executed.\nListing 11-13 Failing Version of the Listing 11-11 Playbook\n::: pre_1 \u0026mdash; - hosts: localhost vars_prompt: - name: filesize prompt: \u0026ldquo;specify a file size in megabytes\u0026rdquo; tasks: - name: check if file size is valid assert: that: - filesize \u0026lt;= 100 - filesize \u0026gt;= 1 fail_msg: \u0026ldquo;file size must be between 0 and 100\u0026rdquo; success_msg: \u0026ldquo;file size is good, let\\’s continue\u0026rdquo; - name: create a file command: dd if=/dev/zero of=/bigfile bs=1 count={{ filesize }} :::\nListing 11-14 ansible-playbook listing1113.yaml Failing Result\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing1113.yaml specify a file size in megabytes:\nPLAY [localhost] ***************************************************************** TASK [Gathering Facts] *********************************************************** ok: [localhost] TASK [check if file size is valid] *********************************************** fatal: [localhost]: FAILED! =\u0026gt; {\u0026quot;msg\u0026quot;: \u0026quot;The conditional check ’filesize left caret= 100’ failed. The error was: Unexpected templating type error occurred on ({% if filesize left caret= 100 %} True {% else %} False {% endif %}): ’left caret=’ not supported between instances of ’str’ and ’int’\u0026quot;} PLAY RECAP *********************************************************************** localhost : ok=1 changed=0 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 :::\nAs you can see, the code in Listing 11-13 fails because the \\left caret= test is not supported between a string and an integer.\nIn Exercise 11-2 you work with some of the modules discussed in this section.\n::: box Exercise 11-2 Using Modules for Troubleshooting\n1. Open your editor to create the file exercise112.yaml and define the play header:\n--- - name: using assert to check if volume group vgdata exists hosts: all tasks: 2. Add a task that uses the command vgs vgdata to check whether a volume group with the name vgdata exists. The task should use register to register the command result, and it should continue if this is not the case.\n- name: check if vgdata exists command: vgs vgdata register: vg_result ignore_errors: true 3. To make it easier to use assert in the next step on the right variable, include a debug task to show the value of the variable:\n- name: show vg_result variable debug: var: vg_result 4. Add a task to print a success or failure message, depending on the result of the vgs command from the first task:\n- name: print a message assert: that: - vg_result.rc == 0 fail_msg: volume group not found success_msg: volume group was found 5. Use the command ansible-playbook exercise112.yaml to verify its contents. Assuming that the LVM Volume Group vgdata was not found, it should print \u0026ldquo;volume group not found.\u0026rdquo;\n6. Change the playbook to verify that it will print the success_msg if the requested volume group was found. You can do so by having it run the command vgs cl, which on CentOS 8 should give a positive result. :::\nTroubleshooting Common Scenarios # Apart from the problems that may arise in playbooks, another type of error relates to connectivity issues. To connect to managed hosts, SSH must be configured correctly, and also authentication and privilege escalation must work as expected.\nAnalyzing Connectivity Issues # To be able to connect to a managed host, you need to have an IP network connection. Apart from that, you need to make sure that the host has been set up correctly:\n• The SSH service needs to be accessible on the remote host.\n• Python must be installed.\n• Privilege escalation needs to be set up.\nApart from these, inventory settings may be specified to indicate how to connect to a remote host. Normally, the inventory contains a host name only. If a host resolves to multiple IP addresses, you may want to specify how exactly the remote host must be connected to. The ansible_host parameter can be configured to do so. In inventory, for instance, you may include the following line to ensure that your host is connected in the right way:\nansible5.example.com ansible_host=192.168.4.55 Notice that this setting makes sense only in an environment where a host can be reached on multiple different IP addresses.\nTo test connectivity to remote hosts, you can use the ping module. It checks for IP connectivity, accessibility of the SSH service, sudo privilege escalation, and the availability of a Python stack. The ping module does not take any arguments. Listing 11-18 shows the result of running on the ad hoc command ansible all -m ping where hosts that are available send \u0026ldquo;pong\u0026rdquo; as a reply, and for hosts that are not available, you see why they are not available.\nListing 11-18 Verifying Connectivity Using the ping Module\n::: pre_1 [ansible@control rhce8-book]$ ansible all -m ping ansible2 | SUCCESS =\u0026gt; { \u0026ldquo;ansible_facts\u0026rdquo;: { \u0026ldquo;discovered_interpreter_python\u0026rdquo;: \u0026ldquo;/usr/libexec/platform-python\u0026rdquo; }, \u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;ping\u0026rdquo;: \u0026ldquo;pong\u0026rdquo; } ansible1 | SUCCESS =\u0026gt; { \u0026ldquo;ansible_facts\u0026rdquo;: { \u0026ldquo;discovered_interpreter_python\u0026rdquo;: \u0026ldquo;/usr/libexec/platform-python\u0026rdquo; }, \u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;ping\u0026rdquo;: \u0026ldquo;pong\u0026rdquo; } ansible3 | SUCCESS =\u0026gt; { \u0026ldquo;ansible_facts\u0026rdquo;: { \u0026ldquo;discovered_interpreter_python\u0026rdquo;: \u0026ldquo;/usr/libexec/platform-python\u0026rdquo; }, \u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;ping\u0026rdquo;: \u0026ldquo;pong\u0026rdquo; } ansible4 | FAILED! =\u0026gt; { \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;Missing sudo password\u0026rdquo; } :::\nAnalyzing Authentication Issues # A few settings play a role in authentication on the remote host to execute tasks:\n• The remote_user setting determines which user account to use on the managed nodes.\n• SSH keys need to be configured for the remote_user to enable smooth authentication.\n• The become parameter needs to be set to true.\n• The become_user needs to be set to the root user account.\n• Linux sudo needs to be set up correctly.\nIn Exercise 11-4 you work on troubleshooting some common scenarios.\n::: box Exercise 11-4 Troubleshooting Connectivity Issues\n1. Use an editor to create the file exercise114-1.yaml and give it the following contents:\n--- - name: remove user from wheel group hosts: ansible4 tasks: - user: name: ansible groups: ’’ 2. Run the playbook using ansible-playbook exercise114-1.yaml and use ansible ansible4 -m reboot to reboot node ansible4.\n3. Once the reboot is completed, use ansible all -m ping to verify connectivity. Host ansible4 should give a \u0026ldquo;Missing sudo password\u0026rdquo; error.\n4. Type ansible ansible4 -m raw -a \u0026ldquo;usermod -aG wheel ansible\u0026rdquo; -u root -k to make user ansible a member of the group wheel again.\n5. Repeat the ansible all -m ping command. You should now be able to connect normally to the host ansible4 again. :::\nManaging Ansible Errors and Logs # Using Check Mode # Before actually running a playbook in a way that all changes are implemented, you can start the playbooks in check mode. To do this, you use the --check or -C command-line argument to the ansible or ansible-playbook command. The effect of using check mode is that changes that would have been made are shown but not executed. You should realize, though, that check mode is not supported in all cases. You will, for instance, have problems with check mode if it is applied to conditionals, where a specific task can do its work only after a preceding task has made some changes. Also, to successfully use check mode, the modules need to support it, but some don\u0026rsquo;t. Modules that don\u0026rsquo;t support check mode don\u0026rsquo;t show any result while running check mode, but also they don\u0026rsquo;t make any changes.\nApart from the command-line argument, you can use check_mode: yes or check_mode: no with any task in a playbook. If check_mode: yes is used, the task always runs in check mode (and does not implement any changes), regardless of the use of the --check option. If a task has check_mode: no set, it never runs in check mode and just does its work, even if the ansible-playbook command is used with the --check option. Using check mode on individual tasks might be a good idea if using check mode on the entire playbook gives unpredicted results: you can enable it on just a couple of tasks to ensure that they run successfully before proceeding to the next set of tasks. Notice that using check_mode: no for specific tasks can be dangerous; these tasks will make changes, even if the entire playbook was started with the --check option!\n::: note\nNote\nThe check_mode argument is a replacement for the always_run option that was used in Ansible 2.5 and earlier. In current Ansible versions, you should not use always_run anymore.\nAnother option that is commonly used with the --check option is --diff. This option reports changes to template files without actually applying them. Listing 11-1 shows a sample playbook, Listing 11-2 shows the template that it is processing, and Listing 11-3 shows the result of running this playbook with the ansible-playbook listing111.yaml --check --diff command.\n--- - name: simple template example hosts: ansible2 tasks: - template: src: listing112.j2 dest: /etc/issue ::: **Listing 11-2** Sample Template File ::: pre_1 {# /etc/issue #} Welcome to {{ ansible_facts[’hostname’] }} ::: **Listing 11-3** Running the listing111.yaml Sample Playbook ::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing111.yaml --check --diff PLAY [simple template example] ************************************************* TASK [Gathering Facts] ********************************************************* ok: [ansible2] TASK [template] **************************************************************** --- before +++ after: /home/ansible/.ansible/tmp/ansible-local-4493uxbpju1e/tmpm5gn7crg/listing112.j2 @@ -0,0 +1,3 @@ +Welcome to ansible2 + + changed: [ansible2] PLAY RECAP ********************************************************************* ansible2 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Understanding Output # When you run the ansible-playbook command, output is generated. You\u0026rsquo;ve probably had a glimpse of it before, but let\u0026rsquo;s look at the output in a more structured way now. Listing 11-4 shows some typical sample output generated by running the ansible-playbook command.\nListing 11-4 ansible-playbook Command Output\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook listing52.yaml\nPLAY [install start and enable httpd] ****************************************** TASK [Gathering Facts] ********************************************************* ok: [ansible2] ok: [ansible1] ok: [ansible3] ok: [ansible4] TASK [install package] ********************************************************* changed: [ansible2] changed: [ansible1] changed: [ansible3] changed: [ansible4] TASK [start and enable service] ************************************************ changed: [ansible2] changed: [ansible1] changed: [ansible3] changed: [ansible4] PLAY RECAP ********************************************************************* ansible1 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible2 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible3 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 ansible4 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 :::\nIn the output of any ansible-playbook command, you can see different items:\n{width=\u0026ldquo;64\u0026rdquo; height=\u0026ldquo;51\u0026rdquo;}\n• An indicator of the play that is started\n• If not disabled, the Gathering Facts task that is executed for each play\n• Each individual task, including the task name if that was specified\n• The Play Recap, which summarizes the play results\nIn the Play Recap, different results can be shown. Table 11-2 gives an overview.\n::: group Table 11-2 Playbook Recap Overview\n{width=\u0026ldquo;941\u0026rdquo; height=\u0026ldquo;338\u0026rdquo;} :::\nAs discussed before, when you use the ansible-playbook command, you can increase the output verbosity level using one or more -v options. Table 11-3 lists what these options accomplish. For generic troubleshooting, you might want to consider using -vv, which shows output as well as input data. In particular cases using the -vvv option can be useful because it adds connection information as well.\nThe -vvvv option just brings too much information in many cases but can be useful if you need to analyze which exact scripts are executed or whether any problems were encountered in privilege escalation. Make sure to capture the output of any command that runs with -vvvv to a text file, though, so that you can read it in an easy way. Even for a simple playbook, it can easily generate more than 10 screens of output.\n::: group Table 11-3 Verbosity Options Overview\n{width=\u0026ldquo;941\u0026rdquo; height=\u0026ldquo;209\u0026rdquo;} :::\nIn Listing 11-5 you can see the output of a small playbook that runs different tasks on the managed hosts. Listing 11-5 shows details about execution of one task on host ansible4, and as you can see, it goes deep in the amount of detail that is shown. One component is worth looking at, and that is the escalation succeeded that you can see in the output. This means that privilege escalation was successful and tasks were executed because become_user was defined in ansible.cfg. Failing privilege escalation is one of the common reasons why playbook execution may go wrong, which is why it\u0026rsquo;s worth keeping an eye on this indicator.\nListing 11-5 Analyzing Partial -vvvv Output\n\u0026lt;ansible4\u0026gt; ESTABLISH SSH CONNECTION FOR USER: ansible \u0026lt;ansible4\u0026gt; SSH: EXEC ssh -vvv -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ’User=\u0026#34;ansible\u0026#34;’ -o ConnectTimeout=10 -o ControlPath=/home/ansible/.ansible/cp/859d5267e3 ansible4 ’/bin/sh -c ’\u0026#34;’\u0026#34;’chmod u+x /home/ansible/.ansible/tmp/ansible-tmp-1587544652.4716983-118789810824208/ /home/ansible/.ansible/tmp/ansible-tmp-1587544652.4716983-118789810824208/AnsiballZ_systemd.py \u0026amp;\u0026amp; sleep 0’\u0026#34;’\u0026#34;’’ Escalation succeeded \u0026lt;ansible4\u0026gt; (0, b’’, b\u0026#34;OpenSSH_8.0p1, OpenSSL 1.1.1c FIPS 28 May 2019\\r\\ndebug1: Reading configuration data /etc/ssh/ssh_config\\r\\ndebug3: /etc/ssh/ssh_config line 51: Including file /etc/ssh/ssh_config.d/05-redhat.conf depth 0\\r\\ndebug1: Reading configuration data /etc/ssh/ssh_config.d/05-redhat.conf\\r\\ndebug2: checking match for ’final all’ host ansible4 originally ansible4\\r\\ndebug3: /etc/ssh/ssh_config.d/05-redhat.conf line 3: not matched ’final’\\r\\ndebug2: match not found\\r\\ndebug3: /etc/ssh/ssh_config.d/05-redhat.conf line 5: Including file /etc/crypto-policies/back-ends/openssh.config depth 1 (parse only)\\r\\ndebug1: Reading configuration data /etc/crypto-policies/back-ends/openssh.config\\r\\ndebug3: gss kex names ok: [gss-gex-sha1-,gss-group14-sha1-]\\r\\ndebug3: kex names ok: [curve25519-sha256,curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group14-sha256,diffie-hellman-group16-sha512,diffie-hellman-group18-sha512,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1]\\r\\ndebug1: configuration requests final Match pass\\r\\ndebug1: re-parsing configuration\\r\\ndebug1: Reading configuration data /etc/ssh/ssh_config\\r\\ndebug3: /etc/ssh/ssh_config line 51: Including file /etc/ssh/ssh_config.d/05-redhat.conf depth 0\\r\\ndebug1: Reading configuration data /etc/ssh/ssh_config.d/05-redhat.conf\\r\\ndebug2: checking match for ’final all’ host ansible4 originally ansible4\\r\\ndebug3: /etc/ssh/ssh_config.d/05-redhat.conf line 3: matched ’final’\\r\\ndebug2: match found\\r\\ndebug3: /etc/ssh/ssh_config.d/05-redhat.conf line 5: Including file /etc/crypto-policies/back-ends/openssh.config depth 1\\r\\ndebug1: Reading configuration data /etc/crypto-policies/back-ends/openssh.config\\r\\ndebug3: gss kex names ok: [gss-gex-sha1-,gss-group14-sha1-]\\r\\ndebug3: kex names ok: [curve25519-sha256,curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group14-sha256,diffie-hellman-group16-sha512,diffie-hellman-group18-sha512,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1]\\r\\ndebug1: auto-mux: Trying existing master\\r\\ndebug2: fd 4 setting O_NONBLOCK\\r\\ndebug2: mux_client_hello_exchange: master version 4\\r\\ndebug3: mux_client_forwards: request forwardings: 0 local, 0 remote\\r\\ndebug3: mux_client_request_session: entering\\r\\ndebug3: mux_client_request_alive: entering\\r\\ndebug3: mux_client_request_alive: done pid = 4764\\r\\ndebug3: mux_client_request_session: session request sent\\r\\ndebug3: mux_client_read_packet: read header failed: Broken pipe\\r\\ndebug2: Received exit status from master 0\\r\\n\u0026#34;) \u0026lt;ansible4\u0026gt; ESTABLISH SSH CONNECTION FOR USER: ansible \u0026lt;ansible4\u0026gt; SSH: EXEC ssh -vvv -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ’User=\u0026#34;ansible\u0026#34;’ -o ConnectTimeout=10 -o ControlPath=/home/ansible/.ansible/cp/859d5267e3 -tt ansible4 ’/bin/sh -c ’\u0026#34;’\u0026#34;’sudo -H -S -n -u root /bin/sh -c ’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’echo BECOME-SUCCESS-muvtpdvqkslnlegyhoibfcrilvlyjcqp ; /usr/libexec/platform-python /home/ansible/.ansible/tmp/ansible-tmp-1587544652.4716983-118789810824208/AnsiballZ_systemd.py’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’\u0026#34;’ \u0026amp;\u0026amp; sleep 0’\u0026#34;’\u0026#34;’’ Escalation succeeded Optimizing Command Output Error Formatting # You might have noticed that the formatting of error messages in Ansible command output can be a bit hard to read. Fortunately, there\u0026rsquo;s an easy way to make it a little more readable by including two options in the ansible.cfg file. These options are stdout_callback = debug and stdout_callback = error. After including these options, you\u0026rsquo;ll notice it\u0026rsquo;s a lot easier to read error output and distinguish between its different components!\nLogging to Files # By default, Ansible does not write anything to log files. The reason is that the Ansible commands have all the options that may be useful to write output to the STDOUT. If so required, it\u0026rsquo;s always possible to use shell redirection to write the command output to a file.\nIf you do need Ansible to write log files, you can set the log_path parameter in ansible.cfg. Alternatively, Ansible can log to the filename that is specified as the argument to the $ANSIBLE_LOG_PATH variable. Notice that Ansible logs can grow big very fast, so if logging to output files is enabled, make sure that Linux log rotation is configured to ensure that files cannot grow beyond a specific maximum size.\nRunning Task by Task # When you analyze playbook behavior, it\u0026rsquo;s possible to run playbook tasks one by one or to start running a playbook at a specific task. The ansible-playbook --step command runs playbooks task by task and prompts for confirmation before running the next task. Alternatively, you can use the ansible-playbook --start-at-task=\u0026quot;task name\u0026quot; command to start playbook execution as a specific task. Before using this command, you might want to use ansible-playbook --list-tasks for a list of all tasks that have been configured. To use these options in an efficient way, you must configure each task with its own name. In Listing 11-6 you can see what running playbooks this way looks like. This listing first shows how to list tasks in a playbook and next how the --start-at-task and --step options are used.\nListing 11-6 Running Tasks One by One\n::: pre_1 [ansible@control rhce8-book]$ ansible-playbook \u0026ndash;list-tasks exercise81.yaml\nplaybook: exercise81.yaml play #1 (ansible1): testing file manipulation skills. TAGS: [] tasks: create a new file TAGS: [] check status of the new file TAGS: [] for debugging purposes only TAGS: [] change file owner if needed TAGS: [] play #2 (ansible1): fetching a remote file. TAGS: [] tasks: fetch file from remote machine. TAGS: [] play #3 (localhost): adding text to the file that is now on localhost TAGS: [] tasks: add a message. TAGS: [] play #4 (ansible2): copy the modified file to ansible2. TAGS: [] tasks: copy motd file. TAGS: [] [ansible@control rhce8-book]$ ansible-playbook --start-at-task \u0026quot;add a message\u0026quot; --step exercise81.yaml PLAY [testing file manipulation skills] **************************************** PLAY [fetching a remote file] ************************************************** PLAY [adding text to the file that is now on localhost] ************************ Perform task: TASK: Gathering Facts (N)o/(y)es/(c)ontinue: :::\nIn Exercise 11-1 you learn how to apply check mode while working with templates.\n::: box Exercise 11-1 Using Templates in Check Mode\n1. Locate the file httpd.conf; you can find it in the rhce8-book directory, which you can download from the GitHub repository at https://github.com/sandervanvugt/rhce8-book. Use mv httpd.conf exercise111-httpd.j2 to rename it to a Jinja2 template file.\n2. Open the exercise111-httpd.j2 file with an editor, and apply modifications to existing parameters so that they look like the following:\nServerRoot \u0026#34;{{ apache_root }}\u0026#34; User {{ apache_user }} Group {{ apache_group }} 3. Write a playbook that takes care of the complete Apache web server setup and installation, starts and enables the service, opens a port in the firewall, and uses the template module to create the /etc/httpd/conf/httpd.conf file based on the template that you created in step 2 of this exercise. The complete playbook with the name exercise111.yaml looks like the following (make sure you have the exact contents shown below and do not correct any typos):\n--- - name: perform basic apache setup hosts: ansible2 vars: apache_root: /etc/httpd apache_user: httpd apache_group: httpd tasks: - name: install RPM package yum: name: httpd state: latest - name: copy template file template: src: exercise111-httpd.j2 dest: /etc/httpd/httpd.conf - name: start and enable service service: name: httpd state: started enabled: yes - name: open port in firewall firewalld: service: http permanent: yes state: enabled immediate: yes 4. Run the command ansible-playbook --syntax-check exercise111.yaml. If no errors are found in the playbook syntax, you should just see the name of the playbook.\n5. Run the command ansible-playbook --check --diff exercise111.yaml. In the output of the command, pay attention to the task copy template file. After the line that starts with +++ after, you should see the lines in the template that were configured to use a variable, using the right variables.\n6. Run the playbook to perform all its tasks step by step, using the command ansible-playbook --step exercise111.yaml. Press y to confirm the first step. Next, press c to automatically continue. The playbook will fail on the copy template file task because the target directory does not exist. Notice that the --syntax-check and the --check options do not check for any logical errors in the playbook and for that reason have not detected this problem.\n7. Edit the exercise111.yaml file and ensure the template task contains the following corrected line: (replace the old line starting with dest:):\ndest: /etc/httpd/conf/httpd.conf 8. Type ansible-playbook --list-tasks exercise111.yaml to list all the tasks in the playbook.\n9. To avoid running the entire playbook again, use ansible-playbook --start-at-task=\u0026quot;copy template file\u0026quot; exercise111.yaml to run the playbook to completion. :::\n","externalUrl":null,"permalink":"/tech/ansible/troubleshooting-in-ansible/","section":"Teches","summary":"\u003ch3 class=\"relative group\"\u003eUsing Modules for Troubleshooting and Testing\n    \u003cdiv id=\"using-modules-for-troubleshooting-and-testing\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#using-modules-for-troubleshooting-and-testing\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eModules used for troubleshooting\u003c/p\u003e\n\u003cp\u003edebug - Write debug information. Useful for checking variable values\nuri - Test answer coming from any URL. Useful for API calls\nfail - Uses when conditional to determine when a module should be considered failing\nscript - Allows you to execute a shell script on the managed host\nstat - Gathers status information about files\nassert - tests whether the expected result is present and otherwise fails\u003c/p\u003e","title":"Using Modules for Troubleshooting and Testing","type":"tech"},{"content":"Vagrant is software that lets you set up multiple, pre-configured virtual machines in a flash. I am going to show you how to do this using Linux and Virtual Box. But you can do this on MacOS and Windows as well.\nDownload Vagrant, VirtualBox and Git.\nVagrant link.\nVirtualbox link.\nYou may want to follow another tutorial for setting up VirtualBox.\nGit link.\nInstalling git will install ssh on windows. Which you will use to access your lab. Just make sure you select the option to add git and unit tools to your PATH variable.\nMake a Vagrant project folder.\nNote: All of these commands are going to be in a Bash command prompt.\nmkdir vagranttest Move in to your new directory.\ncd vagranttest Add and Initialize Your Vagrant Project.\nYou can find preconfigured virtual machines here.\nWe are going to use ubuntu/trusty64.\nAdd the Vagrant box\nvagrant box add ubuntu/trusty64 Initialize your new Vagrant box\nvagrant init ubuntu/trusty64 Use the dir command to see the contents of this directory.\nWe are going to edit this Vagrantfile to set up our multiple configurations.\nvim Vagrantfile Here is the new config without all of the commented lines. Add this (minus the top line) under Vagrant.configure(\u0026ldquo;2\u0026rdquo;) do |config|.\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/trusty64\u0026#34; config.vm.define \u0026#34;server1\u0026#34; do |server1| server1.vm.hostname = \u0026#34;server1\u0026#34; server1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;10.1.1.2\u0026#34; end config.vm.define \u0026#34;server2\u0026#34; do |server2| server2.vm.hostname = \u0026#34;server2\u0026#34; server2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;10.1.1.3\u0026#34; end end Now save your Vagrant file in Vim.\nBring up your selected vagrant boxes:\nvagrant up Now if you open virtual box, you should see the new machines running in headless mode. This means that the machines have no user interface..\nSsh into server1\nvagrant ssh server1 You are now in serve1\u0026rsquo;s terminal.\nFrom server1, ssh into server2\nssh 10.1.1.3 Success! You are now in server2 and can access both machines from your network. Just enter \u0026ldquo;exit\u0026rdquo; to return to the previous terminal.\nAdditional Helpful Vagrant Commands.\nWithout the machine name specified, vagrant commands will work on all virtual machines in your vagrant folder. I\u0026rsquo;ve thrown in a couple examples using [machine-name] at the end.\nShut down Vagrant machines\nvagrant halt Shut down only one machine\nvagrant halt [machine-name] Suspend and resume a machine\nvagrant suspend vagrant resume Restart a virtual machine\nvagrant reload Destroy a virtual machine\nvagrant detstroy [machine-name] Show running vms\nvagrant status List Vagrant options\nvagrant Playground for future labs\nThis type of deployment is going to be the bedrock of many Linux and Red Hat labs. You can easily use pre-configured machines to create a multi-machine environment. This is also a quick way to test your network and server changes without damaging anything.\nNow go set up a Vagrant lab yourself and let me know what you plan to do with it!\nWhat is Vagrant? # Easy to configure, reproducible environments Provisions virtualbox vms Vagrant box: OS image Syntax:\nvagrant box add user/box Add centos7 box\nvagrant box add jasonc/centos7 Many public boxes to download\nVagrant project = folder with a vagrant file\nInstall Vagrant here: https://www.vagrantup.com/downloads\nMake a vagrant folder:\nmkdir vm1 cd vm1 initialize vagrant project:\nvagrant init jasonc/centos7 bring up all vms defined in the vagrant file)\nvagrant up vagrant will import the box into virtualbox and start it\nthe vm is started in headless mode\n(there is no user interfaces)\nVagrant up / multi machine\nBring up only one specific vm\nvagrant up [vm-name] SSH Vagrant\nvagrant ssh [vm_name] or vagrant ssh if there is only one vm in the vagrant file Need to download ssh for windows\ndownloading git will install this:\nhttps://desktop.github.com/\nShut down vagrant machines vagrant halt\nShutdown only one machine vagrant halt [vm]\nSaves present state of the machine\njust run vagrant up without having to import tha machines again\nSuspend the machine vagrant suspend [VM]\nResume vagrant resume [VM]\nDestroy VM vagrant destroy [VM]\nList options vagrant\nVagrant command works on the vagrant folder that you are in\nVagrant File\nVagrant.configure (2) do | config | config.vm.box = \u0026#34;jasonc/centos7\u0026#34; config.vm.hostname = \u0026#34;linuxsvr1\u0026#34; (default files) config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;10.2.3.4\u0026#34; config.vm.provider \u0026#34;virtualbox\u0026#34; do | vbi vb.gui = true vb.memory = \u0026#34;1024\u0026#34; (shell provisioner) config.vm.provision \u0026#34;shell\u0026#34;, path: \u0026#34;setup.sh\u0026#34; end end Configuring a multi machine setup:\nSpecify common configurations at the top of the file\nVagrant.configure (2) do | config | config.vm.box = \u0026#34;jasonc/centos7\u0026#34; config.vm.define = \u0026#34;server1\u0026#34; do | server1 | server1.vm.hostname = \u0026#34;server1\u0026#34; server1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;10.2.3.4\u0026#34; end config.vm.define = \u0026#34;server2\u0026#34; do | server2 | server2.vm.hostname = \u0026#34;server2\u0026#34; server2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;10.2.3.5\u0026#34; end end You can search for vagrant boxes at https://app.vagrantup.com/boxes/search\nCourse software downloads: http://mirror.linuxtrainingacademy.com/\nInstall Git: https://git-scm.com/download/win\nmake sure to check option for git and unit tools to be added to the PATH vagrant ssh\nto connect to the vagrant machine in the folder that you are in default password in vagrant tyoe \u0026rsquo;exit to return to prompt vagrant halt\nstop the vm and save it\u0026rsquo;s current state vagrant reload\nrestarts the vm vagrant status\nshows running vms in that folder You can access files in the vagrant directory from both VMs\nExample RHEL8 Config # Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;generic/rhel8\u0026#34; config.vm.define \u0026#34;server1\u0026#34; do |server1| server1.vm.hostname = \u0026#34;server1.example.com\u0026#34; server1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.1.110\u0026#34; config.disksize.size = \u0026#39;10GB\u0026#39; end config.vm.define \u0026#34;server2\u0026#34; do |server2| server2.vm.hostname = \u0026#34;server2.example.com\u0026#34; server2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.1.120\u0026#34; config.disksize.size = \u0026#39;16GB\u0026#39; end config.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end end Plugin to change the disk size:\nvagrant plugin install vagrant-disksize ","externalUrl":null,"permalink":"/tech/tools/using-vagrant-on-linux/","section":"Teches","summary":"\u003cp\u003eVagrant is software that lets you set up multiple, pre-configured virtual machines in a flash. I am going to show you how to do this using Linux and Virtual Box. But you can do this on MacOS and Windows as well.\u003c/p\u003e","title":"Using Vagrant on Linux","type":"tech"},{"content":"Using and working with variables\nCapture command output using register Variables # Three types of variables:\nFact Variable Magic Variable Variables make Ansible really flexible. Especially when used in combination with conditionals. These are defined at the discretion of the user.:\n--- - name: create a user using a variable hosts: ansible1 vars: users: lisa \u0026lt;-- defaults value for this play tasks: - name: create a user {{ users }} on host {{ ansible_hostname }} \u0026lt;-- ansible fact variable user: name: \u0026#34;{{ users }}\u0026#34; \u0026lt;-- If value starts with variable, the whole line must have double quotes Working with Variables # Variables can be used to refer to a wide range of dynamic data, such as names of files, services, packages, users, URLs to specific servers, etc. Defining Variables # To define a variable\nkey: value structure in a vars section in the play header. --- - name: using variables hosts: ansible1 vars: \u0026lt;------------- ftp_package: vsftpd \u0026lt;------------ tasks: - name: install package yum: name: \u0026#34;{{ ftp_package }}\u0026#34; \u0026lt;------------ state: latest As the variable is the first item in the value, its name must be placed between double curly brackets as well as double quotes. Variable equirements:\n• Must start with a letter. • Case sensitive. • Can contain only letters, numbers, and underscores.\nUsing Include Files # It is common to define variables in include files. Specific host and host group variables can be used as include files it\u0026rsquo;s also possible to include an arbitrary file as a variable file, using the vars_files: statement. The vars_files: parameter can have a single value or a list providing multiple values. If a list is used, each item needs to start with a dash When you include variables from files, it\u0026rsquo;s a good idea to work with a separate directory that contains all variables because that makes it easier to manage as your projects grow bigger. --- - name: using a variable include file hosts: ansible1 vars_files: vars/common \u0026lt;-------------- tasks: - name: install package yum: name: \u0026#34;{{ my_package }}\u0026#34; \u0026lt;------------ state: latest vars/common\nmy_package: nmap my_ftp_service: vsftpd my_file_service: smb If variables are defined in individual playbooks, they are spread all over, and it may be difficult to get an overview of all variables that are used on a site. Managing Host and Group Variables # host_vars and group_vars\nset variables for specific hosts or specific host groups. In older versions of Ansible, it was common to set host variables and group variables in inventory, but this practice is now deprecated. host_vars\nMust create a subdirectory with the name host_vars within the Ansible project directory. In this directory, create a file that matches the inventory name of the host to which the variables should be applied. So the variables for host ansible1 are defined in host_vars/ansible1. group_vars\nMust create a directory with the name group_vars. In this directory, a file with the name of the host group is created, and in this file all variables are defined. ie: group_vars/webservers If no variables are defined at the command prompt, it will use the variable set for the play. You can also define the variables with the -e flag when running the playbook:\n[ansible@control base]$ ansible-playbook variable-pb.yaml -e users=john PLAY [create a user using a variable] ************************************************************************************************************************ TASK [Gathering Facts] *************************************************************************************************************************************** ok: [ansible1] TASK [create a user john on host ansible1] ******************************************************************************************************************* changed: [ansible1] PLAY RECAP *************************************************************************************************************************************************** ansible1 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 LAB: Using Host and Host Group Variables # 1. Create a project directory in your home directory. Type mkdir ~/chapter6 to create the chapter6 project directory, and use cd ~/chapter6 to go into this directory.\n2. Type cp ../ansible.cfg . to copy the ansible.cfg file that you used before. No further modifications to this file are required.\n3. Type vim inventory to create a file with the name inventory, and ensure it has the following contents:\n[webservers] ansible1 [dbservers] ansible2 4. Create the file webservers.yaml, containing the following contents. Notice that nothing is really changed by running this playbook. It just uses the debug module to show the current value of the variables.\n--- - name: configure web services hosts: webservers tasks: - name: this is the {{ web_package }} package debug: msg: \u0026#34;Installing {{ web_package }}\u0026#34; - name: this is the {{ web_service }} service debug: msg: \u0026#34;Starting the {{ web_service }}\u0026#34; 5. Create the file group_vars/webservers with the following contents:\nweb_package: httpd web_service: httpd 6. Run the playbook with some verbosity to verify it is working by using ansible-playbook -vv webservers.yaml\nUsing Multivalued Variables # Two types of multivalued variables:\narray (list)\nkey that can have multiple items as its value. Each item in a list starts with a dash (-). Individual items in a list can be addressed using the index number (starting at zero), as in {{ users\\[1\\] }} (which would print the key-value pairs that are set for user lisa) users: - linda: username: linda homedir: /home/linda shell: /bin/bash - lisa: username: lisa homedir: /home/lisa shell: /bin/bash - anna: username: anna homedir: /home/anna shell: /bin/bash dictionary (hash)\nUnordered collection of items, a collection of key-value pairs. In Python, a dictionary is defined as my_dict = { key1: \u0026lsquo;car\u0026rsquo;, key2:\u0026lsquo;bike\u0026rsquo; }. Because it is based on Python, Ansible lets users use dictionaries as an alternative notation to arrays not as common in use as arrays. Items in values in a dictionary are not started with a dash. users: linda: username: linda homedir: /home/linda shell: /bin/bash lisa: username: lisa homedir: /home/lisa shell: /bin/bash anna: username: anna homedir: /home/anna shell: /bin/bash Addressing Specific Keys in a Dictionary Multivalued Variable:\n--- - name: show dictionary also known as hash hosts: ansible1 vars_files: - vars/users-dictionary tasks: - name: print dictionary values debug: msg: \u0026#34;User {{ users.linda.username }} has homedirectory {{ users.linda.homedir }} and shell {{ users.linda.shell }}\u0026#34; Using the Square Brackets Notation to Address Multivalued Variables (recommended method)\n--- - name: show dictionary also known as hash hosts: ansible1 vars_files: - vars/users-dictionary tasks: - name: print dictionary values debug: msg: \u0026#34;User {{ users[’linda’][’username’] }} has homedirectory {{ users[’linda’][’homedir’] }} and shell {{ users[’linda’][’shell’] }}\u0026#34; Magic Variables\nVariables that are set automatically by Ansible to reflect an Ansible internal state. There are about 30 magic variables Common Magic Variables you cannot use their name for anything else. If you try to set a magic variable to another value anyway, it always resets to the default internal value. Debug module can be used to show the current values assigned to the hostvars magic variable.\nShows many settings that you can change by modifying the ansible.cfg configuration file. If local facts are defined on the host, you will see them also. [ansible@control ~]$ ansible localhost -m debug -a \u0026#39;var=hostvars[\u0026#34;ansible1\u0026#34;]\u0026#39; localhost | SUCCESS =\u0026gt; { \u0026#34;hostvars[\\\u0026#34;ansible1\\\u0026#34;]\u0026#34;: { \u0026#34;ansible_check_mode\u0026#34;: false, \u0026#34;ansible_diff_mode\u0026#34;: false, \u0026#34;ansible_facts\u0026#34;: {}, \u0026#34;ansible_forks\u0026#34;: 5, \u0026#34;ansible_inventory_sources\u0026#34;: [ \u0026#34;/home/ansible/inventory\u0026#34; ], \u0026#34;ansible_playbook_python\u0026#34;: \u0026#34;/usr/bin/python3.6\u0026#34;, \u0026#34;ansible_verbosity\u0026#34;: 0, \u0026#34;ansible_version\u0026#34;: { \u0026#34;full\u0026#34;: \u0026#34;2.9.5\u0026#34;, \u0026#34;major\u0026#34;: 2, \u0026#34;minor\u0026#34;: 9, \u0026#34;revision\u0026#34;: 5, \u0026#34;string\u0026#34;: \u0026#34;2.9.5\u0026#34; }, \u0026#34;group_names\u0026#34;: [ \u0026#34;ungrouped\u0026#34; ], \u0026#34;groups\u0026#34;: { \u0026#34;all\u0026#34;: [ \u0026#34;ansible1\u0026#34;, \u0026#34;ansible2\u0026#34; ], \u0026#34;ungrouped\u0026#34;: [ \u0026#34;ansible1\u0026#34;, \u0026#34;ansible2\u0026#34; ] }, \u0026#34;inventory_dir\u0026#34;: \u0026#34;/home/ansible\u0026#34;, \u0026#34;inventory_file\u0026#34;: \u0026#34;/home/ansible/inventory\u0026#34;, \u0026#34;inventory_hostname\u0026#34;: \u0026#34;ansible1\u0026#34;, \u0026#34;inventory_hostname_short\u0026#34;: \u0026#34;ansible1\u0026#34;, \u0026#34;omit\u0026#34;: \u0026#34;__omit_place_holder__38849508966537e44da5c665d4a784c3bc0060de\u0026#34;, \u0026#34;playbook_dir\u0026#34;: \u0026#34;/home/ansible\u0026#34; } } Variable Precedence # Avoid using variables with the same names that are defined at different levels. If a variable with the same name is defined at different levels, the most specific variable always wins. Variables that are defined while running the playbook command using the -e key=value command-line argument have the highest precedence. After variables that are passed as command-line options, playbook variables are considered. Next are variables that are defined for inventory hosts or host groups. Consult the Ansible documentation item \u0026ldquo;Variable precedence\u0026rdquo; for more details and an overview of the 22 different levels where variables can be set and how precedence works for them. 1. Variables passed on the command line 2. Variables defined in or included from a playbook 3. Inventory variables\nCapturing Command Output Using register # The result of commands can also be used as a variable byusing the register parameter in a task.\n--- - name: test register hosts: ansible1 tasks: - shell: cat /etc/passwd register: passwd_contents - debug: var: \u0026#34;passwd_contents\u0026#34; The cat /etc/passwd command is executed by the shell module. Notice that in this playbook no names are used for tasks. Using names for tasks is not mandatory; it\u0026rsquo;s just recommended in more complex playbooks because this convention makes identification of the tasks easier. The entire contents of the command are next stored in the variable passwd_contents.\nThis variable contains the output of the command, stored in different keys. Table 6-7 provides an overview of the most useful keys, and Listing 6-19 shows the partial result of the ansible-playbook listing618.yaml command.\nKeys Used with register cmd\nCommand that was used rc Return code of the command stderr Error messages stderr_lines Errors line by line stdout command output stdout_line Command output line by line [ansible@control ~]$ ansible-playbook listing618.yaml PLAY [test register] ******************************************************************* TASK [Gathering Facts] ***************************************************************** ok: [ansible2] ok: [ansible1] TASK [shell] *************************************************************************** changed: [ansible2] changed: [ansible1] TASK [debug] *************************************************************************** ok: [ansible1] =\u0026gt; { \u0026#34;passwd_contents\u0026#34;: { \u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: \u0026#34;cat /etc/passwd\u0026#34;, \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.004149\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2020-04-02 02:28:10.692306\u0026#34;, \u0026#34;failed\u0026#34;: false, \u0026#34;rc\u0026#34;: 0, \u0026#34;start\u0026#34;: \u0026#34;2020-04-02 02:28:10.688157\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;root:x:0:0:root:/root:/bin/bash\\nbin:x:1:1:bin:/bin:/sbin/nologin\\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\\nadm:x:3:4:adm:/var/adm:/sbin/nologin\\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\\nsync:x:5:0:sync:/sbin:/bin/sync\\nshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\\nhalt:x:7:0:halt:/sbin:/sbin/halt\\nansible:x:1000:1000:ansible:/home/ansible:/bin/bash\\napache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin\\nlinda:x:1002:1002::/home/linda:/bin/bash\\nlisa:x:1003:1003::/home/lisa:/bin/bash\u0026#34;, \u0026#34;stdout_lines\u0026#34;: [ \u0026#34;root:x:0:0:root:/root:/bin/bash\u0026#34;, \u0026#34;bin:x:1:1:bin:/bin:/sbin/nologin\u0026#34;, \u0026#34;daemon:x:2:2:daemon:/sbin:/sbin/nologin\u0026#34;, \u0026#34;adm:x:3:4:adm:/var/adm:/sbin/nologin\u0026#34;, \u0026#34;lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\u0026#34;, \u0026#34;sync:x:5:0:sync:/sbin:/bin/sync\u0026#34;, \u0026#34;shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\u0026#34;, \u0026#34;halt:x:7:0:halt:/sbin:/sbin/halt\u0026#34;, \u0026#34;ansible:x:1000:1000:ansible:/home/ansible:/bin/bash\u0026#34;, \u0026#34;apache:x:48:48:Apache:/usr/share/httpd:/sbin/nologin\u0026#34;, \u0026#34;linda:x:1002:1002::/home/linda:/bin/bash\u0026#34;, \u0026#34;lisa:x:1003:1003::/home/lisa:/bin/bash\u0026#34; ] } } Ensure that a task runs only if a command produces a specific result by using register with conditionals.\nregister shows the values that are returned by specific tasks. Tasks have common return values, but modules may have specific return values. That means you cannot assume, based on the result of an example using a specific module, that the return values you see are available for all modules. Consult the module documentation for more information about specific return values.\nAnsible Facts # An Ansible fact is a variable that contains information about a target system.This information can be used in conditional statements to tailor playbooks to that system. Systems facts are system property values. Custom facts are user-defined variables stored on managed hosts. system.\nFacts are collected when Ansible executes on the remote system. You\u0026rsquo;ll see a \u0026ldquo;Gathering Facts\u0026rdquo; task everytime you run a playbook. These facts are then stored in the variable ansible_facts.\nUse the debug module to check the value of variables. This module requires variables to be enclosed in curly brackets. This example shows a large list of facts from managed nodes:\n--- - name: show facts hosts: all tasks: - name: show facts debug: var: ansible_facts There are two supported formats for using Ansible fact variables:\nIt\u0026rsquo;s recommended to use square brackets: ansible_facts['default_ipv4']['address'] but dotted notation is also supported for now: ansible_facts.default_ipv4.address\nCommonly used ansible_facts:\nThere are additional Ansible modules for gathering more information. See `ansible-doc -l | grep fact\npackage_facts module collects information about software packages installed on managed hosts.\nTwo ways facts are displayed # Ansible_facts variable (current way)\nAll facts are stored in a dictionary with the name ansible_facts, and items in this dictionary are addressed using the notation with square brackets ie: ansible_facts['distribution_version'] Recommended to use this. injected variables (old way)\nVariable are prefixed with the string ansible_\nWill lose support eventually\nOld approach and the new approach both still occur.\nansible ansible1 -m setup command Ansible facts are injected as variables. ansible1 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_all_ipv4_addresses\u0026#34;: [ \u0026#34;192.168.122.1\u0026#34;, \u0026#34;192.168.4.201\u0026#34; ], \u0026#34;ansible_all_ipv6_addresses\u0026#34;: [ \u0026#34;fe80::e564:5033:5dec:aead\u0026#34; ], \u0026#34;ansible_apparmor\u0026#34;: { Comparing ansible_facts Versus Injected Facts as Variables\nansible_facts Injected Variable -------------------------------------------------------------- ansible_facts[\u0026#39;hostname\u0026#39;] ansible_hostname ansible_facts[\u0026#39;distribution\u0026#39;] ansible_distribution ansible_facts[\u0026#39;default_ipv4\u0026#39;][\u0026#39;address\u0026#39;] ansible_default_ipv4[\u0026#39;address\u0026#39;] ansible_facts[\u0026#39;interfaces\u0026#39;] ansible_interfaces ansible_facts[\u0026#39;devices\u0026#39;] ansible_devices ansible_facts[\u0026#39;devices\u0026#39;][\u0026#39;sda\u0026#39;]\\ [\u0026#39;partitions\u0026#39;][\u0026#39;sda1\u0026#39;][\u0026#39;size\u0026#39;] ansible_devices[\u0026#39;sda\u0026#39;][\u0026#39;partitions\u0026#39;][\u0026#39;sda1\u0026#39;][\u0026#39;size\u0026#39;] ansible_facts[\u0026#39;distribution_version\u0026#39;] ansible_distribution_version Different notations can be used in either method, the listings address the facts in dotted notation, not in the notation with square brackets.\nAddressing Facts with Injected Variables:\n- hosts: all tasks: - name: show IP address debug: msg: \u0026gt; This host uses IP address {{ ansible_default_ipv4.address }} Addressing Facts Using the ansible_facts Variable\n--- - hosts: all tasks: - name: show IP address debug: msg: \u0026gt; This host uses IP address {{ ansible_facts.default_ipv4.address }} If, for some reason, you want the method where facts are injected into variables to be the default method, you can use inject_facts_as_vars=true in the \\[default\\] section of the ansible.cfg file.\n• In Ansible versions since 2.5, all facts are stored in one variable: ansible_facts. This method is used while gathering facts from a playbook.\n• Before Ansible version 2.5, facts were injected into variables such as ansible_hostname. This method is used by the setup module. (Note that this may change in future versions of Ansible.)\n• Facts can be addressed in dotted notation: {{ansible_facts.default_ipv4.address }}\n• Alternatively, facts can be addressed in square brackets notation: {{ ansible_facts['default_ipv4']['address'] }}. (preferred)\nManaging Fact Gathering # By default, upon execution of each playbook, facts are gathered. This does slow down playbooks, and for that reason, it is possible to disable fact gathering completely. To do so, you can use the gather_facts: no parameter in the play header. If later in the same playbook it is necessary to gather facts, you can do this by running the setup module in a task.\nEven if it is possible to disable fact gathering for all of your Ansible configuration, this practice is not recommended. Too many playbooks use conditionals that are based on the current value of facts, and all of these conditionals would stop working if fact gathering were disabled altogether.\nAs an alternative to make working with facts more efficient, you can disable a fact cache. To do so, you need to install an external plug-in. Currently, two plug-ins are available for this purpose: jsonfile and redis. To configure fact caching using the redis plug-in, you need to install it first. Next, you can enable fact caching through ansible.cfg.\nThe following procedure describes how to do this:\n1. Use yum install redis.\n2. Use service redis start.\n3. Use pip install redis.\n4. Edit /etc/ansible/ansible.cfg and ensure it contains the following parameters:\n[defaults] gathering = smart fact_caching = redis fact_caching_timeout = 86400 Note\nFact caching can be convenient but should be used with caution. If, for instance, a playbook installs a certain package only if a sufficient amount of disk space is available, it should not do this based on information that may be up to 24 hours old. For that reason, using a fact cache is not recommended in many situations.\nCustom Facts # Used to provide a host with arbitrary values that Ansible can use to change the behavior of plays.\ncan be provided as static files.\nfiles must\nbe in either INI or JSON format, have the extension .fact, and on the managed hosts must be stored in the /etc/ansible/facts.d directory. can be generated by a script, and\nin that case the only requirement is that the script must generate its output in JSON format. Dynamic custom facts are useful because they allow the facts to be determined at the moment that a script is running. provides an example of a static custom fact file.\nCustom Facts Sample File:\n[packages] web_package = httpd ftp_package = vsftpd [services] web_service = httpd ftp_service = vsftpd To get the custom facts files on the managed hosts, you can use a playbook that copies a local custom fact file (existing in the current Ansible project directory) to the appropriate location on the managed hosts. Notice that this playbook uses variables, which are explained in more detail in the section titled \u0026ldquo;Working with Variables.\u0026rdquo;\n--- - name: Install custom facts hosts: all vars: remote_dir: /etc/ansible/facts.d facts_file: listing68.fact tasks: - name: create remote directory file: state: directory recurse: yes path: \u0026#34;{{ remote_dir }}\u0026#34; - name: install new facts copy: src: \u0026#34;{{ facts_file }}\u0026#34; dest: \u0026#34;{{ remote_dir }}\u0026#34; Custom facts are stored in the variable ansible_facts.ansible_local. In this variable, you use the filename of the custom fact file and the label in the custom fact file. For instance, after you run the playbook in Listing 6-9, the web_package fact that was defined in listing68.fact is accessible as\n{{ ansible_facts[’ansible_local’][’listing67’][’packages’][’web_package’] }} To verify, you can use the setup module with the filter argument. Notice that because the setup module produces injected variables as a result, the ad hoc command to use is ansible all -m setup -a \u0026quot;filter=ansible_local\u0026quot; . The command ansible all -m setup -a \u0026quot;filter=ansible_facts\\['ansible_local'\\]\u0026quot; does not work.\nLab Working with Ansible Facts # 1. Create a custom fact file with the name custom.fact and the following contents:\n[software] package = httpd service = httpd state = started enabled = true 2. Write a playbook with the name copy_facts.yaml and the following contents:\n--- - name: copy custom facts become: yes hosts: ansible1 tasks: - name: create the custom facts directory file: state: directory recurse: yes path: /etc/ansible/facts.d - name: copy the custom facts copy: src: custom.fact dest: /etc/ansible/facts.d 3. Apply the playbook using ansible-playbook copy_facts.yaml -i inventory\n4. Check the availability of the custom facts by using ansible all -m setup -a \u0026quot;filter=ansible_local\u0026quot; -i inventory\n5. Use an ad hoc command to ensure that the httpd service is not installed on any of the managed servers: ansible all -m yum -a \u0026quot;name=httpd state=absent\u0026quot; -i inventory -b\n6. Create a playbook with the name setup_with_facts.yaml that installs and enables the httpd service, using the custom facts:\n--- - name: install and start the web service hosts: ansible1 tasks: - name: install the package yum: name: \u0026#34;{{ ansible_facts[\u0026#39;ansible_local\u0026#39;][\u0026#39;custom\u0026#39;][\u0026#39;software\u0026#39;][\u0026#39;package\u0026#39;] }}\u0026#34; state: latest - name: start the service service: name: \u0026#34;{{ ansible_facts[\u0026#39;ansible_local\u0026#39;][\u0026#39;custom\u0026#39;][\u0026#39;software\u0026#39;][\u0026#39;service\u0026#39;] }}\u0026#34; state: \u0026#34;{{ ansible_facts[\u0026#39;ansible_local\u0026#39;][\u0026#39;custom\u0026#39;][\u0026#39;software\u0026#39;][\u0026#39;state\u0026#39;] }}\u0026#34; enabled: \u0026#34;{{ ansible_facts[\u0026#39;ansible_local\u0026#39;][\u0026#39;custom\u0026#39;][\u0026#39;software\u0026#39;][\u0026#39;enabled\u0026#39;] }}\u0026#34; 7. Run the playbook to install and set up the service by using ansible-playbook setup_with_facts.yaml -i inventory -b\n8. Use an ad hoc command to verify the service is running: ansible ansible1 -a \u0026quot;systemctl status httpd\u0026quot; -i inventory -b\n","externalUrl":null,"permalink":"/tech/ansible/using-variables-in-ansible/","section":"Teches","summary":"\u003cp\u003eUsing and working with variables\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCapture command output using register\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eVariables\n    \u003cdiv id=\"variables\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#variables\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eThree types of variables:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFact\u003c/li\u003e\n\u003cli\u003eVariable\u003c/li\u003e\n\u003cli\u003eMagic Variable\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eVariables\u003c/strong\u003e make Ansible really flexible. Especially when used in combination with conditionals. These are defined at the discretion of the user.:\u003c/p\u003e","title":"Using Variables In Ansible","type":"tech"},{"content":" Using when to Run Tasks Conditionally # Use a when statement to run tasks conditionally. you can test whether: a variable has a specific value whether a file exists whether a minimal amount of memory is available etc. Working with when # Install the right software package for the Apache web server, based on the Linux distribution that was found in the Ansible facts. Notice that\nwhen used in when statements, the variable that is evaluated is not placed between double curly braces. --- - name: conditional install hosts: all tasks: - name: install apache on Red Hat and family yum: name: httpd state: latest when: ansible_facts[’os_family’] == \u0026#34;RedHat\u0026#34; - name: install apache on Ubuntu and family apt: name: apache2 state: latest when: ansible_facts[’os_family’] == \u0026#34;Debian\u0026#34; not a part of any properties of the modules on which it is used\nmust be indented at the same level as the module itself.\nFor a string test, the string itself must be between double quotes.\nWithout the double quotes, it would be considered an integer test.\nUsing Conditional Test Statements # Common conditional tests that you can perform with the when statement:\nVariable exists\nvariable is defined Variable does not exist\nvariable is not defined First variable is present in list mentioned as second\nansible_distribution in distributions Variable is true, 1 or yes\nvariable Variable is false, 0 or no\nnot variable Equal (string)\nkey == \u0026ldquo;value\u0026rdquo; Equal (numeric)\nkey == value Less than\nkey \u0026lt; value Less than or equal to\nkey \u0026lt;= value Greater than\nkey \u0026gt; value Greater than or equal to\nkey \u0026gt;= value Not equal to\nkey != value\nLook for \u0026ldquo;Tests\u0026rdquo; in the Ansible documentation, and use the item that is found in Templating (Jinja2). https://docs.ansible.com/projects/ansible/latest/playbook_guide/playbooks_tests.html#tests\nWhen referring to variables in when statements, you don\u0026rsquo;t have to use curly brackets because items in a when statement are considered to be variables by default.\nSo you can write when: text == \u0026ldquo;hello\u0026rdquo; instead of when: \u0026ldquo;{{ text }}\u0026rdquo; == \u0026ldquo;hello\u0026rdquo;.\nThere are roughly four types of when conditional tests: • Checks related to variable existence • Boolean checks • String comparisons • Integer comparisons\nThe first type of test checks whether a variable exists or is a part of another variable, such as a list.\nChecks for the existence of a specific disk device, using variable is defined and variable is not defined. All failing tests result in the message \u0026ldquo;skipping.\u0026rdquo;\n--- - name: check for existence of devices hosts: all tasks: - name: check if /dev/sda exists debug: msg: a disk device /dev/sda exists when: ansible_facts[’devices’][’sda’] is defined - name: check if /dev/sdb exists debug: msg: a disk device /dev/sdb exists when: ansible_facts[’devices’][’sdb’] is defined - name: dummy test, intended to fail debug: msg: failing when: dummy is defined - name: check if /dev/sdc does not exist debug: msg: there is no /dev/sdc device when: ansible_facts[’devices’][’sdc’] is not defined Lab: Check that finds whether the first variable value is present in the second variable\u0026rsquo;s list. # executes the debug task if the variable my_answer is in supported_packages. vars_prompt is used. This stops the playbook, asks the user for input, and stores the input in a variable with the name my_answer. --- - name: test if variable is in another variables list hosts: all vars_prompt: - name: my_answer prompt: which package do you want to install vars: supported_packages: - httpd - nginx tasks: - name: something debug: msg: you are trying to install a supported package when: my_answer in supported_packages Boolean check\nWorks on variables that have a Boolean value (not very common) Should not be defined with the check for existence. Used to check whether a variable is defined. string comparisons and integer comparisons\nIe: Check if more than 1 GB of disk space is available. When doing checks on available disk space and available memory, carefully look at the expected value. Memory is shown in megabytes, by default, whereas disk space is expressed in bytes. Lab: integer check, install vsftpd if more than 50 MB of memory is available. # --- - name: conditionals test hosts: all tasks: - name: install vsftpd if sufficient memory available package: name: vsftpd state: latest when: ansible_facts[’memory_mb’][’real’][’free’] \u0026gt; 50 Testing Multiple Conditions # when statements can also be used to evaluate multiple conditions. To do so, you can group the conditions with parentheses and combine them with and and or keywords. and runs if both conditionals are ture or runs if one of the conditions are true Lab: and is used and runs the task only if both conditions are true. # --- - name: testing multiple conditions hosts: all tasks: - name: showing output debug: msg: using CentOS 8.1 when: ansible_facts[’distribution_version’] == \u0026#34;8.1\u0026#34; and ansible_facts[’distribution’] == \u0026#34;CentOS\u0026#34; You can make more complex statements by grouping conditions together in parentheses. Group the when statement starts with a \u0026gt; sign to wrap the statement over the next five lines for readability. Lab: Combining complex statements # --- - name: using multiple conditions hosts: all tasks: - package: name: httpd state: removed when: \u0026gt; ( ansible_facts[’distribution’] == \u0026#34;RedHat\u0026#34; and ansible_facts[’memfree_mb’] \u0026lt; 512 ) or ( ansible_facts[’distribution’] == \u0026#34;CentOS\u0026#34; and ansible_facts[’memfree_mb’] \u0026lt; 256 ) Combining loop and when # Lab: Combining loop and when, Perform a kernel update only if /boot is on a dedicated mount point and at least 200 MBis available in the mount. # --- - name: conditionals test hosts: all tasks: - name: update the kernel if sufficient space is available in /boot package: name: kernel state: latest loop: \u0026#34;{{ ansible_facts[’mounts’] }}\u0026#34; when: item.mount == \u0026#34;/boot\u0026#34; and item.size_available \u0026gt; 200000000 Combining loop and register # Lab: Combining register and loop # --- - name: test register hosts: all tasks: - shell: cat /etc/passwd register: passwd_contents - debug: msg: passwd contains user lisa when: passwd_contents.stdout.find(’lisa’) != -1 passwd_contents.stdout.find,\npasswd_contents.stdout does not contain any item with the name find. Construction that is used here is variable.find, which enables a task to search a specific string in a variable. (thefind function in Python is used) When the Python find function does not find a string, it returns a value of −1. If the requested string is found, the find function returns an integer that returns the position where the string was found. For instance, if the string lisa is found in /etc/passwd, it returns an unexpected value like 2604, which is the position in the file, expressed as a byte offset from the beginning, where the string is found for the first time. Because of the behavior of the Python find function, variable.find needs not to be equal to −1 to have the task succeed. So don\u0026rsquo;t write passwd_contents.stdout.find(\u0026rsquo;lisa\u0026rsquo;) = 0 (because it is not a Boolean), but instead write passwd_contents.stdout.find(\u0026rsquo;lisa\u0026rsquo;) != -1. Lab: Practice working with conditionals using register. # When using register, you might want to define a task that runs a command that will fail, just to capture the return code of that command, after which the playbook should continue. If that is the case, you must ensure that ignore_errors: yes is used in the task definition.\n1. Use your editor to create a new file with the name exercise72.yaml. Start writing the play header as follows:\n--- - name: restart sshd service if httpd is running hosts: ansible1 tasks: 2. Add the first task, which checks whether the httpd service is running, using command output that will be registered. Notice the use of ignore_errors: yes. This line makes sure that if the service is not running, the play is still executed further.\n--- - name: restart sshd service if httpd is running hosts: ansible1 tasks: - name: get httpd service status command: systemctl is-active httpd ignore_errors: yes register: result 3. Add a debug task that shows the output of the command so that you can analyze what is currently in the registered variable:\n--- - name: restart sshd service if httpd is running hosts: ansible1 tasks: - name: get httpd service status command: systemctl is-active httpd ignore_errors: yes register: result - name: show result variable contents debug: msg: printing contents of the registered variable {{ result }} 4. Complete the playbook by including the service task, which is started only if the value stored in result.rc (which is the return code of the command that was registered) contains a 0. This is the case if the previous command executed successfully.\n--- - name: restart sshd service if httpd is running hosts: ansible1 tasks: - name: get httpd service status command: systemctl is-active httpd ignore_errors: yes register: result - name: show result variable contents debug: msg: printing contents of the registered variable {{ result }} - name: restart sshd service service: name: sshd state: restarted when: result.rc == 0 5. Use an ad hoc command to make sure the httpd service is installed: ansible ansible1 -m yum -a \u0026quot;name=httpd state=latest\u0026quot;.\n6. Use an ad hoc command to make sure the httpd service is stopped: ansible ansible1 -m service -a \u0026quot;name=httpd state=stopped\u0026quot;.\n7. Run the playbook using ansible-playbook exercise72.yaml and analyze the result. You should see that the playbook skips the service task.\n8. Type ansible ansible1 -m service -a \u0026quot;name=httpd state=started\u0026quot; and run the playbook again, using ansible-playbook exercise72.yaml. Playbook execution at this point should be successful.\n","externalUrl":null,"permalink":"/tech/ansible/using-when-to-run-tasks-conditionally/","section":"Teches","summary":"Using when to Run Tasks Conditionally","title":"Using when to Run Tasks Conditionally","type":"tech"},{"content":"Vim (Vi Improved)\nVim stands for vi (Improved) just like its name it stands for an improved version of the vi text editor command.\nLightweight\nStart Vim\nvim Vim Search Patterns # Moving the Cursor # h or left arrow - move left one character k or up arrow - move up one line j or down arrow - move down one line l or right arrow - will move you right one character\nDifferent Vim Modes # I - Enter INSERT mode from command mode esc - Go back to command mode v - visual mode\nVim Appending Text # In enter while in command mode and will bring you to insert mode.\nI - insert text before the cursor O - insert text on the previous line o - insert text on the next line a - append text after cursor A - append text at the end of the line\nVim editing # x - used to cut the selected text also used for deleting characters dd - used to delete the current line y - yank or copy whatever is selected yy - yank or copy the current line p - paste the copied text before the cursor\nVim Saving and exiting # :w - writes or saves the file :q - quit out of vim :wq - write and then quit :q! - quit out of vim without saving the file ZZ - equivalent of :wq, but one character faster\nu - undo your last action Ctrl-r - redo your last action :% sort - Sort lines\nVim Splits # Add to .vimrc for different key mappings for easy navigation between splits to save a keystroke. So instead of ctrl-w then j, it’s just ctrl-j:\nnnoremap \u0026lt;C-J\u0026gt; \u0026lt;C-W\u0026gt;\u0026lt;C-J\u0026gt; nnoremap \u0026lt;C-K\u0026gt; \u0026lt;C-W\u0026gt;\u0026lt;C-K\u0026gt; nnoremap \u0026lt;C-L\u0026gt; \u0026lt;C-W\u0026gt;\u0026lt;C-L\u0026gt; nnoremap \u0026lt;C-H\u0026gt; \u0026lt;C-W\u0026gt;\u0026lt;C-H\u0026gt; Open file in new split # :vsp filename https://github.com/preservim/nerdtree\nFind and Replace # https://linuxize.com/post/vim-find-replace/\nFind and Replace Text in File(s) with Vim # Find and replace in a single file # Open the file in Vim, this command will replace all occurances of the word \u0026ldquo;foo\u0026rdquo; with \u0026ldquo;bar\u0026rdquo;.\n:%s/foo/bar/g % - apply to whole file s - substitution g - operate on all results\nFind and replace a string in all files in current directory # In vim, select all files with args. Use regex to select the files you want. Select all files with *\n:args * You can also select all recursively:\n:args ** Run :args to see which files are selected\u0026quot;\n:args Perform substitution with argdo # This applies the replacement command to all selected args:\n:argdo %s/foo/bar/g | update Nerd Tree Plugin # Add to .vimrc\ncall plug#begin() Plug \u0026#39;preservim/nerdtree\u0026#39; call plug#end() nnoremap \u0026lt;leader\u0026gt;n :NERDTreeFocus\u0026lt;CR\u0026gt; nnoremap \u0026lt;C-n\u0026gt; :NERDTree\u0026lt;CR\u0026gt; nnoremap \u0026lt;C-t\u0026gt; :NERDTreeToggle\u0026lt;CR\u0026gt; nnoremap \u0026lt;C-f\u0026gt; :NERDTreeFind\u0026lt;CR\u0026gt; Vim Calendar # dhttps://blog.mague.com/?p=602\nAdd to vim.rc\n:auto FileType vim/wiki map d :Vim/wikiMakeDiaryNote function! ToggleCalendar() execute \u0026#34;:Calendar\u0026#34; if exists(\u0026#34;g:calendar_open\u0026#34;) if g:calendar_open == 1 execute \u0026#34;q\u0026#34; unlet g:calendar_open else g:calendar_open = 1 end else let g:calendar_open = 1 end endfunction :auto FileType vim/wiki map c :call ToggleCalendar()i Vimwiki # Cheat sheet # http://thedarnedestthing.com/vimwiki%20cheatsheet\nSet up # Make sure git is installed? https://github.com/git-guides/install-git\nCheck git version # git --version Check git version\ndnf git install # sudo dnf install git-all\nhttps://github.com/junegunn/vim-plug\nDownload plug.vim and put it in ~/.vim/autoload\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim Create ~/.vimrc # touch ~/.vimrc\nAdd to ~/.vimrc # Installation using Vim-Plug # Install Vim Plug\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs \\https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim Add the following to the plugin-configuration in your vimrc:\nset nocompatible filetype plugin on syntax on call plug#begin() Plug \u0026#39;vimwiki/vimwiki\u0026#39; call plug#end() let mapleader=\u0026#34; \u0026#34; let wiki_1 = {} let wiki_1.path = \u0026#39;~/Documents/PerfectDarkMode/\u0026#39; let wiki_1.syntax = \u0026#39;markdown\u0026#39; let wiki_1.ext = \u0026#39;\u0026#39; let wiki_2 = {} let wiki_2.path = \u0026#39;~/Documents/vim/wiki_personal/\u0026#39; let wiki_2.syntax = \u0026#39;markdown\u0026#39; let wiki_2.ext = \u0026#39;\u0026#39; let g:vimwiki_list = [wiki_1, wiki_2] Then run :PlugInstall.\n(leader)ws select which wiki to use\nBasic Markup # = Header1 = == Header2 == === Header3 === *bold* -- bold text _italic_ -- italic text [wiki link](wiki%20link) -- wiki link [description](wiki%20link) -- wiki link with description Lists # * bullet list item 1 - bullet list item 2 - bullet list item 3 * bullet list item 4 * bullet list item 5 * bullet list item 6 * bullet list item 7 - bullet list item 8 - bullet list item 9 1. numbered list item 1 2. numbered list item 2 a) numbered list item 3 b) numbered list item 4 For other syntax elements, see :h vimwiki-syntax\nVimwiki Table of Contents # :VimwikiTOC Create or update the Table of Contents for the current wiki file. See |vimwiki-toc|.\nTable of Contents vimwiki-toc vimwiki-table-of-contents\nYou can create a \u0026ldquo;table of contents\u0026rdquo; at the top of your wiki file. The command |:VimwikiTOC| creates the magic header \u0026gt; = Contents = in the current file and below it a list of all the headers in this file as links, so you can directly jump to specific parts of the file.\nFor the indentation of the list, the value of |vimwiki-option-list_margin| is used.\nIf you don\u0026rsquo;t want the TOC to sit in the very first line, e.g. because you have a modeline there, put the magic header in the second or third line and run :VimwikiTOC to update the TOC.\nIf English is not your preferred language, set the option |g:vimwiki_toc_header| to your favorite translation.\nIf you want to keep the TOC up to date automatically, use the option |vimwiki-option-auto_toc|.\nvimwiki-option-auto_toc\nKey Default value Values~ auto_toc 0 0, 1\nDescription~ Set this option to 1 to automatically update the table of contents when the current wiki page is saved: \u0026gt; let g:vimwiki_list = [{\u0026lsquo;path\u0026rsquo;: \u0026lsquo;~/my_site/\u0026rsquo;, \u0026lsquo;auto_toc\u0026rsquo;: 1}]\nvimwiki-option-list_margin\nKey Default value~ list_margin -1 (0 for markdown)\nDescription~ Width of left-hand margin for lists. When negative, the current \u0026lsquo;shiftwidth\u0026rsquo; is used. This affects the appearance of the generated links (see |:VimwikiGenerateLinks|), the Table of contents (|vimwiki-toc|) and the behavior of the list manipulation commands |:VimwikiListChangeLvl| and the local mappings |vimwiki_glstar|, |vimwiki_gl#| |vimwiki_gl-|, |vimwiki_gl-|, |vimwiki_gl1|, |vimwiki_gla|, |vimwiki_glA|, |vimwiki_gli|, |vimwiki_glI| and |vimwiki_i__|.\nNote: if you use Markdown or MediaWiki syntax, you probably would like to set this option to 0, because every indented line is considered verbatim text.\ng:vimwiki_toc_header_level\nThe header level of the Table of Contents (see |vimwiki-toc|). Valid values are from 1 to 6.\nThe default is 1.\ng:vimwiki_toc_link_format\nThe format of the links in the Table of Contents (see |vimwiki-toc|).\nValue Description~ 0 Extended: The link contains the description and URL. URL references all levels. 1 Brief: The link contains only the URL. URL references only the immediate level.\nDefault: 0\nKey bindings # Normal mode # Note: your terminal may prevent capturing some of the default bindings listed below. See :h vimwiki-local-mappings for suggestions for alternative bindings if you encounter a problem.\nBasic key bindings # \u0026lt;Leader\u0026gt;ww \u0026ndash; Open default /wiki index file. \u0026lt;Leader\u0026gt;wt \u0026ndash; Open default /wiki index file in a new tab. \u0026lt;Leader\u0026gt;ws \u0026ndash; Select and open /wiki index file. \u0026lt;Leader\u0026gt;wd \u0026ndash; Delete /wiki file you are in. \u0026lt;Leader\u0026gt;wr \u0026ndash; Rename /wiki file you are in. \u0026lt;Enter\u0026gt; \u0026ndash; Follow/Create /wiki link. \u0026lt;Shift-Enter\u0026gt; \u0026ndash; Split and follow/create /wiki link. \u0026lt;Ctrl-Enter\u0026gt; \u0026ndash; Vertical split and follow/create /wiki link. \u0026lt;Backspace\u0026gt; \u0026ndash; Go back to parent(previous) /wiki link. \u0026lt;Tab\u0026gt; \u0026ndash; Find next /wiki link. \u0026lt;Shift-Tab\u0026gt; \u0026ndash; Find previous /wiki link. Advanced key bindings # Refer to the complete documentation at :h vimwiki-mappings to see many more bindings.\nCommands # :Vimwiki2HTML \u0026ndash; Convert current wiki link to HTML. :VimwikiAll2HTML \u0026ndash; Convert all your wiki links to HTML. :help vimwiki-commands \u0026ndash; List all commands. :help vimwiki \u0026ndash; General vimwiki help docs. Diary # alias # alias todo=\u0026lsquo;vim -c VimwikiDiaryIndex\u0026rsquo;\nHotkeys # :VimwikiDiaryGenerateLinks ^w^i Generate links ^w^w open today ^wi Open diary index ctrl + up previous day ctrl + down next day\nHow to create Weekly, Monthly, and yearly notes How to do a template for daily set folder location for diary Diary Template # https://frostyx.cz/posts/vimwiki-diary-template\nNested folder structure # [dev](dev/ndex)\nSay yes to make new directory\nwiki # Convert to html live and shows some design stuff https://www.youtube.com/watch?v=A1YgbAp5YRc\nhttps://github.com/Dynalonwiki\nTaskwarrior # https://www.youtube.com/watch?v=UuHJloiDErM requires neovim?\ntaskwiki # vimwiki integration with task warrior https://github.com/tools-life/taskwiki https://www.youtube.com/watch?v=UuHJloiDErM\nCtrl P # Install # Plug \u0026lsquo;ctrlpvim/ctrlp.vim\u0026rsquo;\n","externalUrl":null,"permalink":"/tech/tools/vimguide/","section":"Teches","summary":"\u003cp\u003eVim (Vi Improved)\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eVim\u003c/code\u003e stands for vi (Improved) just like its name it stands for an improved version of the \u003ccode\u003evi\u003c/code\u003e text editor command.\u003c/p\u003e\n\u003cp\u003eLightweight\u003c/p\u003e\n\u003cp\u003eStart Vim\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003evim\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2 class=\"relative group\"\u003eVim Search Patterns\n    \u003cdiv id=\"vim-search-patterns\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#vim-search-patterns\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\n\u003ch3 class=\"relative group\"\u003eMoving the Cursor\n    \u003cdiv id=\"moving-the-cursor\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#moving-the-cursor\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eh or left arrow - move left one character\nk or up arrow - move up one line\nj or down arrow - move down one line\nl or right arrow - will move you right one character\u003c/p\u003e","title":"Vim Guide","type":"tech"},{"content":" Used for storage optimization Device driver layer that sits between the Linux kernel and the physical storage devices. Conserve disk space, improve data throughput, and save on storage cost. Employs thin provisioning, de-duplication, and compression technologies to help realize the goals. How VDO Conserves Storage # Stage 1\nMakes use of thin provisioning to identify and eliminate empty (zero-byte) data blocks. (zero-block elimination) Removes randomization of data blocks by moving in-use data blocks to contiguous locations on the storage device. Stage 2\nIf it detects that new data is an identical copy of some existing data, it makes an internal note of it but does not actually write the redundant data to the disk. (de-duplication) Implemented with the inclusion of a kernel module called UDS (Universal De-duplication Service). Stage 3\nCalls upon another kernel module called kvdo, which compresses the residual data blocks and consolidates them on a lower number of blocks. Results in a further drop in storage space utilization. Runs in the background and processes inbound data through the three stages on VDO-enabled volumes. Not a CPU or memory-intensive process VDO Integration with LVM # LVM utilities have been enhanced to include options to support VDO volumes. VDO Components # Utilizes the concepts of pool and volume. pool logical volume that is created inside an LVM volume group using a deduplicated storage space. volume Just like a regular LVM logical volume, but it is provisioned in a pool. Needs to be formatted with file system structures before it can be used. vdo and kmod-kvdo Commands # Create, mount, and manage LVM VDO volumes Installed on the system by default. vdo\nInstalls the tools necessary to support the creation and management of VDO volumes kmod-kvdo\nImplements fine-grained storage virtualization, thin provisioning, and compression. Not installed by default? Exercise 13-12: Create an LVM VDO Volume # Initialize the 5GB disk (sdf) for use in LVM VDO. Create a volume group called vgvdo and add the physical volume to it. List and display the volume group and the physical volume. Create a VDO volume called lvvdo with a virtual size of 20GB. 1. Initialize the sdf disk using the pvcreate command:\n[root@server2 ~]# sudo pvcreate /dev/sdf Physical volume \u0026#34;/dev/sdf\u0026#34; successfully created. 2. Create vgvdo volume group using the vgcreate command:\n[root@server2 ~]# sudo vgcreate vgvdo /dev/sdf Volume group \u0026#34;vgvdo\u0026#34; successfully created 3. Display basic information about the volume group:\n[root@server2 ~]# sudo vgdisplay vgvdo Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. --- Volume group --- VG Name vgvdo System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size \u0026lt;5.00 GiB PE Size 4.00 MiB Total PE 1279 Alloc PE / Size 0 / 0 Free PE / Size 1279 / \u0026lt;5.00 GiB VG UUID tED1vC-Ylec-fpeR-KM8F-8FzP-eaQ4-AsFrgc 4. Create a VDO volume called lvvdo using the lvcreate command. Use the -l option to specify the number of logical extents (1279) to be allocated and the -V option for the amount of virtual space.\n[root@server2 ~]# sudo dnf install kmod-kvdo [root@server2 ~]# sudo lvcreate --type vdo -l 1279 -n lvvdo -V 20G vgvdo The VDO volume can address 2 GB in 1 data slab. It can grow to address at most 16 TB of physical storage in 8192 slabs. If a larger maximum size might be needed, use bigger slabs. Logical volume \u0026#34;lvvdo\u0026#34; created. 5. Display detailed information about the volume group including the logical volume and the physical volume:\n[root@server2 ~]# sudo vgdisplay -v vgvdo Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. --- Volume group --- VG Name vgvdo System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size \u0026lt;5.00 GiB PE Size 4.00 MiB Total PE 1279 Alloc PE / Size 1279 / \u0026lt;5.00 GiB Free PE / Size 0 / 0 VG UUID tED1vC-Ylec-fpeR-KM8F-8FzP-eaQ4-AsFrgc --- Logical volume --- LV Path /dev/vgvdo/vpool0 LV Name vpool0 VG Name vgvdo LV UUID yGAsK2-MruI-QGy2-Q1IF-CDDC-XPNT-qkjJ9t LV Write Access read/write LV Creation host, time server2, 2024-06-16 09:35:46 -0700 LV VDO Pool data vpool0_vdata LV VDO Pool usage 60.00% LV VDO Pool saving 100.00% LV VDO Operating mode normal LV VDO Index state online LV VDO Compression st online LV VDO Used size \u0026lt;3.00 GiB LV Status NOT available LV Size \u0026lt;5.00 GiB Current LE 1279 Segments 1 Allocation inherit Read ahead sectors auto --- Logical volume --- LV Path /dev/vgvdo/lvvdo LV Name lvvdo VG Name vgvdo LV UUID nnGTW5-tVFa-T3Cy-9nHj-sozF-2KpP-rVfnSq LV Write Access read/write LV Creation host, time server2, 2024-06-16 09:35:47 -0700 LV VDO Pool name vpool0 LV Status available # open 0 LV Size 20.00 GiB Current LE 5120 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:4 --- Physical volumes --- PV Name /dev/sdf PV UUID 0oAXHG-C4ub-Myou-5vZf-QxIX-KVT3-ipMZCp PV Status allocatable Total PE / Free PE 1279 / 0 The output reflects the creation of two logical volumes: a pool called /dev/vgvdo/vpool0 and a volume called /dev/vgvdo/lvvdo.\nExercise 13-13: Remove a Volume Group and Uninitialize Physical Volume(Server2) # remove the vgvdo volume group along with the VDO volumes uninitialize the physical volume /dev/sdf. confirm the deletion. 1. Remove the volume group along with the VDO volumes using the vgremove command:\n[root@server2 ~]# sudo vgremove vgvdo -f Logical volume \u0026#34;lvvdo\u0026#34; successfully removed. Volume group \u0026#34;vgvdo\u0026#34; successfully removed Remember to proceed with caution whenever you perform erase operations.\n2. Execute sudo vgs and sudo lvs commands for confirmation.\n[root@server2 ~]# sudo vgs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 [root@server2 ~]# sudo lvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g 3. Remove the LVM structures from sdf using the pvremove command:\n[root@server2 ~]# sudo pvremove /dev/sdf Labels on physical volume \u0026#34;/dev/sdf\u0026#34; successfully wiped. 4. Confirm the removal by running sudo pvs.\n[root@server2 ~]# sudo pvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 The disk is now back to its raw state and can be repurposed.\n5. Verify that the sdf disk used in the previous exercises has returned to its original raw state using the lsblk command:\n[root@server2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─rhel-root 253:0 0 17G 0 lvm / └─rhel-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 250M 0 disk sdc 8:32 0 250M 0 disk sdd 8:48 0 250M 0 disk sde 8:64 0 250M 0 disk sdf 8:80 0 5G 0 disk sr0 11:0 1 9.8G 0 rom This brings the exercise to an end.\nStorage DYI Labs # Lab 13-1: Create and Remove Partitions with parted # Create a 100MB primary partition on one of the available 250MB disks (lsblk) by invoking the parted utility directly at the command prompt. Apply label \u0026ldquo;msdos\u0026rdquo; if the disk is new.\n[root@server20 ~]# sudo parted /dev/sdb mklabel msdos Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? yes Information: You may need to update /etc/fstab. [root@server20 ~]# sudo parted /dev/sdb mkpart primary 1 101m Information: You may need to update /etc/fstab. Create another 100MB partition by running parted interactively while ensuring that the second partition won\u0026rsquo;t overlap the first.\n[root@server20 ~]# parted /dev/sdb GNU Parted 3.5 Using /dev/sdb Welcome to GNU Parted! Type \u0026#39;help\u0026#39; to view a list of commands. (parted) mkpart primary 101 201m Verify the label and the partitions.\n(parted) print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdb: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 101MB 99.6MB primary 2 101MB 201MB 101MB primary Remove both partitions at the command prompt.\n[root@server20 ~]# sudo parted /dev/sdb rm 1 rm 2 Lab 13-2: Create and Remove Partitions with gdisk # Create two 80MB partitions on one of the 250MB disks (lsblk) using the gdisk utility. Make sure the partitions won\u0026rsquo;t overlap.\nCommand (? for help): o This option deletes all partitions and creates a new protective MBR. Proceed? (Y/N): y Command (? for help): p Disk /dev/sdb: 512000 sectors, 250.0 MiB Model: VBOX HARDDISK Sector size (logical/physical): 512/512 bytes Disk identifier (GUID): 226F7476-7F8C-4445-9025-53B6737AD1E4 Partition table holds up to 128 entries Main partition table begins at sector 2 and ends at sector 33 First usable sector is 34, last usable sector is 511966 Partitions will be aligned on 2048-sector boundaries Total free space is 511933 sectors (250.0 MiB) Number Start (sector) End (sector) Size Code Name Command (? for help): n Partition number (1-128, default 1): First sector (34-511966, default = 2048) or {+-}size{KMGTP}: Last sector (2048-511966, default = 511966) or {+-}size{KMGTP}: +80M Current type is 8300 (Linux filesystem) Hex code or GUID (L to show codes, Enter = 8300): Changed type of partition to \u0026#39;Linux filesystem\u0026#39; Command (? for help): n Partition number (2-128, default 2): 2 First sector (34-511966, default = 165888) or {+-}size{KMGTP}: 165888 Last sector (165888-511966, default = 511966) or {+-}size{KMGTP}: +80M Current type is 8300 (Linux filesystem) Hex code or GUID (L to show codes, Enter = 8300): Changed type of partition to \u0026#39;Linux filesystem\u0026#39; Verify the partitions.\nCommand (? for help): p Disk /dev/sdb: 512000 sectors, 250.0 MiB Model: VBOX HARDDISK Sector size (logical/physical): 512/512 bytes Disk identifier (GUID): 226F7476-7F8C-4445-9025-53B6737AD1E4 Partition table holds up to 128 entries Main partition table begins at sector 2 and ends at sector 33 First usable sector is 34, last usable sector is 511966 Partitions will be aligned on 2048-sector boundaries Total free space is 184253 sectors (90.0 MiB) Number Start (sector) End (sector) Size Code Name 1 2048 165887 80.0 MiB 8300 Linux filesystem 2 165888 329727 80.0 MiB 8300 Linux filesystem Save\nCommand (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): y OK; writing new GUID partition table (GPT) to /dev/sdb. The operation has completed successfully. Delete the partitions\nCommand (? for help): d Partition number (1-2): 1 Command (? for help): d Using 2 Command (? for help): w Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING PARTITIONS!! Do you want to proceed? (Y/N): y OK; writing new GUID partition table (GPT) to /dev/sdb. The operation has completed successfully. Lab 13-3: Create Volume Group and Logical Volumes # initialize 1x250MB disk for use in LVM (use lsblk to identify available disks).\nroot@server2 ~]# sudo parted /dev/sdd mklabel msdos Warning: The existing disk label on /dev/sdd will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? yes Information: You may need to update /etc/fstab. [root@server2 ~]# sudo parted /dev/sdd mkpart primary 1 250m Information: You may need to update /etc/fstab. [root@server2 ~]# sudo parted /dev/sdd print Model: ATA VBOX HARDDISK (scsi) Disk /dev/sdd: 262MB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 250MB 249MB primary [root@server2 ~]# sudo pvcreate /dev/sdd1 Physical volume \u0026#34;/dev/sdd1\u0026#34; successfully created. (Can also just use the full disk without making it into a partition first.)\nCreate volume group vg100 with PE size 16MB and add the physical volume.\n[root@server2 ~]# sudo vgcreate -vs 16 vg100 /dev/sdd1 Wiping signatures on new PV /dev/sdd1. Adding physical volume \u0026#39;/dev/sdd1\u0026#39; to volume group \u0026#39;vg100\u0026#39; Creating volume group backup \u0026#34;/etc/lvm/backup/vg100\u0026#34; (seqno 1). Volume group \u0026#34;vg100\u0026#34; successfully created Create two logical volumes lvol0 and swapvol of sizes 90MB and 120MB.\n[root@server2 ~]# sudo lvcreate -vL 90 vg100 Creating logical volume lvol0 Archiving volume group \u0026#34;vg100\u0026#34; metadata (seqno 1). Activating logical volume vg100/lvol0. activation/volume_list configuration setting not defined: Checking only host tags for vg100/lvol0. Creating vg100-lvol0 Loading table for vg100-lvol0 (253:2). Resuming vg100-lvol0 (253:2). Wiping known signatures on logical volume vg100/lvol0. Initializing 4.00 KiB of logical volume vg100/lvol0 with value 0. Logical volume \u0026#34;lvol0\u0026#34; created. Creating volume group backup \u0026#34;/etc/lvm/backup/vg100\u0026#34; (seqno 2). [root@server2 ~]# sudo lvcreate -l 8 -n swapvol vg100 Logical volume \u0026#34;swapvol\u0026#34; created. Use the vgs, pvs, lvs, and vgdisplay commands for verification.\n[root@server2 ~]# lvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g lvol0 vg100 -wi-a----- 90.00m swapvol vg100 -wi-a----- 120.00m [root@server2 ~]# vgs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 vg100 1 2 0 wz--n- 225.00m 15.00m [root@server2 ~]# pvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 /dev/sdd1 vg100 lvm2 a-- 225.00m 15.00m [root@server2 ~]# vgdisplay Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. --- Volume group --- VG Name vg100 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 5 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size 225.00 MiB PE Size 15.00 MiB Total PE 15 Alloc PE / Size 14 / 210.00 MiB Free PE / Size 1 / 15.00 MiB VG UUID fEUf8R-nxKF-Uxud-7rmm-JvSQ-PsN1-Mrs3zc --- Volume group --- VG Name rhel System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size \u0026lt;19.00 GiB PE Size 4.00 MiB Total PE 4863 Alloc PE / Size 4863 / \u0026lt;19.00 GiB Free PE / Size 0 / 0 VG UUID UiK3fy-FGOc-2fnP-C1Y6-JS0l-irEe-Sq3c4h Lab 13-4: Expand Volume Group and Logical Volume # Create a partition on an available 250MB disk and initialize it for use in LVM (use lsblk to identify available disks).\n[root@server2 ~]# parted /dev/sdb mklabel msdos Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue? Yes/No? yes Information: You may need to update /etc/fstab. [root@server2 ~]# parted /dev/sdb mkpart primary 1 250m Information: You may need to update /etc/fstab. Add the new physical volume to vg100.\n[root@server2 ~]# sudo vgextend vg100 /dev/sdb1 Device /dev/sdb1 has updated name (devices file /dev/sdd1) Physical volume \u0026#34;/dev/sdb1\u0026#34; successfully created. Volume group \u0026#34;vg100\u0026#34; successfully extended Expand the lvol0 logical volume to size 300MB.\n[root@server2 ~]# lvextend -L +210 /dev/vg100/lvol0 Size of logical volume vg100/lvol0 changed from 90.00 MiB (6 extents) to 300.00 MiB (20 extents). Logical volume vg100/lvol0 successfully resized. Use the vgs, pvs, lvs, and vgdisplay commands for verification.\n[[root@server2 ~]# lvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g lvol0 vg100 -wi-a----- 90.00m swapvol vg100 -wi-a----- 120.00m](\u0026lt;[root@server20 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- %3C17.00g swap rhel -wi-ao---- 2.00g lvol0 vg100 -wi-a----- 300.00m swapvol vg100 -wi-a----- 120.00m\u0026gt;) [root@server2 ~]# vgs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 vg100 2 2 0 wz--n- 450.00m 30.00m [root@server2 ~]# pvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- \u0026lt;19.00g 0 /dev/sdb1 vg100 lvm2 a-- 225.00m 30.00m /dev/sdd1 vg100 lvm2 a-- 225.00m 0 [root@server2 ~]# lvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g lvol0 vg100 -wi-a----- 300.00m swapvol vg100 -wi-a----- 120.00m [root@server2 ~]# vgdisplay Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. --- Volume group --- VG Name vg100 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 7 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 450.00 MiB PE Size 15.00 MiB Total PE 30 Alloc PE / Size 28 / 420.00 MiB Free PE / Size 2 / 30.00 MiB VG UUID fEUf8R-nxKF-Uxud-7rmm-JvSQ-PsN1-Mrs3zc --- Volume group --- VG Name rhel System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size \u0026lt;19.00 GiB PE Size 4.00 MiB Total PE 4863 Alloc PE / Size 4863 / \u0026lt;19.00 GiB Free PE / Size 0 / 0 VG UUID UiK3fy-FGOc-2fnP-C1Y6-JS0l-irEe-Sq3c4h Lab 13-5: Add a VDO Logical Volume # initialize the sdf disk for use in LVM and add it to vgvdo1.\n[root@server2 ~]# pvcreate /dev/sdc Physical volume \u0026#34;/dev/sdc\u0026#34; successfully created. [root@server2 ~]# sudo vgextend vgvdo1 /dev/sdc Volume group \u0026#34;vgvdo1\u0026#34; successfully extended Create a VDO logical volume named vdovol using the entire disk capacity.\n[root@server2 ~]# lvcreate --type vdo -n vdovol -l 100%FREE vgvdo1 WARNING: LVM2_member signature detected on /dev/vgvdo1/vpool0 at offset 536. Wipe it? [y/n]: y Wiping LVM2_member signature on /dev/vgvdo1/vpool0. Logical blocks defaulted to 523108 blocks. The VDO volume can address 2 GB in 1 data slab. It can grow to address at most 16 TB of physical storage in 8192 slabs. If a larger maximum size might be needed, use bigger slabs. Logical volume \u0026#34;vdovol\u0026#34; created. Use the vgs, pvs, lvs, and vgdisplay commands for verification.\n[root@server2 ~]# vgs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB123ecea1-63467dee PVID RjcGRyHDIWY0OqAgfIHC93WT03Na1WoO last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 PVID qeP9dCevNnTy422I8p18NxDKQ2WyDodU last seen on /dev/sdf1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID brKVLFEG3AoBzhWoso0Sa1gLYHgNZ4vL last seen on /dev/sdb1 not found. VG #PV #LV #SN Attr VSize VFree rhel 1 2 0 wz--n- \u0026lt;19.00g 0 vgvdo1 2 2 0 wz--n- \u0026lt;5.24g 248.00m Lab 13-6: Reduce and Remove Logical Volumes # reduce the size of vdovol logical volume to 80MB.\n[root@server2 ~]# lvreduce -L 80 /dev/vgvdo1/vdovol No file system found on /dev/vgvdo1/vdovol. WARNING: /dev/vgvdo1/vdovol: Discarding 1.91 GiB at offset 83886080, please wait... Size of logical volume vgvdo1/vdovol changed from 1.99 GiB (510 extents) to 80.00 MiB (20 extents). Logical volume vgvdo1/vdovol successfully resized. [root@server2 ~]# lvs Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID none last seen on /dev/sdd2 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB123ecea1-63467dee PVID RjcGRyHDIWY0OqAgfIHC93WT03Na1WoO last seen on /dev/sdd1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VBa5e3cbf7-10921e08 PVID qeP9dCevNnTy422I8p18NxDKQ2WyDodU last seen on /dev/sdf1 not found. Devices file sys_wwid t10.ATA_VBOX_HARDDISK_VB428913dd-446a194f PVID brKVLFEG3AoBzhWoso0Sa1gLYHgNZ4vL last seen on /dev/sdb1 not found. LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rhel -wi-ao---- \u0026lt;17.00g swap rhel -wi-ao---- 2.00g vdovol vgvdo1 vwi-a-v--- 80.00m vpool0 0.00 vpool0 vgvdo1 dwi------- \u0026lt;5.00g 60.00 [root@server2 ~]# erase logical volume vdovol.\n[root@server2 ~]# lvremove /dev/vgvdo1/vdovol Do you really want to remove active logical volume vgvdo1/vdovol? [y/n]: y Logical volume \u0026#34;vdovol\u0026#34; successfully removed. Confirm the deletion with vgs, pvs, lvs, and vgdisplay commands.\nLab 13-7: Remove Volume Group and Physical Volumes # \\remove the volume group and uninitialized the physical volumes.\n[root@server2 ~]# vgremove vgvdo1 Volume group \u0026#34;vgvdo1\u0026#34; successfully removed [root@server2 ~]# pvremove /dev/sdc Labels on physical volume \u0026#34;/dev/sdc\u0026#34; successfully wiped. [root@server2 ~]# pvremove /dev/sdf Labels on physical volume \u0026#34;/dev/sdf\u0026#34; successfully wiped. Confirm the deletion with vgs, pvs, lvs, and vgdisplay commands.\nUse the lsblk command and verify that the disks used for the LVM labs no longer show LVM information.\n[root@server2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─rhel-root 253:0 0 17G 0 lvm / └─rhel-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 250M 0 disk sdc 8:32 0 250M 0 disk sdd 8:48 0 250M 0 disk sde 8:64 0 250M 0 disk sdf 8:80 0 5G 0 disk sr0 11:0 1 9.8G 0 rom ","externalUrl":null,"permalink":"/tech/linux/virtual-data-optimizer-vdo/","section":"Teches","summary":"\u003cul\u003e\n\u003cli\u003eUsed for storage optimization\u003c/li\u003e\n\u003cli\u003eDevice driver layer that sits between the Linux kernel and the physical storage devices.\u003c/li\u003e\n\u003cli\u003eConserve disk space, improve data throughput, and save on storage cost.\u003c/li\u003e\n\u003cli\u003eEmploys \u003cstrong\u003ethin provisioning\u003c/strong\u003e, \u003cstrong\u003ede-duplication\u003c/strong\u003e, and \u003cstrong\u003ecompression\u003c/strong\u003e technologies to help realize the goals.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 class=\"relative group\"\u003eHow VDO Conserves Storage\n    \u003cdiv id=\"how-vdo-conserves-storage\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#how-vdo-conserves-storage\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStage 1\u003c/strong\u003e\u003c/p\u003e","title":"Virtual Data Optimizer (VDO)","type":"tech"},{"content":" Virtual LAN Concepts # reasons for choosing to create smaller broadcast domains (VLANs):\n-reduce CPU overhead on each devicereduce security risks different security policies per VLAN more flexible designs that -group users by department, or by groups that work together, instead of by physical location solve problems more quickly-failure domain for many problems is the same set of devices as those in the same broadcast domain reduce the workload for the Spanning Tree Protocol (STP) -by limiting a VLAN to a single access switch\n802.1q and ISL\n802.1Q -inserts a 4-byte 802.1Q VLAN header into the Ethernet header\n12 - bit VLAN ID field inside the 802.1Q header \u0026ndash;supports a theoretical maximum of 212 (4096) VLANs, but in practice it supports a maximum of 4094. Both 802.1Q and ISL use 12 bits to tag the VLAN ID, withtwo reserved values [0 and 4095]. 802.1q header includes Type, priority, Flag, Vlan ID\nnormal\u0026ndash;1 to 1005. all switches can use-range\nOnly some switches can use 1006 to 4094\ndepends on the configuration of the VLAN Trunking Protocol (VTP) Extended range Cisco switches break the range of VLAN IDs (1–4094) into the normal range and the extended range. 231852+ 802.1Q simply does not add an 802.1Q header to frames in the native VLAN\n#show vlan brief\nVLAN Trunking Protocol (VTP)\nvtp mode transparent vtp mode off -The server switches can configure VLANs in the standard range only (1The client switches cannot configure VLANs. –1005). Both servers and clients may be learning new VLANs from other switches and seeing their VLANs deleted by other switches because of VTP. If your switch usesVTP server or client mode show running - does not list any vlan commands - confi g show vtp status If possible to learn more about VTP for other purposes.in the lab, switch to disable VTP and ignore VTP for your switch configuration practice until you decide negotiate ISL or 802.1q If both switches support both protocols, they use ISL; -otherwise, they use the protocol that both support. Dynamic Trunking Protocol (DTP). switchport trunk encapsulation {dot1q | isl | negotiate} - configure the type or allow DTP to negotiate the type. Access-always access trunk-always trunk dynamic desirable--initiates negotiation messages and responds to negotiation messagesAccess if other side is access, otherwise trunk dynamic auto-passively waits to receive trunk negotiation messages VLAN Trunking Configuration\nQuick Commands #show vtp status VTP Trunking #switchport trunk encapsulation dot1q/isl/negotiate#switchport mode access/trunk/dynamic desirable/dynamic auto #switchport trunk allowed vlan#show interfaces trunk output #show interfaces trunk#show interfaces switchport Voice#switchport trunk native vlan 2 #show int f0/4 trunk#switchport voice vlan 13 VLAN #show vlan brief#show vlan #show spanning-tree vlan 2 VLANSFriday, July 2, 2021 2:50 PM -passively waits to receive trunk negotiation messagesdefault setting -access if both ends use thistrunk if other end is trunk or Dynamic desirable On a switch that supports both ISL and 802.1Q, this value would by default list “negotiate,” to mean that the type of encapsulation is negotiated. Cisco recommends disabling trunk negotiation on most ports for better security\n(config Disable DTP - if) switchport nonegotiate\nData and Voice VLAN Concepts\nswitchport voice - -can configure on the same access port that has a normal vlan assignedCDP must be enabled* vlan 11 Voice Data is tagged with 802.1Q header -see the voice vlanadministrative and operational mode access mode vlan show interfaces FastEthernet 0/4 switchport show interfaces trunkshow interfaces f0/4 trunk\nvlans allowed on trunk\u0026ndash; (^1) minus vlans removed by the - (^4094) switchport trunk allowed command\nvlans allowed and active in management domain\u0026ndash;the first list minus vlans that are not configuredminus vlans that are shutdown\nvlans in spanning tree forwarding state and not (VTP) pruned\u0026ndash;minus vlans that are in a STP blocking stateminus vlans that are VTP pruned\nShow interfaces trunk will not show the voice VLAN as a trunk, it will only show it if you specify the interface.\nConfirm that all VLANs are both defined and active. show vlanShow vlan brief\nCheck the allowed VLAN lists on both ends of each trunk\nshow interfaces - lists information about currently operational trunks interface-id trunk\n#switchport trunk allowed vlan\nShow vlan - (does the vlan exist and is it active?\nHas the vlan been vtp pruned?\nIs the vlan in an STP forwarding state? #show spanning-tree vlan 2\nCheck for incorrect trunk configuration settings that result in one switch operating as a trunk, with the neighboring switch not operating as a trunk.\n#show interfaces trunk\n#show interfaces switchport. The trunk is in an STP forwarding state in that VLAN (as also seen in the -check administrative and operational modes show spanning-tree vlan vlan-id\ncommand). #switchport trunk allowed vlan\nDTP on one switch but not the other\nCheck the native VLAN settings on both ends\nNative vlan must match on both switches. #switchport trunk native vlan 2 vlan hoppinga frame being sent in one vlan but then being believed to be in a different vlan Troubleshooting VLANS and VLAN trunks\n","externalUrl":null,"permalink":"/tech/networking/vlans/","section":"Teches","summary":"\u003ch2 class=\"relative group\"\u003eVirtual LAN Concepts\n    \u003cdiv id=\"virtual-lan-concepts\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#virtual-lan-concepts\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003ereasons for choosing to create smaller broadcast domains (VLANs):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e-reduce CPU overhead on each devicereduce security risks\u003c/li\u003e\n\u003cli\u003edifferent security policies per VLAN\u003c/li\u003e\n\u003cli\u003emore flexible designs that -group users by department, or by groups that work together, instead of by physical location\u003c/li\u003e\n\u003cli\u003esolve problems more quickly-failure domain for many problems is the same set of devices as those in the same broadcast domain\u003c/li\u003e\n\u003cli\u003ereduce the workload for the Spanning Tree Protocol (STP) -by limiting a VLAN to a single access switch\u003cbr\u003e\n802.1q and ISL\u003cbr\u003e\n802.1Q -inserts a 4-byte 802.1Q VLAN header into the Ethernet header\u003cbr\u003e\n12 - bit VLAN ID field inside the 802.1Q header \u0026ndash;supports a theoretical maximum of 212 (4096) VLANs, but in practice it supports a maximum of 4094. Both 802.1Q and ISL use 12 bits to tag the VLAN ID, withtwo reserved values [0 and 4095].\u003c/li\u003e\n\u003cli\u003e802.1q header includes Type, priority, Flag, Vlan ID\u003cbr\u003e\nnormal\u0026ndash;1 to 1005. all switches can use-range\u003cbr\u003e\nOnly some switches can use 1006 to 4094\u003cbr\u003e\ndepends on the configuration of the VLAN Trunking Protocol (VTP)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eExtended range\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eCisco switches break the range of VLAN IDs (1–4094) into the normal range and the extended range.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e231852+\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e802.1Q simply does not add an 802.1Q header to frames in the native VLAN\u003cbr\u003e\n\u003cstrong\u003e#show vlan brief\u003c/strong\u003e\u003cbr\u003e\nVLAN Trunking Protocol (VTP)\u003cbr\u003e\n\u003cstrong\u003evtp mode transparent vtp mode off\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e-The server switches can configure VLANs in the standard range only (1The client switches cannot configure VLANs. –1005).\u003c/li\u003e\n\u003cli\u003eBoth servers and clients may be learning new VLANs from other switches and seeing their VLANs deleted by other switches because of VTP.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eIf your switch usesVTP server or client mode\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow running - does not list any vlan commands - confi g\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshow vtp status\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eIf possible to learn more about VTP for other purposes.in the lab, switch to disable VTP and ignore VTP for your switch configuration practice until you decide\n\u003cul\u003e\n\u003cli\u003enegotiate ISL or 802.1q\u003c/li\u003e\n\u003cli\u003eIf both switches support both protocols, they use ISL; -otherwise, they use the protocol that both support.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eDynamic Trunking Protocol (DTP).\nswitchport trunk encapsulation {dot1q | isl | negotiate} - configure the type or allow DTP to negotiate the type.\nAccess-always access\ntrunk-always trunk\ndynamic desirable--initiates negotiation messages and responds to negotiation messagesAccess if other side is access, otherwise trunk\ndynamic auto-passively waits to receive trunk negotiation messages\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eVLAN Trunking Configuration\u003c/p\u003e","title":"VLANs","type":"tech"},{"content":"1.0 Network Fundamentals\n1.2 Describe the characteristics of network topology architecture\n1.2.d WAN\n5.0 Security Fundamentals\n5.5 Describe remote access and site-to-site VPNs\nMetro Ethernet\nCustomers connect to a Metro Ethernet service with either routers or Layer 3 switches.\nMetro Ethernet Physical Design and Topology # A device physically near to customer sites is called a point of presence (PoP).\nThe physical link between the customer and the SP is called an access link.\nWhen using Ethernet specifically, an Ethernet access link. Everything that happens on that link falls within the definition of the user network interface, or UNI.\nThe switch will look at the Ethernet header’s MAC address fields and at 802.1Q trunking headers for VLAN tags, but the details inside the network remain hidden.\nEthernet WAN Services and Topologies # E-Line (point-to-point), E-LAN (full mesh), and E-Tree (point-to-multipoint, hub and spoke).\nVirtual Private Wire Service (VPWS) used for what MEF defines as E-line service and Virtual Private LAN Service (VPLS) used for what MEF defines as E-LAN service and Ethernet over MPLS (EoMPLS) refer to cases in which the SP uses MPLS internally to create what the customer sees as an Ethernet WAN service.\nEthernet Line Service (Point-to-Point) # The customer connects two sites with access links. Then the MetroE service allows the two customer devices to send Ethernet frames to each other.\n▪ The routers would use physical Ethernet interfaces. ▪ The routers would configure IP addresses in the same subnet as each other. ▪ Their routing protocols would become neighbors and exchange routes.\nEthernet Virtual Connection, or EVC, to define which user (customer) devices can communicate with which.\nCreates a point-to-point EVC, meaning that the service allows two endpoints to communicate.\nDesigns like Figure 14-4, with multiple E-Line services on a single access link, use 802.1Q trunking, with a different VLAN ID for each E-Line service. As a result, the router configuration can use a typical router configuration with trunking and subinterfaces.\nWith MetroE, the LAN and WAN are both Ethernet, so an Ethernet switch becomes an option.\nEthernet LAN Service (Full Mesh) # allows all devices connected to that service to send Ethernet frames directly to every other device An E-LAN service connects the sites in a full mesh\n##### Ethernet Tree Service (Hub and Spoke) (E-Tree) creates a WAN topology in which the central site device can send Ethernet frames directly to each remote (leaf) site, but the remote (leaf) sites can send only to the central site\npartial mesh, hub and spoke, and point-to-multipoint works well for designs with a central site plus many remote sites.\n##### Layer 3 Design Using Metro Ethernet Layer 3 Design with E-Line Service\nLayer 3 Design with E-LAN Service\n#### Multiprotocol Label Switching (MPLS) a WAN service that routes IP packets between customer sites the middle four routers could represent the SP’s MPLS network, with the numbered routers on the edges being routers owned by one company. the SP builds its IP network to also use Multiprotocol Label Switching (MPLS), in particular MPLS VPNs allow the SP to build one large MPLS network, which also creates a private IP-based WAN for each of its customers. The routers on the edge of the MPLS network add and remove an MPLS header to packets as they enter and exit the MPLS network.The devices inside the MPLS network then use the label field inside that MPLS header when forwarding data across the MPLS network. The choices of the labels to use, along with other related logic, allow the MPLS VPN to create separate VPNs to keep different customers’ traffic separate. layer 3 service but sometimes called a Layer 2.5 protocol because it adds the MPLS header between the data-link header (Layer 2) and the IP header (Layer 3). SP’s MPLS VPN network: Will use a routing protocol to build routing protocol neighbor relationships with customer routers ###### ○ ○ Will learn customer subnets/routes with those routing protocols ○ Will advertise a customer’s routes with a routing protocol so that all routers that customer connects to the MPLS VPN can learn all routes as advertised through the MPLS VPN network Will make decisions about MPLS VPN forwarding, including what MPLS labels to add and remove, based on the customer’s IP address space and customer IP routes\n###### ○ MPLS VPNs create a private network by keeping customer data separate, but not by encrypting the data.\n#### MPLS VPN Physical Design and Topology two important MPLS terms in context: customer edge (CE) and provider edge (PE).\ntwo important MPLS terms in context: customer edge (CE) and provider edge (PE). The customer edge device is typically a router, and it sits at a customer site access linkprovider edge devices sit at the edge of the SP’s network, on the other end of the. any data-link protocol could in theory be used on MPLS access links. ##### MPLS and Quality of Service TheMPLS WAN provider would then configure its QoS tools to react for packets that have that marking, typically sending that packet as soon as possible.\nthe enterprise will want to work with the SP to define other features of the service. The customer and SP will need to work through the details of some Layer 3 design choices (as discussed in more depth in the next section). The customer will also likely want to ask for QoS services from the MPLS provider and define those details.\n##### Layer 3 with MPLS VPN MPLS must be aware of the customer IP addressing. The SP will even use routing protocols and advertise those customer routes across the WAN CE routers need to exchange routes with the PE routers in the MPLS network.all the CE routers need to learn routes from the other CE routers—a process that relies on Additionally, the PE routers. MPLS allows for many familiar routing protocols on the edge of the MPLS network: RIPv2, EIGRP, OSPF, and even eBGP\n○ A CE router does become neighbors with the PE router on the other end of the access link. ○ A CE router does not become neighbors with other CE routers. The MPLS network does advertise the customer’s routes between the various PE routers so that the CE routers can learn all customer routes through their PE-CE routing protocol neighbor relationship.\n###### ○ To advertise the customer routes between the PE routers, the PE routers use another routing protocol along with a process called route redistribution. Route redistribution happens inside one router, taking routes from one routing protocol process and injecting them into another.MPLS does route redistribution in the PE routers between the routing\nthem into another.protocol used by the customer and a variation of BGP called Multiprotocol BGP (MPBGP)MPLS does route redistribution in the PE routers between the routing. (Redistribution is needed when the PE-CE routing protocol is not BGP.\nMPBGP can advertise routes from multiple customers while keeping the routes logically separated\n#### Internet VPNs ##### Internet Access Digital Subscriber Line earlier Internet access technologies (analog modems and Integrated Services Digital Network, or ISDN). DSL modem sends and receives the data, as digital signals, at higher frequencies, over the same local loop Because DSL sends analog (voice) and digital (data) signals on the same line, the telco has to somehow split those signals on the telco side of the connection. To do so, the local loop must be connected to a DSL access multiplexer (DSLAM) located in the nearby telco central office (CO). TheDSLAM splits out the digital data over to the router on the lower right in Figure 14-17, which completes the connection to the Internet. The DSLAM also splits out the analog voice signals over to the voice switch on the upper right.\nCable Internet Like DSL, cable Internet uses asymmetric speeds, sending data faster downstream than upstream Wireless WAN (3G, 4G, LTE, 5G) Fiber (Ethernet) Internet Access Internet VPN Fundamentals Internet VPNs can provide important security features, such as the following: ○ **Confidentiality (privacy)** middle) from being able to read the data: Preventing anyone in the middle of the Internet (man in the **Authentication** : Verifying that the sender of the VPN packet is a legitimate device and not a device used by an attacker ###### ○ ○ Data integrity Internet : Verifying that the packet was not changed as the packet transited the ○ Anti sent by a legitimate user, for the purpose of appearing to be a legitimate user - replay : Preventing a man in the middle from copying and later replaying the packets site-to-site VPN connects two sites of a company.\na. Host PC1 (10.2.2.2) on the right sends a packet to the web server (10.1.1.1), just as it would without a VPN. The router encrypts the packet, adds some VPN headers, adds another IP header (with public IP addresses), and forwards the packet.\nb.\npublic IP addresses), and forwards the packet. An attacker in the Internet copies the packet (called a manthe attacker cannot change the packet without being noticed and cannot read the contents -in-the-middle attack). However, of the original packet.\nc.\nFirewall FW1 receives the packet, confirms the authenticity of the sender, confirms that the packet has not been changed, and then decrypts the original packet.\nd.\ne. Server S1 receives the unencrypted packet. tunnel refers to any protocol’s packet that is sent by encapsulating the packet inside another packet.The term VPN tunnel may or may not imply that the tunnel also uses encryption.\nSite-to-Site VPNs withIpsec an architecture or framework for security services for IP networks more generally called IP Security IPsec defines how two devices, both of which connect to the Internet, can achieve the main goals of a VPN as listed at the beginning of this section: confidentiality, authentication, data integrity, and anti-replay its role as an architecture allows it to be added to and changed over time as improvements to individual security functions are made. IPsec encryption uses a pair of encryption algorithms, which are essentially math formulas, to meet a couple of requirements One to hide (encrypt) the data Another to re-create (decrypt) the original data based on the encrypted data if an attacker did happen to decrypt one packet, that information would not give the attacker any advantages in decrypting the other packets. process for encrypting data for an IPsec VPN the encryption key is also known as the session key, shared key, or shared session key. The sending VPN device (like the remote office router in Figure 14original packet and the session key into the encryption formula, calculating the -21) feeds the encrypted data. The sending device encapsulates the encrypted data into a packet, which includes the new IP header and VPN header. The sending device sends this new packet to the destination VPN device (FW1 back in Figure 14-21). The receiving VPN device runs the corresponding decryption formula, using the encrypted data and session keydevice—to decrypt the data. —the same key value as was used on the sending VPN\ndevices use some related VPN technology like concept of a tunnel (a virtual link between the routers),Generic Routing Encapsulation (GRE)to create the\nWithout IPsec, each GRE tunnel could be used to forward unencrypted traffic over the Internet.\nIPsec adds the security features to the data that flows over the tunnel\nthe figure shows IPsec and GRE, but IPsec teams with other VPN technologies as well\nRemote Access VPNs with TLS\nremote access VPNs often use theTransport Layer Security (TLS) protocol to create a secure VPN\nsession.\nTLS provides the security features of HTTP Secure (HTTPS). Today’s web browsers support HTTPS\n(with TLS) as a way to dynamically create a secure connection from the web browser to a web server,\nTo do so, the browser creates a TCP connection to server well-known port 443 (default) and then\ninitializes a TLS session.authenticating the user. Then, the HTTP messages flow over the TLS VPN connection.TLS encrypts data sent between the browser and the server and\nSSL has been deprecated (see RFC 7568) and has been replaced by TLS.\neach session secures only the data sent in that session\ncan be used to create a client VPN that secures all packets from the device to a site by using a Cisco VPN client\nCisco AnyConnect Secure Mobility Client (or AnyConnect Client for shortuser’s PC and uses TLS to create one end of a VPN remote-access tunnel) is software that sits on a\nwhile the figure shows a firewall used at the main enterprise site, many types of devices can\nbe used on the server side of a TLS connection as well.\nVPN Comparisons\n","externalUrl":null,"permalink":"/tech/networking/wanarchitecture/","section":"Teches","summary":"\u003cp\u003e1.0 Network Fundamentals\u003cbr\u003e\n1.2 Describe the characteristics of network topology architecture\u003cbr\u003e\n1.2.d WAN\u003cbr\u003e\n5.0 Security Fundamentals\u003cbr\u003e\n5.5 Describe remote access and site-to-site VPNs\u003c/p\u003e\n\u003cp\u003eMetro Ethernet\u003c/p\u003e\n\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"\"\n    src=\"../../../../img/Pasted%20image%2020231205144234.png\"\n    \u003e\u003c/figure\u003e\n\u003cp\u003eCustomers connect to a Metro Ethernet service with either routers or Layer 3 switches.\u003c/p\u003e","title":"WAN Architecture","type":"tech"},{"content":"Leased-Line WANs\nPhysical Details of Leased Lines\npredetermined speed Full DuplexUses two pairs of wires one for each direction Conceptually crossover\nLeased Circuit Electrical circuit (line) between 2 endpoints\nSerial Link (line) Bits flow seriallyRouters use serial interfaces\nPoint to point link (line) two points only\nT 1.544 Mbps\nWAN link General term\nPrivate Line Data is private Leased line specifies layer 1HDLC and PPP are the most popular Layer 2 protocols used on leased lines\nHDLC HDLC Data-Link Details of Leased Lines\n3. WANs and IP Routing # Friday, June 11, 2021 10:24 AM\nless work than ethernet because of point to point leased line has an address field, but the destination is impliedCant use between cisco and non cisco Cisco HDLC type field is proprietary\nComparing HDLC Header Fields to Ethernet\nHDLC Header\nFlag Like preamble, SFD1 byte 1 byte Destination address Control No longer used 1 byte\nType Type of layer 3 packet inside the frame2 bytes Data FCS Error detection2 bytes How Router Use a WAN Data Link\nHow routers use HDLC when sending data\nLAN1 802.3header/IP Packet/ 802.3trailer \u0026gt;HDLC HDLCheader/IP Packet/ HDLC Trailer \u0026gt; LAN2 802.3header/Ippacket/802.3trailer \u0026gt; Higher cost and long install timesSlow speeds Leased line negatives\nCustomer Router CPE Ethernet as a WAN technology\nCPE\nService provider\npoint of Presence (PoP)- Where the fiber connects at the provider Common ethernet WAN names Ethernet WANEthernet Line Service (E-Line) Ethernet emulation Acts like simple ethernet link between two routers Ethernet over MPLS ethernet service for a customer.(EoMPLS) (Multi protocol label switching)A technology used to create How Routers Route IP Packets Using Ethernet Emulation\nEoMPLS WAN(Provider network simulating an ethernet link)\n802.3 header and trailer Use FCS field to ensure that the frame had no errors; if errors occurred, discard the frame. Discard the old data-link header and trailer, leaving the IP packet.\nCompare the IP packet’s destination IP address to the routing table, and find the route that best matches the destination address. This route identifies the outgoing interface of the router and\npossibly the next-hop router IP address. 3. Encapsulate the IP packet inside a new datainterface, and forward the frame -link header and trailer, appropriate for the outgoing IP Routing\nThe IP Header (20 bytes)\nVersion, Length, DS Field, Packet Length (4 bytes)Identification, Flags, Fragment Offset (4 bytes) Time to Live, Protocol, Header Checksum (4 bytes)Source IP (4 bytes) Destination IP (4 bytes)\nIP Routing Protocols add route for each directly connected subnet tell neighbors about routes in routing tableadd new routes learned to routing table, with next hop as the router the address was learned from ARP\nSender IP, sender MAC, Target IP, target MAC ??\nEthernet broadcast arp request \u0026gt;\nSender IP, sender MAC, Target IP, target MAC ??\nTarget IP, Target MAC, Sender IP, sender MAC\n\u0026lt; Ethernet unicast ARP reply\narp -a to see arp cache on most operating systems ","externalUrl":null,"permalink":"/tech/networking/wansandiprouting/","section":"Teches","summary":"\u003cp\u003eLeased-Line WANs\u003cbr\u003e\nPhysical Details of Leased Lines\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epredetermined speed\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eFull DuplexUses two pairs of wires one for each direction\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eConceptually crossover\u003cbr\u003e\nLeased Circuit\u003c/li\u003e\n\u003cli\u003eElectrical circuit (line) between 2 endpoints\u003cbr\u003e\nSerial Link (line)\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eBits flow seriallyRouters use serial interfaces\u003cbr\u003e\nPoint to point link (line)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003etwo points only\u003cbr\u003e\nT\u003c/li\u003e\n\u003cli\u003e1.544 Mbps\u003cbr\u003e\nWAN link\u003c/li\u003e\n\u003cli\u003eGeneral term\u003cbr\u003e\nPrivate Line\u003c/li\u003e\n\u003cli\u003eData is private\u003c/li\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003eLeased line specifies layer 1HDLC and PPP are the most popular Layer 2 protocols used on leased lines\u003cbr\u003e\nHDLC\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHDLC Data-Link Details of Leased Lines\u003c/p\u003e","title":"WANs and IP Routing","type":"tech"},{"content":"It\u0026rsquo;s easy to get overwhelmed with options after completing your CCNA. What do you learn next? If you are trying to get a job as a Network Engineer, you will want to check this out.\nI went through dozens of job listings that mentioned CCNA. Then, tallied up the main devices/vendors, certifications, and technologies mentioned. And left out anything that wasn\u0026rsquo;t mentioned more than twice.\nCore CCNA technologies such as LAN, WAN, OSPF, Spanning Tree, VLANs, etc. have been left out. The point here is to target the most sought after technologies and skills by employers. I also left out soft skills and any job that wasn\u0026rsquo;t a networking specific role.\nDevices/ Vendors # Palo Alto is huge! I\u0026rsquo;m not suprised by this. Depending on the company, a network engineer may be responsible for firewall configuration and troubleshooting. It also looks like Network Engineers with a wide variety of skills are saught after.\nDevice/Vendor Times Mentioned Palo Alto 9 Cisco ASA 6 Juniper 6 Office 365 5 Meraki 4 Vmware 4 Linux 4 Ansible 4 AWS 3 Wireshark 3 Technologies # Firewall comes in first again. Followed closely by VPN skills. Every interview I had for a Network Engineer position asked if I knew how to configure and troubleshoot VPNs.\nTechnology Times Mentioned Firewall 19 VPN 16 Wireless 12 BGP 12 Security 12 MPLS 10 Load balancers 8 Ipsec 7 ISE 6 DNS 5 SDWAN 5 Cloud 4 TACACS+ 4 ACL 4 SIEM 4 IDS/IPS 4 RADIUS 3 ITIL 3 Ipam 3 VOIP 3 EIGRP 3 Python 3 Certifications # CCNP blew every other cert out of the water. Companies will be very interested if you are working towards this cert. Security + comes highly recommended as well.\nCertification Times Mentioned CCNP 18 Security+ 6 JNCIA 4 JNCIP 4 Network + 4 CCIE 4 PCNSA 3 So what do you do after CCNA? # It depends\u0026hellip;\nAre you trying to get a new job ASAP? Are there opportunities at your current role that you can use your new skills to leverage? Do you have some study time before you are ready to take the next step?\nCCNP Enterprise is a good bet if you really want to stand out in Network Engineering interviews.\nDon\u0026rsquo;t want to be a Network engineer? # Continue to build a good base of IT skills. This will open you up to a larger variety of jobs and open skill paths that you need a good foundation to unlock.\nCore skills include:\nLinux/ Operating systems Networking General Cybersecurity Programming/ Scripting A good Linux certification like the RHCSA would be great to learn more about Linux, scripting, and operating systems. Security + would be good if you want to get a solid foundation of cyber security. And Python skills will give you a gold star in any IT interview.\nDon\u0026rsquo;t get paralyzed by choices. # Pick something that interests you and go for it. That is the only way to get it right. Doing what you enjoy is better than not doing anything at all because you can\u0026rsquo;t decide the best path.\nHopefully we can revisit this post after learning Python to get a much bigger sample size.\n","externalUrl":null,"permalink":"/tech/tools/what-to-learn-after-ccna/","section":"Teches","summary":"\u003cp\u003eIt\u0026rsquo;s easy to get overwhelmed with options after \u003ca\n  href=\"Resources%20for%20Passing%20CCNA.md\"\u003ecompleting\u003c/a\u003e your CCNA. What do you learn next? If you are trying to get a job as a Network Engineer, you will want to check this out.\u003c/p\u003e","title":"What to Learn After CCNA","type":"tech"},{"content":"The data path from the wireless network to the wired network is very short; the autonomous AP links the two networks. Data to and from wireless clients does not have to travel up into the cloud and back; the cloud is used to bring management functions into the data plane. the network in Figure 27-3 consists of two distinct paths—one for data traffic and another for management traffic, corresponding to the following two functions:\nA control plane: Traffic used to control, configure, manage, and monitor the AP itself A data plane: End-user traffic passing through the AP Split-MAC Architectures The realmessages. -time processes involve802.11 data encryption is also handled in real time, on a persending and receiving 802.11 frames, beacons, and probe -packet basis. The AP must interact with wireless clients on some low level, known as the Media Access Control (MAC) layer. These functions must stay with the AP hardware, closest to the clients. The management functionsare not integral to handling frames over the RF channels, but are things that should be centrally administered.centrally located platform away from the AP.Therefore, those functions can be moved to a When the functions of an autonomous AP are divided, the AP hardware is known as a lightweight access point, and performs only the real-time 802.11 operation. domain. the AP is left with duties in Layers 1 and 2, where frames are moved into and out of the RF The AP becomes totally dependent on the WLC for every other WLAN function,such as authenticating users, managing security policies, and even selecting RF channels and output power. The lightweight APMAC operations are pulled apart into two distinct locations.-WLC division of labor is known as a split-MAC architecture, where the normal the AP and WLC can be located on the same VLAN or IP subnet, but they do not have to be CAPWAP relationship actually consists of two separate tunnels, as follows: CAPWAP control messagesits operation. The control messages are authenticated and encrypted, so the AP is securely : Carries exchanges that are used to configure the AP and manage its operation. The control messages are authenticated and encrypted, so the AP is securely controlled by only the appropriate WLC, then transported over the control tunnel. CAPWAP data: with the AP. Data packets are transported over the data tunnel but are not encrypted by Used for packets traveling to and from wireless clients that are associated default. When data encryption is enabled for an AP, packets are protected with Datagram Transport Layer Security (DTLS). Every AP and WLC must also authenticate each other with digital certificates. An X.509 certificate is preinstalled in each device when it is purchased. Notice how VLAN 100 exists at the WLC and in the air as SSID 100, near the wireless clientsnot in between the AP and the WLC —but Becauseaddress for both management and tunnelingthe AP sits on the access layer where its CAPWAP tunnels terminate, it can use one IP. No trunk link is needed because all of the VLANs it supports are encapsulated and tunneled as Layer 3 IP packetsVLANs. ,rather than individual Layer 2 As the wireless network grows, the WLC simply builds more CAPWAP tunnels to reach more Aps WLC can begin offering a variety of additional functions: Dynamic channel assignment : The WLC canautomatically choose and configure the RF channel used by each AP, based on other active access points in the area. Transmit power optimization AP based on the coverage area needed.:The WLC can automatically set the transmit power of each Self-healing wireless coverage :If an AP radio dies, the coverage hole can be “healed” by turning up the transmit power of surrounding APs automatically. Flexible client roaming : Clients can roam between APs with very fast roaming times. Dynamic client load balancing geographic area, the WLC can associate clients with the least used AP. This distributes the : If two or more APs are positioned to cover the same client load across the APs. RF monitoring usage. By listening to a channel, the WLC can remotely gather information about RF : TheWLC manages each AP so that it scans channels to monitor the RF interference, noise, signals from neighboring APs, and signals from rogue APs or ad hoc clients. Security management : The WLC can authenticate clients from a central service and can require wireless clients to obtain an IP address from a trusted DHCP server before allowing them to associate and access the WLAN. Wireless intrusion protection system client data to detect and prevent malicious activity.: Leveraging its central location, the WLC can monitor\nComparing Wireless LAN Controller Deployments Comparing Wireless LAN Controller Deployments locate the WLC in a central location so that you can maximize the number of APs joined to itis usually called a unified or centralized WLC deployment. This Typical unified WLCs can support a maximum of 6000 APs A WLC can also be located in a central position in the network, inside a data center in a private cloud, as shown in Figure 27-9. This is known as a cloud-based WLC deployment, where the WLC exists as a virtual machine rather than a physical device. Such a controller can typically support up to 3000 APs.\nFor small campuses or distributed branch locations, where the number of APs is relatively small in\nFor small campuses or distributed branch locations, where the number of APs is relatively small in each, the WLC can be co-located with a stack of switches, as shown in Figure 27-10. This is known as an embedded WLC deployment Typical Cisco embedded WLCs can support up to 200 APsto be connected to the switches that host the WLC. The APs do not necessarily have the a Cisco WLC function can be coMobility Express WLC deployment-located with an AP that is installed at the branch site. This is known as A Mobility Express WLC can support up to 100 APs.\nCisco AP Modes WLC, you can also configure a lightweight AP to operate in one of the following specialmodes: -purpose **Local** :The default lightweight mode that offers one or more functioning BSSs on a specific channel. measure the level of noise, measure interference, discover rogue devices, and match against During times that it is not transmitting, the AP will scan the other channels to intrusion detection system (IDS) events. **Monitor** sensor. The AP checks for IDS events, detects rogue access points, and determines the : The AP does not transmit at all, but its receiver is enabled to act as a dedicated position of stations through location-based services. **FlexConnect** its CAPWAP tunnel to the WLC is down and if it is configured to do so.: An AP at a remote site can locally switch traffic between an SSID and a VLAN if **Sniffer** sniffer or packet capture device. The captured traffic is then forwarded to a PC running : An AP dedicates its radios to receiving 802.11 traffic from other sources, much like a network analyzer software such as Wildpackets OmniPeek or WireShark, where it can be analyzed further. **Rogue detector** addresses heard on the wired network with those heard over the air. Rogue devices are : An AP dedicates itself to detecting rogue devices by correlating MAC those that appear on both networks. **Bridge** two networks. Two APs in bridge mode can be used to link two locations separated by a : An AP becomes a dedicated bridge (point-to-point or point-to-multipoint) between two networks. Two APs in bridge mode can be used to link two locations separated by a distance. Multiple APs in bridge mode can form an indoor or outdoor mesh network. **Flex+Bridge** : FlexConnect operation is enabled on a mesh AP. **SE-Connect** :The AP dedicates its radios to spectrum analysis on all wireless channels. You can remotely connect a PC running software such as MetaGeek Chanalyzer or Cisco Spectrum Expert to the AP to collect and analyze the spectrum analysis data to discover sources of interference. Remember thatclient devices to associate to wireless LANs. When an AP is configured to operate in one of the a lightweight AP is normally in local mode when it is providing BSSs and allowing other modes, local mode (and the BSSs) is disabled. This chapter covers the following exam topics: 1.0 Network Fundamentals 1.11 Describe wireless principles 1.11.d Encryption 5.0 Security Fundamentals 5.9 Describe wireless security protocols (WPA, WPA2, and WPA3) A comprehensive approach to wireless security focuses on the following areas: Identifying the endpoints of a wireless connection Identifying the end user (authentication) Protecting the wireless data from eavesdroppers (encryption) Protecting the wireless data from tampering (integrity) Authentication To use a wireless network, it. Clients should be authenticated by some means before they can become functioning members of the wireless LAN.clients must first discover a basic service set (BSS) and then request permission to associate with A fake AP could also send spoofed management frames to disassociate or deauthenticate legitimate and active clients, just to disrupt normal network operation. To prevent this type of manauthenticated -in-the-middle attack, the client should authenticate the AP before the client itself is Message Privacy data should be encrypted for its journey through free space. This is accomplished by encrypting the data payload in each wireless frame just prior to being transmitted, then decrypting it as it is received. The idea is to use an encryption method that the transmitter and receiver share, so the data can be encrypted and decrypted successfully. the AP should securely negotiate a unique encryption key to use for each associated client. No other device should know about or be able to use the same keys to eavesdrop and decrypt the data. The Each of the associated clients uses the same group key to decrypt the data.AP also maintains a “group key” that it uses when it needs to send encrypted data to all clients in its cell at one time. Message Integrity what if someone managed to alter the contents along the way? The recipient would have a very difficult time discovering that the original data had been modified. ## 28 Wireless security Thursday, September 2, 2021 11:46 AM Amessage integrity check (MIC)is a security tool that can protect against data tampering. a way for the sender to add a secret stamp inside the encrypted data frame. The stamp is based on the contents of the data bits to be transmitted. Once the recipient decrypts the frame, it can compare the secret stamp to its own idea of what the stamp should be, based on the data bits that were received. If the two stamps are identical, the recipient can safely assume that the data has not been tampered with.\nWireless Client Authentication Methods 1. Open Authentication original 802.11 standard offered only two choices to authenticate a client: open authentication and WEP. Open authentication is true to its name; it offers use an 802.11 authentication request before it attempts to associate with an AP. No other credentials are needed.open access to a WLAN. The only requirement is that a client must Most client operating systems flag such networks to warn you that your wireless data will not be secured in any way if you join. 2. WEP open authentication offers nothing that can obscure or encrypt the data being sent between a client and an AP. Wired Equivalent Privacy (WEP) WEP same algorithm encrypts data at the sender and decrypts it at the receiver.uses the RC4 cipher algorithm to make every wireless data frame private and hidden from eavesdroppers. The The algorithm uses a string of bits as a key, commonly called a WEP key, to derive other encryption keysreceiver have an identical key, one can decrypt what the other encrypts.—one per wireless frame. As long as the sender and WEP is ahead of time, so that each can derive other mutually agreeable encryption keysknown as a shared-key security method. The same key must be shared between the sender and receiver. In fact, every potential client and AP must share the same key ahead of time so that any client can associate with the AP. The WEP key use the correct WEP key, it cannot associate with an AP. The AP tests the client’s knowledge of the WEP key by can also be used as an optional authentication method as well as an encryption tool. Unless a client can sending it a random challenge phrase. The client encrypts the challenge phrase with WEP and returns the result to the AP. The AP can compare the client’s encryption with its own to see whether the two WEP keys yield identical results. WEP keys can be either 40 or 104 bits long, represented by a string of 10 or 26 hex digits longer keys offer more unique bits for the algorithm, resulting in more robust encryption longer keys offer more unique bits for the algorithm, resulting in more robust encryption WEP was officially deprecated. Both WEP encryption and WEP sharedweak methods to secure a wireless LAN. -key authentication are widely considered to be\n3. 802.1x/EAP With only open authentication and WEP available in the original 802.11 standard, a more secure authentication method was needed. Extensible Authentication Protocol (EAP) EAP is extensible and does not consist of any one authentication method. EAP defines through this section, notice how many authentication methods have EAP in their names. Each method is unique and a set of common functions that actual authentication methods can use to authenticate users. As you read different, but each one follows the EAP framework. itnetwork media until a client authenticates.can integrate with the IEEE 802.1x port-based access control standardThis means that a wireless client might be able to associate with an AP but. When 802.1x is enabled, it limits access to a will not be able to pass data to any other part of the network until it successfully authenticates. with 802.1x; the client uses open authentication to associate with the AP, and then the actual client authentication process occurs at a dedicated authentication server shows the three-party 802.1x arrangement that consists of the following entities: Supplicant: The client device that is requesting access Authenticator[WLC]) : The network device that provides access to the network (usually a wireless LAN controller Authentication server (AS)access based on a user database and policies (usually a RADIUS server): The device that takes user or client credentials and permits or denies network The wireless LAN controller becomes a middleman in the client authentication process, controlling user access with 802.1x and communicating with the authentication server using the EAP framework. The goal here is to become aware of the many methods without trying to memorize them all even when you configure user authentication on a wireless LAN, you will not have to select a specific method. Instead, you select 802.1x on the WLC so that it is ready to handle a variety of EAP methods. It is then up to the client and the authentication server to use a compatible method.\nand the authentication server to use a compatible method.\n4. LEAP proprietary wireless authentication method called supply username and password credentials. Both the authentication server and the client exchange challenge Lightweight EAP (LEAP). To authenticate, the client must messages that are then encrypted and returned. the method used to encrypt the challenge messages was found to be vulnerable, so LEAP has since been deprecated. Even though wireless clients and controllers still offer LEAP, you should not use it. 5. EAP-FAST Cisco developed a more secure method called Authentication credentials are protected by passing aEAP Flexible Authentication by Secure Tunneling (EAPprotected access credential (PAC)between the AS and -FAST). the supplicant. The PAC is authentication. a form of shared secret that is generated by the AS and used for mutual EAP-FAST is a sequence of three phases: Phase 0: The PAC is generated or provisioned and installed on the client. Phase 1: Security (TLS) tunnel.After the supplicant and AS have authenticated each other, they negotiate a Transport Layer Phase 2: The end user can then be authenticated through the TLS tunnel for additional security. two separate authentication processes occur in EAPwith the end user. These occur in a nested fashion, as an outer authentication (outside the TLS tunnel) and an -FAST—one between the AS and the supplicant and another inner authentication (inside the TLS tunnel). a RADIUS server is required. However, the RADIUS server must also operate as an EAPgenerate PACs, one per user. -FAST server to be able to 6. PEAP Protected EAP (PEAP) method uses an inner and outer authentication the AS presents a digital certificate to authenticate itself with the supplicant in the outer authentication. If the supplicant is satisfied with the identity of the AS, the two will build a TLS tunnel to be used for the inner client authentication and encryption key exchange. The digital certificate of the AS consists of data in a standard format that identifies the owner and is “signed” or validated by a third party. The third party is known as a certificate authority (CA) and is known and trusted by both the AS and the supplicants. The supplicant must also possess the CA certificate just so that it can validate the one it receives from the AS. The certificate is also used to pass a public key, in plain view, which can be used to help decrypt messages from the AS. only the AS has a certificate for PEAP it must be authenticated within the TLS tunnel using one of the following two methods: MSCHAPv2: Microsoft Challenge Authentication Protocol version 2 GTCmanually generated password: Generic Token Card; a hardware device that generates one-time passwords for the user or a 7. EAP-TLS PEAP leverages a digital certificate on the AS as a robust method to authenticate the RADIUS server EAP Transport Layer Security (EAP-TLS) goes one step further by requiring certificates on the AS and on every client device. the AS and the supplicant exchange certificates and can authenticate each other. A TLS tunnel is built afterward so that encryption key material can be securely exchanged. EAP-TLS is considered to be the most secure wireless authentication method available; implementing it can sometimes be complex. Along with the AS, each wireless client must obtain and install a certificate EAPsuch as communicators, medical devices, and RFID tags, have an underlying operating system that cannot -TLS is practical only if the wireless clients can accept and use digital certificates. Many wireless devices, interface with a CA or use certificates.\noriginal method to secure wireless data from eavesdroppers: WEP WEP has been compromised, deprecated, and can no longer be recommended.\nTemporal Key Integrity Protocol (TKIP) TKIP adds the following security features using legacy hardware and the underlying WEP encryption: MIC:commonly called “Michael” as an informal reference to MIC.This efficient algorithm adds a hash value to each frame as a message integrity check to prevent tampering; Time stamp: have already been sent.A time stamp is added into the MIC to prevent replay attacks that attempt to reuse or replay frames that Sender’s MAC address:The MIC also includes the sender’s MAC address as evidence of the frame source. TKIP sequence counter: from being replayed as an attack.This feature provides a record of frames sent by a unique MAC address, to prevent frames Key mixing algorithm: This algorithm computes a unique 128-bit WEP key for each frame. Longer initialization vector (IV)WEP keys by brute-force calculation.: The IV size is doubled from 24 to 48 bits, making it virtually impossible to exhaust all\nTKIP\nWireless Privacy and Integrity Methods should be avoided if a better method is available.In fact, TKIP was deprecated in the 802.11-2012 standard. CCMP Counter/CBC-MAC Protocol (CCMP) more secure than TKIP.CCMP consists of two algorithms: AES counter mode encryption Cipher Block Chaining Message Authentication Code (CBC-MAC) used as a message integrity check (MIC) Cipher Block Chaining Message Authentication Code (CBC-MAC) used as a message integrity check (MIC) (AES) is the current encryption algorithm adopted by U.S. National Institute of Standards and Technology (NIST) and the U.S. government, and widely used around the world AES is open, publicly accessible, and represents the most secure encryption method available today. client devices and APs must support the AES counter mode and CBClegacy devices that support only WEP or TKIP. How can you know if a device supports CCMP? Look for the WPA2 -MAC in hardware. CCMP cannot be used on designation, which is described in the following section. GCMP Galois/Counter Mode Protocol (GCMP) more efficient than CCMP GCMP consists of two algorithms: AES counter mode encryption Galois Message Authentication Code (GMAC) used as a message integrity check (MIC) GCMP is used in WPA3 WPA, WPA2, and WPA3 Wi-Fi Protected Access (WPA) industry certifications. WPA was based on parts of 802.11i and included 802.1x authentication, TKIP, and a method for dynamic encryption key management. WPA2 is based around the superior AES CCMP algorithms, rather than the deprecated TKIP from WPA WPA3 leverages stronger encryption by AES with the Galois/Counter Mode Protocol (GCMP). It Management Frames (PMF)to secure important 802.11 management frames between APs and clients, to prevent malicious also uses Protected activity that might spoof or tamper with a BSS’s operation. WPA3 includes other features beyond WPA and WPA2secrecy, and Protected management frames (PMF). , such asSimultaneous Authentication of Equals (SAE), Forward You should avoid using WPA and use WPA2 insteaddevices, APs, and WLCs. —at least until WPA3 becomes widely available on wireless client all three WPA versions support two client authentication modes: a pre-shared key (PSK) or 802.1x These are also known as personal mode and enterprise mode, respectively. With a key string must be shared or configured on every client and AP before the clients can connect to the wireless network. The pre-shared key is normally kept confidential so that unauthorized users have no knowledge of it. The key string is never sent over the air. Instead, clients and APs work through a fourthe pre-shared key string to construct and exchange encryption key material that can be openly exchanged.-way handshake procedure that uses\npersonal mode,\na malicious user can eavesdrop and capture the fourthen use a dictionary attack to automate guessing the pre-way handshake between a client and an AP. That user can -shared key. If he is successful, he can then decrypt the wireless data or even join the network posing as a legitimate user.\nWPA-Personal and WPA2-Personal modes\nWPA3-Personal avoids such an attack by strengthening the key exchange between clients and APs through a method known as Simultaneous Authentication of Equals (SAE). Rather than a client authenticating against a server or AP, the client and AP can initiate the authentication process equally and even simultaneously. Even if a password or key is compromised, WPA3from being able to use a key to unencrypt data that has already been transmitted over the air-Personal offers forward secrecy, which prevents attackers. The Personal mode of any WPA version is usually easy to deploy in a small environment or with clients that are embedded in certain devices because a simple text key string is all that is needed to authenticate the clients.Be aware thaupdate or change the key, you must touch every device to do so. As well, the pret every device using the WLAN must be configured with an identical pre-shared key should remain a well -shared key. If you ever need to kept secret; you should never divulge the pre-shared key to any unauthorized person. WPA, WPA2, and WPA3 also support 802.1x or enterprise authentication. EAP-based authentication WPA versions do not require any specific EAP method WPA versions do not require any specific EAP method interoperability with well-known EAP methods like EAP-TLS, PEAP, EAP-TTLS, and EAP-SIM Enterprise authentication is more complex to deploy than personal mode because authentication servers must be set up and configured as a critical enterprise resource. You should always select the highest WPA version that the clients and wireless infrastructure in your environment will support.\nan effective wireless security strategy includes a method to authenticate clients and a method to provide data privacy and integrity. These two types of methods are listed in the leftmost column. Work your way to the right to remember what types of authentication and privacy/integrity are available. The table also expands the name of each acronym as a memory tool. WPA, WPA2, and WPA3 simplify wireless network configuration and compatibility because they limit which authentication and privacy/integrity methods can be used. multiple VLANs must be brought to it over a trunk link. The wireless side of an AP inherently trunks 802.11 frames by marking them with the BSSID of the WLAN where they belong. lightweight AP Wired VLANs that terminate at the WLC can be mapped to WLANs that emerge at the AP. The AP needs only an access link to connect to the network infrastructure and terminate its end of the tunnel you can also use Telnet or SSH to connect to its CLI over the wired networkbrowser-based management sessions via HTTP and HTTPS.You can manage lightweight APs from a .Autonomous APs support browser session to the WLC. you can also use Telnet or SSH to connect to its CLI over the wired network. Autonomous APs support browser-based management sessions via HTTP and HTTPS. You can manage lightweight APs from a browser session to the WLC. Accessing a Cisco WLC management users to log in. Users can be authenticated against an internal list of local usernames or against an authentication, authorization, and accounting (AAA) server, such as TACACS+ or RADIUS. When you are successfully logged in, the WLC will display a monitoring dashboard must click on the Advanced link in the upper-right corner. This will bring up the full WLC GUI ## 29 Wireless LAN Wednesday, November 24, 2021 11:34 AM Monitor, WLANs, Controller, Wireless, Security, and so on. Connecting a Cisco WLC Cisco wireless controllers differ a bit; ports and interfaces refer to different concepts. Controller ports are physical connections made to an external wired or switched network, whereas interfaces are logical connections made internally within the controller Using WLC Ports You can connect several different types of controller ports to your network, as shown in Figure 29 - 5 and discussed in the following list: **Service port:** functions; always connects to a switch port in access modeUsed for out-of-band management, system recovery, and initial boot **Distribution system port:** to a switch port in 802.1Q trunk modeUsed for all normal AP and management traffic; usually connects **Console port:** functions; asynchronous connection to a terminal emulator (9600 baud, 8 data bits, 1 stop Used for out-of-band management, system recovery, and initial boot bit, by default) **Redundancy port:** Used to connect to a peer controller for high availability (HA) operation Controllers can have a single service port that must be connected to a switched network. Usually, the service port is assigned to a management VLAN so that you can access the controller with SSH or a web browser to perform initial configuration or for maintenance. Notice that the service port supports only a single VLAN, so the corresponding switch port must be configured for access mode only. Controllers also have multiple distribution system ports that you must connect to the network. These ports carry most of the data coming to and going from the controller. For example, the CAPWAP tunnels (control and data) that extend to each of a controller’s APs pass across the distribution system ports. Client data also passes from wireless LANs to wired VLANs over the ports. In addition, any management traffic using a web browser, SSH, Simple Network Management Protocol (SNMP), Trivial File Transfer Protocol (TFTP), and so on, normally reaches the controller in-band through the ports. the distribution system ports always operate in 802.1Q trunking mode. The distribution system ports can operate independently, each one transporting multiple VLANs to a unique group of internal controller interfaces. For resiliency, you can configure distribution system ports in redundant pairs. One port is primarily used; if it fails, a backup distribution system ports in redundant pairs. One port is primarily used; if it fails, a backup port is used instead. Controller distribution system ports can be configured as a link aggregation group (LAG) such that they are bundled together to act as one larger link traffic can be load-balanced across the individual ports that make up the LAG LAG offers resiliency; if one individual port fails, traffic will be redirected to the remaining working ports instead. Cisco WLCs do not support any link aggregation negotiation protocol, like LACP or PAgP, at all.\nUsing WLC Interfaces the controller must somehow map those wired VLANs to equivalent logical wireless networks APs.VLAN must be connected to a unique wireless LAN that exists on a controller and its associated Cisco wireless controllers provide the necessary connectivity through internal logical interfaces, which must be configured with an IP address, subnet mask, default gateway, and a Dynamic Host Configuration Protocol (DHCP) server. Each interface is then assigned to a physical port and a VLAN ID. You can think of an interface as a Layer 3 termination on a VLAN. Cisco controllers support the following interface types: **Management interface:** authentication, WLC-to-WLC communication, webUsed for normal management traffic, such as RADIUS user -based and SSH sessions, SNMP, Network Time Protocol (NTP), syslog, and so on. The management interface is also used to terminate CAPWAP tunnels between the controller and its APs. **Redundancy management** a high availability pair of controllers. The active WLC uses the management interface : The management IP address of a redundant WLC that is part of address, while the standby WLC uses the redundancy management address. **Virtual interface:** DHCP requests, performing client web authentication, and supporting client mobility.IP address facing wireless clients when the controller is relaying client **Service port interface:** Bound to the service port and used for out-of-band management. **Dynamic interface:** Used to connect a VLAN to a WLAN. The management interface faces the switched network, where management users and APs are located. Management traffic will usually consist of protocols like HTTPS, SSH, SNMP, NTP, TFTP, and so on consists of CAPWAP packets that carry control and data tunnels to and from the APs. The virtual interface is used only for certain client-facing operations. when a wireless client issues a request to obtain an IP address, the controller can relay the request on to an actual DHCP server that can provide the appropriate IP address DHCP server appears to be the controller’s virtual interface address. Clients may see the virtual interface’s address, but that address is never used when the controller communicates with other devices on the switched network. Because the virtual interface is used only for some client management functions, you should configure it with a unique, nonroutable address. For example, you might use 10.1.1.1 because it is within a private address space defined in RFC 1918. The virtual interface address is also used to support client mobility. every controller that exists in the same mobility group should be configured with a virtual address that is identical to the others. By using one common virtual address, all the controllers will appear to operate as a cluster as clients roam from controller to controller. Dynamic interfaces map WLANs to VLANs, making the logical connections between wireless and wired networks. You will configure one dynamic interface for each wireless LAN that is offered by the controller’s APs and then map the interface to the WLAN. Each dynamic interface must also be configured with its own IP address and can act as a DHCP relay for wireless clients. To filter traffic passing through a dynamic interface, you can configure an optional access list. Configuring a WLAN To complete the path between the SSID and the VLAN, as illustrated in Figure 29define a WLAN on the controller. -7, you must first The controller will bind the WLAN to one of its interfaces and then push the WLAN configuration out to all of its APs by default. Like VLANs, you can use WLANs to segregate wireless users and their traffic into logical networks. Users associated with one WLAN cannot cross over into another one unless their traffic is bridged or routed from one VLAN to another through the wired network infrastructure. Cisco controllers support a maximum of 512 WLANs, but only 16 of them can be actively configured on an AP. Advertising each WLAN to potential wireless clients uses up valuable airtime. Every AP must broadcast beacon management frames at regular intervals to advertise the existence of a BSS Beacons are normally sent 10 times per second, or once every 100 ms, at the lowest mandatory data rate if you create too many WLANs, a channel can be starved of any usable airtime always limit the number of WLANs to five or fewer; a maximum of three WLANs is best Before you create a new WLAN, think about the following parameters it will need to have: SSID string Controller interface and VLAN number Type of wireless security needed you will create the appropriate dynamic controller interface to support the new WLAN; then you will enter the necessary WLAN parameters Step 1. Configure a RADIUS Server RADIUS server, such as WPA2-Enterprise or WPA3-Enterprise, Select Security \u0026gt; AAA \u0026gt; RADIUS \u0026gt; Authentication to see a list of servers that have already been configured If multiple servers are defined, the controller will try them in sequential order. Click New to create a new server.\nNext, enter the server’s IP address, shared secret key, and port number Because the controller already had two other RADIUS servers configured, the server at 192.168.200.30 will be index number 3. Be sure to set the server status to Enabled so that the controller can begin using it. At the bottom of the page, you can select the type of user that will be authenticated with the server. Check Network User to authenticate wireless clients or Management to authenticate wireless administrators that will access the controller’s management functions. Click Apply to complete the server configuration.\nStep 2. Create a Dynamic Interface create a new dynamic interface, navigate to Controller \u0026gt; Interfaces. You should see a list of all the controller interfaces that are currently configured Click the New button to define a new interface. Enter a name for the interface and the VLAN number it will be bound to. In Figure 29-11, the interface named Engineering is mapped to wired VLAN 100. Click the Apply button Next, enter the IP address, subnet mask, and gateway address for the interface. You should also define primary and secondary DHCP server addresses that the controller will use when it relays DHCP requests from clients that are bound to the interface Click the Apply button to complete the interface configuration and return to the list of interfaces.\nStep 3. Create a New WLAN selecting WLANs from the top menu bar (displays current WLANs) You can create a new WLAN by selecting Create New from the dropand then clicking the Go button. -down menu Next, enter a descriptive name as the profile name and the SSID text string. The ID number is used as an index into the list of WLANs that are defined on the controller. The ID number becomes useful when you use templates in Prime Infrastructure (PI) to configure WLANs on multiple controllers at the same time. WLAN templates are applied to specific WLAN ID numbers on controllers. The WLAN ID is only locally significant and is not passed between controllers. As a rule, you should keep the sequence of WLAN names and IDs consistent across multiple controllers so that any configuration templates you use in the future will be applied to the same WLANs on each controller. Click the Apply button to create the new WLAN. The next page will allow you to edit four categories of parameters, corresponding to the tabs across the top\nYou can control whether the WLAN is enabled or disabled with the Status check box Under Radio Policy, select the type of radio that will offer the WLAN. By default, the WLAN will be offered on all radios that are joined with the controller. You can select a more specific policy with 802.11a only, 802.11a/g only, 802.11g only, or 802.11b/g only. Next, select which of the controller’s dynamic interfaces will be bound to the WLAN. By default, the management interface is selected. The drop-down list contains all the interface names that are available Finally, use the Broadcast SSID check box to select whether the APs should broadcast the SSID name in the beacons they transmit. Hiding the SSID name, by not broadcasting it, does not really provide any worthwhile security\nConfiguring WLAN Security Select the Security tab to configure the security settings. By default, the Layer 2 Security tab is selected. From the Layer 2 Security drop-down menu, select the appropriate security scheme to use. Table 29-2 lists the types that are available. Further down the screen, you can select which specific WPA, WPA2, and WPA3 methods to support on the WLAN To use WPA2would be used to authenticate wireless clients against one or more RADIUS servers.-Enterprise, the 802.1X option would be selected. In that case, 802.1x and EAP To specify which servers the WLAN should use, you would select the Security tab and then the AAA Servers tab in the WLAN edit screen. You can identify up to six specific RADIUS servers in the WLAN configuration. Beside each server, select a specific server IP address from the drop-down menu of globally defined servers. The servers are tried in sequential order until one of them responds By default, a controller will contact a RADIUS server from its management interface. You can override this behavior by checking the box next to Radius Server Overwrite Interface so that the controller sources RADIUS requests from the dynamic interface that is associated with the WLAN.\nConfiguring WLAN QoS Select the QoS tab to configure quality of service settings for the WLAN By default, the controller will consider all frames in the WLAN to be normal data, to be handled in a “best effort” manner. You can set the Quality of Service (QoS) drop-down menu to classify all frames in one of the following ways: Platinum (voice) Gold (video) Silver (best effort) Bronze (background) You can also set the Wibandwidth parameters on the QoS page-Fi Multimedia (WMM) policy, call admission control (CAC) policies, and\nbandwidth parameters on the QoS page Configuring Advanced WLAN Settings Advanced Tab can enable functions such as coverage hole detection, peerexclusion, client load limits, and so on. -to-peer blocking, client By default, client sessions with the WLAN are limited to 1800 seconds (30 minutes). Once that session time expires, a client will be required to reauthenticate. This setting is controlled by the Enable Session Timeout check box and the Timeout field. all clients are subject to the policies configured under Security \u0026gt; Wireless Protection Policies \u0026gt; Client Exclusion Policies. These policies include excessive 802.11 association failures, 802.11 authentication failures, 802.1x authentication failures, web authentication failures, and IP address theft or reuse. Offending clients will be automatically excluded or blocked for 60 seconds, as a deterrent to attacks on the wireless network. Finalizing WLAN Configuration by default, a controller will not allow management traffic that is initiated from a WLAN. That means you (or anybody else) cannot access the controller GUI or CLI from a wireless device that is associated to the WLAN. This is considered to be a good security practice because the controller is kept isolated from networks that might be easily accessible or where someone might eavesdrop on the management session traffic You can change the default behavior on a global basis (all WLANs) by selecting the Management\nYou can change the default behavior on a global basis (all WLANs) by selecting the Management tab and then selecting Mgmt Via Wireless, as shown in Figure 29-21. Check the box to allow management sessions from any WLAN that is configured on the controller. Chapter 1. Introduction to TCP/IP Transport and Applications This chapter covers the following exam topics: 1.0 Network Fundamentals 1.5 Compare TCP to UDP 4.0 IP Services 4.3 Explain the role of DHCP and DNS in the network\nTCP/IP Layer 4 Protocols: TCP and UDP\n- - TCP provides retransmission (error recovery) and TCP helps to avoid congestion (flow control), - - UDP needs fewer bytes in its header (less overhead)UDP software does not slow down data transfer - - TCP can purposefully slow down data transferVoice over IP (VoIP) and video over IP, do not need error recovery, so they use UDP - Multiplexing using ports UDP Supports:\n- Multiplexing using ports - Error recovery (reliability) ○ numbering and acknowledging data with sequence and acknowledgment header fields - Flow control using windowing ○ use window sizes to protect buffer space ○ initialize port numbers and sequence and acknowledgment fields\n- Connection establishment and termination - Ordered data transfer and segmentation TCP Supports:\n○○ source port/ destination portSequence Number ○ Acknowledgement Number ○ Offset, Reserved, Flag bits, Window ○ Checksum, Urgent\n- TCP header is 20 bytes (without options) Transmission Control Protocol\n*TCP segment, Layer 4 PDU, or L4PDU\n","externalUrl":null,"permalink":"/tech/networking/wirelessarchitectures/","section":"Teches","summary":"\u003cp\u003eThe data path from the wireless network to the wired network is very short; the autonomous AP links the two networks. Data to and from wireless clients does not have to travel up into the cloud and back; the cloud is used to bring management functions into the data plane.\nthe network in Figure 27-3 consists of two distinct paths—one for data traffic and another for\nmanagement traffic, corresponding to the following two functions:\u003c/p\u003e","title":"Wireless Architectures","type":"tech"},{"content":"multiple VLANs must be brought to it over a trunk link.\nThe wireless side of an AP inherently trunks 802.11 frames by marking them with the BSSID of the WLAN where they belong.\nlightweight AP\nWired VLANs that terminate at the WLC can be mapped to WLANs that emerge at the AP.\nThe AP needs only an access link to connect to the network infrastructure and terminate its end of the tunnel\nyou can also use Telnet or SSH to connect to its CLI over the wired network. Autonomous APs support browser-based management sessions via HTTP and HTTPS. You can manage lightweight APs from a browser session to the WLC.\nTrunk Link Autonomous AP WLC Switched LAN Access Link Lightweight AP\nyou can also use Telnet or SSH to connect to its CLI over the wired network. Autonomous APs support browser-based management sessions via HTTP and HTTPS. You can manage lightweight APs from a browser session to the WLC.\nAccessing a Cisco WLC\nmanagement users to log in. Users can be authenticated against an internal list of local usernames or against an authentication, authorization, and accounting (AAA) server, such as TACACS+ or RADIUS.\nWhen you are successfully logged in, the WLC will display a monitoring dashboard\nmust click on the Advanced link in the upper-right corner. This will bring up the full WLC GUI\nMonitor, WLANs, Controller, Wireless, Security, and so on.\nConnecting a Cisco WLC\nCisco wireless controllers differ a bit; ports and interfaces refer to different concepts.\nController ports are physical connections made to an external wired or switched network, whereas interfaces are logical connections made internally within the controller\nUsing WLC Ports\nService port: Used for out-of-band management, system recovery, and initial boot functions; always connects to a switch port in access mode\nDistribution system port: Used for all normal AP and management traffic; usually connects to a switch port in 802.1Q trunk mode\nConsole port: Used for out-of-band management, system recovery, and initial boot functions; asynchronous connection to a terminal emulator (9600 baud, 8 data bits, 1 stop bit, by default)\nRedundancy port: Used to connect to a peer controller for high availability (HA) operation\nControllers can have a single service port that must be connected to a switched network. Usually, the service port is assigned to a management VLAN so that you can access the controller with SSH or a web browser to perform initial configuration or for maintenance. Notice that the service port supports only a single VLAN, so the corresponding switch port must be configured for access mode only.\nControllers also have multiple distribution system ports that you must connect to the network. These ports carry most of the data coming to and going from the controller. For example, the CAPWAP tunnels (control and data) that extend to each of a controller’s APs pass across the distribution system ports. Client data also passes from wireless LANs to wired VLANs over the ports. In addition, any management traffic using a web browser, SSH, Simple Network Management Protocol (SNMP), Trivial File Transfer Protocol (TFTP), and so on, normally reaches the controller in-band through the ports.\nthe distribution system ports always operate in 802.1Q trunking mode.\nThe distribution system ports can operate independently, each one transporting multiple VLANs to a unique group of internal controller interfaces. For resiliency, you can configure distribution system ports in redundant pairs. One port is primarily used; if it fails, a backup port is used instead.\nController distribution system ports can be configured as a link aggregation group (LAG) such that they are bundled together to act as one larger link\ntraffic can be load-balanced across the individual ports that make up the LAG\nLAG offers resiliency; if one individual port fails, traffic will be redirected to the remaining working ports instead.\nCisco WLCs do not support any link aggregation negotiation protocol, like LACP or PAgP, at all.\nUsing WLC Interfaces\nthe controller must somehow map those wired VLANs to equivalent logical wireless networks\nVLAN must be connected to a unique wireless LAN that exists on a controller and its associated APs.\nCisco wireless controllers provide the necessary connectivity through internal logical interfaces, which must be configured with an IP address, subnet mask, default gateway, and a Dynamic Host Configuration Protocol (DHCP) server. Each interface is then assigned to a physical port and a VLAN ID. You can think of an interface as a Layer 3 termination on a VLAN.\nCisco controllers support the following interface types:\nManagement interface: Used for normal management traffic, such as RADIUS user authentication, WLC-to-WLC communication, web-based and SSH sessions, SNMP, Network Time Protocol (NTP), syslog, and so on. The management interface is also used to terminate CAPWAP tunnels between the controller and its APs.\nRedundancy management: The management IP address of a redundant WLC that is part of a high availability pair of controllers. The active WLC uses the management interface address, while the standby WLC uses the redundancy management address.\nVirtual interface: IP address facing wireless clients when the controller is relaying client DHCP requests, performing client web authentication, and supporting client mobility.\nService port interface: Bound to the service port and used for out-of-band management.\nDynamic interface: Used to connect a VLAN to a WLAN.\nSwitch VLAN a VLAN n VLAN x VLAN y Dynamic Interface Dynamic Interface Management Redundancy Management Service Port WLC WI-AN 1 WI-AN n SSID SSID\nThe management interface faces the switched network, where management users and APs are located. Management traffic will usually consist of protocols like HTTPS, SSH, SNMP, NTP, TFTP, and so on\nconsists of CAPWAP packets that carry control and data tunnels to and from the APs.\nThe virtual interface is used only for certain client-facing operations.\nwhen a wireless client issues a request to obtain an IP address, the controller can relay the request on to an actual DHCP server that can provide the appropriate IP address\nDHCP server appears to be the controller’s virtual interface address. Clients may see the virtual interface’s address, but that address is never used when the controller communicates with other devices on the switched network.\nBecause the virtual interface is used only for some client management functions, you should configure it with a unique, nonroutable address. For example, you might use 10.1.1.1 because it is within a private address space defined in RFC 1918.\nThe virtual interface address is also used to support client mobility.\nevery controller that exists in the same mobility group should be configured with a virtual address that is identical to the others. By using one common virtual address, all the controllers will appear to operate as a cluster as clients roam from controller to controller.\nDynamic interfaces map WLANs to VLANs,\nmaking the logical connections between wireless and wired networks. You will configure one dynamic interface for each wireless LAN that is offered by the controller’s APs and then map the interface to the WLAN.\nEach dynamic interface must also be configured with its own IP address and can act as a DHCP relay for wireless clients. To filter traffic passing through a dynamic interface, you can configure an optional access list.\nConfiguring a WLAN\n192.16B.199.0/24 WLC Interface Engineering VI-AN 100 192.168.199.199/24 CAPWAP W LAN V LAN\nThe controller will bind the WLAN to one of its interfaces and then push the WLAN configuration out to all of its APs by default.\nLike VLANs, you can use WLANs to segregate wireless users and their traffic into logical networks. Users associated with one WLAN cannot cross over into another one unless their traffic is bridged or routed from one VLAN to another through the wired network infrastructure.\nCisco controllers support a maximum of 512 WLANs, but only 16 of them can be actively configured on an AP.\nAdvertising each WLAN to potential wireless clients uses up valuable airtime.\nEvery AP must broadcast beacon management frames at regular intervals to advertise the existence of a BSS\nBeacons are normally sent 10 times per second, or once every 100 ms, at the lowest mandatory data rate\nif you create too many WLANs, a channel can be starved of any usable airtime\nalways limit the number of WLANs to five or fewer; a maximum of three WLANs is best\nBefore you create a new WLAN, think about the following parameters it will need to have:\nSSID string\nController interface and VLAN number\nType of wireless security needed\nyou will create the appropriate dynamic controller interface to support the new WLAN; then you will enter the necessary WLAN parameters\nStep 1. Configure a RADIUS Server\nRADIUS server, such as WPA2-Enterprise or WPA3-Enterprise,\nSelect Security \u0026gt; AAA \u0026gt; RADIUS \u0026gt; Authentication to see a list of servers that have already been configured\nIf multiple servers are defined, the controller will try them in sequential order. Click New to create a new server.\nS AutNnOcaOon Servers 10 EAP • web\nNext, enter the server’s IP address, shared secret key, and port number Because the controller already had two other RADIUS servers configured, the server at 192.168.200.30 will be index number 3. Be sure to set the server status to Enabled so that the controller can begin using it. At the bottom of the page, you can select the type of user that will be authenticated with the server. Check Network User to authenticate wireless clients or Management to authenticate wireless administrators that will access the controller’s management functions. Click Apply to complete the server configuration.\nSec urity • uo:us MONITOR WLANs WIRELESS SECURITY RADIUS Authentication Servers \u0026gt; New MANAGE\u0026quot; go HELP EEEOBACK Index (priority) She re ded Net MAC C lie ts rd EAP Advanced EAP priornv Order Certificate ACROSS Control L istS Win P web Auth policies 192.168.200.30 ASCII r for FIPS Map enabled \u0026ldquo;\u0026gt;\nStep 2. Create a Dynamic Interface\ncreate a new dynamic interface, navigate to Controller \u0026gt; Interfaces. You should see a list of all the controller interfaces that are currently configured\nCISCO\nCISCO ess secuurv\nNext, enter the IP address, subnet mask, and gateway address for the interface. You should also define primary and secondary DHCP server addresses that the controller will use when it relays DHCP requests from clients that are bound to the interface\nClick the Apply button to complete the interface configuration and return to the list of interfaces.\nc Isco Conu\nStep 3. Create a New WLAN\nselecting WLANs from the top menu bar (displays current WLANs)\nYou can create a new WLAN by selecting Create New from the drop-down menu and then clicking the Go button.\nNext, enter a descriptive name as the profile name and the SSID text string.\nThe ID number is used as an index into the list of WLANs that are defined on the controller. The ID number becomes useful when you use templates in Prime Infrastructure (PI) to configure WLANs on multiple controllers at the same time.\nCISCO KLAN s\nWLAN templates are applied to specific WLAN ID numbers on controllers. The WLAN ID is only locally significant and is not passed between controllers. As a rule, you should keep the sequence of WLAN names and IDs consistent across multiple controllers so that any configuration templates you use in the future will be applied to the same WLANs on each controller.\nClick the Apply button to create the new WLAN. The next page will allow you to edit four categories of parameters, corresponding to the tabs across the top\nWLANS •Enginering\nYou can control whether the WLAN is enabled or disabled with the Status check box\nUnder Radio Policy, select the type of radio that will offer the WLAN. By default, the WLAN will be offered on all radios that are joined with the controller. You can select a more specific policy with 802.11a only, 802.11a/g only, 802.11g only, or 802.11b/g only.\nNext, select which of the controller’s dynamic interfaces will be bound to the WLAN. By default, the management interface is selected. The drop-down list contains all the interface names that are available\nFinally, use the Broadcast SSID check box to select whether the APs should broadcast the SSID name in the beacons they transmit.\nHiding the SSID name, by not broadcasting it, does not really provide any worthwhile security\nConfiguring WLAN Security\nSelect the Security tab to configure the security settings. By default, the Layer 2 Security tab is selected. From the Layer 2 Security drop-down menu, select the appropriate security scheme to use. Table 29-2 lists the types that are available.\nOption None WPA+WPA2 802.1x Static WEP Static WEP + 802.1x CKIP None + EAP Passthrough Description Open authentication Wi-Fi protected access YVPA or YVPA2 EAP authentication with dynamic WEP WEP key security EAP authentication or static WEP Cisco Key Integrity Protocol Open authentication with remote EAP authentication \u0026ldquo;\u0026gt;\nFurther down the screen, you can select which specific WPA, WPA2, and WPA3 methods to support on the WLAN\nMAN. Edit •Engin..nng•\nTo use WPA2-Enterprise, the 802.1X option would be selected. In that case, 802.1x and EAP would be used to authenticate wireless clients against one or more RADIUS servers.\nTo specify which servers the WLAN should use, you would select the Security tab and then the AAA Servers tab in the WLAN edit screen. You can identify up to six specific RADIUS servers in the WLAN configuration. Beside each server, select a specific server IP address from the drop-down menu of globally defined servers. The servers are tried in sequential order until one of them responds\n凹 ONIT ( 浓 MANAGEMENT 0 OS 0 Be , h 〔 De 〔 K BC-me WLANS \u0026gt; Radius ~ , 、 0 「 「 ENbÄ•d Fi , re2g -17s e - RAD TSSe \u0026quot; ” - r Ⅵ 4 Ⅳ Au a 0 \u0026ldquo;\u0026gt;\nBy default, a controller will contact a RADIUS server from its management interface. You can override this behavior by checking the box next to Radius Server Overwrite Interface so that the controller sources RADIUS requests from the dynamic interface that is associated with the WLAN.\nConfiguring WLAN QoS\nSelect the QoS tab to configure quality of service settings for the WLAN\nBy default, the controller will consider all frames in the WLAN to be normal data, to be handled in a “best effort” manner. You can set the Quality of Service (QoS) drop-down menu to classify all frames in one of the following ways:\nPlatinum (voice)\nGold (video)\nSilver (best effort)\nBronze (background)\nYou can also set the Wi-Fi Multimedia (WMM) policy, call admission control (CAC) policies, and bandwidth parameters on the QoS page\nConfiguring Advanced WLAN Settings\nAdvanced Tab\ncan enable functions such as coverage hole detection, peer-to-peer blocking, client exclusion, client load limits, and so on.\nCisco m. N s CONTROWFA WLANs Edit •Engin Pap ESS _u\nBy default, client sessions with the WLAN are limited to 1800 seconds (30 minutes). Once that session time expires, a client will be required to reauthenticate. This setting is controlled by the Enable Session Timeout check box and the Timeout field.\nall clients are subject to the policies configured under Security \u0026gt; Wireless Protection Policies \u0026gt; Client Exclusion Policies. These policies include excessive 802.11 association failures, 802.11 authentication failures, 802.1x authentication failures, web authentication failures, and IP address theft or reuse. Offending clients will be automatically excluded or blocked for 60 seconds, as a deterrent to attacks on the wireless network.\nFinalizing WLAN Configuration\nc Isco KLAN. WI RECESS secuRm\nby default, a controller will not allow management traffic that is initiated from a WLAN. That means you (or anybody else) cannot access the controller GUI or CLI from a wireless device that is associated to the WLAN. This is considered to be a good security practice because the controller is kept isolated from networks that might be easily accessible or where someone might eavesdrop on the management session traffic\nCISCO Management SNMP serial port WLAKs Management Via Wireless wtRELESS SEuRr FEE D SACK Enable t. O Software\n","externalUrl":null,"permalink":"/tech/networking/wireless_lan/","section":"Teches","summary":"\u003cp\u003emultiple VLANs must be brought to it over a trunk link.\u003c/p\u003e\n\u003cp\u003eThe wireless side of an AP inherently trunks 802.11 frames by marking them with the BSSID of the WLAN where they belong.\u003c/p\u003e","title":"Wireless LAN","type":"tech"},{"content":"the operation of a BSS hinges on the AP, the BSS is bounded by usable. This is known as thebasic service area (BSA)or cell. the area where the AP’s signal is\nthe cell is shown as a simple shaded circular area that centers around the AP itself.\nIt advertises the existence of the BSS so that devices can find it and try to join. To do that, the AP\nuses a unique BSS identifier (BSSID) that is based on the AP’s own radio MAC address.\nWireless devices must also have unique MAC addresses to send wireless frames at Layer 2 over\nthe air.\nthe AP advertises the wireless network with a Service Set Identifier (SSID), which is a text string containing a logical name.Think of the BSSID as a machine-readable name tag that uniquely\nidentifies the BSS ambassador (the AP), and the SSID as a nonunique, humanthat identifies the wireless service. -readable name tag\nMembership with the BSS is called an association. request to the AP and the AP must either grant or deny the request. Once associated, a device A wireless device must send an association\nbecomes a client, or an 802.11 station (STA), of the BSS.\nAs long as a wireless client remains associated with a BSS, most communications to and from the client must pass through the AP,\nBy using the BSSID as a source or destination address, data frames can be relayed to or from the AP.\nBy sending data through the AP first, the BSS remains stable and under control.\nDistribution System\nThe 802.11 standard refers to the upstream wired Ethernet as the distribution system (DS) for the wireless BSS,\nthe AP is in charge of mapping a virtual local-area network (VLAN) to an SSID.\nThe AP must be connected to the switch by a trunk link that carries the VLANs. The AP uses the 802.1Q tag to map the VLAN numbers to the appropriate SSIDs. The AP then appears as multiple logical APs—one per BSS—with a unique BSSID for each. With Cisco APs, this is usually accomplished byfor each SSID. incrementing the last digit of the radio’s MAC address Even though wireless clients can be distributed across many SSIDs, all of those clients must share the same AP’s hardware and must contend for airtime on the same channel. Extended Service Set\nWhen APs are placed at different geographic locations, they can all be interconnected by a switched infrastructure. The 802.11 standard calls this an extended service set (ESS),\nIdeally,any SSIDs that are defined on one AP should be defined on all the APs in an ESS\neach cell in Figure 26-8 has a unique BSSID, but both cells share one common SSID.\nPassing from one AP to anotheris called roaming Each AP offers its own BSS on its own channel, to prevent interference between the APs. As a client device roams from one AP to another, it must scan the available channels to find a new AP (and BSS) to roam toward. (and BSS) to roam toward. The 802.11 standard allows two or more wireless clients to communicate directly with each other, with no other means of network connectivity. This is known as an ad hoc wireless network, or an independent basic service set (IBSS), One of the devices must take the lead and begin advertising a network name and the necessary radio parameters, much like an AP would do. Independent Basic Service Set\nOther Wireless Topologies\nRepeater\nA wireless repeater takes the signal it receives and repeats or retransmits it in a new cell\narea around the repeater.\nSome repeaters can use two transmitters and receivers to keep the original and repeated signals isolated on different channels. One transmitter and receiver pair is dedicated to signals in the AP’s cell, while the other pair is dedicated to signals in the repeater’s own cell. Workgroup Bridge WGB acts as an external wireless network adapter for a device that has none. You might encounter two types of workgroup bridges: You might encounter two types of workgroup bridges: Universal workgroup bridge (uWGB): wireless network. A single wired device can be bridged to a Workgroup bridge (WGB)wired devices to be bridged to a wireless network.:A Cisco-proprietary implementation that allows multiple Outdoor Bridge\nact as a bridge to form a single wireless link from one LAN to another over a long distance.\nOne AP configured in bridge mode is needed on each end of the wireless link. Special purpose antennas are normally used with the bridges to focus their signals in one direction\nA point-to-multipoint bridged link allows a central site to be bridged to several other sites Mesh Network\nIn a mesh topology, wireless traffic is bridged from AP to AP, in a daisy-chain fashion, using\nanother wireless channel.\nMesh APs can leverage dual radiosone a different range. Each mesh AP usually maintains a BSS on one channel, with which —one using a channel in one range of frequencies and\nwireless clients can associate. Client traffic is then usually bridged from AP to AP over other channels as a backhaul network.\nThe mesh network runs its own dynamic routing protocol to work out the best path for backhaul traffic to take across the mesh APs.\nRF Overview\nElectromagnetic waves do not travel in a straight line. Instead, they travel by expanding in all directions away from the antenna.\nfrequencyin 1 secondof the wave, or the number of times the signal makes one complete up and down cycle\nA second.hertz (Hz)is themost commonly used frequency unit and is nothing other than one cycle per 3 kHz to 300 GHz is commonly called radio frequency (RF).\nWireless Bands and Channels\nWireless Bands and Channels\nBecause a range of frequencies might be used for the same purpose, it is customary to refer to the range as a band of frequencies.\nthe range from 530 kHz to around 1710 kHz is used by AM radio stations; therefore, it is commonly called the AM band or the AM broadcast band.\nOne of the two main frequency ranges used for wireless LAN communication lies between 2.400\nand 2.4835 GHz. This is usually called the 2.4entire range between 2.4 and 2.5 GHz -GHz band, even though it does not encompass the\nThe other wireless LAN range is usually called the 55.825 GHz. The 5-GHz band actually contains the following four separate and distinct bands:-GHz band because it lies between 5.150 and\n5.150 to 5.250 GHz\n5.250 to 5.350 GHz\n5.470 to 5.725 GHz\n5.725 to 5.825 GHz\nGap between 5.350 and 5.470 this gap exists and cannot be used for wireless LANs.\nDo not worry about memorizing the band names or exact frequency ranges; just be aware of the two main bands at 2.4 and 5 GHz.\nA frequency band contains a continuous range of frequencies.\nbands are usually divided into a number of distinct channels. Each channel is known by a channel\nnumber and is assigned to a specific frequency.\nIn the 5not encroach on or overlap the frequencies allocated for any other channel. In other words, the 5-GHz band, this is the case because each channel is allocated a frequency range that does -\nGHz band consists of nonoverlapping channels.\nAPs and Wireless Standards\nWireless client devices and APs can be compatible with one or more amendments; APs can usually operate on both bands simultaneously to support any clients that might be present on each band. wireless clients typically associate with an AP on one band at a time, while scanning for potential wireless clients typically associate with an AP on one band at a time, while scanning for potential APs on both bands. The band used to connect to an AP is chosen according to the operating\nsystem, wireless adapter driver, and other internal configuration.\nCisco APs have dual radios (sets of transmitters and receivers) to support BSSs on one 2.4channel and other BSSs on one 5-GHz channel simultaneously. Some models also have two 5-GHz -GHz\nradios that can be configured to operate BSSs on two different channels at the same time,\nproviding wireless coverage to higher densities of users that are located in the same vicinity.\nRF signals propagate or reach further on the 2.4to penetrate indoor walls and objects easier at 2.4 GHz than 5 GHz. However, the 2.4-GHz band than on the 5-GHz band. They also tend -GHz band is\ncommonly more crowded with wireless devices. Remember that only three nonoverlapping channels are available, so the chances of other neighboring APs using the same channels is\ngreater. In contrast, the 5-GHz band has many more channels available to use, making channels\nless crowded and experiencing less interference.\nThis chapter covers the following exam topics:\n2.0 Network Access\n2.6 Compare Cisco Wireless Architectures and AP modes\nAutonomous AP Architecture\nVLANs must be trunked from the distribution layer switch (where routing commonly takes place) to the access layer, where they are extended further over a trunk link to the AP.\nTwo wireless users that are associated to the same autonomous AP can reach each other through the AP without having to pass up into the wired network\nconfigure SSIDs, VLANs, and many RF parameters like the channel and transmit power to be used. An autonomous AP must also be configured with a management IP address so that you can remotely manage it. management address is not normally part of any of the data VLANs, so a dedicated management\nVLAN (i.e., VLAN 10) must be added to the trunk links to reach the AP.\nEach AP must be configured and maintained individually unless you leverage a management platform such as Cisco Prime Infrastructure or Cisco DNA Center.\nThat might sound straightforward until you have to add a new VLAN and configure every switch\nand AP in your network to carry and support it.\nCloud-based AP Architecture\nCisco Meraki is cloudnetworks built from Meraki products. For example, through the cloud networking service, you can -based and offers centralized management of wireless, switched, and security\nconfigure and manage APs, monitor wireless performance and activity, generate reports, and so\non.\nCisco Meraki APs can be deployed automatically, once you register with the Meraki cloud. Each AP will contact the cloud when it powers up and will self-configure. From that point on, you can\nmanage the AP through the Meraki cloud dashboard.\nFrom the cloud, you can push out code upgrades and configuration changes to the APs in the\nenterprise\nadds the intelligence needed to automatically instruct each AP on which channel and transmit power level to use.\nCan also collect information from all of the APs about things such as RF interference, rogue or\nunexpected wireless devices that were overheard, and wireless usage statistics.\n","externalUrl":null,"permalink":"/tech/networking/wirelessnetworking/","section":"Teches","summary":"\u003cp\u003ethe operation of a BSS hinges on the AP, the BSS is bounded by usable. This is known as thebasic service area (BSA)or cell. the area where the AP’s signal is\u003c/p\u003e","title":"Wireless Networking","type":"tech"},{"content":"A comprehensive approach to wireless security focuses on the following areas:\nIdentifying the endpoints of a wireless connection\nIdentifying the end user (authentication)\nProtecting the wireless data from eavesdroppers (encryption)\nProtecting the wireless data from tampering (integrity)\nAuthentication\nTo use a wireless network, clients must first discover a basic service set (BSS) and then request permission to associate with it. Clients should be authenticated by some means before they can become functioning members of the wireless LAN.\nA fake AP could also send spoofed management frames to disassociate or deauthenticate legitimate and active clients, just to disrupt normal network operation.\nTo prevent this type of man-in-the-middle attack, the client should authenticate the AP before the client itself is authenticated\nMessage Privacy\ndata should be encrypted for its journey through free space. This is accomplished by encrypting the data payload in each wireless frame just prior to being transmitted, then decrypting it as it is received. The idea is to use an encryption method that the transmitter and receiver share, so the data can be encrypted and decrypted successfully.\nthe AP should securely negotiate a unique encryption key to use for each associated client.\nNo other device should know about or be able to use the same keys to eavesdrop and decrypt the data.\nThe AP also maintains a “group key” that it uses when it needs to send encrypted data to all clients in its cell at one time. Each of the associated clients uses the same group key to decrypt the data.\nMessage Integrity\nwhat if someone managed to alter the contents along the way? The recipient would have a very difficult time discovering that the original data had been modified.\nA message integrity check (MIC) is a security tool that can protect against data tampering.\na way for the sender to add a secret stamp inside the encrypted data frame. The stamp is based on the contents of the data bits to be transmitted. Once the recipient decrypts the frame, it can compare the secret stamp to its own idea of what the stamp should be, based on the data bits that were received. If the two stamps are identical, the recipient can safely assume that the data has not been tampered with.\n![1. Original Data 2. Compute MIC nihaol 23 nihao 123 6. Compare MICs 5. Compute MIC 3. Encrypt Data + MIC Client 4. Decrypt niha0123 741 fcb64901d\nWireless Client Authentication Methods\nOpen Authentication original 802.11 standard offered only two choices to authenticate a client: open authentication and WEP.\nOpen authentication is true to its name; it offers open access to a WLAN. The only requirement is that a client must use an 802.11 authentication request before it attempts to associate with an AP. No other credentials are needed.\n","externalUrl":null,"permalink":"/tech/networking/wireless_security/","section":"Teches","summary":"\u003cp\u003eA comprehensive approach to wireless security focuses on the following areas:\u003c/p\u003e\n\u003cp\u003eIdentifying the endpoints of a wireless connection\u003c/p\u003e\n\u003cp\u003eIdentifying the end user (authentication)\u003c/p\u003e\n\u003cp\u003eProtecting the wireless data from eavesdroppers (encryption)\u003c/p\u003e","title":"Wireless Security","type":"tech"},{"content":"https://www.youtube.com/watch?v=RzAkjX_9B7E\u0026t=295s\nMan (manual) pages are the built in help system for Linux. They contain documentation for most commands.\nRun the man command on a command to get to it\u0026rsquo;s man page. man man\nNavigating a man page h\nGet help q\nQuit out of the man page Man uses less\n^ mean ctrl\n^f Forward one page\n^b backward one page\ncan use # followed by command to repeat that many times\ng first line in file\nG last line in file\nCR means press enter\nSearching # /searchword\npress enter to jump first occurance of searched word\nn to jump to next match\nN to go to previous match\n?searchword to do a backward search (n and N are reversed when going through results)\nMan page conventions # bold text type as shown\nitalic text replace with arguments\nItalic may not render in terminal and may be underlined or colored text instead. [-abc] optional\n-a | -b Options separated by a pipe symbol cannot be used together.\nargument \u0026hellip; (followed by 3 dots) can be repeated. (Argument is repeatable)\n[expression] \u0026hellip; entire expression within [ ] is repeatable.\nParts of a man page # Name\nname of command Synopsis\nHow to use the command When you see file in a man page, think file and or directory\nDescription short and long options do the same thing Current section number is printed at the top left of the man page.\n-k to search sections using apropos\n[root@server30 ~]# man -k unlink mq_unlink (2) - remove a message queue mq_unlink (3) - remove a message queue mq_unlink (3p) - remove a message queue (REALT... sem_unlink (3) - remove a named semaphore sem_unlink (3p) - remove a named semaphore shm_open (3) - create/open or unlink POSIX s... shm_unlink (3) - create/open or unlink POSIX s... shm_unlink (3p) - remove a shared memory object... unlink (1) - call the unlink function to r... unlink (1p) - call theunlink() function unlink (2) - delete a name and possibly th... unlink (3p) - remove a directory entry unlinkat (2) - delete a name and possibly th...] Shows page number in ()\nThe sections that end in p are POSIX documentation. Theese are not specific to Linux.\n[root@server30 ~]# man -k \u0026#34;man pages\u0026#34; lexgrog (1) - parse header information in man pages man (7) - macros to format man pages man-pages (7) - conventions for writing Linux man pages man.man-pages (7) - macros to format man pages [root@server30 ~]# man man-pages Use man-pages to learn more about man pages\nSections within a manual page The list below shows conventional or suggested sections. Most manual pages should include at least the highlighted sections. Arrange a new manual page so that sections are placed in the order shown in the list. NAME LIBRARY [Normally only in Sections 2, 3] SYNOPSIS CONFIGURATION [Normally only in Section 4] DESCRIPTION OPTIONS [Normally only in Sections 1, 8] EXIT STATUS [Normally only in Sections 1, 8] RETURN VALUE [Normally only in Sections 2, 3] ERRORS [Typically only in Sections 2, 3] ENVIRONMENT FILES ATTRIBUTES [Normally only in Sections 2, 3] VERSIONS [Normally only in Sections 2, 3] STANDARDS HISTORY NOTES CAVEATS BUGS EXAMPLES AUTHORS [Discouraged] REPORTING BUGS [Not used in man-pages] COPYRIGHT [Not used in man-pages] SEE ALSO Shell builtins do not have man pages. Look at the shell man page for info on them. man bash\nSearch for the Shell Builtins section: /SHELL BUILTIN COMMANDS\nYou can find help on builtins with the help command: david@fedora:~$ help hash hash: hash [-lr] [-p pathname] [-dt] [name ...] Remember or display program locations. Determine and remember the full pathname of each command NAME. If no arguments are given, information about remembered commands is displayed. Options: -d\tforget the remembered location of each NAME -l\tdisplay in a format that may be reused as input -p pathname\tuse PATHNAME as the full pathname of NAME -r\tforget all remembered locations -t\tprint the remembered location of each NAME, preceding each location with the corresponding NAME if multiple NAMEs are given Arguments: NAME\tEach NAME is searched for in $PATH and added to the list of remembered commands. Exit Status: Returns success unless NAME is not found or an invalid option is given. help without any arguments displays commands you can get help on.\ndavid@fedora:~/Documents/davidvargas/davidvargasxyz.github.io$ help help help: help [-dms] [pattern ...] Display information about builtin commands. Displays brief summaries of builtin commands. If PATTERN is specified, gives detailed help on all commands matching PATTERN, otherwise the list of help topics is printed. Options: -d\toutput short description for each topic -m\tdisplay usage in pseudo-manpage format -s\toutput only a short usage synopsis for each topic matching PATTERN Arguments: PATTERN\tPattern specifying a help topic Exit Status: Returns success unless PATTERN is not found or an invalid option is given. type command tells you what type of command something is.\nUsing man on some shell builtins brings you to the bash man page Shell Builtin Section\nMany commands support -h or --help options to get quick info on a command.\n","externalUrl":null,"permalink":"/tech/tools/using-man-pages/","section":"Teches","summary":"\u003cp\u003e\u003ca\n  href=\"https://www.youtube.com/watch?v=RzAkjX_9B7E\u0026amp;t=295s\"\n    target=\"_blank\"\n  \u003ehttps://www.youtube.com/watch?v=RzAkjX_9B7E\u0026t=295s\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMan (manual) pages are the built in help system for Linux. They contain documentation for most commands.\u003c/p\u003e\n\u003cp\u003eRun the \u003ccode\u003eman\u003c/code\u003e command on a command to get to it\u0026rsquo;s man page.\n\u003ccode\u003eman man\u003c/code\u003e\u003c/p\u003e","title":"You Need to Learn Man Pages","type":"tech"}]